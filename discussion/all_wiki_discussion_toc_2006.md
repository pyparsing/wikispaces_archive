## Pyparsing Wikispaces Discussion - 2006

[Note: these entries are fairly old, and predate many new features of pyparsing,
and are predominantly coded using Python 2.
They are captured here for historical benefit, but may not contain
the most current practices or features. We will try to add editor
notes to entries to indicate when discussions have been 
overtaken by development events.]

[2006-05-17 08:01:32 - knguyen - Grammar Suggestion](all_wiki_discussion_toc_2006.md#2006-05-17-080132---knguyen---grammar-suggestion)  
[2006-05-31 00:26:48 - ideefixe - Parse actions return values](all_wiki_discussion_toc_2006.md#2006-05-31-002648---ideefixe---parse-actions-return-values)  
[2006-06-04 02:51:18 - prologic - Wiki Parser](all_wiki_discussion_toc_2006.md#2006-06-04-025118---prologic---wiki-parser)  
[2006-06-16 05:22:10 - peter21081944 - Exceptions](all_wiki_discussion_toc_2006.md#2006-06-16-052210---peter21081944---exceptions)  
[2006-07-19 10:29:15 - lenx - bug report: wrong pos when strings contain '\t'](all_wiki_discussion_toc_2006.md#2006-07-19-102915---lenx---bug-report-wrong-pos-when-strings-contain-\t)  
[2006-07-23 14:12:47 - Qman - Help with parsing](all_wiki_discussion_toc_2006.md#2006-07-23-141247---qman---help-with-parsing)  
[2006-07-23 14:13:41 - Qman - problem](all_wiki_discussion_toc_2006.md#2006-07-23-141341---qman---problem)  
[2006-08-11 07:31:58 - kndyer - Matching question](all_wiki_discussion_toc_2006.md#2006-08-11-073158---kndyer---matching-question)  
[2006-08-26 14:00:10 - kwm - Plural possessives](all_wiki_discussion_toc_2006.md#2006-08-26-140010---kwm---plural-possessives)  
[2006-08-28 12:17:57 - cie - a bug? combine and recursion](all_wiki_discussion_toc_2006.md#2006-08-28-121757---cie---a-bug-combine-and-recursion)  
[2006-09-04 10:21:40 - cie - Opposite of CharsNotIn](all_wiki_discussion_toc_2006.md#2006-09-04-102140---cie---opposite-of-charsnotin)  
[2006-09-14 07:30:36 - nycterent - parser fails when http auth username is email](all_wiki_discussion_toc_2006.md#2006-09-14-073036---nycterent---parser-fails-when-http-auth-username-is-email)  
[2006-09-15 23:52:29 - CosmicStars - Idea how to skip masked linefeeds?](all_wiki_discussion_toc_2006.md#2006-09-15-235229---cosmicstars---idea-how-to-skip-masked-linefeeds)  
[2006-09-16 12:28:17 - metaperl - whitespace field discrim, adding ParseResults attributes](all_wiki_discussion_toc_2006.md#2006-09-16-122817---metaperl---whitespace-field-discrim-adding-parseresults-attributes)  
[2006-09-16 18:57:17 - metaperl - Debug output goes to stdout?](all_wiki_discussion_toc_2006.md#2006-09-16-185717---metaperl---debug-output-goes-to-stdout)  
[2006-09-27 14:22:29 - metaperl - a line of quoted and unquoted terms](all_wiki_discussion_toc_2006.md#2006-09-27-142229---metaperl---a-line-of-quoted-and-unquoted-terms)  
[2006-09-27 17:45:53 - metaperl - why is parseString returning a tuple consisting of a list and dictionary?](all_wiki_discussion_toc_2006.md#2006-09-27-174553---metaperl---why-is-parsestring-returning-a-tuple-consisting-of-a-list-and-dictionary)  
[2006-09-28 12:09:43 - chuckbiscuito - parsing config files with incomplete grammar](all_wiki_discussion_toc_2006.md#2006-09-28-120943---chuckbiscuito---parsing-config-files-with-incomplete-grammar)  
[2006-10-23 08:34:58 - korvus - setParseAction problems](all_wiki_discussion_toc_2006.md#2006-10-23-083458---korvus---setparseaction-problems)  
[2006-10-28 15:43:58 - akkartik - backtracking?](all_wiki_discussion_toc_2006.md#2006-10-28-154358---akkartik---backtracking)  
[2006-11-04 07:15:46 - ptmcg - re: coding style](all_wiki_discussion_toc_2006.md#2006-11-04-071546---ptmcg---re-coding-style)  
[2006-11-09 03:03:40 - akkartik - Chomping lines without removing leading whitespace](all_wiki_discussion_toc_2006.md#2006-11-09-030340---akkartik---chomping-lines-without-removing-leading-whitespace)  
[2006-11-11 15:15:40 - CosmicStars - How raise exception by ](all_wiki_discussion_toc_2006.md#2006-11-11-151540---cosmicstars---how-raise-exception-by-)  
[2006-11-14 18:53:24 - korvus - max recursion depth exceeded](all_wiki_discussion_toc_2006.md#2006-11-14-185324---korvus---max-recursion-depth-exceeded)  
[2006-11-14 23:45:21 - hyry - how to do some replacement with HTML](all_wiki_discussion_toc_2006.md#2006-11-14-234521---hyry---how-to-do-some-replacement-with-html)  
[2006-12-06 14:36:59 - aJanuary - Repeating within a range of repetitions](all_wiki_discussion_toc_2006.md#2006-12-06-143659---ajanuary---repeating-within-a-range-of-repetitions)  
[2006-12-06 16:00:31 - aJanuary - Avoiding greedy behaviour.](all_wiki_discussion_toc_2006.md#2006-12-06-160031---ajanuary---avoiding-greedy-behaviour)  
[2006-12-14 11:29:54 - metaperl - multi-line key-value pair parsing help needed](all_wiki_discussion_toc_2006.md#2006-12-14-112954---metaperl---multi-line-key-value-pair-parsing-help-needed)  
[2006-12-15 06:39:05 - metaperl - verbose parsing](all_wiki_discussion_toc_2006.md#2006-12-15-063905---metaperl---verbose-parsing)  
[2006-12-25 18:35:56 - akkartik - Whitespace at the very start of a parse](all_wiki_discussion_toc_2006.md#2006-12-25-183556---akkartik---whitespace-at-the-very-start-of-a-parse)  


---
## 2006-05-17 08:01:32 - knguyen - Grammar Suggestion
I am trying to come up with a grammar that describes the following:

    record = f1,f2,...,fn END_RECORD

All the fi has to be in that order.

Any fi can be absent (e.g. f1,,f3,f4,,f6 END_RECORD)

Number of fi's can vary. For example, the following are allowed:

    f1,f2 END_RECORD
    f1,f2,,f4,,f6 END_RECORD


Any suggestions?



Thanks,

Khoa

#### 2006-05-18 21:45:28 - ptmcg
In the discussion on c.l.py, you indicated that not all fi's are created equal, that each has a different matching expression.  You might try building up a list of expressions and then construct an And with the list.  Something like:


    f1 = (whatever this matches, for instance, if f1 is an integer:
    f1 = Word(nums).setResultsName('F1')
    f2 = ...
    f3 = ...
    ...

    expr = And( [ Optional(',') + Optional(f) for f in (f1,f2,f3,f4,f5,f6) ] ) + 'END_RECORD'



Use setResultsName to help keep track of which items have been included in the input list.



-- Paul

---
## 2006-05-31 00:26:48 - ideefixe - Parse actions return values
Hi!



First of all, thank you for your cute and witty pyparsing module. I'm using it to parse the output of fluid (fltk) designer.



During parsing I have to turn lists of paired tokens (eg 'name1 val1 name2 val2 name3 val3') into python dicts. So some of my grammar rules are stuff like:



    prop = name + val
    props = ZeroOrMore(prop)
    props.setParseAction(lambda s,l,t: dict(t.asList()))
    props.setResultsName('properties')

(here I'm using dict(iter_of_pairs) to build the dict)


When trying to parse properties using these rules an exception is thrown. That's because of pyparsing trying to index a dict (line 215 in version 1.4.2):



    try:
        self[name] = toklist[0]
    except TypeError:
        self[name] = toklist


The raised exception is KeyError instead of TypeError, so it's not catched by the except clause.



Anyway, I'm not sure about what a parse action is expected to return. Since I'm implementing the action as a lambda, I'm not able to modify the received ParseResults; so I must return something. I guess a returned list would be handled by pyparsing just as if it were a ParseResults. From the examples, it's crystal clear that simple scalars (as floats or ints) would be taken as 'atomic' results. But what about returning tuples, dicts or other indexable objects? Should these be avoided? Should I rewrite the action as:

    props.setParseAction(lambda s,l,t: [dict(t.asList())])

?



Could you clarify the above in the reference documentation? I think the doc is a bit vague in this respect as of today.



Thank you in advance.

Regards,

Carlos

#### 2006-05-31 01:23:53 - ideefixe
It's me again.



Just to make it a bit more juicy, a couple of troublesome cases follow:



1) returning a tuple from a parse action. Pyparsing moans when:



  if isinstance(tokens,tuple):

     tokens = tokens[1]

     warnings.warn('Returning loc from parse actions is deprecated...')



2) returning a dict from a parse action. It's just the case described on my previous post. Pyparsing groans when trying to index the dict, which leads to piteux failure announced by the raising of a KeyError exception.



So I'm stucked here. I need to return both tuples and dicts. Perhaps it would be better to handle every returned object as a 'scalar', despite of its type and/or sequence/map protocol implementation, wouldn't it?



Nevertheless I'd like to get this right just for now, even if by means of an eye-injuring patch. Could you help me?



Thank you again.

Best regards.
#### 2006-05-31 04:36:19 - ideefixe
I've written a patch to allow the creation of ParseResults from common python objects (as tuples or dicts) avoiding the problems described above.



I tried to be the less intrusive I could. Briefly, any python object is handled as an scalar (ie. single-token) by ParseResults, being lists the only exception. If you pass a list, its items would become the tokens in <u>toklist. How do you pass a list as a single-token then? You wrap it inside a OneToken (more on this in a minute).





    
    class OneToken(tuple):
        def __new__(cls, tok, off = 0):
            return tuple.__new__(cls, (tok, off))
    
    class ParseResults:
        def __init__(self, toklist, ...):
           [...]
           if isinstance(toklist, list):
               self.__toklist = toklist[:]
           elif isinstance(toklist, OneToken):
               self.__toklist = [toklist[0]]
           else:
               self.__toklist = [toklist]
    



Even so if you try to pass a tuple it would be remorselessly dismembered by </u>getitem<u>/</u>setitem__:





    
    def __setitem__( self, k, v ):
        if isinstance(v,tuple):
           self.__tokdict[k] = self.__tokdict.get(k,list()) + [v]
        [...]
    
    def __getitem__( self, i ):
        [...]
        return ParseResults([ v[0] for v in self.__tokdict[i] ])
    



So for the same price you just get more from petty OneToken. As it stands out, a second optional argument allows the caller to specify an offset. Being a dummy tuple subclass, OneToken is no more, but no less, that our (tok, off) fellow in disguise. Since the subclass act as a tag, you retain the ability to put tuples into another more mundane uses:





    def __setitem__( self, k, v ):
       if isinstance(v,OneToken):
          self.__tokdict[k] = self.__tokdict.get(k,list()) + [v]



There are another changes here and there, mainly makeover for turning (tok, off) into OneToken(tok, off). And that's all. The distributed examples seem to have worked out fine after the change.



If you're interested in the patch, I could send you the diffs.



Best regards,

Carlos.
#### 2006-06-01 20:28:44 - ptmcg
Carlos -



Thanks for your detailed comments (and compliments, of course!).  I will take a look at your proposed enhancements to ParseResults - I actually have some nagging problems with ParseResults, which I've not gotten to look at in a while, so I appreciate your comments and suggestions.



Early in pyparsing's life, parse actions would return a pair of values: a modified parse location, and modified parse tokens.  At the time, I had in mind that parse actions could not only return modified tokens, but could also reposition the parse location before parsing would continue.  Since a pair of values was returned, Python wraps these as a tuple.  This idea never really went anywhere, so I deprecated this behavior (after warning about it about 2 years ago).  The biggest problem with this 'in-between' backwards-compatbility state is that parse actions can return lists, dicts, ints, ParseResults - but *not* tuples!  So as of the next release, I am removing the backwards-compatibility support, so that parse actions can return tuples and they will be treated as tuples (and not unpacked into a location and list of tokens).



But except for this tuple issue, my intent is that parse actions should be able to return just about anything, even custom application objects (like the Command objects created in my adventure game presentation, which now ships with pyparsing).



An alternative approach might be to try using the pyparsing Dict class (shown here being created with the dictOf helper factory method).  For your data, I think this would look something like:





from pyparsing import *



data = 'name1 val1 name2 val2 name3 val3'



key = Word(alphanums)

value = Word(alphanums)

proplist = dictOf(key, value)



props = proplist.parseString(data)



print props.items()

for k in sorted(props.keys()):

    print k,'-\>',props[k]



Giving:



[('name2', 'val2'), ('name3', 'val3'), ('name1', 'val1')]

name1 -\> val1

name2 -\> val2

name3 -\> val3





I hope to get the next release out sometime in June, if work will just let up for a few days!



-- Paul
#### 2006-06-01 23:06:18 - ideefixe
Oh, thank you for your support. No doubt that I'll put that dictOf() goody to proper use.



\>backwards-compatbility state is that parse actions can

\>return lists, dicts, ints, ParseResults - but *not* 

\>tuples! So as of the next release, I am removing the



Regarding parse actions returning dicts, if you let me repeat myself, there is at least a problem when they are named, which shows off at ParseResults construction time:





    try:
      self[name] = toklist[0]
    except TypeError:
      self[name] = toklist



(a KeyError but not a TypeError is raised)



Nice to hear that you would get a new release out about June.



Best regards,

Carlos
#### 2006-06-04 17:33:53 - ptmcg
Yes, this is very good, I've got a placeholder fix in my code, so this fix will be included in 1.4.3.



(Do you mind if I delete your discussion comment on the Feedback page?  It looks like a duplicate of this comment.)



-- Paul

---
## 2006-06-04 02:51:18 - prologic - Wiki Parser
Hi all,



Thanks for such a great library :) I've been looking at PyParsing to build a Wiki Parser for quite a while not, but unfortunately I don't quite know enough about how it works to do this, I keep getting stuck :/



Was wondering if someone would be so kind as to provide a simple example ?



cheers

James

#### 2006-06-04 17:26:07 - ptmcg
Pyparsing can quickly crank out a *simple* Wiki parser, such as bold, or italic, or http URL link.  Things get trickier and more ambiguous when you have something that is both bold *and* italic, unless you define your own wiki emphasis syntax for this.



Here is a *very* crude wiki marker-upper.



-- Paul





    from pyparsing import *
    
    wikiInput = '''
    Here is a simple Wiki input:
      *This is in italics.*
      **This is in bold!**
      ***This is in bold italics!***
      Here's a URL to {{Pyparsing's Wiki Page-\>http://pyparsing.wikispaces.com}}
    '''
    
    def convertToHTML(opening,closing):
        def conversionParseAction(s,l,t):
            return opening + t[0] + closing
        return conversionParseAction
    
    italicized = QuotedString('*').setParseAction(convertToHTML('\<I\>','\</I\>'))
    bolded = QuotedString('**').setParseAction(convertToHTML('\<B\>','\</B\>'))
    boldItalicized = QuotedString('***').setParseAction(convertToHTML('\<B\>\<I\>','\</I\>\</B\>'))
    def convertToHTML_A(s,l,t):
        try:
            text,url=t[0].split('-\>')
        except ValueError:
            raise ParseFatalException(s,l,'invalid URL link reference: ' + t[0])
        return '\<A href='%s'\>%s\</A\>' % (url,text)
    
    urlRef = QuotedString('{{',endQuoteChar='}}').setParseAction(convertToHTML_A)
    
    wikiMarkup = urlRef | boldItalicized | bolded | italicized
    
    print wikiInput
    print
    print wikiMarkup.transformString(wikiInput)
    
    prints:
    
    
    Here is a simple Wiki input:
      *This is in italics.*
      **This is in bold!**
      ***This is in bold italics!***
      Here's a URL to {{Pyparsing's Wiki Page-\>http://pyparsing.wikispaces.com}}
    
    
    
    Here is a simple Wiki input:
      \<I\>This is in italics.\</I\>
      \<B\>This is in bold!\</B\>
      \<B\>\<I\>This is in bold italics!\</I\>\</B\>
      Here's a URL to \<A href='http://pyparsing.wikispaces.com'\>Pyparsing's Wiki Page\</A\>


#### 2006-06-04 17:35:17 - prologic
Thank you for your response Paul.

I knew using PyParsing would be a good choice :) Ihmo, parsers that use traditional regex or custom built are too tedious to extend or modify for some other syntax.



Just a few thoughts though...

How would you handle such cases that you described above ? How about keeping track of depth (ie: headings) ? And table syntax ?



Thanks again, I'll keep hacking away till I understand PyParsing back-to-front :)



cheers

James
#### 2006-06-05 06:40:14 - ptmcg
Overall, I think transformString is the best way to do a Wiki, since it modifies only the matched text, and passes everything else through unharmed.



So when working with transformString, the general process is to layout the pattern you will search for, define the transformation, implement that in a parse action, and then attach the parse action to the pattern's corresponding parse expression.



Since you are doing your own wiki marker upper, you have the luxury/burden of designing your own markup patterns (much like I made up bold, italic, and URL link off the top of my head).  To keep track of depth for headings, you'll need to keep a stack of headings, and push and pop the stack as necessary in a parse action.  For tables, I would define a parse expression that matches the entire table, with nested expressions for table rows and column fields, and then build the appropriate HTML in that expression's parse action.  If you use Group to wrap the rows and column fields, then the tokens argument to the parse action will have the correct hierarchy to it, and you can just iterate through that, assembling \<TR\>, \<TD\>, etc. tags as you go.  Then emit the whole mess from the parse action, so that transformString will replace the marked-up table text with the HTML-ified table text.



-- Paul
#### 2006-06-05 06:44:57 - ptmcg
But as I mentioned earlier, things can get trickier the more elaborate your wiki becomes.  For instance, the next thing you will want to do is have bold text embedded in the headings of your table.  If you are going to get this from the markup, then the parser will have to handle nested constructs within the table - this is not impossible, just a bit more complex.  An alternative would be for your parser to implicitly bold table headings, without requiring **'s around the heading text.  This makes your wiki less flexible, but simple wiki users might very well appreciate it.



-- Paul
#### 2006-06-05 06:49:45 - prologic
Thanks Paul for your comments.



Just one question. Could you show

an example of handling a nexted construct ? (By hand I'd write a recursive parser!)



cheers

James
#### 2006-06-07 08:33:08 - ptmcg
James -



I'm a little short on time today, but look at the list parsing examples on slides 21-26 of my PyCon presentation, at .



This goes through the steps for parsing a stringified Python list back into a list, including a nested list of lists (of lists...).



Hope this helps,

-- Paul
#### 2008-07-31 13:28:38 - abalter
This link is dead:



    ptmcg re: Wiki Parser

James -



I'm a little short on time today, but look at the list parsing examples on slides 21-26 of my PyCon presentation, at .
#### 2008-07-31 15:30:53 - ptmcg
Sorry, try this one:







Note slides 21-26 on how to parse nested ()'s or []'s.



-- Paul

---
## 2006-06-16 05:22:10 - peter21081944 - Exceptions
In the  example below pyParsing fails to parse the input string with the error ' Expected 'value' (at char 12), (line:1, col:13)': 






```
from pyparsing import *<br />
<br />
A = Keyword('Quote') + QuotedString('&quot;')<br />
B = Word(alphas) + Literal('thing') + Literal('value')<br />
S = A | B<br />
print S.parseString('Quote thing here?')
```




I have no problem with this as default behaviour but I have a use case where I want to fail the parse when the

QuotedString fails. What is the best way to do this? I was sort of looking for a SetFailAction to call when a match failed analogous to SetParseAction when it succeeded. 



Thanks for any help

#### 2006-06-16 06:21:58 - ptmcg
Hmm, SetFailAction sounds interesting.  Two things you can try:

- add a parse action to QuotedString, no wait, that wont work..., parse actions only fire when the expression matches (which it doesn't here)

- add a failure debug action to QuotedString (see setDebugActions) and throw ParseFatalException in the body of your debug method - ParseExceptions are routine, and signal when an alternative wont work.  ParseFatalExceptions will kill parsing immediately.



But it feels kind of hacky to use a debugging mechanism for handling of a functional use case.  Why not post a Feature Request on SF for me, describing setFailAction?  Do fail actions need to support chaining the way parse actions do now?



Thanks for using pyparsing!

-- Paul
#### 2006-06-16 07:47:07 - peter21081944
OK - I submitted a request. What I did for my use-case (which was to kill the parse if a quoted string with the wrong delimniters was enterd) was to subclass QuotedString and then call QuotedString.parseImpl(..) and catch the ParseException and then terminate with sys.exit(). 



BTW For my purposes it is <em>much</em> better than the other 'compiler' options. It is flexible, maintainable and convenient. So thanks for providing pyparsing!



--Peter
#### 2006-07-01 11:19:56 - peter21081944
I got the new release with setFailAction call which works fine. Many thanks. A couple of questions: what is the 'RetryFailedExpressionException' and where is the 'countedArray helper'? You mention these in your post on sourceforge  ()



Cheers

-- Peter
#### 2006-07-01 15:57:00 - ptmcg
'RetryFailedExpressionException' does not yet exist, but it is what I have in mind for how a fail action would signal that it wanted the parse to be reattempted.



countedArray exists today, it was released in 1.4.2, I think.  Here is my unit test for it:



    from pyparsing import Word,nums,OneOrMore,countedArray
    
    testString = '2 5 7 6 0 1 2 3 4 5 0 3 5 4 3'
    
    integer = Word(nums).setParseAction(lambda t: int(t[0]))
    countedField = countedArray(integer)
    
    r = OneOrMore(countedField).parseString( testString )
    print r.asList()

Which prints:

[[5,7],[0,1,2,3,4,5],[],[5,4,3]]
#### 2006-07-02 07:08:32 - peter21081944
Ah, I see. `retryFailedExpressionException` sounds a good idea. I found the `countedArray` as well,



Thanks



-- Peter

---
## 2006-07-19 10:29:15 - lenx - bug report: wrong pos when strings contain '\t'
sorry, I don't find a right place to report bug, so put it here...



pyparsing 1.4.2



<ul class="quotelist"><ul class="quotelist"><ul class="quotelist"><li>from pyparsing import *</li><li>pr = Word(alphas, alphanums)</li><li>str = '123 a1'</li><li>for r, start, end in pr.scanString(str):</li></ul></ul></ul>...     print str[start:end]

...     

a1

>>>

<ul class="quotelist"><ul class="quotelist"><ul class="quotelist"><li>str= '\t123 a1 12345678'</li><li>for r, start, end in pr.scanString(str):</li></ul></ul></ul>...     print str[start:end]

...     

56

<ul class="quotelist"><ul class="quotelist"><ul class="quotelist"><li>

</li></ul></ul></ul>

#### 2006-07-24 11:58:10 - ptmcg
For the sake of column-sensitivity, pyparsing implicitly uses a copy of the input string, with expanded tabs. You can suppress this behavior by calling parseWithTabs, as in:



pr.parseWithTabs().scanString(str)



(Note, 'str' is not the greatest variable name, since it masks the built-in str type.)



-- Paul
#### 2006-07-24 18:08:06 - lenx
Thanks :)
#### 2011-03-17 22:18:22 - ahatchkins
I've also run into this problem.

This feature is not documented in the O'Reilly book. 

It is also sort of counter-intuitive, because grammar 'a\tb' doesn't parse 'a\tb'.

I would suggest either documenting it in the next version of 'Getting Started with Pyparsing' (near leaveWhitespace and Co) or disabling by default.

Thanks )

---
## 2006-07-23 14:12:47 - Qman - Help with parsing
Hi I have this problem where I can't figure out how to get pass.



\<outer part\>

stuff

\<inner part\>

stuff

\<end\>

more stuff

\<end\>



Say I want to capture this entire outer part and the only things I have to identify it is \<outer part\> and \<end\>. The problem is that the \<inner part\> has the same ending. How can do this by somehow skipping over the inner \<end\>?

#### 2007-06-29 14:05:44 - vineshp
I'd say:



inner = '\<inner part\>' + stuff + '\<end\>'

outer = '\<outer part\>' + stuff + Optional(inner) + stuff + '\<end\>'



where stuff is what's allowed there, say 

stuff = CharsNotIn('\<')



This way you combine the \<inner part\> ... \<end\> into a single block that must be evaluated together.



-Shawn
#### 2016-04-18 02:54:59 - yosra_ben_amor
Hello, 

I am beginner in the pyparsing, and i have a problem with parsing  my log file wich begin with a white ligne space and i tried to ignore the ligne but i couldn't with the method of setWhitespaceChars() 

this is my log file : 



Memory: svmem(total=4014465024, available=3359850496, percent=16.3, used=1995816960, free=2018648064, active=854323200, inactive=939483136, buffers=105562112, cached=1235640320)

CPU: 35.40 %





Elapsed execution time: 0.658702850342 seconds

please it there someone who could help me to resolve this problem ?

---
## 2006-07-23 14:13:41 - Qman - problem
Hi I have this problem where I can't figure out how to get pass.



\<outer part\>

stuff

\<inner part\>

stuff

\<end\>

more stuff

\<end\>



Say I want to capture this entire outer part and the only things I have to identify it is \<outer part\> and \<end\>. The problem is that the \<inner part\> has the same ending. How can do this by somehow skipping over the inner \<end\>?

#### 2006-07-24 12:04:32 - ptmcg
Qman -



How complicated is 'stuff'? Can you create expressions for them, so that you could define something like:



stuff = ~Literal('\<end\>') + Word(printables)

innerExpr = '\<inner part\>' + ZeroOrMore( stuff ) + '\<end\>'

outerExpr = '\<outer part\>' + ZeroOrMore( stuff | innerExpr ) + '\<end\>'



-- Paul
#### 2006-07-24 14:41:20 - Qman
Thanks. That was what I was looking for.



For ZeroOrMore( stuff | innerExpr ), wouldn't the innerExpr have to come first?
#### 2006-07-24 18:57:40 - Qman
Also to follow up with another question, are there certain rules for Combine? When I try

Combine (ZeroOrMore( stuff | innerExpr )), it doesn't work. Any way around this?
#### 2006-07-24 19:43:47 - ptmcg
The main purpose for Combine is to convert a series of tokens into a single string. For instance, if you defined a real number as Word(nums)+'.'+Word(nums), parsing '3.14159' would return ['3','.','14159'].  So instead, we wrap our definition in a Combine, as in Combine(Word(nums)+'.'+Word(nums)), then we get '3.14159'.  The default behavior for Combine is to join the tokens with an empty string, and to require the input tokens to be adjacent.



I'm guessing that there is whitespace separating the stuff tokens in your input string.  Combine will work if you specify Combine (ZeroOrMore( stuff | innerExpr ), adjacent=False ).  You can also specify the joinString parameter, in case you want to join the tokens with something other than an empty string.



-- Paul
#### 2006-07-24 19:44:49 - ptmcg
<ul class="quotelist"><li>wouldn't the innerExpr have to come first</li></ul>

Yes, good catch!



-- Paul
#### 2006-07-24 20:23:12 - Qman
Thanks for your help.



I really appreciate everything you've done and all the hours you have spent on this parser.

---
## 2006-08-11 07:31:58 - kndyer - Matching question
I have a grammer like this:



item = Combine( Word(alphas,max=1) + Word(nums, max=3) + Word( alphas, max=1) )



list = Forward()

list \<\< (item + Literal(',').suppress() + list) | item



This fails as expected:

list.parseString( 'a3333b' )



However, I don't get parse exceptions for:

list.parseString( 'a22b, ')

list.parseString( 'a22B, a b' )



How do I construct a grammar that will validate subsequent list elements?  



IE: If there is a comma, the following tokens must also be valid

#### 2006-08-13 08:05:37 - ptmcg
There are two aspects to this question.  The first is in the literal implementation of the common BNF idiom:



thing :: blah

listOfThings:: thing listOfThings | thing



BNF does this as an alternative to regex * and ? repetition operators.  Pyparsing has the classes ZeroOrMore and OneOrMore.  While your recursive implementation using Forward *will* work (successfully finds the single-element lists a22b and a22B), you will find the repetition classes to run faster.  In this case it would translate to (I've changed your expression list to listOfItems, so as to not mask Python's own builtin list type):



listOfItems = item + ZeroOrMore(',' + item)



In fact, this is such a common expression, that there is a helper method in pyparsing called delimitedList:



listOfItems = delimitedList( item )



A comma delimiter is the default, but an optional delim argument can be passed containing an expression for other delimiters.



Note also that a list of items that are just separated by whitespace don't even need to be in a delimitedList.  A list such as this:



aaa bba ccdd  efeeb



Can be parsed with just OneOrMore(Word(alphas)), since pyparsing implicitly skips over whitespace.



The second aspect to your question has to do with pyparsing not raising an exception if it runs out of matching input text before it gets to the end of the string.  If pyparsing is at a valid ending place in the grammar, and then finds nonmatching text, then it will simply return the results found so far.  To force pyparsing to *have* to read all the way to the end, terminate your grammar with stringEnd, as in:



listOfItems = delimitedList( item )

grammar = listOfItems + stringEnd



Now grammar will succeed with:

a22b

b123c, e456d,b134



but will fail with:

a22b,

b123c,e456d!



Because the list of items is not immediately followed by the end of the string.  There are also stringStart, lineStart, and lineEnd expressions defined in pyparsing.



Also, check out the FAQ's - this comes up pretty often, really.



Good luck!

-- Paul

---
## 2006-08-26 14:00:10 - kwm - Plural possessives
I've built (and have been using for some time) a SQL parser based on pyparsing.  To this point, however, I've only been handling a very high volume of small messages.  Thus, not a lot of edge cases at all.



The one not-so-edge case that I can't seem to deal with is quoted-strings that contain plural possessives.  For example:



INSERT INTO foo(uno) VALUES ('The farmers' intention was to pool their crop.') WHERE id = 1;



Pyparsing raises an exceptions because it expects the WHERE clause to begin immediately following the single quote after 'farmers''.



I'm using the single- and double-quoted string builtins.  I've defined a valid value as (quotedString | realNum | intNum).



Any thoughts or suggestions would be greatly appreciated!



Cheers



Keith

#### 2006-08-26 20:03:17 - ptmcg
What version of pyparsing are you using?  Are you sure you're using the built-ins?  Here's my interactive Python session:

    import pyparsing
    print pyparsing.__version__
    1.4.3

    data = '''('The farmers' intention was to pool their crop.') '''
    print pyparsing.quotedString.searchString(data)
    [[''The farmers\' intention was to pool their crop.'']]


-- Paul
#### 2006-08-31 04:38:25 - kwm
Paul,



Thanks for the reply.  As it turns out, the data that we're getting differs from the data that was submitted, in some cases.  For example, many SQL-backed web appliations will escape single quotes by adding another single quote.  This is what appears to be (rightfully) causing me problems.



I've yet to try, but I believe that I should be able to handle this by defining escapedSingleQuote as two single quotes with nothing in between, correct?
#### 2006-08-31 07:04:11 - ptmcg
Keith -



I'm still not able to reproduce your problem.  Here is another interactive Python session, with SQL-style quote escaping (both single and double quotes):

    >>> from pyparsing import *
    >>> print dblQuotedString.parseString(''''This string has an doubled ('') quote character'''')
    [''This string has an doubled ('') quote character'']
    >>> print sglQuotedString.parseString(''''This string has an doubled ('') quote character'''')
    [''This string has an doubled ('') quote character'']
    >>> print quotedString.parseString(''''This string has an doubled ('') quote character'''')
    [''This string has an doubled ('') quote character'']
    >>> print quotedString.parseString(''''This string has an doubled ('') quote character'''')
    [''This string has an doubled ('') quote character'']



Perhaps the problem is that the resulting text is not 'unescaped', that is, the doubled-up quote characters are still there.  You can post-process the strings after parsing, or use a parse action to do the unescaping during parsing.  Here's a simple parse action to do the trick:


    >>> quotedString.setParseAction(lambda t:t[0].replace('''',''').replace('''','''))
    quotedString using single or double quotes
    >>> print quotedString.parseString(''''This string has an doubled ('') quote character'''')
    [''This string has an doubled (') quote character'']
    >>> print quotedString.parseString(''''This string has an doubled ('') quote character'''')
    [''This string has an doubled (') quote character'']



You can pass multiple parse actions to be executed as a chain, for example, unescaping embedded doubled quote characters, and removing the outer enclosing quote characters (using the pyparsing helper method removeQuotes):

    >>> quotedString.setParseAction(lambda t:t[0].replace('''',''').replace('''','''), removeQuotes)
    >>> print quotedString.parseString(''''This string has an doubled ('') quote character'''')
    ['This string has an doubled (') quote character']
    >>> print quotedString.parseString(''''This string has an doubled ('') quote character'''')
    ['This string has an doubled (') quote character']



Is any of this helping?



-- Paul

---
## 2006-08-28 12:17:57 - cie - a bug? combine and recursion
hi, I think this is not as expected:



    from lib.pyparsing import *
    Stream = Forward()
    Stream << Optional(Word(alphas))+Optional('('+Word(nums)+')'+Stream)
    Forward: {[W:(abcd...)] [{{{'(' W:(0123...)} ')'} ...}]}


    Stream.parseString('myc(114)r(11)dd')
    (['myc', '(', '114', ')', 'r', '(', '11', ')', 'dd'], {})



that's ok, but:



    from lib.pyparsing import *
    Stream = Forward()
    Stream << Combine(Optional(Word(alphas))+Optional('('+Word(nums)+')'+Stream))
    Forward: Combine:({[W:(abcd...)] [{{{'(' W:(0123...)} ')'} Forward: None}]})

    Stream.parseString('myc(114)r(11)dd')
    (['myc'], {})


now what's the matter?



thx

#### 2006-08-28 12:35:23 - ptmcg
Good catch - looks like Combine doesn't like recursion!  Would you mind reporting this as a bug on SourceForge?  Then I'll be sure to take care of it properly.



BTW, is there any reason you are using recursion instead of just using one of the iterative forms, such as OneOrMore?  I notice that many folks porting BNF and EBNF grammars to pyparsing tend to use recursion, as in:





    item :: blah blah
    listOfItems :: item | item listOfItems



The pyparsing form of this is just:



    listOfItems = OneOrMore(item)

(If items are separated by commas or some other non-whitespace delimiter, use delimitedList.)



For your code, these perform as expected:



    print OneOrMore( Word(alphas) | '('+Word(nums)+')').parseString('myc(114)r(11)dd')
    
    print Combine(OneOrMore( Word(alphas) | '('+Word(nums)+')')).parseString('myc(114)r(11)dd')

Giving:



    ['myc', '(', '114', ')', 'r', '(', '11', ')', 'dd']
    ['myc(114)r(11)dd']



I'll still look into that Combine problem, though!



-- Paul
#### 2006-09-03 11:57:34 - cie
Thanks.
#### 2006-09-03 12:23:49 - ptmcg
I've looked into this a little further, and this turns out to be fairly tricky.  The problem stems from pyparsing's whitespace-skipping behavior.  Combine turns off whitespace skipping in embedded expressions, but first makes a copy of them so as not to change the subexpressions' default behavior.  Unfortunately, at the time this is done, the embedded Forward has not yet had its contents assigned, so the copy contains no expression to be matched - Forwards with no content always fail, therefore, the Combine fails.



I'll see if I can make Forwards a little smarter when they get copied.



-- Paul
#### 2006-09-04 03:56:10 - cie
Yes, I think you could also enable using Forward with setResultsName().
#### 2006-09-27 15:30:44 - ptmcg
I have this problem fixed, and it will be in the next pyparsing release (1.4.4).  And it does address the setResultsName problem with Forwards also.



ETA on this release is probably mid-October or so.



-- Paul

---
## 2006-09-04 10:21:40 - cie - Opposite of CharsNotIn
Hi,



How can I generate an opposite of CharsNotIn, eg. CharsIn which parses only those characters which are in the specified string. Word is not good, because it checks whether the word does not exceed the maximal length:



1244 <strong>if</strong> self.maxSpecified <strong>and</strong> loc \< instrlen <strong>and</strong> instring[loc] <strong>in</strong> bodychars:

1245            throwException = <strong>True</strong>



How about something like Word(alphas, max=5, <em>mustEnd</em>=False)

#### 2006-09-04 20:00:26 - ptmcg
It isn't necessary to specify a maximum length when defining a Word instance.  You can just write:



integer = Word(nums)



and it will match as many numeric digits as are in the input text at the current parse position, up to sys.maxint.



-- Paul
#### 2006-09-05 13:00:06 - cie
I am working on a TeX parser, and I need to parse the first letter of a string, not caring about the others.

A simplified version of the grammar:





    element=Forward()
    frac='\\frac'+element+element
    sqrt='\\sqrt'+element
    group='{'+ZeroOrMore(element)+'}'
    element << (frac | sqrt | group | Word(alphas,max=1) | Word(printables,max=1))



But this does not work, eg. \frac34 fails, because Word(printables,max=1) checks if the next char is not in printables.

Can you suggest a solution?
#### 2006-09-05 14:14:12 - ptmcg
How about one of these:

  Word(printables,exact=1)

  oneOf(list(printables))

  Regex('.')



-- Paul
#### 2006-09-05 14:40:23 - cie
Yes, this works. Thank you very much.

---
## 2006-09-14 07:30:36 - nycterent - parser fails when http auth username is email
if using mod_imap to authenticate users, then you can email as username in the log file and example httpServerLogParser.py chokes on that:



pyparsing.ParseException: Expected '['

#### 2006-09-16 14:19:11 - ptmcg
Could you post a sample of what the log file line looks like with such data?  Then we can come up with a modified grammar to handle it.



-- Paul
#### 2006-09-17 10:43:18 - nycterent
127.0.0.1 -  [12/Sep/2006:14:13:53 +0300] 'GET /skins/monobook/external.png HTTP/1.0' 304 - '' 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.0.6) Gecko/20060728 Firefox/1.5.0.6'





i've managed to use  '-' | Word( '.' + '@' + alphas+nums ) for the auth stuff - works fine. 



also i feel that request 'GET /skins/monobook/external.png HTTP/1.0' shoud be split in three: type, request uri and protocol version, because one of the parsing goals is to group by that data :)
#### 2006-09-17 19:32:58 - ptmcg
The cmd data gets parsed as a qutoed string, but these fields could be broken up in the parse action, such as:



    def getCmdFields( s, l, t ):
        t['method'],t['requestURI'],t['protocolVersion'] = t[0].strip(''').split()



I'll add this and your auth changes in this example for the next release.



-- Paul
#### 2006-09-17 22:53:06 - nycterent
thanks ;) pyparsing is very cool.

---
## 2006-09-15 23:52:29 - CosmicStars - Idea how to skip masked linefeeds?
Hi,



I read a file in bin mode so I don't loose

or transform any bytes.



I got clauses over more than one lines

sepereated by maked LFs (\\\n). 

The clause itselfs ends with an unmasked

LF '\n'.



I want to parse these and need the

whole clause by clause back.



Here an example of an clause(don't ask me

about my english :-)



An fish is living in the water. It can live \\\n

    in a sea or in the ocean.\n

A human is in the water to watch the fishes \\\n

    living in the sea (or ocean).\n

Some fishes are food for other fishes. But \\\n

\tonly seldom a fish eats an human.\n



I've found only an solution for clause-breaks

at \\\n with suppressed \n, but not in the other

way (\n with suppressed \\\n)



Any idea?



How I can parse these?

#### 2006-09-16 14:30:51 - ptmcg
If I were to try to parse this in Python, would this give me the string you are working with?





    '''An fish is living in the water. It can live \\\n
    in a sea or in the ocean.\n
    A human is in the water to watch the fishes \\\n
    living in the sea (or ocean).\n
    Some fishes are food for other fishes. But \\\n
    \tonly seldom a fish eats an human.\n'''



If so, can you explain what you want to do with this?  Perhaps if you post a little more of your code, we would have a better idea what problem you are having.



-- Paul
#### 2006-09-16 19:29:36 - CosmicStars
I hope this short code will explain enough:



filetext = open('testtext.txt', 'rb').read()

notend = pp.Combine(pp.Literal('\\') + pp.Literal('\n') )

<ol><li>I don't need this</li></ol>delit = pp.SkipTo( notend ).suppress() 



<ol><li>Like this is what I need but didn't work</li></ol>#line = pp.lineStart + delit + pp.SkipTo( pp.Literal('\n') )



<ol><li>This gives every line</li></ol>#line = pp.lineStart + pp.SkipTo( notend )



<ol><li>This is working but not how I need it.</li></ol>line = pp.lineStart + pp.SkipTo( notend )



for logline,begin,end in  line.scanString(filetext):

    p([logline,begin,end])
#### 2006-09-16 19:34:21 - CosmicStars
Hope this will looks better.





    from pprint import pprint as p
    import pyparsing as pp
    
    filetext = open('testtext.txt', 'rb').read()
    notend = pp.Combine(pp.Literal('\\') + pp.Literal('\n') )
    delit = pp.SkipTo( notend ).suppress() 
    # 
    #line = pp.lineStart + delit + pp.SkipTo( pp.Literal('\n') )
    #line = pp.lineStart + pp.SkipTo( pp.Literal('\n' )
    line = pp.lineStart + pp.SkipTo( notend )
    
    for logline,begin,end in  line.scanString(filetext):
        p([logline,begin,end])
    


#### 2006-09-16 21:14:08 - ptmcg
I think I understand what you are doing (I'm not totally sure this requires opening the file in binary mode, but shouldn't matter either way).



It looks like you have some lines that end with '\' characters, and you want to merge these lines with the following line wherever they occur.



I think the simplest way is to use notend as you have defined it, add a parse action that replaces '\\\n' with '', and then call transformString.  This gives you a new string that removes any \n that is preceded by a \.  To then get the actual lines in a list (as marked by unmasked \n's), call split('\n') on the result.  Here is my update to your version of the code:





    from pprint import pprint as p
    import pyparsing as pp
    
    #~ filetext = open('testtext.txt', 'rb').read()
    filetext = r'''An fish is living in the water. It can live \
    in a sea or in the ocean.
    A human is in the water to watch the fishes \
    living in the sea (or ocean).
    Some fishes are food for other fishes. But \
    \tonly seldom a fish eats an human.'''
    
    notend = pp.Combine(pp.Literal('\\') + pp.Literal('\n') )
    notend.setParseAction( pp.replaceWith('') )
    
    p( notend.transformString( filetext ).split('\n') )



Prints:



    ['An fish is living in the water. It can live in a sea or in the ocean.',
     'A human is in the water to watch the fishes living in the sea (or ocean).',
     'Some fishes are food for other fishes. But \\tonly seldom a fish eats an human.']



Is this what you're looking for?



-- Paul
#### 2006-09-16 21:24:03 - ptmcg
Note that the wiki is displaying the code example by joining the lines that end with '\', so it is not easy to see the program in action.  But try it out on your input file and see if it is doing what you want.



-- Paul
#### 2006-09-17 02:36:04 - CosmicStars
Hello,



thank you. 



1. I have now my lines seperated :-)

2. I got more information about transformString.



Thank you.

Pierre

---
## 2006-09-16 12:28:17 - metaperl - whitespace field discrim, adding ParseResults attributes
Hi,



First, I am parsing lines which represent the information about a single song:



-\>[0/1314] Realtime - Lost in Space (07:30)



and the way to locate the artist and title is not very precise, because an artist or title can have dashes and parentheses. But for now, the question is whether or not I can choose the artist and title directly from the grammar if I specify that they are separated by ' - '... meaning a space, a dash and a space.



Also, I tried to add some attributes to the ParseResults object but was not allowed. Is there some way to add them? I manually parsed the artist and title into separate fields and wanted to store them in the ParseObject. Code follows:



<ol><li>parse song data</li><li>-\>[0/1314] Realtime - Lost in Space (07:30)</li></ol>

class song_parse:

    '''Designed to parse lines like this:

    -\>[0/1314] Realtime - Lost in Space (07:30)

    '''

    playlist_id = Regex(r'\d+')               .setResultsName('playlist_id')

    medialib_id = Regex(r'\d+')               .setResultsName('medialib_id')



    minutes = Regex(r'\d+')                   .setResultsName('minutes')

    seconds = Regex(r'\d+')                   .setResultsName('seconds')



    current_marker = Literal('-\>')            .setResultsName('current_marker')

    song_id = '[' + playlist_id + '/' + medialib_id + ']'

    artist_title = Regex(r'[^(]+')            .setResultsName('artist_title')



    time    = '(' + minutes + ':' + seconds + ')'





    song_data = Optional(current_marker) + song_id + artist_title + time

    song_data.setDebug()







    def <u>init</u>(my, song_line):

        my.scan = my.song_data.parseString(song_line)

        artist, title = my.scan.artist_title.split(' - ')

#### 2006-09-16 15:42:19 - ptmcg
Terrence -



Welcome to the Python and pyparsing worlds!  I look forward to getting your perspective on pyparsing, fresh eyes are always good to get on a project.



Pyparsing's ParseResults are somewhat schizophrenic objects - you can treat them like lists, like dicts ('hashes' in Perl), or like objects with named attributes.  They are *really* meant to be mostly read-only, to facilitate access to the parsed data after parsing has finished.  If you want to add keyed attributes, you must do this using the dict-like access.



Here is your program, with some Python cleanup  (to post code in wikispaces and retain the spacing, use \[\[code\]\] delimiters about your source).



Some Python fixes:

- 'init' should be '<u>init</u>'

- have your classes inherit from object, avoids some legacy behavior

- 'my' is okay for the 'me' reference variable, and really any varname will work (I've seen some people use '_'), but 'self' is the normal Python convention





    from pyparsing import *
    
    class song_parse(object):
        '''Designed to parse lines like this:
        -\>[0/1314] Realtime - Lost in Space (07:30)
        '''
        playlist_id = Regex(r'\d+') .setResultsName('playlist_id')
        medialib_id = Regex(r'\d+') .setResultsName('medialib_id')
    
        minutes = Regex(r'\d+') .setResultsName('minutes')
        seconds = Regex(r'\d+') .setResultsName('seconds')
    
        current_marker = Literal('-\>') .setResultsName('current_marker')
        song_id = '[' + playlist_id + '/' + medialib_id + ']'
        artist_title = Regex(r'[^(]+') .setResultsName('artist_title')
    
        time = '(' + minutes + ':' + seconds + ')'
        song_data = Optional(current_marker) + song_id + artist_title + time
        #~ song_data.setDebug()
    
        def __init__(my, song_line):
            my.scan = my.song_data.parseString(song_line)
            artist, title = my.scan.artist_title.split(' - ')
    
    
    song_line = '-\>[0/1314] Realtime - Lost in Space (07:30)'
    sp = song_parse(song_line)
    print sp.scan.dump()
    sp.scan['artist'],sp.scan['title'] = sp.scan.artist_title.split(' - ')print sp.scan.dump()



Will print:



    ['-\>', '[', '0', '/', '1314', ']', 'Realtime - Lost in Space ', '(', '07', ':', '30', ')']
    - artist_title: Realtime - Lost in Space 
    - current_marker: -\>
    - medialib_id: 1314
    - minutes: 07
    - playlist_id: 0
    - seconds: 30
    ['-\>', '[', '0', '/', '1314', ']', 'Realtime - Lost in Space ', '(', '07', ':', '30', ')']
    - artist: Realtime
    - artist_title: Realtime - Lost in Space 
    - current_marker: -\>
    - medialib_id: 1314
    - minutes: 07
    - playlist_id: 0
    - seconds: 30
    - title: Lost in Space 



Here are some suggestions on your program:



1. If you like, you can do the artist_title splitting inside a parse action.  Something like:





    def splitArtistAndTitle(toks):
        toks['artist'],toks['title'] = toks.artist_title.split(' - ')
    
    class song_parse(object):
        '''Designed to parse lines like this:
        -\>[0/1314] Realtime - Lost in Space (07:30)
        '''
        playlist_id = Regex(r'\d+') .setResultsName('playlist_id')
        medialib_id = Regex(r'\d+') .setResultsName('medialib_id')
    
        minutes = Regex(r'\d+') .setResultsName('minutes')
        seconds = Regex(r'\d+') .setResultsName('seconds')
    
        current_marker = Literal('-\>') .setResultsName('current_marker')
        song_id = '[' + playlist_id + '/' + medialib_id + ']'
        artist_title = Regex(r'[^(]+') .setResultsName('artist_title')
        artist_title.setParseAction(splitArtistAndTitle)
    
        time = '(' + minutes + ':' + seconds + ')'
        song_data = Optional(current_marker) + song_id + artist_title + time
        #~ song_data.setDebug()
    
        def __init__(my, song_line):
            my.scan = my.song_data.parseString(song_line)
    
    sp = song_parse(song_line)
    print sp.scan.dump()



Prints the same as the above.



2. Alternatively, see if the SkipTo pyparsing class might help in defining separate fields for artist and title, something like:





    artist_title = SkipTo(' - ').setResultsName('artist') + Literal(' - ').leaveWhitespace() + SkipTo('(').setResultsName('title')





3. Parse actions are also good for converting data values during parsing.  In general, the philosophy behind parse actions is something like 'we've already verified that the incoming text conforms to some expected pattern, if this is going to be converted to a more meaningful object later, might as well do it now'.  So your minutes and seconds can be converted to actual ints using this format:





    integer = Regex(r'\d+').setParseAction(lambda t:int(t[0]))
    minutes = integer .setResultsName('minutes')
    seconds = integer .setResultsName('seconds')



I have seen other users use parse actions to actually compile their text on the fly, and return an object that is immediately executable in their application.



Hope this helps, write back if you have more questions,



-- Paul

---
## 2006-09-16 18:57:17 - metaperl - Debug output goes to stdout?
FWICT, debug output is going to STDOUT which makes  pyparsing.py painful for web use.



Perhaps this could be parameterized?

#### 2006-09-16 20:55:44 - ptmcg
Good thought!  What if I add a setDebugOutput static method on ParserElement?



-- Paul
#### 2006-09-17 22:44:29 - ptmcg
Oh, you can also define your own debugging routines - see ParserElement.setDebugActions, and the method signatures for _defaultStartDebugAction, _defaultSuccessDebugAction, and _defaultExceptionDebugAction.
#### 2012-01-14 10:45:05 - DiaaFayed
idea



add logger class to output 

debugger out to logger file



and not stopping parsing if

more than lines of strings.

---
## 2006-09-27 14:22:29 - metaperl - a line of quoted and unquoted terms
Hi, my pyparsing function is not tokenizing the unquoted terms into discrete tokens:




```
<br />
def tokenize(s):<br />
    &quot;&quot;&quot;Take a single line of terms and return as a list of terms<br />
    For example:<br />
      &gt;&gt;&gt; sqlgen.tokenize(&quot;Phase I&quot; cerv* cerc* &quot;ovarian cyst&quot;)<br />
      &gt;&gt;&gt; ['&quot;Phase I&quot;', 'cerc*', 'cerv*', '&quot;ovarian cancer&quot;']<br />
    &quot;&quot;&quot;<br />
    quote_term = Regex('&quot;[^&quot;]+&quot;') # not needed<br />
    other_term = Regex('[^&quot;]+')<br />
<br />
    search_term = quotedString | other_term<br />
<br />
    line = OneOrMore(search_term)<br />
    return line.parseString(s)<br />

```


#### 2006-09-27 14:25:56 - metaperl
Too bad you cant edit posts. But I'm going to get this right :)





    
    def tokenize(s):
        '''Take a single line of terms and return as a list of terms
        For example:
          >>> sqlgen.tokenize('Phase I' cerv* cerc* 'ovarian cyst')
          >>> [''Phase I'', 'cerc*', 'cerv*', ''ovarian cancer'']
        '''
        quote_term = Regex(''[^']+'') # not needed
        other_term = Regex('[^']+')
    
        search_term = quotedString | other_term
    
        line = OneOrMore(search_term)
        return line.parseString(s)
    


#### 2006-09-27 15:16:54 - metaperl
This thread is a complete embarrassment:





    
        other_term = Regex('[^'\s]+')
    
    



that fixes it. No help needed.
#### 2006-09-27 15:17:59 - ptmcg
For the record, the provided example and source give these results:

[''Phase I'', 'cerv* cerc* ', ''ovarian cyst'']



Your regex for {other_term} says simply, 'accept all characters that are not a ' character', and spaces are not ' characters.



Since you want to read in words/tokens made up of non-space characters, the easiest is to create a pyparsing {Word} composed of characters in the pyparsing string constant {printables}.  Or expand your Regex re string to disallow spaces as well as ' characters.



Here are two versions of other_term that do what you want:



    other_term = Regex('[^' ]+')
        other_term = Word(printables)



I would also guess that the enclosing quotes on {''Phase I''} and {''ovarian cyst''} aren't really desirable parts of the string content; the quotes were just there to delimit some tokens with embedded spaces.  Pyparsing provides the {removeQuotes} parse action for attaching to quotedString expressions, to clean up the returned text.  So by defining {search_term} as (after having fixed {other_term} per one of the suggestions above) as:



    search_term = quotedString.setParseAction(removeQuotes) | other_term



you get:



    ['Phase I', 'cerv* cerc* ', 'ovarian cyst']



-- Paul
#### 2006-09-27 15:20:28 - ptmcg
Doh!  I mean, you get:



    ['Phase I', 'cerv*', 'cerc*', 'ovarian cyst']



-- Paul

(If you like, I'll delete your earlier misformatted posts.)
#### 2006-09-27 17:34:08 - metaperl
Yes, delete all the earlier ones.



But the quotes have to be preserved because it means something to MS SQL's CONTAINS clause, as seen in example C below:





---
## 2006-09-27 17:45:53 - metaperl - why is parseString returning a tuple consisting of a list and dictionary?
I am wondering why I am not just getting back a list of tokens.





    def tokenize(s):
        '''Take a single line of terms and return as a list of terms
        For example:
          >>> sqlgen.tokenize(''Phase I' cerv* cerc* 'ovarian cyst'')
          >>> [''Phase I'', 'cerc*', 'cerv*', ''ovarian cancer'']
        '''
        quote_term = Regex(''[^']+'')  # not needed
        other_term = Word(printables) #Regex('[^'\s]+')
    
        search_term = quotedString | other_term
    
        line = OneOrMore(search_term) + StringEnd()
    
        return line.parseString(s)
    



#### 2006-09-27 18:41:23 - ptmcg
Ah!  parseString <em>actually</em> returns a pyparsing-defined type called a ParseResults.  ParseResults is a multi-personalitied type, that allows access to its data by list, dict, or object attribute access methods.



Here's the canonical 'Hello, World!' program.  Note the list access to the individual tokens:



    # define grammar
    greet = Word( alphas ) + ',' + Word( alphas ) + oneOf('! ? .')
    
    # input string
    hello = 'Hello, World!'
    
    # parse input string
    greetTokens = greet.parseString( hello )
    print hello
    print greetTokens
    print repr(greetTokens)
    print greetTokens[0]
    print greetTokens[2]
    if greetTokens[-1] == '.':
        print 'That was a sentence'
    elif greetTokens[-1] == '!':
        print 'That was an exclamation'
    elif greetTokens[-1] == '?':
        print 'That was a question'
    else:
        print 'That wasn't a valid greeting'



We get the following output:



    Hello, World!
    ['Hello', ',', 'World', '!']
    (['Hello', ',', 'World', '!'], {})
    Hello
    World
    That was an exclamation



If we modify the grammar to define name some of the results tokens:



    greet = Word( alphas ).setResultsName('salutation') + \
        ',' + Word( alphas ).setResultsName('greetee') + oneOf('! ? .')
    
    greetTokens = greet.parseString( hello )
    print hello
    print greetTokens
    print repr(greetTokens)
    print greetTokens.salutation
    print greetTokens.greetee



We see repr gets much more complicated, but we can also reference these fields like they were attributes of an object:



    Hello, World!
    ['Hello', ',', 'World', '!']
    (['Hello', ',', 'World', '!'], {'salutation': [('Hello', 0)], 'greetee': [('World', 2)]})
    Hello
    World



We could also use dict-like access, such as greetTokens['salutation'], as well as dict-like methods, such as keys(), items(), values(), etc.



Lastly, to help with debugging, ParseResults will emit a complete string of its token list and defined names by calling dump():





    print greetTokens.dump()



gives:





    - greetee: World
    - salutation: Hello
    ['Hello', ',', 'World', '!']





-- Paul

---
## 2006-09-28 12:09:43 - chuckbiscuito - parsing config files with incomplete grammar
I'm trying to make use of SkipTo to work on a configuration file with an incomplete grammar, and I'm lacking understanding of the rules.



Say I have a configuration file:



--snip--

hostname router.pop



ip access-list blah

lots_of_other_lines_go_here



router bgp 1234

 neighbor 1.2.3.4 remote-as 1234

--snip--



and the parser grammar:



--snip--

  def MakeBnf(self, debug=None):

    '''Returns an IOS configuration lexer/parser'''

    # ios lexical tokens

    _t_hostname = Literal('hostname')

    _t_router = Literal('router')

    _t_bgp = Literal('bgp')

    _t_neighbor = Literal('neighbor')

    _t_remote_as = Literal('remote-as')

    _t_peer_group = Literal('peer-group')

    # ip re not working? TODO: fix

    ##_t_ipv4_addr = Regex(r'((25[0-5]|2[0-4][0-9]|1[0-9]{2}|[1-9][0-9]|[0-9])\.){3}(25[0-5]|2[0-4][0- 9]|1[0-9]{2}|[1-9][0-9]|[0-9])')

    _t_ipv4_addr = Regex(r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}')

    _t_asn = Word(nums)

    # parser constructs

    p_ios_bgp_neigh = _t_neighbor + _t_ipv4_addr 

    p_ios_bgp = _t_router + _t_bgp + _t_asn 

    p_hostname = _t_hostname + Word(printables)

    p_config = OneOrMore(p_hostname) + (SkipTo(Optional(p_ios_bgp)) | 

        SkipTo(ZeroOrMore(p_ios_bgp_neigh)))



    if debug: return p_config.setDebug(True)

    else: return p_config.setDebug(False)



--snip--



when i parse this grammar with the following wrapping:



  def ParseConfigFile(self, file):

    f = open(file, 'r')

    for tokens, start, end in self.bnf.scanString(f.read()):

      print tokens.asList()



For the configuration file above, i get:



['hostname', 'router.pop', '']



How do I use SkipTo properly to allow me to skip over unknown pieces of text before I get to what I do want?  (the 'router bgp' bit will have one of those lines, followed by ZeroOrMore of the 'neighbor 1.2.3.4 remote-as 1234' type lines).

#### 2006-09-28 20:25:44 - ptmcg
Chuck -



Welcome to pyparsing!  Overall, it looks like you are putting together the pieces pretty well, with just a few minor glitches.



I think you are confusing SkipTo and scanString.  Let's forget for a minute that there are things we are going to skip over, and focus on the things we want to recognize.



Looking at your grammar, it looks like the significant pieces are:

hostname

ios_bgp

ios_bgp_neigh



You already have definitions for these, and they seem to work okay.



Now the next thing you want is to scan through your input file and sift through the file content, and lift out matches of any of these patterns.  scanString is perfect for this.  But instead of using SkipTo to skip over the chaff/uninteresting text, we should let scanString do the work for us.  We essentially say, 'I want to pick out any text that matches A or B or C.'  And in fact, if we create a grammar that matches this exactly, that is:



    p_config = (p_hostname | 
                    p_ios_bgp | 
                    p_ios_bgp_neigh )



and then use this as the grammar for scanning, we get:





    ['hostname', 'router.pop']
    ['router', 'bgp', '1234']
    ['neighbor', '1.2.3.4']



Now the next trick is to know for any given match, <em>which</em> of the grammar expressions was the one that matched.  For this, setResultsName can help us out.  By changing p_config to:



    p_config = (p_hostname.setResultsName('hostname') | 
                    p_ios_bgp.setResultsName('ios_bgp') | 
                    p_ios_bgp_neigh.setResultsName('ios_bgp_neigh'))



we can call getName() on the returned results and see which pattern was matched.  Change:



    print tokens.asList()

to:



    print tokens.getName(), ':', tokens.asList()



giving us:



    hostname : ['hostname', 'router.pop']
    ios_bgp : ['router', 'bgp', '1234']
    ios_bgp_neigh : ['neighbor', '1.2.3.4']



Very rarely do I ever see SkipTo combined with scanString - usually one or the other is enough 'flexibility' in the grammar to get what's needed.



Please write back if I missed your point, or if you have other questions!



-- Paul

---
## 2006-10-23 08:34:58 - korvus - setParseAction problems
Ok, so I have some code that looks like this:



def buildUseList(s,l,t):

global useList

print '>>> ',

print t.asList()

#useList = useList + [t[1]]



#...in another function...

idTok = Word(alphas,alphanums+'_')

allTok = defKW('all')

fullNameTok = Combine(idTok + ZeroOrMore(Literal('.') + idTok) + Optional(Literal('.') + allTok))

useTok = defKW('use')

useDef = Suppress(useTok) + fullNameTok + Suppress(';')

useDef.setName('useDef')

useDef.setDebug(d)

useDef.setParseAction( buildUseList )



So, this code works, and the whole file parses (there are MANY more statements than I showed above). But if I uncomment that last line in buildUseList ('#useList = useList + [t[1]]'), I get a 'pyparsing.ParseException: Expected end of text (at char 2513), (line:45, col:1)'. Why would appending something to a global list in a parse action cause it to fail like this? I am really confused.



Jeff

#### 2006-10-23 09:23:26 - korvus
Ok, I figured it out.  That line was throwing an exception since t[1] didn't exist (I changed the format while I was doing this, so I really wanted t[0]).  I'm still pretty new to Python...I'm used to Java that usually lets exceptions like that flow to the top level.
#### 2006-10-27 06:48:19 - ptmcg
What version of pyparsing are you using?  I think the latest version behaves like you expect.





    from pyparsing import *
    A_expr = Word('A')
    A_expr.setParseAction( lambda t: t[1] )
    print OneOrMore(A_expr).parseString('AAA AAA')



throws this exception



    Traceback (most recent call last):
      File 'paExceptions.py', line 7, in ?
        print OneOrMore(A_expr).parseString('AAA AAA')
      File 'C:\dev\parserFwk\pyparsing.py', line 803, in parseString
        loc, tokens = self._parse( instring.expandtabs(), 0 )
      File 'C:\dev\parserFwk\pyparsing.py', line 698, in _parseNoCache
        loc,tokens = self.parseImpl( instring, preloc, doActions )
      File 'C:\dev\parserFwk\pyparsing.py', line 2232, in parseImpl
        loc, tokens = self.expr._parse( instring, loc, doActions )
      File 'C:\dev\parserFwk\pyparsing.py', line 720, in _parseNoCache
        tokens = fn( instring, tokensStart, retTokens )
      File 'C:\dev\parserFwk\pyparsing.py', line 590, in tmp
        return f(t)
      File 'paExceptions.py', line 5, in \<lambda\>
        A_expr.setParseAction( lambda t: t[1] )
      File 'C:\dev\parserFwk\pyparsing.py', line 230, in __getitem__
        return self.__toklist[i]
    IndexError: list index out of range



-- Paul
#### 2006-10-27 09:12:45 - korvus
<u>version</u> = '1.4.4'

<u>versionTime</u> = '19 October 2006 23:11'



Looks like a pretty recent one.  I should note that useDef is NOT the top level expression, so perhaps that is somehow hiding it?  The top level expression is:



  vhdlCode = OneOrMore(libraryDef | useDef | architectureDef | entityDef | packageBodyDef | packageDef| configurationDeclarationDef) + StringEnd()





I just changed it back and when I try to parse a file, I get:



File vhd/work/system_pkg.vhd changed.  Reparsing.

Traceback (most recent call last):

  File './generateTools.py', line 46, in ?

    proj.findVHDLfiles('vhd')

  File '/cygdrive/c/fpga/tools/FPGAProject.py', line 480, in findVHDLfiles

    vh.parse()

  File '/cygdrive/c/fpga/tools/VHDLFile.py', line 545, in parse

    result = vhdlCode.parseString(self.filecontents)

  File '/cygdrive/c/fpga/tools/pyparsing.py', line 803, in parseString

    loc, tokens = self._parse( instring.expandtabs(), 0 )

  File '/cygdrive/c/fpga/tools/pyparsing.py', line 694, in _parseNoCache

    loc,tokens = self.parseImpl( instring, preloc, doActions )

  File '/cygdrive/c/fpga/tools/pyparsing.py', line 1845, in parseImpl

    loc, exprtokens = e._parse( instring, loc, doActions )

  File '/cygdrive/c/fpga/tools/pyparsing.py', line 698, in _parseNoCache

    loc,tokens = self.parseImpl( instring, preloc, doActions )

  File '/cygdrive/c/fpga/tools/pyparsing.py', line 1725, in parseImpl

    raise exc

pyparsing.ParseException: Expected end of text (at char 14), (line:2, col:1)





If I turn on debugging for useDef and vhdlCode (and whatever else is in my list of statements I turn debugging on for), I get this:

File vhd/work/system_pkg.vhd changed.  Reparsing.

Match `{libraryDef | useDef | architectureDef | entityDef | packageBodyDef | packageDef | configurationDeclarationDef`... StringEnd} at loc 0 (1,1)

Match useDef at loc 14 (2,1)

Match fullNameTok at loc 17 (2,4)

Matched fullNameTok -\> ['IEEE.std_logic_1164.all']

Match architectureDef at loc 14 (2,1)

Exception raised: Expected 'architecture' (at char 14), (line:2, col:1)

Match entityDef at loc 14 (2,1)

Exception raised: Expected 'entity' (at char 14), (line:2, col:1)

Match packageBodyDef at loc 14 (2,1)

Exception raised: Expected 'package' (at char 14), (line:2, col:1)

Match packageDef at loc 14 (2,1)

Exception raised: Expected 'package' (at char 14), (line:2, col:1)

Match configurationDeclarationDef at loc 14 (2,1)

Exception raised: Expected 'configuration' (at char 14), (line:2, col:1)

Exception raised: Expected end of text (at char 14), (line:2, col:1)

Traceback (most recent call last):

  File './generateTools.py', line 46, in ?

    proj.findVHDLfiles('vhd')

  File '/cygdrive/c/fpga/tools/FPGAProject.py', line 480, in findVHDLfiles

    vh.parse()

  File '/cygdrive/c/fpga/tools/VHDLFile.py', line 545, in parse

    result = vhdlCode.parseString(self.filecontents)

  File '/cygdrive/c/fpga/tools/pyparsing.py', line 803, in parseString

    loc, tokens = self._parse( instring.expandtabs(), 0 )

  File '/cygdrive/c/fpga/tools/pyparsing.py', line 674, in _parseNoCache

    loc,tokens = self.parseImpl( instring, preloc, doActions )

  File '/cygdrive/c/fpga/tools/pyparsing.py', line 1845, in parseImpl

    loc, exprtokens = e._parse( instring, loc, doActions )

  File '/cygdrive/c/fpga/tools/pyparsing.py', line 698, in _parseNoCache

    loc,tokens = self.parseImpl( instring, preloc, doActions )

  File '/cygdrive/c/fpga/tools/pyparsing.py', line 1725, in parseImpl

    raise exc

pyparsing.ParseException: Expected end of text (at char 14), (line:2, col:1)



I can only assume it is moving on to trying to match architectureDef because the useDef parseAction threw an exception that it somehow interpreted as being a failed match.



On a side note, I've written almost a full VHDL parser using pyparsing.  Right now I'm mostly parsing structure, so I'm ignoring a lot of expressions that can be written (just using SkipTo(';') once I know what the expression is), but if I ever fill it out more I'll send it to you if you want to use it as an example.  It takes a little over three mins to parse this project I have, though it does consist of about 80,000 lines of code over 108 files.  Overall it seems to do quite well -- if speed was the first priority, I probably would have used a predictive parser instead of a full recursive descent one.  But I'm definately pleased with how easy it is to read the grammar I have written.  VHDL has horrible syntax.
#### 2006-10-27 09:14:47 - korvus
Since the formatting there did some odd things, here is that second output again:





    File vhd/work/system_pkg.vhd changed.  Reparsing.
    Match {{{libraryDef | useDef | architectureDef | entityDef | packageBodyDef | packageDef | configurationDeclarationDef}}... StringEnd} at loc 0 (1,1)
    Match useDef at loc 14 (2,1)
    Match fullNameTok at loc 17 (2,4)
    Matched fullNameTok -\> ['IEEE.std_logic_1164.all']
    Match architectureDef at loc 14 (2,1)
    Exception raised: Expected 'architecture' (at char 14), (line:2, col:1)
    Match entityDef at loc 14 (2,1)
    Exception raised: Expected 'entity' (at char 14), (line:2, col:1)
    Match packageBodyDef at loc 14 (2,1)
    Exception raised: Expected 'package' (at char 14), (line:2, col:1)
    Match packageDef at loc 14 (2,1)
    Exception raised: Expected 'package' (at char 14), (line:2, col:1)
    Match configurationDeclarationDef at loc 14 (2,1)
    Exception raised: Expected 'configuration' (at char 14), (line:2, col:1)
    Exception raised: Expected end of text (at char 14), (line:2, col:1)
    Traceback (most recent call last):
      File './generateTools.py', line 46, in ?
        proj.findVHDLfiles('vhd')
      File '/cygdrive/c/fpga/tools/FPGAProject.py', line 480, in findVHDLfiles
        vh.parse()
      File '/cygdrive/c/fpga/tools/VHDLFile.py', line 545, in parse
        result = vhdlCode.parseString(self.filecontents)
      File '/cygdrive/c/fpga/tools/pyparsing.py', line 803, in parseString
        loc, tokens = self._parse( instring.expandtabs(), 0 )
      File '/cygdrive/c/fpga/tools/pyparsing.py', line 674, in _parseNoCache
        loc,tokens = self.parseImpl( instring, preloc, doActions )
      File '/cygdrive/c/fpga/tools/pyparsing.py', line 1845, in parseImpl
        loc, exprtokens = e._parse( instring, loc, doActions )
      File '/cygdrive/c/fpga/tools/pyparsing.py', line 698, in _parseNoCache
        loc,tokens = self.parseImpl( instring, preloc, doActions )
      File '/cygdrive/c/fpga/tools/pyparsing.py', line 1725, in parseImpl
        raise exc
    pyparsing.ParseException: Expected end of text (at char 14), (line:2, col:1)



Also, I'm using Python under cygwin if that matters...
#### 2006-10-28 03:19:15 - ptmcg
Ok, well, this is part of the dark side of pyparsing, that of tracking down exceptions.  Your exception may have been masked by a construct such as OneOrMore(alternative1 | alternative2 | ...), which will tolerate exceptions as long as there is at least one alternative found.  I've tried to add some more troubleshooting features to pyparsing, but sometimes you just need to 'be the ball'.



One issue to be aware of is that '|' is an eager matcher, not a greedy one; that is, it will match the first successful match, and not evaluate all options and choose the longest one.  In a construct such as this:



    OneOrMore(libraryDef | useDef | architectureDef | entityDef | 
              packageBodyDef | packageDef | configurationDeclarationDef)

you must be careful that an early expression might mask a later one in the list.  To work around this, replace the '|'s with '^'s - you'll take a hit in parsing speed, but this may help you identify some subtle issues in your grammar.



One of my design goals with pyparsing has been to stay portable by staying 100% Python, so I've not heard anyone have any issues with pyparsing that were unique to any platform or OS.  Also, I've really wanted to keep pyparsing down to a single file, to make it easy for others to embed in their own software.



I can appreciate your efforts on developing a VHDL - I spent about 8 weeks developing a grammar for Verilog.  I'd be very happy to include your VHDL grammar with the examples shipped with pyparsing!



Please write back if you have other questions, and welome to pyparsing!



-- Paul
#### 2009-06-24 07:04:04 - krb04r
HI folks,



Its been almost 3 yrs since this thread started.

So I would like to know if there is a pythonic VHDL parser available.



Thanks

---
## 2006-10-28 15:43:58 - akkartik - backtracking?
Anybody have a solution to this problem?





#### 2006-10-29 21:23:13 - ptmcg
Kartik -



Thanks for the vote of confidence!



I also appreciated the link to the doc on Parsec.  I've read quite a number of references to Haskell/Parsec, but this was the first readable reference for me to assimilate. :)



However, I think we are talking a little 'apples and oranges' with respect to Haskell's 'try'.  Here is the description of the merits of 'try':







    Consider the following example parser: 
    
    testOr  =   string '(a)'
            \<|\> string '(b)'
    
    
    The (string s) parser accepts input that equals the string s. Since 
    both strings in the example start with the same initial character, 
    the behaviour is probably not what the user expected: 
    
    Main\> run testOr '(b)'      
    parse error at (line 1, column 2):
    unexpected 'b'
    expecting 'a'
    
    
    Since the first parser has already consumed some input (the '('), the 
    second alternative is never tried allthough it would succeed! The 
    preferred solution to this problem is to left-factor the grammar by 
    merging common prefixes. The following parser works as expected: 
    
    testOr1 = do{ char '('
                ; char 'a' \<|\> char 'b'
                ; char ')'
                }
    
    
    Unfortunately, it is not always possible to left-factor the grammar and 
    in many other cases the grammar becomes more complex to understand. 
    There is a primitive combinator called try that handles these cases in 
    an elegant manner. The parser (try p) behaves just like p except that 
    it pretends that it hasn't consumed any input whenever p fails. In 
    combination with (\<|\>) this allows for infinite look-ahead in the 
    grammar. Here is the above parser written using try: 
    
    testOr2 =   try (string '(a)')
            \<|\> string '(b)'
    





Pyparsing already does this - the 'try' behavior is implied in both the Or and MatchFirst classes.  From the Python interactive prompt:





    >>> from pyparsing import *
    >>>
    >>> testOr = Literal('(a)') | Literal('(b)')
    >>> print testOr.parseString('(a)')
    ['(a)']
    >>> print testOr.parseString('(b)')
    ['(b)']
    >>>
    



The 'backtracking' that we discussed earlier (although it seems more like 'lookahead' than 'backtracking' to me) is more like a qualifier on one of the repetitive classes like OneOrMore, to designate a special value to stop the repetition.  Pyparsing does include FollowedBy and NotAny to do some lookahead testing during parsing, so OneOrMore(Word(alphas)) could be modified to read all words up to the word STOP by changing it to OneOrMore(~Literal('STOP') + Word(alphas)).  



Classes like Word and CharsNotIn are a little trickier, since their repetition behavior is internal to their parseImpl methods.  Adding something that would have Word(alphas+'.') stop when it sees '.jpg' would require extending the class with some kind of terminateOnExpression qualifier (maybe 'until' would be easier to type), as in:





    fileExtension = Combine('.' + oneOf('htm html gif jpg jpeg png'))
    # this doesn't work in the current version of pyparsing
    filename = Word(alphas+'.', until=fileExtension) + fileExtension



But I need to give this some thought.  Let me know what your impressions are.



-- Paul
#### 2006-10-29 21:35:15 - akkartik
Likewise. Hmm, so it seems there are different kinds of backtracking? I will be in touch, thanks. I also hope to contribute a parser example I've been working on.
#### 2006-10-31 11:39:22 - korvus
Since I've been using a LOT of pyparsing lately, let me give this a shot.  I have a preference for having pyparsing determine what strings are what and then I just use the .setParseAction() to do something appropriate with that data, so this code won't explicitly use ParseActions like Paul's above....





    from pyparsing import *
    
    fileExtension = Combine('.' + oneOf('htm html gif jpg jpeg png'))
    knownName = Word(alphanums+'_-')+ZeroOrMore(NotAny(fileExtension)+Literal('.')+Word(alphanums+'_-'))+fileExtension
    unknownName = Word(alphanums+'._-')
    path = Suppress('/') + ZeroOrMore(Word(alphanums+'._-')+Suppress('/'))
    knownUrl = (Literal('http')|Literal('ftp')) + Literal(':/') + path + knownName
    unknownUrl = (Literal('http')|Literal('ftp')) + Literal(':/') + path + unknownName
    url = knownUrl | unknownUrl
    
    url.setDebug(True)
    url.setName('url')
    knownUrl.setDebug(True)
    knownUrl.setName('knownUrl')
    unknownUrl.setDebug(True)
    unknownUrl.setName('unknownUrl')
    
    url.parseString('http://www.blah.com/some/directories/filename.png')
    url.parseString('http://www.blah.com/some/directories/filename.aaa')



When I run that, I get:



    >>> url.parseString('http://www.blah.com/some/directories/filename.png')
    Match url at loc 0 (1,1)
    Match knownUrl at loc 0 (1,1)
    Matched knownUrl -\> ['http', ':/', 'www.blah.com', 'some', 'directories', 'filename', '.png']
    Matched url -\> ['http', ':/', 'www.blah.com', 'some', 'directories', 'filename', '.png']
    (['http', ':/', 'www.blah.com', 'some', 'directories', 'filename', '.png'], {})
    >>> url.parseString('http://www.blah.com/some/directories/filename.aaa')
    Match url at loc 0 (1,1)
    Match knownUrl at loc 0 (1,1)
    Exception raised: Expected '.' (at char 49), (line:1, col:50)
    Match unknownUrl at loc 0 (1,1)
    Matched unknownUrl -\> ['http', ':/', 'www.blah.com', 'some', 'directories', 'filename.aaa']
    Matched url -\> ['http', ':/', 'www.blah.com', 'some', 'directories', 'filename.aaa']
    (['http', ':/', 'www.blah.com', 'some', 'directories', 'filename.aaa'], {})



You could either use setParseAction() to distinguish between the two results, or you could try knownUrl.parseString(...) and if that fails move to unknownUrl.parseString(...), or whatever else you want to do depending on what your code needs.  



Disclaimer: This obviously isn't robust, since '-_' are not the only non-alphanums characters in normal URLs and I haven't bothered to consider strange cases like query strings or logins, etc.
#### 2006-10-31 15:03:56 - ptmcg
Korvus -



Thanks for pitching in on this question!  Glad to hear that you have been bitten by the pyparsing bug!  (And I'm especially glad that you find the parse action technique to be a useful one - I think it is one of the more powerful aspects of pyparsing.)



Your suggestion is dead-on, let me pass along one more technique for distinguishing the type of results, that does not require the overhead of a parse action.  The string associated with an expression using setResultsName can be read back from the corresponding parsed results using the getName() method defined on ParseResults.  Try this variation on your program.



Change:



    url = knownUrl | unknownUrl

to:



    url = knownUrl.setResultsName('KNOWN') | unknownUrl.setResultsName('UNKNOWN')



Now run your tests using this code.  Note the use of getName() on the returned results.





    tests = [
            'http://www.blah.com/some/directories/filename.png',
            'http://www.blah.com/some/directories/filename.aaa',
            ]
    
    for testUrl in tests:
        res = url.parseString(testUrl)
        print testUrl,'is a URL that is',res.getName()
        print res
        print



This prints out (omitting the debug output):



    http://www.blah.com/some/directories/filename.png is a URL that is KNOWN
    ['http', ':/', 'www.blah.com', 'some', 'directories', 'filename', '.png']
    
    http://www.blah.com/some/directories/filename.aaa is a URL that is UNKNOWN
    ['http', ':/', 'www.blah.com', 'some', 'directories', 'filename.aaa']



I hope this gives you another pyparsing technique for your growing bag-of-tricks!



-- Paul
#### 2006-10-31 17:41:28 - akkartik
Yes, I like that this second solution doesn't use parseActions. However, it fails to handle the following:



    url.parseString('http://www.blah.com/some/directories/filename.png.png')



The second extension is no longer part of the url. This is a little more subtle than the lack of robustness in handling new characters that you refer to. To fix it we have to actually change the definition of knownName, from:





    knownName = Word(alphanums+'_-')+ZeroOrMore(NotAny(fileExtension)+Literal('.')+Word(alphanums+'_-')+fileExtension)



to:





    knownName = ZeroOrMore(CharsNotIn('.') + '.') + oneOf('htm html gif jpg jpeg png')



I *think* this is right.. In any case, it illustrates the subtleties when you try to left-factor grammars because you don't have lookahead.
#### 2006-11-01 08:32:38 - korvus
Paul - Thanks for the advice.  I've been using setResultsName(), but I've been using it so I can look up parts of an expression in the resulting ParseResults dictionary.  One quick question; if I did a setResultsName() on the url object, would it mask the knownUrl/unknownUrl names?  If there were higher level constructs without names, would the name still come through?  



Kartik - Thanks for the correction -- that looks like it works to me.  



I am still inclined to say that pyparsing DOES have lookahead.  For example:





    url.parseString('http://www.blah.com/some/directories/filename.png.pnrty')
    Match url at loc 0 (1,1)
    Match knownUrl at loc 0 (1,1)
    Match knownName at loc 37 (1,38)
    Exception raised: Expected Re:('html|htm|gif|jpg|jpeg|png') (at char 50), (line:1, col:51)
    Exception raised: Expected Re:('html|htm|gif|jpg|jpeg|png') (at char 50), (line:1, col:51)
    Match unknownUrl at loc 0 (1,1)
    Matched unknownUrl -\> ['http', ':/', 'www.blah.com', 'some', 'directories', 'filename.png.pnrty']
    Matched url -\> ['http', ':/', 'www.blah.com', 'some', 'directories', 'filename.png.pnrty']
    (['http', ':/', 'www.blah.com', 'some', 'directories', 'filename.png.pnrty'], {})



Note that it fails on the first pattern at char 50 then goes back to char 0 to try and match.  While there is some functionality for explicitly looking ahead (things like FollowedBy() and NotAny() ), I've never found that to be graceful to use.  It seems easier to tell it to try and match, and if that fails, try something else.  The main disadvantage to using the recursive descent to try to match strings instead of using a lookahead is that I suspect if you use lookaheads correctly, it could be faster.  Especially if you are doing longest-match ('^' operator) and you can 'pre-sort' the line somehow (such as only trying expressions that start with an alphabetic character instead of a symbol if the lookahead shows that it starts with an alphabetic character).  Or, in my VHDL parser where almost every line can start with an optional 'label :', I could perhaps determine if there is a label on a line before trying to parse it, so every expression doesn't need an Optional(idTok + Literal(':')) that it has to try before trying to match the real expression...
#### 2006-11-01 08:56:51 - ptmcg
<ul class="quotelist"><li>One quick question; if I did a setResultsName() on the url object,</li><li>would it mask the knownUrl/unknownUrl names? If there were higher</li><li>level constructs without names, would the name still come through?</li></ul>

Technically, that's 2 quick questions, but they both bear answering. :)  Actually, I'll start by answering the 2nd question first.



2. You have hit on the absolute single reason why ParseResults is such a complicated little class.  Pyparsing makes it so easy to unknowingly define a nested grammar, and I didn't want pyparsing users to have to keep score about 'how deep is this nested because I used an And of a MatchFirst and a Combine with a Group...?'  I saw this problem in rev 0.5 of pyparsing, when I wrote my first expression with a results name:





    floatNum = Word(nums).setResultsName('intPart') + '.' + \ 
               Word(nums).setResultsName('fracPart')



At that point in time, the resulting expression actually looked like:





    And(And(Word(nums).setResultsName('intPart'),'.'),Word(nums).setResultsName('fracPart'))



I later added the autostreamlining that converted this to:





    And(Word(nums).setResultsName('intPart'),'.',Word(nums).setResultsName('fracPart'))



but the naming issue remains.  Tokens parsed and built into a ParseResults for the first Word(nums) get labeled as 'intPart', but this gets nested inside the containing ParseResults for the enclosing And.



To resolve this, the ParseResults class does a lot of bookkeeping to propagate names upward *as appropriate*.  You can see the results of some of this bookkeeping when looking at the repr of a ParseResults.





    from pyparsing import *
    
    floatNum = Word(nums).setResultsName('intPart') + '.' + \ 
               Word(nums).setResultsName('fracPart')
    
    parseNumber = floatNum.parseString('3.14159')
    print repr(parseNumber)
    print parseNumber.dump()



prints:



    (['3', '.', '14159'], {'fracPart': [('14159', 2)], 'intPart': [('3', 0)]})
    ['3', '.', '14159']
    - fracPart: 14159
    - intPart: 3





1. Results names at higher levels will mask lower level names, unless you use Group's to maintain the intervening token structure.  With use of Groups and intermediate naming, you can reference nested token names as 'outer.inner1.inner2.inner3' and so on, or even have repeated names, such as 'knownUrl.knownUrl'.  But you should experiment a bit with this, especially with the behavior of Group.



-- Paul
#### 2006-11-01 10:20:22 - korvus
Ok, that makes sense.  I did a quick testcase to see how things work:



    line = (Word(nums)+OneOrMore('.'+Word(nums))).setResultsName('IP')+(Word(alphanums).setResultsName('Hostname')+ZeroOrMore('.'+Word(alphanums))).setResultsName('Hostname')
    
    r=line.parseString('1.2.3.4 blah.example.com')
    
    r
    (['1', '.', '2', '.', '3', '.', '4', 'blah', '.', 'example', '.', 'com'], {'IP': [((['1', '.', '2', '.', '3', '.', '4'], {}), -1)], 'Hostname': [('blah', 7), ((['blah', '.', 'example', '.', 'com'], {'Hostname': [('blah', 0)]}), 7)]})
    
    r.Hostname
    (['blah', '.', 'example', '.', 'com'], {'Hostname': [('blah', 0)]})
    
    r.Hostname.Hostname
    'blah' 



One more question while I'm at it. :)  If I have multiple results with the same names, how do I get to them all?  Right now I am trying to parse things like:



    signal sig1,sig2,sig3 : std_logic;

And I would love to be able to pull out all the signal names.  I suppose I might be able to do that with a Group and then iterating over the list?  But I tried this test case and I see that all the results are stored there (in the <u>repr</u>()), but I can only seem to get to the last one:





    stuff = OneOrMore(Word(nums).setResultsName('Integer') | Word(alphas).setResultsName('Alpha') )
    
    r=stuff.parseString('21342 lalala 1111 qwerty splat 777')
    
    r
    (['21342', 'lalala', '1111', 'qwerty', 'splat', '777'], {'Integer': [('21342', 0), ('1111', 2), ('777', 5)], 'Alpha': [('lalala', 1), ('qwerty', 3), ('splat', 4)]})
    
    r.Integer
    '777'
    
    r['Integer']
    '777'
    
    r.asXML()
    '\n\<Integer\>\n  \<Integer\>21342\</Integer\>\n  \<Alpha\>lalala\</Alpha\>\n  \<Integer\>1111\</Integer\>\n  \<Alpha\>qwerty\</Alpha\>\n  \<Alpha\>splat\</Alpha\>\n  \<Integer\>777\</Integer\>\n\</Integer\>'



The .asXML() method seems to return them all (though enclosed in an Integer...I suppose that's the logic trying to figure out what name should flow to the top at work), but I don't want to have to then parse the XML to make sense of it.  Is there a way for me to have it return the lists of results to me that it apparently has?  Or do I have to explicitly group the results and use the token list it returns for that group?  While grouping the results would work for parsing a list of signal names, if I could get the list of tokens associated with a name out of the ParseResults object it would handle more general cases like the Alpha/Integer one.



FYI, right now I'm trying to build an abstract syntax tree out of my ParseResults, so I'm actually trying to make sense of what it's getting me instead of just using ParseActions to pull out small pieces of information here and there.





- Jeff
#### 2006-11-01 10:22:01 - korvus
I need to be more careful what I put in code blocks...  I'm starting to make this page pretty ugly.  Sorry about that.
#### 2006-11-01 11:52:13 - akkartik
korvus: _'I am still inclined to say that pyparsing DOES have lookahead.'_



Absolutely. It's just that there are subtle places right now where lookahead is missing. This goes back to Paul's comments earlier in the thread. At a high level our goal is to be able to specify a grammar in a top-down as a series of subtrees and the places where they're permitted. We would like individual definitions to specify only what is permitted in them *in isolation*, without concerning ourselves with what comes before or after. I think this is what  refers to as doing the parser's job.



In the example of this thread, what we would like to be able to say is something simple: 'An image_url is a url (previously defined) that ends in .png'. Currently we have 2 ways to do this:



<ul><li>Paul's technique:</li></ul>

    image_url = url.setResultsName('image_url').setParseAction(
        pathEndsWithOneOf('.gif','.png','.jpg') )

Here we don't have to say anything about the previous token not containing .gif/.png/.jpg in the end, but we have part of our grammar specification in this extra setParseAction block.



<ul><li>korvus's technique, which can be reduced to:</li></ul>

    knownName = ZeroOrMore(CharsNotIn('.') + '.') + oneOf('htm html gif jpg jpeg png')

As he points out, this is more elegant in that we no longer have semantic actions affecting parse tree results. But now we have to explicitly say things like 'this doesn't have period until now, and now it does.' In other words, this grammar has been a little left-factored, increasing its complexity.



Ideally we want left-factoring to be an optional optimization to improve performance rather than a necessity for correct parsing.



I think haskell's parsec does have something to teach here. 

You still have to turn on backtracking using the '\<|\>' operator (it's not the try keyword like I said before), but now you have a way to avoid left-factoring and yet have a nice grammar that reflects the true underlying structure of the language.



---



Re making things ugly: it's not your fault korvus, just wikispaces treating discussions as second-hand pages without the wysiwyg editor or a preview button. And anybody know how I can get back the grey background when I specify format='python' in a code block?
#### 2006-11-01 16:20:54 - ptmcg
Some ideas on returning multiple values for a given results name.  The case you describe sounds most like a candidate for the listAllMatches qualifier.  See the examples below (I think the examples dir also includes more listAllMatches sample code).



-- Paul





    from pyparsing import *
    
    input = 'signal sig1,sig2,sig3 : std_logic;'
    
    signals1 = 'signal' + delimitedList(Word(alphanums)).setResultsName('sigs') + 
        ':' + Word(alphanums+'_')
    print signals1.parseString(input).sigs
    
    signalName = Combine(Word(alphas)+Word(nums)).setResultsName('sigName')
    print signalName.searchString(input)
    
    stuff = OneOrMore(Word(nums).setResultsName('Integer',listAllMatches=True) | 
        Word(alphas).setResultsName('Alpha',listAllMatches=True) )
    
    r=stuff.parseString('21342 lalala 1111 qwerty splat 777')
    print r.Integer
    print r.Alpha

prints



    ['sig1', 'sig2', 'sig3']
    [['sig1'], ['sig2'], ['sig3']]
    ['21342', '1111', '777']
    ['lalala', 'qwerty', 'splat']


#### 2006-11-02 09:49:48 - korvus
It looks like listAllMatches is exactly what I was hoping for.  Of course, I think I'm already using delimitedList in the current grammar for that specific example, so your first example will probably do the job for me in that case.  Thanks again!
#### 2006-10-29 20:22:54 - ptmcg
Well, I'm not sure this is a totally informed opinion.  A recursive-descent parser is not the same as a regex, and many of the assumptions you would make about re behavior are not applicable in a recursive-descent parser.



Here is the other post's comments/complaints about the difficulty of writing a URL parser using pyparsing:




```
<br />
from pyparsing import *<br />
protocol =  (Literal(&quot;http&quot;) | &quot;ftp&quot;) + Suppress(&quot;:<em>&quot;)<br />
domain = OneOrMore(CharsNotIn(&quot;/&quot;))<br />
path = Combine(Literal(&quot;/&quot;) + OneOrMore(CharsNotIn(&quot;&quot;)))<br />
url = protocol + domain + path<br />
<br />
print url.parseString(&quot;<!-- ws:start:WikiTextUrlRule:50:http://web.de/index --><a class="wiki_link_ext" href="http://web.de/index" rel="nofollow">http://web.de/index</a><!-- ws:end:WikiTextUrlRule:50 -->&quot;)<br />
<br />
Everything ok, until here.<br />
<br />
image_url = url + Or(&quot;.png&quot;, &quot;.gif&quot;)<br />
print image_url.parseString(&quot;<!-- ws:start:WikiTextUrlRule:51:http://web.de/index.png --><a class="wiki_link_ext" href="http://web.de/index.png" rel="nofollow">http://web.de/index.png</a><!-- ws:end:WikiTextUrlRule:51 -->&quot;)<br />
<br />
This will throw an exception, because url will parse until the end of the string and nothing is left for the Or(&quot;.png&quot;, &quot;.gif&quot;).<br />
<br />
PyParsing is probably nice for a little screenscraping, but it isn't a full parser.<br />

```






Well, um, pyparsing is only doing what it has been told to do.  Note that the definition of path is:



    path = Combine(Literal('/') + OneOrMore(CharsNotIn('')))



CharsNotIn('')??  What would one </em>expect<em> of this kind of expression.  Of course it will eat everything until the end of the string.  For that matter, wrapping it inside a OneOrMore is kind of overkill - a single CharsNotIn('') expression will suffice.



Pyparsing does </em>not// do lookaheads (or backtracking) the way re's do.  Following an expression that equates to 'match all remaining characters' with anything other than StringEnd is guaranteed to fail.  But I take exception to the comment that the absence of backtracking means that pyparsing is not a full parser.



Let's look at what the OP (Other Poster) was trying to do.  In fact, the original expression definition for url was nearly accurate - in fact, the path field should be defined as:



    path = Combine(Literal('/') + CharsNotIn('?'))



This will still include any .png or .gif extension, but it will correctly break if there is a trailing query expression in the url.  As it happens, the path field includes any file extension, so defining image_url as 'a url followed by one of several special extensions' is not correct.  Given the current (accurate) definition of url, a more precise definition of image_url would be 'a url in which the path field ends with one of several special extensions'.  This could be done with a parse action, which supports the addition of specialized logic during the parsing process.



url = (definition as above).setResultsName('url')

image_url = url.setResultsName('image_url).setParseAction( pathEndsWithOneOf('.gif','.png','.jpg') )



Here's the full program:



    from pyparsing import *
    protocol =  (Literal('http') | 'ftp') + Suppress('://')
    domain = OneOrMore(CharsNotIn('/'))
    # add a results name to make access to this url field a little easier
    path = Combine(Literal('/') + OneOrMore(CharsNotIn(''))).setResultsName('path')
    url = protocol + domain + path
    
    print url.parseString('http://web.de/index')
    
    def pathEndsWithOneOf(*fileExts):
        def parseAction(s,l,t):
            for ext in fileExts:
                if t.path.endswith(ext):
                    return t
            raise ParseException(s,l,'Path does not end with required extension')
        return parseAction
    
    url = url.setResultsName('url')
    image_url = url.setResultsName('image_url').setParseAction(
        pathEndsWithOneOf('.gif','.png','.jpg') )
    html_url = url.setResultsName('html_url').setParseAction(
        pathEndsWithOneOf('.htm','.html') )
    
    
    urlType = image_url | html_url | url
    testUrls = '''\
        http://web.de/index.png
        http://web.de/index
        http://web.de/index.htm
        http://web.de/index.gif'''.split('\n')
    for u in testUrls:
        print u, 'is a', urlType.parseString(u).getName()
        print 'path:', urlType.parseString(u).path
        print
    



which prints:





    ['http', 'web.de', '/index']
    http://web.de/index.png is a image_url
    path: /index.png
    
    http://web.de/index is a url
    path: /index
    
    http://web.de/index.htm is a html_url
    path: /index.htm
    
    http://web.de/index.gif is a image_url
    path: /index.gif



It turns out, Python includes a built-in module for parsing url's, named urlparse.  Just for kicks, I reimplemented the urlparse module's internal urlsplit method using pyparsing, and the grammar passed all the internal test urls, including relative urls, nonstandard protocols, with/without query strings, etc.



So please don't be hasty in discarding pyparsing as a viable parsing tool.  The absence of backtracking is not the end of the world.



-- Paul
#### 2006-10-29 20:34:50 - akkartik
Thanks Paul. Fear not, you now have a defender on reddit.com :) The other poster was clearly over-reacting; 'not a good thing' sounds like deep design problems, rather than speaking from a superficial understanding like he was.



You're right that all these use cases are doable. However, backtracking for predictive parsing is nice to have, right? How hard would it be to add backtracking support like  so that we don't have to left-factor grammars?



Thanks,

Kartik

---
## 2006-11-04 07:15:46 - ptmcg - re: coding style
Well, anyway, here are some comments:



1. Check out the techniques used by Seo Sangheyon in the ebnf.py example.  He uses a list of variable names to access the Python vars() list, and automate the setting of expression names, parse actions, and debug flags.



2. A variation on #1, instead of using a list of names, iterate over vars() and if issubclass of ParserElement, then call setName and setDebug.  Here's a sample of this technique:



    def enableDebugging():
        l = globals()
        for k, x in l.items():
            if isinstance(x, ParserElement):
                x.setDebug(True)
                if not hasattr(x, 'name'):
                    x.setName(k)
    
    if 0:  # change to 1 to turn on debugging
        enableDebugging()



Hope these give you some ideas.



-- Paul

#### 2006-11-06 07:34:57 - korvus
I think this will be more readable:





    componentDef = componentTok + idTok + Optional(isTok) + 
      Optional(genericDef) + Optional(portBlockDef) + 
      Suppress(endTok) + Suppress(componentTok) + 
      Optional(Suppress(idTok)) + Suppress(';')
    componentDef.setDebug(d)
    componentDef.setName('componentDef')
    componentDef.setParseAction( self.buildClassNode(Component) )



If you delete my original post, I'll note here that I was looking for a way to not write all four lines every time.  Right now my parsing code is something like 8 Emacs screens, which makes it hard to jump around in the code when I'm adding features or debugging.



Anyway, those are some great suggestions.  I'll probably still call setParseAction right after creating the definition so I can tell what I'm putting ParseActions on, but the setName() and setDebug() statements I should be able to do elsewhere.  I'll see what I can do.  Thanks!



Jeff
#### 2006-11-06 14:09:39 - ptmcg
Great!  And now that those old posts are gone, this thread is a lot more readable.  Unfortunately, the body of text inside the code tags determines the overall width/wrap dimensions for all of the message text, so those long code lines made this thread really hard to read.



Cheers!

-- Paul

---
## 2006-11-09 03:03:40 - akkartik - Chomping lines without removing leading whitespace
As a primitive for reading rcs files, I'd like to read in a number n followed by n lines of text. My current approach is:





    line = Regex(r'.*')
    patch = List(countedArray(line))
    print patch.parseFile(sys.argv[1])



Unfortunately the list of lines returned is missing all the leading spaces. Anybody have any ideas on fixing this? I've experimented with putting leaveWhitespace() at various places (including within the definition of countedArray()) without success.

#### 2006-11-09 12:00:02 - ptmcg
Does this get you closer?



-- Paul





    from pyparsing import *
    
    #~ line = Regex(r'.*')
    # modify line to start at beginning of the line, and 
    # tell Regex not to skip leading whitespace
    line = LineStart() + Regex(r'.*').leaveWhitespace()
    #~ patch = List(countedArray(line))  \<\<--- what is List?
    patch = countedArray(line)
    
    test = '''\
    4
    blah : sldjfdslkfj
        lskflksdjf
        lsjdlskdjf
        sldlskdjf
        '''
    
    for l in patch.parseString(test)[0]:
        print l
    

Prints:



    blah : sldjfdslkfj
        lskflksdjf
        lsjdlskdjf
        sldlskdjf


#### 2006-11-09 20:13:54 - akkartik
es indeed. Your solution still had a problem handling leading whitespace in the first line, before the blah in your example. To fix it I had to create a new definition:





    def countedLines():
        '''Helper to define a counted list of lines.                                 
           This helper defines a pattern of the form::                               
               integer                                                               
               line
               line
               ..
           where the leading integer tells how many lines to read.                   
           The matched tokens returns the array of lines as a list - the leading count
           token is suppressed.
        '''
        arrayExpr = Forward().leaveWhitespace()
        line = lineStart + Regex(r'.*').leaveWhitespace()                            
        def countFieldParseAction(s,l,t):                                            
            n = int(t[0])
            arrayExpr \<\< (n and Group(And([line]*n)) or Group(empty))                
            return []
        return ( Word(nums).setParseAction(countFieldParseAction) + arrayExpr )      



1. Looking back, I'd experimented with LineStart() and leaveWhiteSpace() in isolation before I posted here, but not tried them together. I'm still not clear: why do we need the LineStart()?



2. What is the convention about using lineStart vs LineStart()?



3. In my previous example, List is defined as follows:





    def List(a):
        return Group(ZeroOrMore(a))



This allows me to do the following easily:





    patchList = List(patch).setResultName('patches')
    
    for i in patchList.parseFile('foo').patches:
        print i

Comments welcome :)
#### 2006-11-09 20:31:22 - ptmcg
1. The purpose of the lineStart is to interrupt the whitespace skipping that was eating all of the leading whitespace, which is then consumed by the Regex term.



2. Um, no convention of LineStart() vs. lineStart.  LineStart is the class, lineStart is a globally-defined instance of LineStart.  Use either one.



3. Ah, now I see what List is doing.  Interestingly, List was the original name for the Group class, back in version 0.something.  This definition makes more sense.



-- Paul
#### 2006-11-09 20:37:22 - akkartik
Ah, so the leaveWhiteSpace() stops at any whitespace, and so we need the lineStart to skip past the next newline?
#### 2006-11-10 06:56:10 - ptmcg
I'm not sure I understand what you said, but here's a shot.



As you know, pyparsing skips over whitespace between expression elements, by default.  This skipping is actually done as part of the lead-in to the match.  Let's just say I have a simple grammar OneOrMore(Word(nums)), and I'm parsing this string:



    100   200



After parsing 100, the parse position is at column 4:



    100   200
       ^



Now to match the next Word.  The first thing we do is call preParse, which skips over all whitespace and ignoreable expressions (like comments).  So whitespace skipping is done as part of the preface to actual pattern matching.  After preparse is called, the parse location moves to:



    100   200
          ^

And now parsing continues with the next Word.



In your original grammar, before the introduction of the lineStart term, pyparsing was advancing from the end of the previous line, past the line start and all of the leading whitespace, to the next non-whitespace character.  That is why none of the leading whitespace was being preserved in your parsed tokens.



The two changes I made were:

- insert the lineStart term

- add leaveWhitespace to the Regex term

The lineStart term stopped the whitespace skipping at the beginning of the next line.  By adding leaveWhitespace to the Regex term, we also suppressed the whitespace skipping from there to the first non-white character.



So don't think of leaveWhitespace as <em>stopping</em> at whitespace, it just modifies the parsing expression to not skip over any before starting to do its actual expression parsing.



-- Paul
#### 2006-11-10 10:57:37 - akkartik
Ah, that was most illuminating, thanks! So leaveWhitespace() is modifying preparse behavior *before* the next token. I had only a vague cargo-cultish idea of what was going on, and now you've given it a solid foundation!



I wonder if being able to say



    foo = Word(num) + leaveWhitespace() + line

would feel more natural than



    foo = Word(num).leaveWhitespace() + line



The latter reminds me of C++ iostream manipulators, and at least at first glance it feels more clear that we're modifying streaming behavior rather than a specific token.



Just a thought..
#### 2006-11-10 12:35:06 - ptmcg
Hmmm, this is most intriguing.  Next chance I get (which wont be for a while, unfortunately, I have many hours to bill before I sleep), I'll work up a prototype of this idea.  



Or what if LeaveWhitespace were added as a pyparsing class?  This would be more framework-consistent, but what would this class/instance do?  Hmmm, hmmmm...



For that matter, there <em>is</em> the existing White class (I use it so seldom, I forget that it's there).  Could that meet your leading-whitespace needs?



-- Paul
#### 2006-11-10 14:19:07 - akkartik
I had in fact looked at White earlier, but your nudge was sufficient to get me to take a more successful second look, once I figured out the default initialization is White() not White(''). Draft solution to the problem of parsing a line with leading whitespace without calling leaveWhitespace():





    line = Optional(White('\n').suppress()) + lineStart + Combine (Optional(White()) + Regex(r'.*'))



Test cases that succeed:



    test = '''\
     4
     a
     5'''
    
    foo1 = Word(nums) + line + line
    foo2 = Word(nums) + line + Word(nums)
    print foo1.parseString(test)
    print foo2.parseString(test)



Only remaining issue that I can see - These don't work because of the leading whitespace in the first line:





    foo3 = line + line + word(nums)
    foo4 = line + line + line



The critical nonintuitiveness to me here is that these succeed:



    lineStart.parseString(' 4')
    (lineStart.leaveWhitespace() + Optional(White()) + Regex(r'.*')).parseString(' 4')

but this fails:



    (lineStart+Literal('4')).parseString(' 4')


#### 2006-11-10 14:26:43 - akkartik
addendum: this succeeds as well:





    (lineStart.leaveWhitespace() + Literal('4')).parseString(' 4')


#### 2007-01-25 09:56:13 - akkartik
Suggestion: in countedArray(expr) check if expr.skipWhitespace. That makes it more general, and we no longer need a separate countLines().





    def countedArray( expr ):
        '''...'''
        arrayExpr = Forward()
        if not expr.skipWhitespace:
            arrayExpr.leaveWhitespace()
        ..


#### 2007-01-25 20:28:26 - ptmcg
Seems like this should really be a general change to the Forward() implementation of <u>lshift</u>:





    def __lshift__( self, other ):
            if isinstance( other, basestring ):
                other = Literal(other)
            self.expr = other
            self.mayReturnEmpty = other.mayReturnEmpty
            self.strRepr = None
            # add these lines
            self.mayIndexError = self.expr.mayIndexError
            self.mayReturnEmpty = self.expr.mayReturnEmpty
            self.setWhitespaceChars( self.expr.whiteChars )
            self.skipWhitespace = self.expr.skipWhitespace
            self.saveAsList = self.expr.saveAsList
            return self



These are the same lines in the ParseElementEnhance <u>init</u> method, but Forward can't benefit from them at init time since the contained expr is not defined at that time.  So when the contained expr is defined using the '\<\<' operator, we can include them at that time.



This results in a pretty healthy performance improvement, and passes all my unit tests.



Thanks,

-- Paul

---
## 2006-11-11 15:15:40 - CosmicStars - How raise exception by 
Hello,



I'm parsing text in the form of





    name
    {
    sometext
    sometext
    moresometext
    }
    name2
    {
    moretext
    ...
    }





    # Items without Cc und Whitespaces
    noControlChars = pp.Word(u''.join((unichr(char) for char in xrange(2**16) if
                               not unicodedata.category(unichr(char)) == 'Cc' )))
    
    # Gruppenname
    groupname = pp.lineStart.suppress() + noControlChars + pp.lineEnd.suppress()
    # Begin of group : Gruppenname + {
    initgroup = groupname + pp.lineStart.suppress() + pp.Literal('{').suppress() + pp.lineEnd.suppress()
    # End of group: }
    exitgroup = pp.lineStart + pp.Literal('}') + pp.lineEnd
    # Up to end without itself }
    bodygroup = pp.SkipTo(exitgroup)
    # Complete
    group = initgroup + bodygroup
    # Need still tabs
    group.parseWithTabs()
    # Need all ws as they are
    group.leaveWhitespace()



My problem is I want to raise an exception if the from of the group structure is invalid.

If there is an <strong>initgroup</strong> is thrown in the SkipTo before the single } is found there should raise an exception.

My idea is to use an 'setParseAction' on group or bodygroup object.

Or should I better use an other idea?



Here the text which should fail:





    name
    {
    sometext
    sometext
    moresometext
    }
    
    {
    moretext
    ...
    }



If I have an solution later this should fail, too.





    name
    {
    sometext
    sometext
    moresometext
    
    name2
    {
    moretext
    ...
    }



Thx for an idea.



MfG...

Pierre Bernhardt

#### 2006-11-12 06:00:38 - ptmcg
Pierre,



I got your message, I will get back to you in a day or two - I'm presently on an urgent business trip.



-- Paul



P.S. I noticed that noControlChars includes u' ', did you mean this? If not, change xrange(2**16) to xrange(33,2**16).
#### 2006-11-12 06:33:19 - CosmicStars
Hi,



<em>P.S. I noticed that noControlChars includes u' ', did you mean this? If not, change xrange(2<strong>16) to xrange(33,2</strong>16).</em>



I think you havn't seen the if condition at the end of the generator definition. I really don't need the cc but need the white space chars (unicodedata.category(unicode)[0] == 'Z')



Then I forgotten to paste some imports





    import unicodedata
    import pyparsing as pp
    class ParserError(Exception): 
        pass
    
    
    # This means, give all unicode chars without any Control char (not unicodedata.category is 'Cc')
    noControlChars = pp.Word(u''.join((unichr(char) for char in xrange(2**16) if
                                   not unicodedata.category(unichr(char)) == 'Cc' )))
    # And so on



Here some more I forgotten what I do with the parsed results:





    groupall = ģĥ
    for groupitem,begin,end in group.scanString(rawtext):
        # Check first that item is not already inserted
        if groupitem[0] in groupall:
            raise ParserError, u'Doubled group entry found: ' + groupitem[0]
        # All ok add item in dictionary
        groupall[groupitem[0]] = groupitem[1]
        # No item was get back



The code itself looks like running without any problem with strings who have no structure errors.

But if an structure is wrong, the parsing should fail so I can check and correct them.

For these I want to extend my code.

(This comes from my unittest checks I've declared some failures)



-   <small>Nov 12, 2006</small>
#### 2006-11-12 07:56:45 - CosmicStars
Hello, 



I found an possible solution by searching for an new begin of an group in the result value. I've changed something on the code for better reading.





    # Put all groups in an dictionary
    groupdict = ģĥ
    for groupitem,begin,end in group.scanString(rawtext):
        groupkey, groupvalue = groupitem
        # Check first that item is not already inserted
        if groupkey in groupdict:
            message = u'Doubled group entry found:' + groupkey
            raise ParserError, message
        # Check that no new group begin is found in the result
        if initgroup.searchString(groupvalue):
            message = u'New groupbegin found before old ended:' + groupkey
            raise ParserError, message
        # All ok add item in dictionary
        groupdict[groupkey] = groupvalue
        # No item was get back
    



This will helps me. But is there an better and clearer solution for parsing strings/files with an wrong structure? 

I think the skipTo rule could be a problem.



-   <small>Nov 12, 2006</small>
#### 2006-11-13 13:40:21 - ptmcg
Cosmic Stars - Please bear with me, I will try to give you a thorough reply in the next day or so.



Yes SkipTo is problematic when it has to skip over lots of text, and there is a chance of malformed input.  I think you are better off with a grammar that recognizes the opening and closing braces.  Unfortunately, your definition of noControlChars does not distinguish between 'text', 'moretext', '}' or any other printable string - they are all noControlChars.



Try making your body line expression a bit more restrictive, such as:  lineStart + ~Literal('}') + noControlChars + lineEnd



This way you wont have to use SkipTo.



Again, I'm sorry for being so brief, I'll write a more detailed response this evening.



-- Paul
#### 2006-11-15 10:53:04 - CosmicStars
<em>Again, I'm sorry for being so brief, I'll write a more detailed response this evening.</em>



No problem :-) I must work at my job, too. 

Help is welcome but I cannot damand this.



MfG...

Pierre Bernhardt



You're on irc channel sometimes for live chat?
#### 2006-11-15 14:16:25 - ptmcg
Yes, sometimes on IRC, but my hotel network connection here in Japan will not support it.  I will be back in the US this weekend.  Thanks for your patience.



-- Paul
#### 2006-11-17 00:16:27 - ptmcg
Cosmic -



Okay!  A breather at last!  Let's look at your application again.



First of all, errors in incoming text are naturally signalled with pyparsing's ParseException class.  In the case of an expression like A | B | C, then expressions A, B, and C are tried in turn by the MatchFirst object created by the '|' operators.  If A succeeds, it returns its tokens and this is a match - if A fails, it raises a ParseException, signalling the MatchFirst to move on to the next expression.  If all 3 expressions fail, then the MatchFirst raises its own ParseException.  So pyparsing implicitly raises exceptions when incoming text fails to match a grammar.  You don't have to do anything special to get this behavior.



Next, understand that pyparsing does not assume that the entire input string must successfully parse.  If I define a grammar as 



    OneOrMore('blah')

and parse the string 'blahblahblex', I wont get an error at 'blex' because I successfully parsed one or more 'blah' strings, which is all the OneOrMore class cares about.  (Regular expressions act the same way.)  To enforce successful parsing of the entire string, I must define my grammar so as to say, 'at the end of parsing, I should be at the end of the input string'.  To do this, we would change our test grammar to add an instance of the pyparsing StringEnd class:



    OneOrMore('blah') + StringEnd()

If we parse 'blahblahblex' with this grammar, we now get an error at 'blex' stating that the end of string was expected.



Now let's look at your grammar.  You are parsing something like:



    name
        {
        sometext
        sometext
        moresometext
        }
        name2
        {
        moretext
        ...
        }



That is, you want one or groups, and each group consists of a name, a '{', zero or more lines of text, and a '}'.  The input text should only contain such groups, so we should add a StringEnd() at the end so that pyparsing will signal an error if there is a mismatch after the first successfully parsed group.



It would nice if 'name' could be just a contiguous set of characters, like Word(alphanums+'_'), but I wont restrict the problem without your feedback, so we'll assume that all the lines of text in your example could be just about anything printable.  



Also, I see you've already picked up on the need to follow restOfLine by explicit matching/consumption of the LineEnd.  I'll define the expression nl to account for these newlines.



So heres our first naive approach at a grammar:



    nl = Suppress(LineEnd())
        group = restOfLine + nl + 
                '{' + nl + 
                ZeroOrMore(restOfLine) + nl + 
                '}'
        groupList = OneOrMore(group) + StringEnd()



And our resulting parser never finishes, because the ZeroOrMore(restOfLine) never knows when to stop.  ZeroOrMore does not do lookahead to the next term like regex's do, so it just keeps reading restOfLines until it reaches the end of the string.  Then, not finding a '}', it raises an error.



So to clarify what can be inside the delimiting braces, we should define an expression bodyLine, and then define group using bodyLine.  We want to accept any line of text in the body between braces EXCEPT for a closing brace (let's not deal with nested braces just now - we might save that for the next e-mail if you really need it):



    bodyLine = ~Literal('}') + restOfLine + nl
        group = restOfLine + nl + 
                '{' + nl + 
                ZeroOrMore(bodyLine) + 
                '}'
        groupList = OneOrMore(group) + StringEnd()



Now let's try our parser, on both good and bad input text.  In your previous post, I'll use your same test cases, and call them good1, bad1, and bad2.



groupList.parseString(good1) returns:



    ['name', '{', 'sometext', 'sometext', 'moresometext', '}', 'name2', '{', 'moretext', '...', '}']



groupList.parseString(bad1) returns:



    ['name', '{', 'sometext', 'sometext', 'moresometext', '}', 
        '', '{', 'moretext', '...', '}']



groupList.parseString(bad2) returns:



    ['name', '{', 'sometext', 'sometext', 'moresometext', '', 'name2', '{', 'moretext', '...', '}']



bad1 is succeeding because it is accepting a blank string as a valid groupname.  We can write our own parse action to test for this, and raise a ParseException if the groupname is blank.





    bodyLine = ~Literal('}') + restOfLine + nl
        groupname = restOfLine + nl
        group = groupname + 
                '{' + nl + 
                ZeroOrMore(bodyLine) + 
                '}'
        def mustNotBeBlank(s,loc,tokens):
            if len(tokens[0])==0:
                raise ParseException(s,loc,'found empty string')
        groupname.setParseAction(mustNotBeBlank)



Now for bad1, we get this error message:



    Expected end of text (at char 41), (line:8, col:1)



That is the first term of our grammar, OneOrMore(group) found a valid group, then didn't find a valid group after that so assumed that our OneOrMore was satisfied, then proceeded to look for the StringEnd, and there was still more text.  So the error message was not something like 'found empty string' - we got 'expected end of text'.  Maybe this wasn't the exception we expected, but at least we <em>are</em> getting exceptions!



Let's set aside handling of bad2 for now.  I'm not keen on how good1 is returning just a list of strings.  All we are doing at this point is just tokenizing.  The next step is to add some structure by using Group.  Since Group will structure the parsed results, we can drop the '{}'s from the actual parsed output using Suppress.  Now we're up to:



    bodyLine = ~Literal('}') + restOfLine + nl
        groupname = restOfLine + nl
        group = groupname + 
                Suppress('{') + nl + 
                Group(ZeroOrMore(bodyLine)) + 
                Suppress('}')
        def mustNotBeBlank(s,loc,tokens):
            if len(tokens[0])==0:
                raise ParseException(s,loc,'found empty string')
        groupname.setParseAction(mustNotBeBlank)



And for good1 we get:



    ['name', ['sometext', 'sometext', 'moresometext'], 'name2', ['moretext', '...']]



We could add some more structure with another level of Group around the name and the group, but instead, I want to suggest using pyparsing's Dict class.  Dict will create a dictionary of the parsed results, using the first term of a group as the key, and the second term as the value.  pyparsing includes a helper called dictOf(keyExpr,valueExpr) that makes using Dict a little easier.  Here's a Dict-ified grammar (note use of dictOf in groupList instead of just OneOrMore):



    bodyLine = ~Literal('}') + restOfLine + nl
        groupname = restOfLine + nl
        groupbody =  Suppress('{') + nl + 
                Group(ZeroOrMore(bodyLine)) + 
                Suppress('}') + nl
        def mustNotBeBlank(s,loc,tokens):
            if len(tokens[0])==0:
                raise ParseException(s,loc,'found empty string')
        groupname.setParseAction(mustNotBeBlank)
    
        groupList = dictOf(groupname, groupbody) + StringEnd()



Now parsing good1 gives us:



    [['name', ['sometext', 'sometext', 'moresometext']], ['name2', ['moretext', '...']]]



But we also get from the parsed results tokens that can be accessed as if they were a Python dict, with keys and key lookup:



    res = groupList.parseString(t)
        print res
        print 'Keys:', res.keys()
        for k in res.keys():
            print '-', k, ':', res[k]
    



prints:



    
        [['name', ['sometext', 'sometext', 'moresometext']], ['name2', ['moretext', '...']]]
        Keys: ['name2', 'name']
        - name2 : ['moretext', '...']
        - name : ['sometext', 'sometext', 'moresometext']
    





Well, this has gone on rather long, and I think I've given you enough to chew on.  There are more examples using Dict in the pyparsing examples directory.  Also, check out the configParse.py example - this is very similar to your format.



HTH,

-- Paul
#### 2006-11-19 09:11:03 - CosmicStars
Hi,



thank you for the deatailed explanation.

You gave me some more ideas for my parsing

tool and I will check for usage.



I come back to you at a differend thread

because there is another problem what I

didn't understand, yet. But I must first

write an good example before I will post.



MfG...

Pierre

---
## 2006-11-14 18:53:24 - korvus - max recursion depth exceeded
For some reason, when I add this to my parser, it gets this error: RuntimeError: maximum recursion depth exceeded in cmp



[code]

documentationDef = Literal('--@') + restOfLine

#documentationDef = Literal('--@')



vhdlCode = OneOrMore(libraryDef | useDef 

  | architectureDef | entityDef | packageBodyDef 

  | packageDef| configurationDeclarationDef 

  | documentationDef) + StringEnd()

[code]



If I uncomment that second line, it works (at least, it doesn't go into an infinite recursive loop when I run it on a file with no '--@' in it).  The only thing worth noting is that I'm trying to catch a 'special' type of comment, with my comment def being:



[code]

comment = Literal('--') + ~Literal('@') + restOfLine 

...

vhdlCode.ignore(comment)

[code]



Any ideas?  I realize it is a bit of a hack to try and get pyparsing to ignore all comments except for ones followed by an '@' symbol...



Jeff

#### 2006-11-15 08:07:09 - korvus
Apparently I forgot how to format code here. :)



Anyway, I wanted to mention that the following code appears to fix it.



Replace:



    documentationDef = Literal('--@') + restOfLine

...with:



    documentationDef = Literal('--@') + SkipTo('\n')



I'd think they should do the same thing, but maybe not.  Is this a bug of some sort, or am I missing something?



Jeff
#### 2006-11-15 09:12:58 - ptmcg
Hmm, off the top of my head, restOfLine and SkipTo('\n') *should* be equivalent, so this does seem strange.



As for your left recursion (which is what usually causes pyparsing to blow up with a max recursion limit exceeded error), this is somewhat mysterious too.  Does this help at all?





    comment = Combine(Literal('--') + ~Literal('@')) + restOfLine



The Combine will suppress any whitespace and comment skipping between the '--' and the nonexistent '@', which may be the source of your infinite recursion.



-- Paul



-- Paul
#### 2006-11-15 11:16:04 - korvus
I actually found a much more elegant solution to my problem.  Eventually, I wanted to be able to support 'special' comments anywhere (I'm trying to add javadoc-like support for commenting individual parts of code and then extracting those later to generate documentation).  This turned out to be a great solution:





    def commentAdd(self,s,l,t):
      if t[0]=='@':
        if (len(t)\>1):
          self.docs.extend(t.asList()[1])
        else:
          print 'Empty doc comment!'
      else:
        self.commentList.append( (l,t.asList()) )
    
    # ....
    
    comment = Suppress(Literal('--'))+ Optional(Literal('@')) + SkipTo(Word('\r\n'))
    comment.setName('commentDef')
    comment.setParseAction(self.commentAdd)



That way I don't have to make my 'comment' construct ignore my 'special' comments, but instead have it pick them up and decide if the comment is of the 'special' type in the parseAction.  I also now have a list of normal comments and what lines they are on, which may be useful as well.  And I don't have to put 'documentationDef' in every statement where I might expect someone to use one of those documenting comments.



That said, I'm curious if the Combine() might solve the problem I had before, so I may change my code back just to see if that works...  If I do, I'll let you know.



Jeff

---
## 2006-11-14 23:45:21 - hyry - how to do some replacement with HTML
I want to do some replacement in html file.

The detail is:

1. find all \<img\> tags that the 'src' starts with 'blogdb/'

2. add a \<a\> arround those \<img\>s as the following example.

example:

\<img src='blogdb/test.jpg'\> 

-\>

\<a href='view.py?file=test.jpg'\>\<img src='blogdb/test.jpg'\>\</a\>



\<img src='/others/test.jpg'\> should not be changed.



My test program is :



from pyparsing import *



def addLink(s, l, t):

    tocStr = s[l:s.find('\>',l)+1]   #1

    if t.src.startswith('blogdb/'):

        s = '\<a href='' + t.src[7:] + ''\>' + tocStr + '\</a\>'

        return s

    else:

        return tocStr    #2



imgOpenTag, imgCloseTag = makeHTMLTags('img')

imgOpenTag.setParseAction( addLink )



d = imgOpenTag.transformString(file('blog.htm').read())

file('blognew.htm','w').write(d)



my questions are:

1. how to get the original string of t without use #1

2. If I do not return tocStr at #2, transformString will get an Error, why?

#### 2006-11-14 23:49:06 - hyry
sorry, the code again:



    from pyparsing import *
    
    def addLink(s, l, t):
        tocStr = s[l:s.find('\>',l)+1]   #1
        if t.src.startswith('blogdb/'):
            s = '\<a href='' + t.src[7:] + ''\>' + tocStr + '\</a\>'
            return s
        else:
            return tocStr    #2
    
    imgOpenTag, imgCloseTag = makeHTMLTags('img')
    imgOpenTag.setParseAction( addLink )
    
    d = imgOpenTag.transformString(file('blog.htm').read())
    file('blognew.htm','w').write(d)


#### 2006-11-15 09:01:34 - ptmcg
Here's a modified version of your program:



    from pyparsing import *
    
    test = '''
    \<img src='blogdb/test.jpg'\> 
    
    \<img src='/others/test.jpg'\> should not be changed.
    '''
    
    def addLink2(s, l, t):
        print 'In addLink2'
        # reparse input string into component parts
        # (can't reuse imgOpenTag, will recursively call this parse action)
        imgParts = makeHTMLTags('img')[0].parseString(t[0])
        #~ print imgParts.dump()
        if imgParts.src.startswith('blogdb/'):
            s = '\<a href='view.py?' + imgParts.src.split('/')[1] + ''\>' + t[0] + '\</a\>'
            return s
        else:
            return None    # same as return t[0]
            # or do this:
            #raise ParseException(s,l,'didn't start with blogdb/')
    
    imgOpenTag, imgCloseTag = makeHTMLTags('img')
    imgOpenTag.setParseAction( keepOriginalText, addLink2 )
    
    d = imgOpenTag.transformString(test)
    print d



Both of your questions stem from the over-zealousness of the makeHTMLTags helper in breaking up the matched tag into component parts.  In the process, the original HTML text gets broken up into a ParseResults list of tag attributes, with dict-like access to the attribute values, quotation marks stripped from attribute values, etc.  Which is nice if you're trying to scrape some data from the HTML page, but not so great if you are trying to transform the HTML(like you are doing); the original tag string gets scrambled, sliced and diced, which is not good for transformString - transformString likes nice, neat strings, not lists, so it fails to rebuild the transformed text because instead of a string, there is this crummy list structure thing.  (It's useful to use t.dump() to see the contents of the ParseResults object passed to your parse action.)



I'll use a couple of new pyparsing features to propose one way to address your problem.

1. Parse expressions can now support multiple parse actions, whose intent is to chain results of one to the input of the next.

2. A new parse action in v 1.4.4 named 'keepOriginalText', whose job it is to put Humpty Dumpty back together again, that is, dump that list and get back the original text string that comprises the matched HTML tag.



You'll see that I extend the call to setParseAction to insert another method, the recent keepOriginalText.  Now the input to addLink (which I've copied to addLink2 so you can try them both yourself) is back to the unstructured string '\<img src=whatever'\>'.  Unfortunately we still want some of that structure, and I don't want to hack in some error-prone string slicing thing.  So I reparse this string in the parse action, using a non-parse-action-enhanced parser expression returned as the 0'th element from makeHTMLTags.  Now I have both the original string AND the parsed data fields.



Lastly, to return from your parse action in the do-nothing case, that is, when the image does not start with 'blogdb/'.  You have three choices here:

1. Return t[0]. (Now that it is restored to its original string-like form)

2. Return None (pyparsing's parseImpl interprets None as 'make no changes')

3. Raise a ParseException.



I'm actually partial to the third choice.  Raising a ParseException in a parse action allows us to include some additional semantic qualification beyond just the basic syntax matching of the pyparsing grammar.



Here's another way to filter on non-interesting \<img\> tags, also using ParseExceptions.  In this case, we do the filtering 'up front', or more precisely, upstream in the chain of parse actions.





    def srcMustStartWithBlogdb(s,l,t):
        if not t.src.startswith('blogdb/'):
            raise ParseException(s,l,'didn't start with blogdb/')
    
    def addLink3(s, l, t):
        print 'In addLink3'
        # reparse input string into component parts
        # (can't reuse imgOpenTag, will recursively call this parse action)
        imgParts = makeHTMLTags('img')[0].parseString(t[0])
        # now no need to test for src attribute starting with 'blogdb/' 
        # - we wouldn't have made it this far if it didn't
        s = '\<a href='view.py?' + imgParts.src.split('/')[1] + ''\>' + t[0] + '\</a\>'
        return s
    
    imgOpenTag.setParseAction( srcMustStartWithBlogdb, keepOriginalText, addLink3 )
    
    d = imgOpenTag.transformString(test)
    print d



Note that now, addLink3 has dropped the test for starting with blogdb.



Hope this gives you some new ideas to chew on.  Welcome to Pyparsing!



-- Paul

---
## 2006-12-06 14:36:59 - aJanuary - Repeating within a range of repetitions
I am looking for an example on how to repeat between a range of repetitions, i.e. 'repeat this between 2 and 4 times'. However, I can't find it anywhere. It would fit nicely in this short tutorial (if it's not already in there and I'm just blind)

#### 2006-12-16 04:17:23 - ptmcg
aJanuary -



You didn't miss it because there really isn't such a thing.  Here are some tips on how to implement some repetition constructs:



Must have exactly 'n' copies of expression expr



    And([expr]*n)



Have up to 'n' copies of expression expr



    And([Optional(expr)]*n)



Have 'm' to 'n' copies of expression expr (n\>m)



    And([expr]*m)+And([Optional(expr)]*(n-m))



Have at least 'n' copies of expression expr



    And([expr]*n)+ZeroOrMore(expr)



You could wrap these in your own helper function.



Let us know how these work for you.



-- Paul

(Sorry not to respond sooner, I don't often check the discussions on pages other than the home page.)

---
## 2006-12-06 16:00:31 - aJanuary - Avoiding greedy behaviour.
I have an input that is defined like this:

start:: (the start defined here)

body:: '[' + number + ':' + number + ']' + ZeroOrMore(Word(printables))

end:: (the end defined here)

whole:: start + ZeroOrMore(body) + end

However, due to the greedy behaviour, the body always ends up swallowing the end. This means the end will never match, giving an error of 'expected end at (insert end position of input)'

How would I stop the swallowing?

#### 2006-12-16 04:27:55 - ptmcg
Your body expression is very liberal in what it will match.  pyparsing does not implicitly do lookahead like regexp's do, so you need to tell the ZeroOrMore(Word(printables)) when to stop.  You don't really mean 'take zero or more words of any printable characters'; you mean 'take all the following word composed of printable characters, until you hit the end, or the start of the next body.'  So we expand the content expression of the ZeroOrMore repetition to stop when it reaches either of these:



    body:: '[' + number + ':' + number + ']' + 
        ZeroOrMore(~Literal('[') +~end + Word(printables))



An alternate form is to define body using SkipTo.  This has the added benefit of not breaking up the body into a list of words.  Something like this:



    body_intro:: '[' + number + ':' + number + ']'
    body_body:: SkipTo( body_intro | end | stringEnd )
    body:: body_intro + body_body



Do these give you some ideas on how to proceed from here?



-- Paul
#### 2010-03-23 11:07:02 - shankarlingayya
I just started using the pyparsing module for parsing the C++ header file, currently i am facing one problem with Optional() as below



sample Code :

Optional(functionSpecifierName) + Optional(typeDef) + identifier



here i specified 'functionSpecifierName' &  'typeDef' are optional,



when i give the input to the above code as only the identifier data then it is not parsing the data

even though i specified 'functionSpecifierName' & 'typeDef' as optional

---
## 2006-12-14 11:29:54 - metaperl - multi-line key-value pair parsing help needed
Hi, I am trying to parse some data where the key is syntactically distinct (I have described it with a regular expression) but I don't know of a good way to describe the value, rather than to say: 'take everthing up to the next key or record separator.... here's my code so far:




```
{<br />
<br />
from pyparsing import *<br />
<br />
record_sep = &quot;<strong>* BRS DOCUMENT BOUNDARY </strong>*&quot;<br />
<br />
key   =  Regex (r'\.\.[A-Z]+:')<br />
value = NotAny (key)<br />
<br />
key_value_pair = Group (key + value)<br />
<br />
record = record_sep + OneOrMore (key_value_pair)<br />
recordset = OneOrMore(record)<br />
<br />
s = &quot;&quot;&quot;<strong>* BRS DOCUMENT BOUNDARY </strong>*<br />
..PGP:<br />
     PN_WO2006089317 PN_A1 PN_WO PN_firstpub<br />
..DA1:<br />
     20060831<br />
..DS:<br />
     AE AG AL AM AT AU AZ BA BB BG BR BW BY BZ CA CH CN CO CR CU CZ DE DK DM DZ<br />
     EC EE EG ES FI GB GD GE GH GM HR HU ID IL IN IS JP KE KG KM KN KP KR KZ LC<br />
&quot;&quot;&quot;<br />
<br />
<br />
print recordset.parseString(s)<br />
<br />
<br />
}
```


#### 2006-12-14 14:23:15 - metaperl
Ok, SkipTo works a little better, but even though include defaults to false, it gobbled up the next key:





    
    from pyparsing import *
    
    record_sep = '*** BRS DOCUMENT BOUNDARY ***'
    
    key   =  Regex (r'[.][.][A-Z]+:')
    value = SkipTo (key)
    
    key_value_pair = Group (key + value)
    
    record = record_sep + OneOrMore (key_value_pair)
    recordset = OneOrMore(record)
    
    s = '''*** BRS DOCUMENT BOUNDARY ***
    ..PGP:
         PN_WO2006089317 PN_A1 PN_WO PN_firstpub
    ..DA1:
         20060831
    ..DS:
         AE AG AL AM AT AU AZ BA BB BG BR BW BY BZ CA CH CN CO CR CU CZ DE DK DM DZ
         EC EE EG ES FI GB GD GE GH GM HR HU ID IL IN IS JP KE KG KM KN KP KR KZ LC
    '''
    
    
    print recordset.parseString(s)
    


#### 2006-12-14 18:56:40 - ptmcg
Hey metaperl, good to hear from you.



1. '..DA1:' does not match your regexp.  Try: 



    key   =  Regex (r'[.][.][A-Z0-9]+:')



2. You lose the last key-value because there is no key to 'skip to'.  Change value to:



    value = SkipTo (key | StringEnd() )



Cheers,

-- Paul
#### 2006-12-14 18:59:11 - ptmcg
Or rather, since you can have the separater for the next record terminate a list of key-values, value should probably be:



    value = SkipTo (record_sep | key | StringEnd() )



-- Paul
#### 2006-12-14 19:09:41 - ptmcg
Also, let me put in a plug for using dictOf, since you are parsing key-value pairs.  Try replacing record with:



    record = record_sep + dictOf(key,value)



This will return the same sets of tokens, in the same hierarchy as before, but will now give you dict-like access to them, by key, as in:



    res = recordset.parseString(s)
    print res.dump()
    print res['..PGP:']
    print
    for k in res.keys():
        print 'res['%s'] -\> %s' % (k,res[k])
    



with these results:



    ['*** BRS DOCUMENT BOUNDARY ***', ['..PGP:', 'PN_WO2006089317 PN_A1 PN_WO PN_firstpub'], 
    ['..DA1:', '20060831'], ['..DS:', 'AE AG AL AM AT AU AZ BA BB BG BR BW BY BZ CA CH CN CO CR CU CZ DE DK DM DZ\n
    EC EE EG ES FI GB GD GE GH GM HR HU ID IL IN IS JP KE KG KM KN KP KR KZ LC']]
    - ..DA1:: 20060831
    - ..DS:: AE AG AL AM AT AU AZ BA BB BG BR BW BY BZ CA CH CN CO CR CU CZ DE DK DM DZ
         EC EE EG ES FI GB GD GE GH GM HR HU ID IL IN IS JP KE KG KM KN KP KR KZ LC
    - ..PGP:: PN_WO2006089317 PN_A1 PN_WO PN_firstpub
    PN_WO2006089317 PN_A1 PN_WO PN_firstpub
    
    res['..DA1:'] -\> 20060831
    res['..DS:'] -\> AE AG AL AM AT AU AZ BA BB BG BR BW BY BZ CA CH CN CO CR CU CZ DE DK DM DZ
         EC EE EG ES FI GB GD GE GH GM HR HU ID IL IN IS JP KE KG KM KN KP KR KZ LC
    res['..PGP:'] -\> PN_WO2006089317 PN_A1 PN_WO PN_firstpub





-- Paul
#### 2006-12-14 21:13:48 - metaperl
dictOf rocks! and the 



`value = SkipTo (record_sep | key | StringEnd() ) `



idea came to me in the shower after work... funny how being tied to a desk impairs your thinking.



Thanks a lot for your help on this.
#### 2006-12-14 21:58:13 - ptmcg
Glad this worked out for you!  If you add a parse action to your keys to strip off the leading '.'s and the trailing ':', your keys will be a bit more readable.  Also, since this will make them valid Python identifiers, you'll be able to use attribute-style access instead of dict-style (which I find to be a little nicer-looking): 'res.PGP' vs. 'res['..PGP:']'.  Note, though, that the two have different error semantics: dict-style will raise KeyError just like a regular dict if you use a non-existent key; attribute-style NEVER raises and exception, not even AttributeError.  If you retrieve res.XYZZY (that is, using a key not defined in your input), you will get ''.



Viel Glück!

-- Paul

(I'm just finishing a business trip to Germany and am sitting in Dresden airport on my way back home, so helping out with your problem was a nice bonus for the end of this trip!)
#### 2006-12-15 06:04:23 - metaperl
Hmm, I naively attempted the dictOf suggestion and it leads to an infinite loop. I have a pound sign in front of my attempt to use it:





    from pyparsing import *
    
    record_sep = '*** BRS DOCUMENT BOUNDARY ***'
    
    key   =  Regex (r'[.][.][A-Z0-9]+:')
    value = SkipTo ( record_sep | key | StringEnd() )
    
    key_value_pair = Group (key + value)
    #key_value_pair = dictOf (key,value)
    
    record = record_sep + OneOrMore (key_value_pair)
    recordset = OneOrMore(record)
    
    s = '''*** BRS DOCUMENT BOUNDARY ***
    ..PGP:
         PN_WO2006089317 PN_A1 PN_WO PN_firstpub
    ..DA1:
         20060831
    ..DS:
         AE AG AL AM AT AU AZ BA BB BG BR BW BY BZ CA CH CN CO CR CU CZ DE DK DM DZ
         EC EE EG ES FI GB GD GE GH GM HR HU ID IL IN IS JP KE KG KM KN KP KR KZ LC
    '''
    
    
    print recordset.parseString(s)
    
    


#### 2006-12-15 06:44:37 - metaperl
the setParseAction suggestion is good, but I don't know how to do it. Is pyparsing documented mainly by articles and example programs?



I dont think I missed the docs. I even went into pyparsing.py and looked at the docstring and after reading it, I have no idea how to mutate the dict (assuming I ever get dictOf to work) to trim of the dots and colon.
#### 2006-12-15 13:57:10 - ptmcg
To modify the results of the key expression, just attach the parse action to the key expression, like so:



    key   =  Regex (r'[.][.][A-Z0-9]+:')
    key.setParseAction(lambda tokens : tokens[0][2:-1])



Or for a more explicit version:



    def stripLeadingDotsAndTrailingColon( tokens ):
        # Parse action to convert keys of the form '..XYZ:' to simply 'XYZ'
        # We *know* that all tokens passed to us will begin with two leading
        # periods and end with a colon (otherwise we would not have reached this
        # part of the code), so just return the string from index 2 to index -1
        return tokens[0][2:-1]
    key   =  Regex (r'[.][.][A-Z0-9]+:')
    key.setParseAction( stripLeadingDotsAndTrailingColon )





As for dictOf (and the Dict class it generates), I'm sorry, but this is one of the more complicated/confusing/obscure constructs in pyparsing.  Still it can be quite powerful once you bend it to your will! :)  



Before the inception of dictOf, the raw Dict class required a grammar idiom like:



    record = record_sep + Dict(ZeroOrMore(Group(key+value)))

and this was just too many concepts at once!  But here was the evolution of thought:



1. Begin with the basic, most simple grammar, much as you had (I've expanded the key_value_pair expression of yours to keep this progression more clear).



    record = record_sep + OneOrMore( key + value )



2. You already went past this step, to group the key-value pairs for simpler traversal of the resulting ParseResults structure:



    record = record_sep + OneOrMore( Group(key + value) )



3. At this point, early in the development of pyparsing when I first saw this style of construct, I thought, 'wouldn't it be nice, in addition to returning the tokens as parsed, to also support a dictionary of the parsed keys as if the grammar was able to use setResultsName internally on the returned key-value pairs?'  So the Dict class was born, as a wrapper for expressions like 



    OneOrMore( Group(key + value) )



thus,



    Dict( OneOrMore( Group(key + value) ) )



This gives the set of key-value pairs a more extensive support for all those nice dict-like methods, such as keys(), values(), support for the 'in' keyword, etc.



4. Well, Dict either languished unused, or the few intrepid folks who tried it sent me several e-mails asking about it.  So it seemed that the dictOf helper method would simplify things somewhat.  dictOf(key,value) emits the same expression above, but is a bit more natural to follow within your grammar.  I also added a few examples to the examples directory.





Pyparsing documentation and examples directory is included in the source distribution, but not in the Win32 binary installer.  Since you've already found the examples, it sounds like you have already downloaded the source dist, so let me direct you to some other resources in the distribution:

- PDF files of my presentations at PyCon'06 in the docs directory

- UML class diagrams in the root directory (in PNG and JPG formats)

- 'HowToUsePyparsing.html' in the root directory (albeit a bit out of date)

- The htmldoc directory, containing epydoc-generated class/method/variable documentation.

I'm also guessing that you have found the ONLamp article that I wrote last January.  The message archives at the pyparsing wiki and the SourceForge Support Requests forum also contain fairly extensive explanations for specific questions.



But that is about it for now.  In my software career so far, my impression is that developers get more from coding examples than they do from narrative documentation, and it has been much easier for me to provide the sample code, either from my own dabblings or from those who have submitted them to be included.



But I can see that searching through all these disparate sources is probably not very efficient.  I am starting to compile more formal 'how to' notes, to organize a more comprehensive document for pyparsing. But since all pyparsing activities are done in my 'spare time', I can't promise how soon it will be available.



Thanks for your persistence in working with pyparsing!

-- Paul
#### 2006-12-15 18:22:05 - ptmcg
I looked again at your 'infinitely looping' version, and I now see what you are referring to.



    key_value_pair = Group (key + value)
    #key_value_pair = dictOf (key,value)  \<-- what you tried in using dictOf
    
    record = record_sep + OneOrMore (key_value_pair)
    recordset = OneOrMore(record)



The key_value_pair does not itself compose the Dict, just as a single definition is not a dictionary.  It is the collection of key-value pairs which is the Dict.  In the code I sent you, I did not use key_value_pair at all, hoping that dictOf would have been clearer.  But instead of making any modification to key_value_pair, look at the definition of record:



    record = record_sep + dictOf(key,value)



That is, within a record, after the record_sep header there is a Dict of key-value pairs.  We could have used your key_value_pair as you originally wrote it, with the original Dict class (since dictOf uses the individual key and value expressions, which are merged in your definition of key_value_pair):



    key_value_pair = Group (key + value)
    record = record_sep + Dict( OneOrMore (key_value_pair) )



As I said before, Dict does not change the parsing at all, it just adds key definitions to the returned ParseResults.  The Dict class defines a postParse method, which iterates over the nested groups in the parsed tokens, assumes that the first element of each group is a 'key' and the remaining elements form a 'value', and then inserts that key-value definition into the parse results.



Your 'infinite loop' version was trying to create a Dict out of a single key-value pair.  I'm not exactly sure why this would loop forever, but suffice to say that when Dict is correctly wrapped around a collection of groups of tokens, it builds the proper return value.



-- Paul

---
## 2006-12-15 06:39:05 - metaperl - verbose parsing
I've got a huge file to parse and for a long time I get no output from pyparsing. Is there some way to have it echo all the things it is doing so I know it is running?

#### 2006-12-16 18:59:44 - ptmcg
Try adding setDebug(), or use a custom debugging action to output progress, or add a parse action that simply prints parsing progress.  Since parse actions get called with the current parse location, you could use the col and lineno methods to expand this location to the actual line and column number of the source text.



-- Paul

---
## 2006-12-25 18:35:56 - akkartik - Whitespace at the very start of a parse
Hi Paul,



Did you ever get a chance to look at ? Does that way of including leading whitespace without using leaveWhitespace() seem on the right track?



As I pointed out there, for some reason that combinator dies if there's whitespace in the very first line of parsing. Just to summarize the simplest test cases I don't understand (start each fragment in a new interpreter to clear python's buffers):



<ul><li>Works:</li></ul>

    (lineStart.leaveWhitespace() + Literal('4')).parseString(' 4')



<ul><li>Doesn't work:</li></ul>

    (lineStart + Literal('4')).parseString(' 4')



<ul><li>Works (We've simply introduced a new combinator but we're not actually parsing anything!):</li></ul>

    lineStart.leaveWhitespace()
    (lineStart+Literal('4')).parseString(' 4')



Could someone confirm that these tests behave the same in your environment?



Not urgent, but it seems to be worth getting to the bottom of.

#### 2006-12-25 20:55:43 - ptmcg
Yes, these behave the same for me, with pyparinsg 1.4.5.  Try inserting the statement 'lineStart.setDebug()' to get a little more insight into what is happening.



Then, consider that (lineStart + Literal('4')) is the same as And([lineStart, Literal('4')]).  In earlier versions, even if I invoked leaveWhitespace on lineStart, the whitespace would still be skipped because the containing And was *not* set to skip whitespace.  I fixed this so that And now looks at its 0'th expr element to determine its whitespace-skipping behavior.  Still there seems to be some loophole in this logic, which would be nice to close up.



-- Paul
#### 2006-12-26 01:13:29 - akkartik
A little searching found  by your own self, indicating that the whole purpose of LineStart() is to match items immediately following a newline. That seems reasonable.



I finally figured out how to properly define line, very similar to :





    line = LineStart().leaveWhitespace() + Regex(r'.*').leaveWhitespace()



It seems a little non-intuitive setting leaveWhitespace() twice, but it's not too confusing if one thinks of LineStart() as a zero-length regex pattern.



We could of course create a new variant of LineStart that resets skipWhitespace by default, but a couple of weeks later the whole quest to avoid leaveWhitespace() seems a little quixotic. All it's doing is setting a member to affect behavior; I'm not sure why I thought it was somehow more magical.



The phrasing that seemed confusing to me :





    foo = Word(num).leaveWhitespace() + line



..is confusing only because we're declaring parsers before calling _parse() on them. Methods are executed at declaration time even though the parser itself waits until parse time to execute. Seems almost obvious when I put it like that.



In order to say:





    foo = leaveWhitespace() + Word(num) + line



..we'd have to set skipWhitespace in some sort of global namespace that And can then check, change semantics of next expr, then reset. It hardly seems worth it. It's more elegant to mould my mind around pyparsing rather than the other way around. Not the first time this has happened either.



Well, it was fun to think about even if my primary feeling right now is of mild embarrassment :) Thanks, Paul.
#### 2006-12-27 03:11:16 - remyblank
I just stumbeled on this logic in And() and found it very useful. However, I was wondering if it wouldn't make sense also to set skipWhitespace to False in Or() and MatchFirst() if *any* of its elements have skipWhitespace = False. Indeed, if all of the elements skip leading whitespace, we might as well skip it in the Or() or MatchFirst(). However, if at least one element doesn't, the container shouldn't skip either.



-- Remy
#### 2006-12-27 08:12:17 - ptmcg
Things get a little tricky if different elements define different sets of whitespace chars; most notably LineStarts and LineEnds do not include '\n' in their definitions of whitespace.  Also, the list of items in any ParseExpression subclass (And, Or, MatchFirst, or Each) may be extended after the initial call to <u>init</u>, so the only time to check all the elements and *know* you've gotten them all is at parse time - at which point you might as well just make the skipWhitespace calls.  And is a special case, since, no matter how many elements get appended, the first in the list is always the first in the list.



I'll look into this a little further, I actually think I may have to undo some optimization of whitespace skipping for Or's, MatchFirst's, and Each's, since all alternatives might not have the same whitespace settings.



-- Paul
#### 2006-12-27 08:36:09 - remyblank
Yes, having Or and MatchFirst never skip whitespace, and leave that to their elements would probably solve the problem, at the expense of performance.



