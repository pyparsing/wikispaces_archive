## Pyparsing Wikispaces Discussion - 2007

_[Note: these entries are fairly old, and predate many new features of pyparsing,
and are predominantly coded using Python 2.
They are captured here for historical benefit, but may not contain
the most current practices or features. We will try to add editor
notes to entries to indicate when discussions have been 
overtaken by development events.]_

[2007-01-02 08:04:39 - remyblank - Two bugs with packrat parsing](all_wiki_discussion_toc_2007.md#2007-01-02-080439---remyblank---two-bugs-with-packrat-parsing)  
[2007-01-08 17:58:03 - IanSparks - re: Optional usage. Is this correct output?](all_wiki_discussion_toc_2007.md#2007-01-08-175803---iansparks---re-optional-usage-is-this-correct-output)  
[2007-01-25 07:35:15 - akkartik - v1.4.5 update to And](all_wiki_discussion_toc_2007.md#2007-01-25-073515---akkartik---v145-update-to-and)  
[2007-01-29 15:57:48 - bca48150 - Memory optimization... ](all_wiki_discussion_toc_2007.md#2007-01-29-155748---bca48150---memory-optimization-)  
[2007-01-30 12:07:40 - metaperl - incremental / generator / memory-efficient parsing?](all_wiki_discussion_toc_2007.md#2007-01-30-120740---metaperl---incremental--generator--memory-efficient-parsing)  
[2007-02-11 02:40:14 - Digitalxero - Is Pyparsing right now me?](all_wiki_discussion_toc_2007.md#2007-02-11-024014---digitalxero---is-pyparsing-right-now-me)  
[2007-02-11 14:09:25 - nickzam - bug report: another unicode bug](all_wiki_discussion_toc_2007.md#2007-02-11-140925---nickzam---bug-report-another-unicode-bug)  
[2007-02-12 13:41:52 - metaperl - What does a tilde in front of a token descriptor mean?](all_wiki_discussion_toc_2007.md#2007-02-12-134152---metaperl---what-does-a-tilde-in-front-of-a-token-descriptor-mean)  
[2007-02-12 14:11:49 - metaperl - multi-line CSV with pre-formatted multi-line field](all_wiki_discussion_toc_2007.md#2007-02-12-141149---metaperl---multi-line-csv-with-pre-formatted-multi-line-field)  
[2007-03-08 11:05:59 - utoxin - Full Blown Language Parser Possible?](all_wiki_discussion_toc_2007.md#2007-03-08-110559---utoxin---full-blown-language-parser-possible)  
[2007-03-26 14:14:59 - bkit - tokens.asXML() prints always a nested structure?](all_wiki_discussion_toc_2007.md#2007-03-26-141459---bkit---tokensasxml-prints-always-a-nested-structure)  
[2007-03-31 01:02:17 - kib2 - Does PyParsing suits my needs ?](all_wiki_discussion_toc_2007.md#2007-03-31-010217---kib2---does-pyparsing-suits-my-needs-)  
[2007-03-31 02:32:05 - shinewu - I am a little confused with setParseAction](all_wiki_discussion_toc_2007.md#2007-03-31-023205---shinewu---i-am-a-little-confused-with-setparseaction)  
[2007-04-01 11:00:08 - frediz - Matching names_with_space within known sentences](all_wiki_discussion_toc_2007.md#2007-04-01-110008---frediz---matching-names_with_space-within-known-sentences)  
[2007-04-08 19:57:13 - felixrabe - pyparsing on launchpad](all_wiki_discussion_toc_2007.md#2007-04-08-195713---felixrabe---pyparsing-on-launchpad)  
[2007-04-09 13:11:45 - gunars - preserving original text](all_wiki_discussion_toc_2007.md#2007-04-09-131145---gunars---preserving-original-text)  
[2007-04-09 22:07:41 - gunars - removeQuotes](all_wiki_discussion_toc_2007.md#2007-04-09-220741---gunars---removequotes)  
[2007-04-10 20:26:10 - gunars - SkipTo and include](all_wiki_discussion_toc_2007.md#2007-04-10-202610---gunars---skipto-and-include)  
[2007-04-10 23:10:22 - michelp - searchparser in pyparsing package](all_wiki_discussion_toc_2007.md#2007-04-10-231022---michelp---searchparser-in-pyparsing-package)  
[2007-04-12 22:03:22 - shinewu - A little feature request of delimitedList](all_wiki_discussion_toc_2007.md#2007-04-12-220322---shinewu---a-little-feature-request-of-delimitedlist)  
[2007-04-13 07:30:58 - Claudiu - compact match, separate tokens](all_wiki_discussion_toc_2007.md#2007-04-13-073058---claudiu---compact-match-separate-tokens)  
[2007-04-13 09:25:06 - joncle - prefix and suffixes](all_wiki_discussion_toc_2007.md#2007-04-13-092506---joncle---prefix-and-suffixes)  
[2007-04-15 04:34:12 - Claudiu - getting original matched string](all_wiki_discussion_toc_2007.md#2007-04-15-043412---claudiu---getting-original-matched-string)  
[2007-04-28 09:04:38 - propell - Error in link to ONLamp](all_wiki_discussion_toc_2007.md#2007-04-28-090438---propell---error-in-link-to-onlamp)  
[2007-05-01 11:24:05 - shinewu - A bug in error msg](all_wiki_discussion_toc_2007.md#2007-05-01-112405---shinewu---a-bug-in-error-msg)  
[2007-05-01 11:26:05 - shinewu - Need a preview button](all_wiki_discussion_toc_2007.md#2007-05-01-112605---shinewu---need-a-preview-button)  
[2007-05-15 10:44:09 - droblek - Resursion problem](all_wiki_discussion_toc_2007.md#2007-05-15-104409---droblek---resursion-problem)  
[2007-05-23 13:51:01 - ptmcg - Enhancement proposal - abbreviated setResultsName](all_wiki_discussion_toc_2007.md#2007-05-23-135101---ptmcg---enhancement-proposal---abbreviated-setresultsname)  
[2007-05-28 16:05:59 - Kambiz - operatorPrecedence](all_wiki_discussion_toc_2007.md#2007-05-28-160559---kambiz---operatorprecedence)  
[2007-05-29 05:46:22 - rbosman - Detailed error message](all_wiki_discussion_toc_2007.md#2007-05-29-054622---rbosman---detailed-error-message)  
[2007-05-29 05:53:42 - Kambiz - function call and subscripting with operatorPrecedence](all_wiki_discussion_toc_2007.md#2007-05-29-055342---kambiz---function-call-and-subscripting-with-operatorprecedence)  
[2007-05-30 04:55:03 - bplant - Problem with loc in recursive expressions](all_wiki_discussion_toc_2007.md#2007-05-30-045503---bplant---problem-with-loc-in-recursive-expressions)  
[2007-05-30 13:47:21 - Kambiz - Packrat Parsing](all_wiki_discussion_toc_2007.md#2007-05-30-134721---kambiz---packrat-parsing)  
[2007-06-07 12:14:34 - droblek - Is pyparsing thread safe?](all_wiki_discussion_toc_2007.md#2007-06-07-121434---droblek---is-pyparsing-thread-safe)  
[2007-06-29 13:55:11 - vineshp - Typo leaveWhiteSpace() -> leaveWhitespace()](all_wiki_discussion_toc_2007.md#2007-06-29-135511---vineshp---typo-leavewhitespace---leavewhitespace)  
[2007-07-13 11:03:32 - muralimani - Use Pyparsing in Jython Code](all_wiki_discussion_toc_2007.md#2007-07-13-110332---muralimani---use-pyparsing-in-jython-code)  
[2007-08-01 09:14:14 - BlGene - operatorPrecedence and numTerms problem](all_wiki_discussion_toc_2007.md#2007-08-01-091414---blgene---operatorprecedence-and-numterms-problem)  
[2007-08-02 01:40:43 - aaron-L - Nested tags for asXML()?](all_wiki_discussion_toc_2007.md#2007-08-02-014043---aaron-l---nested-tags-for-asxml)  
[2007-08-02 14:25:52 - aaron-L - PyParsing for real-time syntax highlighting?](all_wiki_discussion_toc_2007.md#2007-08-02-142552---aaron-l---pyparsing-for-real-time-syntax-highlighting)  
[2007-08-17 08:19:01 - tetris - trying to parse very random strings](all_wiki_discussion_toc_2007.md#2007-08-17-081901---tetris---trying-to-parse-very-random-strings)  
[2007-08-29 12:11:54 - jjrael - Sync contents of two files - transformString  examples?](all_wiki_discussion_toc_2007.md#2007-08-29-121154---jjrael---sync-contents-of-two-files---transformstring--examples)  
[2007-08-29 16:13:15 - jjrael - How to match man of two types out of order](all_wiki_discussion_toc_2007.md#2007-08-29-161315---jjrael---how-to-match-man-of-two-types-out-of-order)  
[2007-08-29 18:10:25 - jjrael - scanString - dataStart and dataEnd don't make sense](all_wiki_discussion_toc_2007.md#2007-08-29-181025---jjrael---scanstring---datastart-and-dataend-dont-make-sense)  
[2007-09-04 09:51:56 - luca_politti - newbie problems with pyparsing](all_wiki_discussion_toc_2007.md#2007-09-04-095156---luca_politti---newbie-problems-with-pyparsing)  
[2007-09-07 15:01:34 - jjrael - How to append with setResultsName](all_wiki_discussion_toc_2007.md#2007-09-07-150134---jjrael---how-to-append-with-setresultsname)  
[2007-09-11 04:22:22 - rustin - Newbie problem with Optional()/LineEnd()](all_wiki_discussion_toc_2007.md#2007-09-11-042222---rustin---newbie-problem-with-optionallineend)  
[2007-09-21 04:13:37 - hamidh - Unicode problem perhaps?](all_wiki_discussion_toc_2007.md#2007-09-21-041337---hamidh---unicode-problem-perhaps)  
[2007-09-24 18:53:07 - gefan - combine() question](all_wiki_discussion_toc_2007.md#2007-09-24-185307---gefan---combine-question)  
[2007-10-10 20:16:23 - gasolin - To match python docstring?](all_wiki_discussion_toc_2007.md#2007-10-10-201623---gasolin---to-match-python-docstring)  
[2007-10-11 14:26:57 - kar1107 - Effective uses for pyparsing](all_wiki_discussion_toc_2007.md#2007-10-11-142657---kar1107---effective-uses-for-pyparsing)  
[2007-10-18 08:17:09 - metaperl - Listing on Python parsing tools](all_wiki_discussion_toc_2007.md#2007-10-18-081709---metaperl---listing-on-python-parsing-tools)  
[2007-10-18 16:01:24 - krjackson - simple newbie question](all_wiki_discussion_toc_2007.md#2007-10-18-160124---krjackson---simple-newbie-question)  
[2007-11-02 06:34:09 - louis_nichols - help needed with using Dict](all_wiki_discussion_toc_2007.md#2007-11-02-063409---louis_nichols---help-needed-with-using-dict)  
[2007-11-05 07:03:03 - michael_ramirez44 - Using SkipTo](all_wiki_discussion_toc_2007.md#2007-11-05-070303---michael_ramirez44---using-skipto)  
[2007-11-05 09:08:06 - HJarausch - howto maintain state - a beginner's question](all_wiki_discussion_toc_2007.md#2007-11-05-090806---hjarausch---howto-maintain-state---a-beginners-question)  
[2007-11-06 16:33:25 - jpj1138 - Help with grouping (newbie)](all_wiki_discussion_toc_2007.md#2007-11-06-163325---jpj1138---help-with-grouping-newbie)  
[2007-11-07 00:06:15 - badri - metadata conversion](all_wiki_discussion_toc_2007.md#2007-11-07-000615---badri---metadata-conversion)  
[2007-11-08 02:45:58 - louis_nichols - how to make dicts that can be called like. key.subkey1.subkey2](all_wiki_discussion_toc_2007.md#2007-11-08-024558---louis_nichols---how-to-make-dicts-that-can-be-called-like-keysubkey1subkey2)  
[2007-11-08 10:43:20 - hamidh - re: tab delimited files - sorry long day](all_wiki_discussion_toc_2007.md#2007-11-08-104320---hamidh---re-tab-delimited-files---sorry-long-day)  
[2007-11-13 08:22:15 - robert_cimrman - dealing with optional tokens](all_wiki_discussion_toc_2007.md#2007-11-13-082215---robert_cimrman---dealing-with-optional-tokens)  
[2007-11-13 08:30:30 - robert_cimrman - pyparsing docstrings](all_wiki_discussion_toc_2007.md#2007-11-13-083030---robert_cimrman---pyparsing-docstrings)  
[2007-11-20 04:09:21 - robert_cimrman - description of function arguments](all_wiki_discussion_toc_2007.md#2007-11-20-040921---robert_cimrman---description-of-function-arguments)  
[2007-12-05 15:09:52 - Viserys - Newbie question (is this possible?)](all_wiki_discussion_toc_2007.md#2007-12-05-150952---viserys---newbie-question-is-this-possible)  
[2007-12-06 09:28:09 - virtualjim - Newbie bitten by setResultsName behavior](all_wiki_discussion_toc_2007.md#2007-12-06-092809---virtualjim---newbie-bitten-by-setresultsname-behavior)  
[2007-12-10 12:50:04 - paulsmith - Ambiguous matches](all_wiki_discussion_toc_2007.md#2007-12-10-125004---paulsmith---ambiguous-matches)  
[2007-12-10 18:26:10 - jpj1138 - Is Word() matching too much?](all_wiki_discussion_toc_2007.md#2007-12-10-182610---jpj1138---is-word-matching-too-much)  
[2007-12-13 20:52:39 - tmcw - Accented Characters?](all_wiki_discussion_toc_2007.md#2007-12-13-205239---tmcw---accented-characters)  
[2007-12-18 08:30:04 - wcbarksdale - Spaces at start of line](all_wiki_discussion_toc_2007.md#2007-12-18-083004---wcbarksdale---spaces-at-start-of-line)  
[2007-12-28 13:01:00 - michael_ramirez44 - IronPython 1.1](all_wiki_discussion_toc_2007.md#2007-12-28-130100---michael_ramirez44---ironpython-11)  
[2007-12-29 18:17:12 - nickzam - re: LineEnd strange behavior](all_wiki_discussion_toc_2007.md#2007-12-29-181712---nickzam---re-lineend-strange-behavior)  


---
## 2007-01-02 08:04:39 - remyblank - Two bugs with packrat parsing
I am writing a parser for a numeric expression grammar with quite a few operators. I am using the operatorPrecedence() helper function of pyparsing 1.4.5, but the generated grammar becomes extremely slow: parsing the expression '1 + (2 + 3)' takes 45 seconds!



So I wanted to enable packrat parsing, but I hit two bugs that I could narrow down to the following two test cases.



<h2 id="toc0"> Bug 1 </h2>


The following program:



    from pyparsing import *
    
    exprOperand = Word(nums + '_')
    op = Literal('-')
    
    expr = FollowedBy(op + exprOperand) + Group(op + exprOperand)           # This fails
    #expr = FollowedBy(op + exprOperand) + Group(op.copy() + exprOperand)    # This works
    
    print expr.parseString('-1')[0]
    ParserElement.enablePackrat()
    print expr.parseString('-1')[0]

prints:



    ['-', '1']
    ['-', '1', '1']

That is, the parse result is different if packrat is enabled, even though there are no side-effects in the grammar.



Strangely, removing the comment on the second expr= line fixes the problem. That is, if a copy of op is used in the expression, the problem goes away.



I wasn't able to find the cause of the problem in the library, but a few debug prints showed that the cache is populated correctly, i.e. the entry for the '-' token is initially set to ['-'], but when it is retrieved later, it is retrieved as ['-', '1']. So it seems that the elements in the cache are modified erroneously during parsing.



<h2 id="toc1"> Bug 2 </h2>


The following program:



    from pyparsing import *
    
    def makeInt(s, loc, toks):
        print 'makeInt(%r)' % toks[0]
        return int(toks[0], 0)
    
    exprOperand = Word(nums + '_').setParseAction(makeInt)
    op = Literal('-')
    
    expr = Forward()
    expr << ((FollowedBy(op.copy() + expr) + Group(op + expr)) | exprOperand)
    
    print expr.parseString('-1')[0]
    ParserElement.enablePackrat()
    print expr.parseString('-1')[0]

prints:



    makeInt('1')
    ['-', 1]
    ['-', '1']

So, when packrat is enabled, the makeInt() parse action is not called. This is because FollowedBy() gives a first try at parsing the expression with doActions=False, and succeeds, so it caches the result without running the parse actions. The cached result is retrieved later, so the parse actions are never executed.



I found a fix for this problem that seems to work for me, but I don't know if it will break other grammars. Basically, the idea is that parse actions should always be executed when trying to add to the cache. So the following diff fixes the problem for me:



    --- pyparsing.py.orig   2007-01-02 17:00:23.000000000 +0100
    +++ pyparsing.py        2007-01-02 17:00:44.000000000 +0100
    @@ -768,7 +768,7 @@
             else:
                 try:
                     ParserElement._exprArgCache[ lookup ] = \
    -                    value = self._parseNoCache( instring, loc, doActions, callPreParse )
    +                    value = self._parseNoCache( instring, loc, True, callPreParse )
                     return value
                 except ParseBaseException, pe:
                     ParserElement._exprArgCache[ lookup ] = pe



Please let me know if I can help fixing these problems. The first bug in particular is blocking for me, as I have to use packrat parsing for performance reasons.

#### 2007-01-02 08:07:39 - remyblank
Woops, one line of the patch seems to have slipped due to a trailing backslash. Here's the patch again:



    --- pyparsing.py.orig   2007-01-02 17:00:23.000000000 +0100
    +++ pyparsing.py        2007-01-02 17:00:44.000000000 +0100
    @@ -768,7 +768,7 @@
             else:
                 try:
                     ParserElement._exprArgCache[ lookup ] = \\
    -                    value = self._parseNoCache( instring, loc, doActions, callPreParse )
    +                    value = self._parseNoCache( instring, loc, True, callPreParse )
                     return value
                 except ParseBaseException, pe:
                     ParserElement._exprArgCache[ lookup ] = pe


#### 2007-01-02 08:08:35 - remyblank
Ok, still no luck. I hope the change is obvious enough...
#### 2007-01-02 10:05:30 - ptmcg
Ok, I've looked at this a little bit.  Instead of always calling parse actions, how about adding doActions to the cache key tuple?  That is, change:



    lookup = (self,instring,loc,callPreParse)

to:



    lookup = (self,instring,loc,callPreParse,doActions)



This change also seems to correct your problem, and passes all my unit tests. (It also passes some tests that used to fail with packratting, so this does clean up some bad behavior.)



I also don't understand the purpose of the FollowedBy.  I recognize it from the operatorPrecedence code, but there is a subtle difference - in operatorPrecedence, the FollowedBy is used to try to avoid left recursion with a leading Optional.  In your code, the FollowedBy is not really helpful, try removing it.  That is, change:



    expr << (FollowedBy(a + b) + Group(a + b))

to:



    expr << Group(a + b)



-- Paul
#### 2007-01-02 14:54:06 - ptmcg
Also, please post or e-mail me your operatorPrecedence call.  45 seconds to parse 1+(2+3) sounds like a bit much!



-- Paul
#### 2007-01-03 00:36:04 - remyblank
Hi Paul,



Thanks for your reply. I'll try your proposed change to lookup tonight. At first sight, this will double the space used by the cache, won't it? Wouldn't it also make sense then to cache the result in the case where 'doActions and self.parseAction'? 



The FollowedBy in my example is necessary to reproduce the problem. In my code, the problem happens when I use operatorPrecedence, and I removed as much code as possible while still showing the bug, to simplify the test case. This included removing the Optional in Group. But I have to say that I still don't understand why matchExpr needs the FollowedBy in operatorPrecedence. I'll look at it some more.



I'll mail you my expression grammar, as it has lots of trailing backslashes and the wiki doesn't seem to like them.



Any idea about the first (cache corruption) problem?



-- Remy
#### 2007-01-03 07:59:59 - ptmcg
On my system, making the change to lookup fixes both bugs.



-- Paul
#### 2007-01-03 11:47:32 - remyblank
I can confirm that adding doActions to lookup fixes both bugs, and makes my test suite pass. Removing the special case at the beginning of the function still makes everything pass.



Whereas my proposed fix (to set doActions=True when caching a value) only fixes bug 2. But I'm still a bit worried that bug 1 isn't actually fixed with the change, but only hidden in this particular case, and that cache corruption can still occur. I'll try to investigate this a little more, and post here if I find anything.



Thank you so much for this great library and your excellent support.



-- Remy

---
## 2007-01-08 17:58:03 - IanSparks - re: Optional usage. Is this correct output?
Ouch. Wish I'd read the how to format text link. Lets try again...





    >>> from pyparsing import Keyword,Optional
    >>> world = Keyword('world')
    >>> hello = Keyword('Hello') + Optional(world,default=world)
    >>> hello.parseString('Hello')
    
    (['Hello', world], {})
    
    >>> hello.parseString('Hello world')
    
    (['Hello', 'world'], {})



would have expected that the last line would be :



    (['Hello', world], {})



Since I'm asking Optional() to provide a Keyword() not a string.



Or am I missing something?

#### 2007-01-09 04:42:48 - ptmcg
(I took the liberty of deleting your first post)



Try changing the second line to:



    world = Keyword('world').setName('Keyword containing the word 'world'')

and see if this makes more sense to you.



In the first call, there is no text for the <em>world</em> keyword, so Optional returns the default value, which you specified should be the <em>world</em> variable (very unusual - usually the argument to default is a string, or a user-defined object indicating the absence of the optional element).



More likely, what you want is this:



    hello = Keyword('Hello') + Optional(world,default='world')



Pyparsing allows you to return any type of object as the parsed tokens, you are not limited to returning strings.  So it did not complain when you asked to return the pyparsing Keyword object <em>world</em>.



-- Paul
#### 2007-01-09 05:17:07 - IanSparks
Thanks Paul, I noticed that I could make the default a string as you noted but it seemed a kind of repetition. Nevertheless, this works for me.

---
## 2007-01-25 07:35:15 - akkartik - v1.4.5 update to And
After upgrading to 1.4.5 a while ago (I wanted the support for pickling), I recently ran into an issue that can be traced to the change in the default callPreParse at And::parseImpl. These lines in particular:





    # pass False as last arg to _parse for first element, since we already
    # pre-parsed the string as part of our And pre-parsing
    loc, resultlist = self.exprs[0]._parse( instring, loc, doActions,
    callPreParse=True )



Unfortunately I am unable to distill it down to a simple test case right now. My question is the following: is the above change intended solely as a performance optimization, or am I undoing a bugfix by reverting it?



Thanks.

#### 2007-01-25 20:17:36 - ptmcg
This is essentially a performance optimization.



If you need to undo this change, please send me some sample code.  I'm about to put 1.4.6 out soon, and I'd like to comprehend your problem case.



-- Paul
#### 2007-01-26 00:08:51 - akkartik
Sample code interacts with . Sent by email.

---
## 2007-01-29 15:57:48 - bca48150 - Memory optimization... 
When I created a Verilog parser using pyparsing I ran into memory usage problems.  Though I have discovered that the task was a little too large to accomplish with pyparsing (~100MB Verilog files would consume about 3GB of memory - which is a lot!).  In the process of trying to see if I could squeeze a little better memory performance out of the parse I noticed a little optimization that would help parsing a lot.  In the ParseResults class the <u>parent attribute is a strait reference to the parent which creates a cyclic loop.  If the </u>parent attr is changed to a weakref, the loop is broken.  This has two advantages.  The first is that the memory is reclaimed right away when the reference is dropped to the base element.  The second is that the number of times that the cyclic garbage collector will have to run will be greatly reduced during the parsing stage.  These two elements allowed me to get a noticeable improvement on the performance and the memory usage was slightly reduced.



Patch:


```
<br >
diff -cbB old/pyparsing.py new/pyparsing.py<br >
<ul><ul><ul><li>old/pyparsing.py    Mon Jan 29 17:55:57 2007</li></ul></ul></ul>--- new/pyparsing.py    Mon Jan 29 17:53:33 2007<br >
***<br >
<ul><ul><ul><li>66,71 </li></ul></ul></ul>--- 66,72 ----<br >
  import warnings<br >
  import re<br >
  import sre_constants<br >
+ import weakref<br >
  import xml.sax.saxutils<br >
  #~ sys.stderr.write( &quot;testing pyparsing module, version %s, %s\n&quot; % (<u>version</u>,<u>versionTime</u> ) )<br >
<br >
***<br >
<ul><ul><ul><li>245,251 </li></ul></ul></ul>              self.<u>tokdict[k] = self.</u>tokdict.get(k,list()) + [(v,0)]<br >
              sub = v<br >
          if isinstance(sub,ParseResults):<br >
!             sub.<u>parent = self<br >
<br >
      def </u>delitem<u>( self, i ):<br >
          if isinstance(i,(int,slice)):<br >
--- 246,252 ----<br >
              self.</u>tokdict[k] = self.<u>tokdict.get(k,list()) + [(v,0)]<br >
              sub = v<br >
          if isinstance(sub,ParseResults):<br >
!             sub.</u>parent = weakref.ref(self)<br >
<br >
      def <u>delitem</u>( self, i ):<br >
          if isinstance(i,(int,slice)):<br >
***<br >
<ul><ul><ul><li>296,302 </li></ul></ul></ul>              for k,v in otherdictitems:<br >
                  self[k] = v<br >
                  if isinstance(v[0],ParseResults):<br >
!                     v[0].<u>parent = self<br >
          self.</u>toklist += other.<u>toklist<br >
          self.</u>accumNames.update( other.<u>accumNames )<br >
          del other<br >
--- 297,303 ----<br >
              for k,v in otherdictitems:<br >
                  self[k] = v<br >
                  if isinstance(v[0],ParseResults):<br >
!                     v[0].</u>parent = weakref.ref(self)<br >
          self.<u>toklist += other.</u>toklist<br >
          self.<u>accumNames.update( other.</u>accumNames )<br >
          del other<br >
***<br >
<ul><ul><ul><li>414,420 </li></ul></ul></ul>          if self.<u>name:<br >
              return self.</u>name<br >
          elif self.<u>parent:<br >
!             par = self.</u>parent<br >
              if par:<br >
                  return par.<u>lookup(self)<br >
              else:<br >
--- 415,421 ----<br >
          if self.</u>name:<br >
              return self.<u>name<br >
          elif self.</u>parent:<br >
!             par = self.<u>parent()<br >
              if par:<br >
                  return par.</u>lookup(self)<br >
              else:<br >
***<br >
<ul><ul><ul><li>454,460 </li></ul></ul></ul>      def <u>getstate</u>(self):<br >
          return ( self.<u>toklist,<br >
                   ( self.</u>tokdict.copy(),<br >
!                    self.<u>parent,<br >
                     self.</u>accumNames,<br >
                     self.<u>name ) )<br >
<br >
--- 455,461 ----<br >
      def </u>getstate<u>(self):<br >
          return ( self.</u>toklist,<br >
                   ( self.<u>tokdict.copy(),<br >
!                    self.</u>parent(),<br >
                     self.<u>accumNames,<br >
                     self.</u>name ) )<br >
<br >
***<br >
<ul><ul><ul><li>466,471 </li></ul></ul></ul>--- 467,473 ----<br >
          self.<u>name = state[1]<br >
          self.</u>accumNames = {}<br >
          self.<u>accumNames.update(inAccumNames)<br >
+         self.</u>parent = weakref.ref(self.__parent)<br >
<br >
<br >
  def col (loc,strg):<br >

```


#### 2007-01-29 15:59:55 - bca48150
Ok, so the patch did not work...but I hope the idea is clear.
#### 2007-01-29 20:05:34 - ptmcg
I'm game, but I'm having trouble even getting started with this patch.  When I try to make this change, I get this exception:



TypeError: cannot create weak reference to 'ParseResults' object



Suggestions?



-- Paul
#### 2007-01-29 21:19:43 - bca48150
__weakref__ needs to be added to the __slots__ for the class.  I ported the changes quickly from 1.4.3 and I missed that little detail.
#### 2007-01-29 22:12:27 - ptmcg
Ok, I have a version that passes all unit tests (had to tweak the getstate/setstate methods for pickle support), but I'm not seeing big performance gains.  In fact, my verilog parse test runs about 5% slower.  Next thoughts?



-- Paul
#### 2007-01-30 11:25:51 - bca48150
Not too bad actually.  What used to happen is that if a ParseResults was created and then the reference was dropped (that branch of the expression did not match) then it would stay in memory and not be freed until the garbage collector decided to run (because of the cyclic loop keeping all the objects alive).  Now, because there are no cyclic loops the ParseResults instances are being garbage collected right away (when the reference is dropped to them) which means that the deallocation routines for the instance is happening while the test is running.  If you add the following to the end of the test:

``import gc

gc.collect()``

Then the amount of garbage collection that is being avoided (or saved) by the cyclic references will be handled.  It might happen that for small runs the change may cause a decrease in preformance but for larger runs (with many, many objects that would have to be checked by the cyclic collector) it will run faster.

From my experience (with the large files) it did help with memory consumption and a little on speed.

If you really want to squeeze speed from it change the line:

import weakref

to:

from weakref import ref

and update the rest of the code to follow.  This will save a attribute look up each time a reference is created which can add up.



I'll think of other things but these are the things that come to mind right now.



Brian
#### 2007-01-30 14:39:26 - ptmcg
Any thoughts on how this might coexist/conflict with packratting?  My tests that showed the 5% loss (no measureable diff when using 'from weakref import ref') when testing with packrat parsing enabled.  When I disabled packrat parsing, there is <1% difference with weakrefs or without.



Have you tried using enablePackrat in your app?  Although it is possible this would only add further to your memory utilization woes.



-- Paul
#### 2007-01-30 14:53:41 - bca48150
Never did anything with packrat and so that is why I may not have seen such a difference.  I'll have to read up as to what that is...



Brian
#### 2007-01-30 21:58:08 - ptmcg
Look in the docs for ParserElement.enablePackrat. It is a class-level method.



If you are using psyco, call this immediately after importing pyparsing.



Packrat does a lot of intermediate results cacheing, so that when backtracking is done and the same expression is tested at the same location of the same string, parse results (or exception) are returned instead of making another parse call.



Without packrat, my verilog test runs around 470 lines/sec. With packrat, it jumps to ~700 lines/sec.



Packrat is not the default mode for pyparsing, as there are grammars that will break if packrat is enabled - most specifically, those that use special parse actions that have external side-effects, or use methods that have internal parse actions that affect parse-time validation, such as countedArray or matchPrevious.
#### 2007-01-31 10:49:02 - bca48150
Well I am out of ideas.  Though I say that you shoudl consider it for inclusion.  There is another approch - which depends on the branching characteristics of the particular parser.  To me the cyclic garbage collector is a great thing, but I hate to abuse it. ;)



Do with as you like.  For the large file verilog parsing I had to go to a lex/yacc implementation, however pyparsing is great for all but these extremes.



To go to the lex/yacc I took a few of the concepts from pyparsing and made a wrapper for the lex/yacc engine.  Depending on the contract requirements I may extend this to see if I can create a drop in replacement for a subset of the pyparsing functions/classes.  (Though only slightly compareable - got ~1700 lines/sec).



Great program though and quite useful.  Use it a lot.



Brian
#### 2007-02-10 10:31:30 - ptmcg
Not every problem is a pyparsing problem.  But as you say, for problems beyond basic str.split and str.startsWith approaches, I think pyparsing is at least a good tool in prototyping up a solution.  Even if it does not suffice in the end, it should give some insights into the problem space in general.  I'm glad you find it to be a useful tool.



-- Paul

---
## 2007-01-30 12:07:40 - metaperl - incremental / generator / memory-efficient parsing?
I am wondering if it is possible for pyparsing to return each recordset as it is parsed instead of generating a huge recordset and then returning it as an array. My grammar follows:





    from pyparsing import *
    
    record_sep = '*** BRS DOCUMENT BOUNDARY ***'
    
    
    key   =  Regex (r'[.][.][A-Z0-9]+:')
    value = SkipTo ( record_sep | key | StringEnd() )
    
    key_value_pair = Group (key + value)
    #key_value_pair = dictOf (key,value)
    
    record = Group(record_sep + OneOrMore (key_value_pair))
    recordset = OneOrMore(record)
    
    s = '''*** BRS DOCUMENT BOUNDARY ***
    ..PGP:
         PN_WO2006089317 PN_A1 PN_WO PN_firstpub
    ..DA1:
         20060831
    ..DS:
         AE AG AL AM AT AU AZ BA BB BG BR BW BY BZ CA CH CN CO CR CU CZ DE DK DM DZ
         EC EE EG ES FI GB GD GE GH GM HR HU ID IL IN IS JP KE KG KM KN KP KR KZ LC
    '''
    
    
    def parse(data):
        return recordset.parseString(data)
    
    



#### 2007-01-30 14:44:41 - ptmcg
Yes, scanString does this very thing.  Instead of doing 



    recordset = OneOrMore(record)
    recs = recordset.parseString(data)



use scanString (returns a generator which yields the matching tokens, start, and end location of each match - also skips over unmatched text):



    for rec,recstart,recend in record.scanString(data):
        # ... do something with rec



-- Paul
#### 2007-02-08 13:24:23 - metaperl
Well, this did not work the way I had hoped it would.





    def parse(data):
        return recordset.parseString(data)
        #for rec,recstart,recend in record.scanString(data):
        #yield rec



When I comment out     

return recordset.parseString(data)



and uncomment the lines you suggested, the records are all empty.



Here is how it is called:



    parse = parsing.parse(fp.read())
    txt = textgen.generate(self.storage.output,patent_key, parse, f.basename(), debug=False)



If this does not make sense, then I can
#### 2007-02-08 14:59:53 - ptmcg
Hmm, for some troubleshooting, try just printing recordset.searchString(data).  This will do two things:

1. search the whole string, making a list of the matched recs, and eliminating the generator issue

2. use the same scanning algorithm that scanString uses, so that we verify that the scanning process is working as expected

('expr.searchString(text)' is the same as 'list(rec for rec,recstart,recend in expr.scanString(text))')



You could also make sure that things are working properly by printing out list(parse) to see if you get anything from your generator-calling-a-generator.



-- Paul
#### 2007-02-09 07:09:50 - metaperl
I'm not sure if you want to directly debug this, but I pared down my code so that only the parsing part exists...



The data file is





The pyparsing file is:





And the driver file is:







You simply have to do python proc-ptmcg.py to run it.
#### 2007-02-09 09:20:07 - ptmcg
Had to do a little pruning to run in my environment - I have no such modules as logic or textgen.  Still, I didn't need them to work out the generator items.



The parse implementation works just as you had it.  Extract the files from the attached <div class="objectEmbed"><div>
<ul><li></li><li></li><li style="color: #666">3 MB</li></ul></div></div>, and you will get each record in turn. (I put back in our dictOf changes from a previous thread, and use dump() on each ParseResults returned to see the parsed list and keys.)  I suspect the culprit that is obscuring your record-at-a-time processing is textgen.generate.



-- Paul
#### 2007-02-09 13:20:22 - metaperl
I see well textgen.py is in that same directory:





the parse info is passed in as the variable named datums... This is a huge function but here it is:





    def generate(dir, patent_key, datums, data_file_name, debug=False):
    
        columns = [s.upper() for s in ordering[patent_key]]
        print 'Columns for', patent_key, 'are', sorted(columns)
    
        filename = '%s/%s.csv' % (dir, patent_key)
        f = open(filename, 'wb')
    
        f.write(','.join(columns))
        f.write('\n')
        c = csv.DictWriter(f, columns)
    
    
        for i,e in enumerate(datums):
        #print '----'
            #print 'i,e', i,e
        dd = collections.defaultdict(set)
            for j, e2 in enumerate(e[1:]):
                #print 'j,e2', j, e2
                if type(e2) is str: continue
                dict_key = e2[0][2:-1]
                e2[1].replace('\n', '')
                dict_val = e2[1]
            if debug: print 'K,V', dict_key, dict_val
                if ( (dict_key == 'PGP') or (dict_key == 'PN') ):
    
                    fields = dict_val.split()
    
            tmp = []
            lth = 0
            for f in fields:
                if lth + len(f) > 20: break
                tmp.append(f)
                lth += len(f)
            dict_val = ' '.join(tmp)
    
    
                    dict_key = 'PN'
    
            dict_val = re.sub( r'(firstpub|granted)' , '' , dict_val )
    
            if debug: print 'predictval', dict_val
                    dict_val = dict_val.replace('PN_', '')
            if debug: print 'postdictval', dict_val
    
    
                    patint = patint_from_patrow(fields)
                    dd['PATINT'].add(patint)
                    dd['PATNUM'].add(patint)
    
                if dict_key == 'IN': dict_key = 'INV'
    
                if dict_key == 'PCP':
                    dict_val = dict_val.replace( '/' , '' )
                    dict_val = dict_val.replace( 'WO' , '' )
    
                if dict_key == 'PCN':
                    if valid_dict_key(patent_key, dict_key):
                        print 'input PCN:', dict_val
                        fields = dict_val.split('/')
                        if fields == 3:
                            dict_val = pcn_three(fields)
    
                        print 'output PCN:', dict_val
                    else:
                        continue
    
    
                if ( (dict_key == 'PCIT') and dict_val ):
            #print 'dict_val', dict_val, 'length', len(dict_val)
            fields = dict_val.split()
            del fields[-1]
            yyyymmdd = fields[1]
            yyyy     = int(yyyymmdd[0:4])
            mm       = int(yyyymmdd[4:6])
            if mm < 1: mm = 1
            #print 'YYYY', yyyy, 'MM', mm
            do = mx.DateTime.Date(yyyy, mm, 1)
            fields[1] = do.strftime('%b %Y')
    
            dict_val = ' '.join(fields)
    
    
                if dict_key == 'PAF':
                    m = re.search( r'([A-Z]{2})X?\s*$', dict_val )
                    if m:
                        dd['COUNTRY'].add(m.group(1))
    
                if dict_key in date_fields:
                    dict_val = datetime_string(dict_val)
    
                if dict_key == 'DP':
                    tmp = dict_val
                    dict_val = datetime_string(dict_val)
                    #print 'dt_str', dict_val
                    do = datetime_object(tmp)
                    #print 'DO', do
                    dd['ISSUEMONTH'] = do.strftime('%B')
                    #print 'ISSUEMONTH', do.strftime('%B')
                    dd['ISSUEYEAR']  = do.strftime('%Y')
    
                dict_val = dict_val.strip()
                dict_val = dict_val.replace('\n', ' ')
                dict_val = dict_val.replace('\r', ' ')
                dict_val = data.localpy.strip_html.strip_tags(dict_val)
                dict_val = data.localpy.html_entities.escape(dict_val)
    
            if len(dict_val): dd[dict_key].add(dict_val)
    
    
            dd['SOURCEFILE'].add(data_file_name)
            dd['RECORDDATE'].add( str( mx.DateTime.now() ) )
    
    
            for k in ( set(dd.keys()) - set(columns) ): del(dd[k])
            #print 'keys to write', sorted(dd.keys())
    
        if debug: pprint.pprint(dd)
    
        for k in dd.keys():
                if type(dd[k]) != types.StringType:
                    dd[k] = '<br />'.join(dd[k])
                if k == 'PAF':
                    dd[k] = dd[k][0:500]
    
    
            c.writerow(dd)
    
    


#### 2007-02-09 13:24:15 - metaperl
Also, I had a discussion about generators and maintaining their laziness here:







and it is clear that my code which takes the parse data and sends it to textgen.generate() is not killing laziness...





    parse = parsing.parse(fp.read())
                patent_key = logic.f2patent_key(f)
                txt = textgen.generate(self.storage.output, patent_key, parse, f.basename(), debug=False)
    


#### 2007-02-10 01:12:53 - ptmcg
Ok, I got your code to run, and generate csv files full of data.



The primary culprit was this line, in parsing.py:



    record = Group(record_sep + OneOrMore (key_value_pair))



When using OneOrMore(record).parseString() to extract all the results at once, the use of Group is important to help keep data from each record separate.  But when switching to record.scanString, all the Group does is add another level of nesting.  If you remove Group from this definition, I think the rest of your logic will work ok.



Some other comments:



The record separator is useful for parsing, but not interesting afterward.  You can keep it from cluttering up the returned parse results by enclosing it in a Suppress expression:



    record = (Suppress(record_sep) + OneOrMore (key_value_pair))

This will also ensure that your returned results contain only nested ParseResults, with no leading string.  You currently have to filter this string in parsing.py with this code:



    if type(e2) is str: continue

By adding Suppress, you can remove this test.



In place of creating an external process just to print the current time, try:



    #~ os.system('date')
    # this is a bit friendlier than using the system 'time' command
    print time.asctime()



In textgen.py, you call replace on a string to remove end-of-lines.  replace does not update strings in place, you have to assign the modified string back into the original:



    # e2[1].replace('\n', '')
    # replace doesn't replace in place, have to assign back to e2[1]
    e2[1] = e2[1].replace('\n', '') 



In textgen.py, you use variable f as the original output file, and then again as a list-looping variable.  Change the file f to ff, and at the end of textgen.generate be sure to call ff.close() to close the created csv file.



Hope this helps get your program going again.



-- Paul

---
## 2007-02-11 02:40:14 - Digitalxero - Is Pyparsing right now me?
I want to write a dice roll parser that is flexible enough to support various die systems and methods. Everything it will be parsing is on a single line. Here are a few examples



1+2

(2+17)*10

4d6

4d6+7

q1d20+5

4d6.takeHighest(3)+(2d6*3)-5.5



Here is what I came up with after about 5 min of looking over examples (I have not tested it yet as the computer I am at right now does not have python installed and work frowns on me installing stuff)



caps = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'

lowers = caps.lower()

digits = '0123456789'

mathfn = '+-*/^'



methval = Group('(' + Word(digits) + Optional(OneOrMore(Group(',' + Word(digits))), default='') + ')')

method = Group('.' + Word(caps, lowers) + methval)



die = Group(Word(digits) + 'd' + Word(digits))



float = Group(Word(digits) + '.' + Word(digits))



proll = Group(Optional(Word('Q', 'q')).setResultsName('quiet') + die + OneOrMore(Optional(method)) + Optional(Word(mathfn)) + Optional(Word(digits)) + Optional(float))



sroll = Group('(' + Optional(proll) + Optional(float) + Optional(Word(digits)) + Optional(Word(mathfn)) + Optional(float) + Optional(Word(digits)) + ')')



roll = Group(OneOrMore(Optinal(proll)) + Optional(Word(mathfn)) +  OneOrMore(Optional(sroll)))



roll.parseString('[4d6.takeHighest(3)+(2d6*3)-5.5]')

#### 2007-02-11 20:26:22 - ptmcg
A couple of comments:

<ul><li>post your code nested in \[\[code\]\] tags, so it wont get interpreted as wiki markup.</li><li>'float' is a Python built-in type, I replaced it with 'float_' to not mask the built-in.</li><li>OneOrMore(Optional(x)) is an infinite loop in pyparsing.  Replace with ZeroOrMore(x).</li><li>overall, the code works, after fixing some 'Optinal'->'Optional' typos</li><li>Your math operators (defined in the string mathfn) will only occur one at at time.  I replaced the Word(mathfn) with oneOf(list(mathfn)).</li><li>Your original test string encloses the values in []'s, but the grammar doesn't show any []'s, so I removed them for this test.</li><li>To test out fragments of your grammar, try using searchString, it is good at picking out grammar pieces to prove out your grammar a bit at a time.</li></ul>

You are not the first one to parse die roll strings with pyparsing.  Here is another pass at the same problem: 



I think this is definitely a reasonable application for pyparsing.  I would suggest having pyparsing generate one or more Dice or DieRoll objects, which could then be evaluated to give you actual random die rolls.



Write back if you want more suggestions.  You look to be off to a good start.



-- Paul





    
    from pyparsing import *
    
    caps = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'
    lowers = caps.lower()
    digits = '0123456789'
    mathfn = '+-*/^'
    
    methval = Group('(' + Word(digits) + Optional(OneOrMore(Group(',' + Word(digits))), default='') + ')')
    method = Group('.' + Word(caps, lowers) + methval)
    
    float_ = Group(Word(digits) + '.' + Word(digits))
    
    die = Group(Word(digits) + 'd' + Word(digits))#.setDebug()
    
    proll = Group(Optional(Word('Q', 'q')).setResultsName('quiet') + 
                    die + 
                    #~ OneOrMore(Optional(method)) + 
                    ZeroOrMore(method) + 
                    Optional(Word(mathfn)) + 
                    Optional(Word(digits)) + 
                    Optional(float_)).setName('proll')#.setDebug()
    
    sroll = Group('(' + Optional(proll) + 
                    Optional(float_) + 
                    Optional(Word(digits)) + 
                    #Optional(Word(mathfn)) + 
                    Optional(oneOf(list(mathfn))) + 
                    Optional(float_) + 
                    Optional(Word(digits)) + ')').setName('sroll')#.setDebug()
    
    roll = Group(ZeroOrMore(proll) + Optional(Word(mathfn)) + ZeroOrMore(sroll))
    
    test = '4d6.takeHighest(3)+(2d6*3)-5.5'
    print proll.searchString(test)
    print sroll.searchString(test)
    print roll.parseString(test)

prints 



    [[[['4', 'd', '6']]], [[['2', 'd', '6'], '*', '3']]]
    [[['(', '3', ')']], [['(', [['2', 'd', '6'], '*', '3'], ')']]]
    [[[['4', 'd', '6']]]]


#### 2007-05-15 12:39:39 - Digitalxero
Ok I finally had some time to sit down and play with pyparsing a bit for my Dice roller and I came up with 



from pyparsing import *

quiet   = CaselessLiteral('Q')

point = Literal('.')

comma = Literal(',')

fnumber = Combine(Word(nums) + Optional(point + Optional(Word(nums))))

ident = Word(alphas, alphas+nums+'_$')



plus  = Literal('+')

minus = Literal('-')

mult  = Literal('*')

div   = Literal('/')

lpar  = Literal('(').suppress()

rpar  = Literal(')').suppress()

addop  = plus | minus

multop = mult | div

expop = Literal('^').setParseAction(pushFirst)



expr = Forward()

func = ZeroOrMore(Group(Optional(point).suppress() + ident + Combine(lpar + (Word(nums) | Word(alphas)) + ZeroOrMore(comma + (Word(nums) | Word(alphas))) + rpar))).setResultsName('function')

roll = (Word(nums) + 'd' + Word(nums)).setResultsName('roll')

dice  = ZeroOrMore(Group(Optional('-') + roll + func))

atom = Optional(quiet) + dice + ZeroOrMore(addop | multop) + ZeroOrMore(fnumber | Group(lpar + expr + rpar))



factor = Forward()

factor << atom + ZeroOrMore(( expop + factor ))



term = factor + ZeroOrMore(multop + factor)

expr << term + ZeroOrMore(addop + term)



test = '(2d6*3)-5.5+4d6.minRoll(2).takeHighest(3)'

results = expr.parseString(test)

print results



Prints

[[['2', 'd', '6'], '*', '3'], '-', '5.5', '+', ['4', 'd', '6', ['minRoll', '2'], ['takeHighest', '3']]]



Now I am having problems figuring out where and how to use setParseAction to get the results back out so I can process them
#### 2007-05-16 20:35:58 - ptmcg
You may not want to use parse actions to evaluate this.  Instead, this parsed intermediate form (or some other higher-level one) may be what you want to keep, and then evaluate over and over to get different die rolls for '2d6' and '4d6'.  Instead, I would vote for evaluating this structure for die rolls, and where appropriate, do minRoll or takeHighest functions.



In anticipation of evaluating this structure, I would add a couple of modifications to your parser.  For one, I would convert the numeric literals to ints or floats, using something like:



    fnumber.setParseAction( lambda tokens : float(tokens[0])

Also, I would use either Group or Combine on roll, so that these tokens are a distinctly recognizable element.  Lets use Combine:



    roll = Combine(Word(nums) + 'd' + Word(nums)).setResultsName('roll')



With these changes, your results will change slightly, to:



    [[['2d6', '*', 3.0], '-', 5.5, '+', ['4d6', ['minRoll', [2]], ['takeHighest', [3]]]]]

(note that the numeric constants are now true float values, and not strings any more)



Your captures the ParseResults in the variable 'results', and we'll write a function to evaluate this structure:



    results = expr.parseString( diceString )
    print evalExpr( results )



Now looking at this parsed structure, we see that at the highest list level, the list is made up of:



    something operator something operator something



and as it turns out, your grammar guarantees that whatever operations your expression has in it, the top-most level will be of this alternating form.  Just for the sake of giving the 'somethings' a name, let's call them 'dice nodes'.  A dice node may contain:

- a floating point value

- a dice roll string (of the form <int>d<int>)

- a structure containing a dice roll string with functions

- a structure containing a nested expression



Looking ahead, we'll need a function to roll dice for us.  Since you have some expressions that do dice selection or filtering before summing the dice values, let's just have roll() return the list of dice values:



    from random import randint
    def roll(dieStr):
        count,sides = map(int,dieStr.split('d'))
        ret = [ randint(1,sides) for _ in range(count) ]
        print 'Rolling', dieStr, '->', ret  # debugging statement, comment out later
        return ret



We'll also capture the logic of evaluating 'something op something op ...' inside a function, since we expect that we may need to call this recursively for nested expressions.



We start by evaluating the first dice node of the results variable, and then take the remaining elements in pairs, applying the right operation to add or subtract the next node.  I'll use zip to create a set of tuples representing the remaining terms, a pair at a time.  The dict variable binops sets up a dispatch table of operations to perform depending on the next operator in the structure:



    binops = {
        '+' : (lambda a,b : a+b),
        '-' : (lambda a,b : a-b),
        '*' : (lambda a,b : a*b),
        }
    def evalExpr(a):
        # special handling for dice rolls with attached functions
        if isinstance(a,ParseResults) and \
            len(a)>1 and \
            isinstance(a[1],ParseResults):
            return evalDiceExpr(a)
    
        accum = evalNode(a[0])
        for op,node in zip(a[1::2],a[2::2]):
            # use op to dispatch into binops dict, and 
            # perform the appropriate operation on the 
            # accumulated value, plus the evaluated value
            # of the next item
            accum = binops[op](accum,evalNode(node))
        return accum



evalNode is going to return a value, for any of the four cases above.  The 3rd and 4th cases have been merged into a call to evalExpr, which determines at the beginning if this is case 3 or 4.



    def evalNode(a):
        if isinstance(a, float):
            return a
        if isinstance(a, str):
            return sum(roll(a))
        if isinstance(a, ParseResults):
            return evalExpr(a)



evalDiceExpr does much the same as evalNode, using a dispatch table to perform the suitable function.



    sortedRoll = lambda dstr,revflag=False : sorted(roll(dstr),reverse=revflag)
    takeLowest = lambda d,n : isinstance(d,str) and sortedRoll(d)[:n] or sorted(d)[:n]
    takeHighest = lambda d,n : isinstance(d,str) and sortedRoll(d,revflag=True)[:n] or sorted(d,reverse=True)[:n]
    minRoll = lambda d,n : sorted([ sortedRoll(d) for _ in range(n)])[0]
    maxRoll = lambda d,n : sorted([ sortedRoll(d) for _ in range(n)],reverse=True)[0]
    fns = {
        'takeLowest' : takeLowest,
        'takeHighest' : takeHighest,
        'minRoll' : minRoll,
        'maxRoll' : maxRoll,
        }
    def evalDiceExpr(a):
        dice = a[0]
        for mod in a[1:]:
            dice = fns[mod[0]](dice, mod[1][0])
        return sum(dice)

(I'm guessing that minRoll(2) means 'take the smaller of two rolls of the dice'.  'takeHighest' I could figure out for myself.)



At this point, we've closed the loop on our recursive evaluator, and when put all together, we get results like this:





    >pythonw -u 'dice2.py'
    [[['2d6', '*', 3], '-', 5.5, '+', ['4d6', ['minRoll', [2]], ['takeHighest', [3]]]]]
    Rolling 2d6 -> [5, 5]
    Rolling 4d6 -> [5, 2, 1, 5]
    Rolling 4d6 -> [6, 6, 3, 1]
    36.5
    >Exit code: 0
    >pythonw -u 'dice2.py'
    [[['2d6', '*', 3], '-', 5.5, '+', ['4d6', ['minRoll', [2]], ['takeHighest', [3]]]]]
    Rolling 2d6 -> [1, 3]
    Rolling 4d6 -> [4, 2, 1, 2]
    Rolling 4d6 -> [6, 5, 6, 3]
    14.5
    >Exit code: 0
    >pythonw -u 'dice2.py'
    [[['2d6', '*', 3], '-', 5.5, '+', ['4d6', ['minRoll', [2]], ['takeHighest', [3]]]]]
    Rolling 2d6 -> [3, 6]
    Rolling 4d6 -> [2, 6, 6, 4]
    Rolling 4d6 -> [2, 4, 5, 1]
    32.5
    >Exit code: 0
    



I'll post dice2.py on the example page so you can compare with your own grammar.



-- Paul
#### 2007-05-17 19:04:58 - Digitalxero
Thanks muchly, that helped alot

---
## 2007-02-11 14:09:25 - nickzam - bug report: another unicode bug
Hello, Paul!



I think it would be useful for everyone to move our discussion about pyparsing unicode support from e-mail to wiki page.



It would be a good idea to go through code and cover all possible attempts to use string object methods with _ustr() function.  



I've found another unicode bug:





    def scanString( self, instring, maxMatches=sys.maxint ):
            '''Scan the input string for expression matches.  Each match will return the 
               matching tokens, start location, and end location.  May be called with optional
               maxMatches argument, to clip scanning after 'n' matches are found.'''
            if not self.streamlined:
                self.streamline()
            for e in self.ignoreExprs:
                e.streamline()
    
            if not self.keepTabs:
             #unicode fix   
             instring = _ustr(instring).expandtabs()
            instrlen = len(instring)
            loc = 0
            preparseFn = self.preParse
            parseFn = self._parse
            ParserElement.resetCache()
            matches = 0
            while loc <= instrlen and matches < maxMatches:
                try:
                    preloc = preparseFn( instring, loc )
                    nextLoc,tokens = parseFn( instring, preloc, callPreParse=False )
                except ParseException:
                    loc = preloc+1
                else:
                    matches += 1
                    yield tokens, preloc, nextLoc
                    loc = nextLoc



Nikolai

#### 2007-02-11 23:13:58 - ptmcg

---
## 2007-02-12 13:41:52 - metaperl - What does a tilde in front of a token descriptor mean?
This is not my real question, but I am wondering what a tilde in front of a token descriptor does... For example





    
    _commasepitem = Combine(OneOrMore(Word(_noncomma) + 
                                      Optional( Word(' \t') + 
                                                ~Literal(',') + ~LineEnd() ) ) ).streamline().setName('commaItem')



#### 2007-02-12 14:26:58 - ptmcg
~X is the same as NotAny(X).

---
## 2007-02-12 14:11:49 - metaperl - multi-line CSV with pre-formatted multi-line field
Ok, I have a CSV file.



Each record is terminated with `||| `

Most of the CSV fields are your everyday single-line fields. However it contains certains fields which start with `PRE ` in angle brackets and end with 

`/PRE ` in angle brackets. These are multi-line fields which must have their spacing and all characters preserved.



The program so-far is simple:





    
    #!/usr/bin/env python
    
    from pyparsing import commaSeparatedList
    import sys
    
    f = open('UTEK_Agg.txt')
    
    
    for line in f.read().split('|||'):
        print 'line:', line
        print 'csv:',   commaSeparatedList.parseString(line)
        print 'split:', line.split(',')
        print
        sys.exit()
    



A single record is quite large, so I am afraid to put one in this post. Instead the entire data file is here:





#### 2007-02-12 14:31:06 - metaperl
I created a version of the file with just two records:




#### 2007-02-12 14:36:11 - ptmcg
Ah, so that is why you are looking at _commasepitem. I think you should be able to crib the guts from commaSeparatedList, embellishing _commasepitem to also accept preStart + SkipTo(preEnd) + preEnd, where preStart \= Literal('<PRE>') and preEnd \= Literal('</PRE>') (use CaselessLiteral if you need to, but I wouldn't bother with the expressions created by makeHTMLTags, they are probably more complex than you need - do these tags really have attributes, for instance?)
#### 2007-02-12 14:41:29 - ptmcg
This may actually be simpler than originally thought.  Every field is enclosed in double quotes, and the '<PRE>...</PRE>' are also enclosed in double quotes, and any double quotes in them are escaped using ''.  You may be able to get away with something as simple as delimitedList(QuotedString(''',escQuote='''')) to describe these lists.



-- Paul
#### 2007-02-13 06:09:43 - metaperl
Ok, but the problem is that the parse stops at end of _line_ instead of end of _string_.



In other words, given this data:





    '1225','3801','Batim (the ''Agreement'') Bartech Agencies','Bartech E.M.E.A Sarl','12/28/2000','<PRE>
    EXHIBIT 10.2
    
                                        EXCLUSIVE
    </PRE>'
    



The parse will will only continue up to '12/28/2000' because it encounters end of _line_ while trying to parse the `PRE ` field.



I looking for an instruction which says parse to end of string, regardless of whether you get an EOL character.
#### 2007-02-13 06:28:57 - ptmcg
Do you have the docs that come with the source distribution?  In the htmldoc directory, the documentation for QuotedString includes this description of the init args:



Defined with the following parameters: 

<ul><li>quoteChar - string of one or more characters defining the quote delimiting string</li><li>escChar - character to escape quotes, typically backslash (default=None)</li><li>escQuote - special quote sequence to escape an embedded quote string (such as SQL's '' to escape an embedded ') (default=None)</li><li>multiline - boolean indicating whether quotes can span multiple lines (default=False)</li><li>unquoteResults - boolean indicating whether the matched text should be unquoted (default=True)</li><li>endQuoteChar - string of one or more characters defining the end of the quote delimited string (default=None => same as quoteChar)</li></ul>

You've posted some samples, so I'll do a quick test to see how QuotedString works for this data, but I think adding multiline=True should take care of your EOL problem.



-- Paul
#### 2007-02-13 06:45:44 - ptmcg
This works for me:



    import urllib
    from pyparsing import *
    
    page = urllib.urlopen('http://fyodor.hcoop.net/~terry/tmp/pyparsing/multiline-csv/two_records.txt')
    html = page.read()
    page.close()
    
    record = delimitedList(QuotedString(''',escQuote='''',multiline=True))
    
    recs = record.searchString(html)
    for r in recs:
        print r[:3]
    



prints



    ['1225', '3801', 'Batim (the 'Agreement') Bartech Agencies']
    ['1334', '3880', 'Cytogen Corporation']
    



On your original file, I get 180 records.





Here's another way to crack the ParseResults without having to resort to setResultsName (not easy to do with delimitedList):



    record = delimitedList(QuotedString(''',escQuote='''',multiline=True))
    
    recs = record.searchString(html)
    print 'Found',len(recs),'records'
    for r in recs:
        (recid,
        dataid,
        firstParty,
        secondParty,
        timestamp,
        agreementbody) = r[:6]
        print 'Agreement on %(timestamp)s between %(firstParty)s and %(secondParty)s' % locals()



-- Paul

---
## 2007-03-08 11:05:59 - utoxin - Full Blown Language Parser Possible?
I'm coding up a MOO server as a learning exercise, and stumbled across Pyparsing while looking for libraries to help me.



Would it be reasonable to code a parser for a programming language in Pyparsing, or should I look elsewhere? (I may use Pyparsing anyway, for certain phases of command parsing that it looks to be well suited for)

#### 2007-03-10 02:53:19 - ptmcg
utoxin -



I definitely believe pyparsing can be used to code a parser for a programming language.  I have implemented a Verilog parser using pyparsing, and this language is quite complex.



Here are some hints in your endeavor:

- pyparsing is not so great with indentation-style blocking, like Python uses.  If you define your language to support multiple statements to be performed as a block, please plan on delimiting them with {}'s, or some similar notation.

- think about your domain and consider language constructs that support it directly.  For instance, in a MOO-style language 'X->Y' could signify that player X moves to room Y.  Or 'X-(move)->Y' vs. 'X-(use)->Y' vs. 'X-(take)->Y', etc. means X moves, uses, or takes Y.

- design a BNF for your language.  That is to say, lay out the language syntax for 'if-then-else' or 'switch-case' or whatever control constructs, then mentally work through this BNF to look for ambiguities

- include definition of a comment syntax - pyparsing has very good support for this, using the ignore() method

- use the operatorPrecedence helper to facilitate creating the syntax for arithmetic or boolean expressions.  Look at the examples simpleArith.py and simpleBool.py.

- use pyparsing parse actions to take input text and create executable objects.  This is one of my pet peeves with many parsers that are actually little more than tokenizers - after the text is tokenized into statements and tokens, the resulting structure has to be reprocessed for the actual behavior.  With a suitable syntax, pyparsing knows already that '1.234' represents the value 1.234, and can convert it at parse time.  Otherwise, some postprocessor has to retest '1.234' for its validity as a real number before converting from string to float.  Instead, use parse actions to return executable class instances which can then be invoked or evaluated directly.

- check out my mini adventure game, which I presented at PyCon back in '06.  It is also available on-line at the wiki Documentation page.

- check out grailmud.  This uses pyparsing primarily as an advanced string splitting utility, but to very good effect.



Good luck, keep us posted on your progress!



-- Paul
#### 2007-04-12 12:13:14 - HowardKapustein
Possible? Absolutely. I've been using PyParsing to create a CORBA IDL-ish parser for some time now.



That's 'ish' as what I have is both a subset (we weren't using all of the OMG IDL constructs) and a superset (additional features to make our life easier I started with the CORBA example but quickly grew past it (it was a far smaller subset than even I needed), but a big thank you to whoever created that sample. It was a huge jumpstart when I first used dove into PyParsing.



As for the BNF comment, I can't agree enough. If you can express your language in a form of BNF, it's pretty easy to implement it in PyParsing.



I take PyParsing's output and walk the structured data, building an object graph as an intermediary form of the input, which I use later for the 'code generation' phases. I made a bunch of helper classes as my intermediary objects, and mostly it's just constructor + <u>str</u>() + <u>repr</u>(), with direct attribute access for the properties. [Yeah, it's not 100% Pure Encapsulation. I'm using Python, not Java. And I like it that way :-]



Good luck,
#### 2007-04-13 08:12:46 - ptmcg
Howard -



Thanks for chiming in!  Also, I'm glad the CORBA IDL subset was sufficient for you to get your foothold - I had some misgivings about putting out such a small subset, but it was what I had at the time.  (This actually dates waaaaay back in pyparsing time, there may be some newer features that could simplify this example.)



One thing you might look into is to attach object constructors as parse actions, so that pyparsing builds your structure as it parses, and you can skip the 'walk the structured data' pass afterwards.  Look at the simpleBool.py example and the Command processor of the adventure game for examples of this technique (or not - your approach is working for you as-is, I just wanted to raise an alternative for you).



Cheers!

-- Paul
#### 2007-06-22 11:25:21 - karmazilla
<ul class="quotelist"><li>'- pyparsing is not so great with indentation-style blocking, like</li><li>Python uses. If you define your language to support multiple</li><li>statements to be performed as a block, please plan on delimiting</li><li>them with {}'s, or some similar notation.'</li></ul>

Do you know of a better alternative?
#### 2007-06-22 21:39:31 - ptmcg
Not off hand.  I was looking into adding some Indent and Unindent classes to pyparsing, but it is tricky to combine this kind of grammar item with pyparsing's built-in whitespace skipping.  



Perhaps someone who has used some of the other parsers, like Spark or PLY, might have some ideas on how to approach this problem.  But I think detecting indentation-based code blocks will probably take writing a custom state-machine style parser.



-- Paul

---
## 2007-03-26 14:14:59 - bkit - tokens.asXML() prints always a nested structure?
Hello, 

Consider this Python code:





    from pyparsing import Word, alphas
    grammar = Word(alphas, exact=3).setResultsName('tag')
    tokens = grammar.parseString('foo')
    print tokens.asXML()



it outputs





    
    <tag>
      <tag>foo</tag>
    </tag>



Although only one token has been defined and matched, the asXML() method wraps the token with an outer element. I assume that's to making sure that there's always one single root element in case many non-grouped tokens have to be XMLified. Is there any way to suppress this behaviour if only one token is returned after parsing?



regards,

bk

#### 2007-03-26 18:52:25 - ptmcg
asXML() has always been a stepchild routine in pyparsing - when it works, it does the job, but when it doesn't work, it is more trouble than it is worth.



Your suspicion is correct, there needs to be some root element for the returned results.  asXML() will accept a string argument so that you can give this your own name.  For example, if you change the last line of your test to:



    print tokens.asXML('root')



the generated output is:



    <root>
      <tag>foo</tag>
    </root>



Passing an empty string creates a non-descript root named 'ITEM':



    <ITEM>
      <tag>foo</tag>
    </ITEM>



I hope this option will help you resolve your question.



-- Paul
#### 2007-04-24 14:15:47 - bkit
A little late :) ... thanks for the detailed answer. I was only curious if I'd miss something obvious with the way asXML() works. Originally I stumbled over this caveat when I wrote tests for sections of my grammar, but it was easy to find a workaround. Now that the grammar is almost finished there will be a root element present anyway.

---
## 2007-03-31 01:02:17 - kib2 - Does PyParsing suits my needs ?
Hi,



I wanted to parse a sort of TextMate-like snippets engine.

My first implementation used regexps (I didn't know them a lot before), 

but I then I released that the parsing needs to be recursive.



I considered using PyParsing as it seems a good solution to my problem,

but I'm not really sure if I can make what I wanted to, or if I have to

write a parser by hand (with a sort of stack to count the opened tags).



Sample template:



'whatever ${1:key} whatever2 ${2:'${3:value}' } $1 $0'



The parsing process needs to be space sensitive, and I wanted to match all the placeholders (ie, ${number: default_value}) and their mirrors (ie, $number. The $0 value is a particular case, I just need its exact position in the text) with their exact positions in the original text so as to replace their value with a given dictionnary, ie from the template, the defautls values will return:



'whatever key whatever2 '${3:value}' key'



Now, if I give now these values:



1 set to foo

2 set to bar 

3 set to something



I should obtain :



'whatever foo whatever2 'something' foo'



Is that possible with PyParsing, and if so, any hint ?

Thanks a lot.

#### 2007-04-11 09:21:09 - ptmcg
kib2 -



Well, here's as far as I've gotten.  This method will match nested braces and return them as a string containing the outermost braces and their original content, spaces and all.  It is smart enough not to treat opening or closing braces inside quotes as nesting characters.  It should handle multiline cases, but I've not tested this too hard yet.  To process the nested structure, you'll need to reparse the string, leaving out the opening and closing braces.



Try this method out and see if it helps at all.



-- Paul





    from pyparsing import *
    
    def matchedBraces(openchar='{', closechar='}', multiline=True):
        openBrace = Literal(openchar)
        closeBrace = Literal(closechar)
        if multiline:
            notBrace = CharsNotIn(openchar+closechar,max=1)
        else:
            notBrace = CharsNotIn(openchar+closechar+'\n\r',max=1)
    
        tally = list()
        tally.append(0)
        def incrTally(s,l,t):
            tally[0] += 1
        def decrTally(s,l,t):
            if tally[0] > 0:
                tally[0] -= 1
            else:
                raise ParseFatalException, 'mismatched braces'
        def assertNested(s,l,t):
            if tally[0] == 0:
                raise ParseException, 'no longer nested'
    
        openBrace.setParseAction(incrTally)
        closeBrace.setParseAction(decrTally)
        stillNested = Empty().setParseAction(assertNested)
    
        matchBraceExpr = openBrace + ZeroOrMore( stillNested + 
                                (quotedString.copy() | openBrace | closeBrace | notBrace) ).leaveWhitespace()
        return matchBraceExpr
    
    test = '''\
    'whatever ${1 : key} whatever2 ${2:'${3:value}' } $1 $0'
    
    
    The parsing process needs to be space sensitive, and I wanted to match all the 
    placeholders (ie, ${number: default_value}) and their mirrors (ie, $number. 
    The $0 value is a particular case, I just need its exact position in the text) 
    with their exact positions in the original text so as to replace their value 
    with a given dictionnary, ie from the template, the defautls values will return:
    
    'whatever key whatever2 '${3:value}' key'
    
    {}  {} {{}} ${1 : key}
    ${1 : ${2 : sldflsjd} }   { {abc{} } {def} }
        '''
    
    mbExpr = matchedBraces().setParseAction(keepOriginalText)
    for mb in mbExpr.searchString(test):
        print mb[0]


#### 2007-04-11 12:20:57 - kib2
Hi Paul,



I just ave it a try and the script fails, I don't know why :



TypeError: <u>init</u>() takes at least 4 arguments (2 given)



Here's my current version using the string module. I wonder if PyParsing is faster ?





    #!/usr/bin/env python
    
    import string
    
    STARTER = '${'
    ENDER   = '}'
    ASSIGN = ':'
    
    def tokens(content, token) :
      '''
      Parse a given string and returns a list of
      the starting positions of a given token.
      '''
      cont, last, indexes = content, 0, []
      index = string.find(cont, token, last)
      while index != -1 :
        indexes.append( int(index) )
        last = index + 1
        index = string.find(cont, token, last)
      return indexes
    
    def parse(content, start_token= STARTER, end_token= ENDER):
        '''
        Parse a string and returns a list of
        all Fields delimited by ${...} inside it.
        Fields can be nested.
        '''
        result = []
        opentags = tokens(content, start_token)
        closedtags = tokens(content, end_token)
    
        for st in opentags :
          iterator = iter(closedtags)
          nofound = True
          while nofound :
            ed = iterator.next()
            if ed >= st :
              opentags2 = len(tokens(content[st+len(start_token): ed], start_token))
              closedtags2 = len(tokens(content[st+len(start_token): ed], end_token))
              if opentags2 == closedtags2 :
                result.append( [st, ed, content[st+len(start_token) : ed]] )
                nofound = False
        return result



Thanks,

Kib.
#### 2007-04-11 12:59:05 - ptmcg
Ah, I am using my development version of pyparsing.  The exception constructors are simplified in the next version, so that you can raise a ParseException using:



    raise ParseException,'missing character'



You'll need to change the ParseException construction lines to:



    raise ParseException('',0,'msg')


#### 2007-04-11 13:06:36 - kib2
Corrected, but that was not the only problem, now I've got this error :



NameError: name 'N_TOKENS' is not defined
#### 2007-04-11 14:35:45 - ptmcg
???



Can you post the full traceback?  This will give me more to go on, such as the actual source line with the problem.



-- Paul
#### 2007-04-12 01:11:39 - kib2
Hi Paulh,

sorry for my late :





    File 'C:\Program Files\Wing IDE 3.0\bin\wingdb.py', line 10, in <module>
      '''
    File 'C:\Program Files\Wing IDE 3.0\bin\wingdb.py', line 445, in main
      def main():
    File 'C:\Program Files\Wing IDE 3.0\bin\wingdb.py', line 375, in DebugFile
      def DebugFile(server, filename, err, fs_encoding, sys_path=None):
    File 'C:\Users\kib\Desktop\PyTM\sample_pyparsing1.py', line 1, in <module>
      from pyparsing import *
    File 'C:\Users\kib\Desktop\PyTM\sample_pyparsing1.py', line 49, in <module>
      for mb in mbExpr.searchString(test):
    File 'c:\Python25\Lib\site-packages\pyparsing.py', line 882, in searchString
      return ParseResults([ t for t,s,e in self.scanString( instring, maxMatches ) ])
    File 'c:\Python25\Lib\site-packages\pyparsing.py', line 844, in scanString
      nextLoc,tokens = parseFn( instring, preloc, callPreParse=False )
    File 'c:\Python25\Lib\site-packages\pyparsing.py', line 738, in _parseNoCache
      tokens = fn( instring, tokensStart, retTokens )
    File 'c:\Python25\Lib\site-packages\pyparsing.py', line 2790, in keepOriginalText
      import inspect
    File 'c:\Python25\Lib\inspect.py', line 31, in <module>
      import sys, os, types, string, re, dis, imp, tokenize, linecache
    File 'c:\Python25\Lib\tokenize.py', line 38, in <module>
      COMMENT = N_TOKENS


#### 2007-04-12 06:39:35 - ptmcg
I'm still running Python 2.4.1 - this error is cropping up in Python2.5's tokenize.py module, nothing I can do about this.



Looking at the code in my Python24/Lib directory, N_TOKENS is defined in token.py, which is imported by tokenize.py.  Can you chase this down from there?



-- Paul
#### 2007-04-15 13:14:36 - kib2
Sorry Paul,



I did all I can (even by upgrading to new Python 2.5.1) and it fails again...



But I didn't really understood what you're expecting me to do with tokenize.py ?
#### 2007-04-16 08:39:57 - ptmcg
Ah, I think you have a local module that is overwriting one in the Python lib.  This reads like the genalogy in the Bible:



pyparsing import inspect.py

inspect import tokenize.py

tokenize import tokens.py (which defines N_TOKENS)

<exception in tokenize trying to reference N_TOKENS>



Do you have a local source file named tokens.py?  If so, then when tokenize.py gets imported, it is not importing its expected tokens.py from the Python library, but the one from your local directory.



Is that the problem?  If so, that is a general Python issue, not specific to pyparsing.



-- Paul
#### 2007-04-16 12:26:35 - kib2
You were right Paul, 

I just saw that my script was inside a folder with about 50 other python scripts, one of them is named tokens.py and that was the problem.
#### 2007-04-16 14:43:13 - ptmcg
It seems odd to me that Python still has such a major loophole in its import strategy.  This problem is magnified the more external packages you import, each with its own internal imports.  It seems to me to be a lot for a developer to have to keep track of, and the error messages are *never* obvious.



-- Paul
#### 2007-10-18 10:07:20 - ptmcg
A couple of updates to some of my comments in this thread:



1. The 'more extensive how-to documentation' I alluded to is now out - it is the 'Getting Started with Pyparsing' ShortCut being offered on the O'Reilly online catalog.  There is currently a link to this ShortCut on the pyparsing wiki home page.



2. The matchedBraces helper method has been rewritten and added to the 1.4.8 release of pyparsing, named 'nestedExpr'.  nestedExpr supports the definition of nested expressions using arbitrary opening and closing delimiters (default are '(' and ')').



-- Paul
#### 2007-03-31 03:39:47 - kib2
Paul,



Thank you for providing me such a great sample. 



PyParsing seems very powerful indeed, the one I've written with regexps was longer and doesn't have the recursion process... 



I saw the docs (from Dave Khulman's reST files ), but they don't look nice on screen.

In exchange I can try to help you in providing better-looking docs ( in several formats : pdf, odt, html, tex : see here h**p://kib2.free.fr/documents/DocUtils/tuto_rest.pdf for a sample). 



If you send me the original reST file (mail me at kib2ATfree.fr)



Thanks again,

Kib
#### 2007-03-31 09:36:36 - ptmcg
Great, glad that helped!



Which pyparsing package did you download from SF?  Was it the Windows binary installer (.exe)?  If so, this installs *only* the basic pyparsing runtime module.  There are class docs, diagrams, and sample programs included in the source and docs packages, so I'd encourage you to download one of them and look through the additional files.



The one item that is a little light is the 'how-to' documentation.  Dave Kuhlman's page helps out with that some, and there are also my PyCon presentations, and the ONLamp article.  Most of these are included in the source and docs packages, or there are links on the wiki Documentation page.  I'm working on a project now for some more extensive 'how-to' documentation.



By the way, you mentioned recursive grammars.  Strictly speaking, this example is recursive in that we took the parsing output and reparsed it, but that is more an outcome of the way we applied the grammar.  The grammar itself, as we've implemented it, is not a recursive one.



Recursive grammars come about when some parts of the grammar reference each other, such as the parser in the PyCon presentation that parses a Python list.  The grammar for a list in this simplified version shows that a list can be composed of a comma-separated list of listItems, and that a listItem can be an integer, a quoted string, or a nested list.  Here is a BNF:



    listItem :: integer | quotedString | listExpr
    listExpr :: '[' listItem [ ',' listItem ]... ']'



To implement a recursive grammar, you need to use pyparsing's Forward class:



    listExpr = Forward()
    listItem = Word(nums+'+-',nums) | quotedString | listExpr
    LBRACK = Literal('[').suppress()
    RBRACK = Literal(']').suppress()
    listExpr << Group(LBRACK + delimitedList(listItem ) + RBRACK)



This allows us to define listExpr with a placeholder, so we can reference listExpr within listItem.  Then, to actually define the contents of listExpr, we use the '<<' operator to insert the actual definition into the listExpr placeholder.  This is *truly* a recursive grammar.



This recursive capability can't be done in traditional regexp's (although I thought I read that there was an extended regexp implementation that handles recursive expressions).



-- Paul
#### 2007-03-31 10:15:45 - kib2
Ok,



thanks for the clarification : the forward method was the one I used before (i saw something on O'Reilly codezoo).



Now, I need to catch the beginning/end of each placeholder inside the template, i just had a look at the doc, but found nothing. I suppose it's easily done, but how ?
#### 2007-03-31 10:22:49 - ptmcg
scanString will give the starting location, but not the ending location.  If you look at the source for the pyparsing module, and read how transformString works, you'll see how I use scanString and some bending of the rules to get the ending location.



-- Paul
#### 2007-03-31 11:41:33 - ptmcg
My bad, scanString will return both starting and ending location.  (It's been a while since I used it.)  Here is a snippet from in the examples directory for scanExamples.py.



    ident = Word(alphas, alphanums+'_')
    macroDef = Literal('#define') + ident.setResultsName('name') + '=' + restOfLine.setResultsName('value')
    for t,s,e in macroDef.scanString( testData ):
        print t.name,':', t.value



scanString returns a generator that yields the tokens, start, and end location of each pattern match in the input string.  transformString is just a simple wrapper around scanString that uses these values to do the appropriate string slicing to return a transformed string.



(The end-of-tokens hack I was thinking of was the helper parse action keepOriginalText, which peeks up the stack to see the value of 'loc' in the parent frame.)



-- Paul



-- Paul
#### 2007-04-01 05:40:37 - kib2
Hi Paul,



Thanks for the scanString method.

But in fact the problem is more complex than I thought concerning the positions.

Suppose I've got this simple template:



'def (${1:name}):

    ${2:pass}$0'



witch is rendered like this by default:



'def (name):

    pass'



Now, I need to catch the exact positions of the replaced text-pieces : 'name' and 'pass' ( or whatever  names I give in the dictionnary for replacement).



To give you an idea of what I'm trying to achieve, I've made a little flash-video (this was with my regexp engine, not with PyParsing) :



h**p://kib2.free.fr/reSTedit/rest.htm



Have you got any idea ?
#### 2007-04-01 12:03:41 - ptmcg
Ah, les yeux ont t ouverts! (Courtesy of Altavista/Babelfish)



Now I understand that you are converting templates dynamically while editing!  This explains why you need the start/end locations for all of the replacements.



I still think scanString is what you want, so that you can pick up the individual placeholders and mirrors.  But instead of just returning a string, you should design an object, such as TemplateDef, that contains the start and end locations, the display string, mirror reference, whatever, and have the parse action return that instead.  Then your calling routine would have all of the template information at hand, and could do the right thing with it.  I don't know what editor you are using, but it appears that you can call Python scripts with it, so you should be able to handle a Python object.  You may need to hack your parse action to get the end location, similar to what I did with keepOriginalText - I think I'll add a getEndLocation helper method that simplifies this.



I just checked and I see I don't have any examples of parse actions returning objects.  I think if you look up ldaptor (), it uses parse actions to essential compile an LDAP query into ldap objects.  I think the real issue here is that, once you find the template, you need more than just the replacement string, so returning some sort of object is unavoidable.



Bon chance! (I knew this one without looking it up)

-- Paul
#### 2007-04-02 02:32:18 - kib2
Hi Paul,

I just found a little time to look at getTokensEndLoc() function, but I don't know how to use it.



I've tried this :



 

<ol><li>is this to register getTokensEndLoc ?</li></ol>substitution.setParseAction(getTokensEndLoc)

<ol><li>then I can use 'tokens.getTokensEndLoc' inside</li><li>my function constructPHR</li></ol>substitution.setParseAction(constructPHR)





but it fails.
#### 2007-04-02 06:18:49 - ptmcg
Sorry.  Call getTokensEndLoc() from within your parse action, it will return an int.



    def parseAction(s,l,t):
        print 'Tokens start at %d and end at %d' % (l,getTokensEndLoc())



-- Paul
#### 2007-04-02 09:29:26 - kib2
Thanks Paul,

Now I've got two problems :



1. What about spaces in placeholders ? In fact it's even more complicated as they may contain anything...



2. How to reconize the nested ones, i.e ${1: ${2:something}}?
#### 2007-04-10 18:59:48 - ptmcg
kib2 -



I think you will need some special logic for finding the closing '}' for a template that may contain nested {}'s.  In fact, this sounds like something that may be a good helper method to add to pyparsing.  I'll spend the next hour or two on it and see what I can post.



-- Paul
#### 2007-04-11 02:05:15 - kib2
Hi Paul,



Thanks for giving it a try, I'm waiting for your results.

The only problem I see is that a template has no particular form, in fact you can place whatever value you want inside (spaces, tabs,etc.), and I find it hard to produce such a grammar.



I've started something like this, but it's not sufficient :





    from pyparsing import *
    
    class SnippetParser:
    
        def __init__(self):
            self._parser = self.parser()
    
        def parser(self):
            '''
            This function returns a parser.
            '''
    
            Field = Forward()
    
            operatorBeginField = Literal('$').setResultsName('begin').suppress()
            operatorKey = Word(nums).setResultsName('key')
            operatorBraceLeft = Literal('{').setResultsName('brace_left').suppress()
            operatorBraceRight = Literal('}').setResultsName('brace_right').suppress()
            operatorAssignement = Literal(':').setResultsName('assign')
            operatorRegexp = Literal('\\').setResultsName('regexp')
            operatorStatic = Word(printables).setResultsName('static')
            Field << ZeroOrMore(operatorStatic) + Group( operatorBeginField + \
                           (operatorKey | (operatorBraceLeft + operatorKey + operatorAssignement + Word(alphanums).setResultsName('default') + operatorBraceRight)) ).setResultsName('field')+ ZeroOrMore(operatorStatic) 
    
            return Field.parseString
    
        def Parse(self, text):
            #print self._parser(query)[0]
            #return self._parser.ParseString( text ) 
            return self._parser(text)
    
    intext = 'whatever${1:key} whatever2 ${2:'${3:value}' } $1 $0'
    
    s = SnippetParser()
    print s.Parse( intext )


#### 2007-03-31 02:42:06 - ptmcg
Yes this is absolutely possible with pyparsing.  The key to most templating applications is to look at using transformString to perform substitutions for matched string patterns.



Here is a first cut at your problem:



    intext = 'whatever ${1:key} whatever2 ${2:'${3:value}' } $1 $0'
    
    from pyparsing import *
    substitution = '$' + (Word(nums).setResultsName('key') |
                         '{' + Word(nums).setResultsName('key') + ':' + 
                            ( quotedString | SkipTo('}') ).setResultsName('defaultValue') + 
                         '}' )
    
    def replaceSubs(tokens):
        key = int(tokens.key)
        if key == 0: 
            return ''
        elif key in subs:
            return subs[key]
        elif 'defaultValue' in tokens:
            subs[key] = tokens.defaultValue
            return tokens.defaultValue
        return None
    substitution.setParseAction(replaceSubs)
    
    # use empty subs dict to just get defaults
    subs = {}
    pass1 = substitution.transformString(intext)
    print pass1
    
    # now define values for pass 2
    subs = { 1 : 'foo', 20 : 'bar', 3: 'something'}
    pass2 = substitution.transformString(pass1)
    print pass2



printing:



    whatever key whatever2 '${3:value}' key 
    whatever key whatever2 'something' key 



Was that too much of a hint?



-- Paul

---
## 2007-03-31 02:32:05 - shinewu - I am a little confused with setParseAction
First of all, pyparsing is great.

I try to build some compiler prototype with Python. And I think pyparsing is the easiest and definitely cleanest parser available for Python.

However I get confused about setParseAction. You know in yacc, you can build the tree data structure from bottom up. For example:



expr = expr + expr : $$ = $1 + $2



In this rule, you can easily get the child pointer. With pyparsing, I could not figure our an easy way to mimic this. What I can do is to build the data structure from top down. I.e. I recursively go through the result returned by parseString and build each data structure accordingly. I figure this is pretty wasteful, because all this structure info has been built once by pyparsing. If I dive once more, that should be kind of silly.



I look at the SimpleCalc.py. The author is actually doing the wasteful thing. I.e. pushing everything onto a stack and evaluating the stack recursively ( top-down ), where a bottom-up approach should be much more efficient.

#### 2007-03-31 02:45:39 - ptmcg
I'm not completely sure I understand your question, but try doing this in your program.  At the beginning of your parse action, print out a dump of the tokens argument, that is try something like this:



    def parseAction(tokens):
        print tokens.dump()  # <-- insert this line



If your tokens has some structure to it, you should be able to see what you have when the parse action is called.



-- Paul
#### 2007-03-31 12:34:37 - shinewu
This is working in a limited sense. What would be the best is to support the Yacc style.



For instance:

expr = expr + expr



It would be best if the action handler can have the pointer to the two sub-expression and the char '+' instead of '(a + b - 1 ) * 2 + ( f ** g << 5 )'.
#### 2007-03-31 14:30:22 - ptmcg
Are you looking at the right argument to the parse action?  The first argument contains the full string being parsed, the second isthe current parse location, and the third is the parsed tokens, which should contain the separate elements broken out into their own ParseResults (which you can access just like a list).



-- Paul
#### 2007-03-31 21:38:46 - ptmcg
Could you post some of your grammar, and the parse action that you are using?

---
## 2007-04-01 11:00:08 - frediz - Matching names_with_space within known sentences
I've written a parser with pyparsing, which can read

text files where each line is, more or less, like :

<name> <action>



For ex :

name = Word(alphanums)

rule_eat = name+ Literal('is eating')

would match 'John is eating'



For each action, I wrote a rule : rule_<action>

Each line, is parsed with all the action rules :

rule_eating | rule_walking |....

This used to work until I encountered names with spaces

'John Doe is eating'

since the expression for names was :name = Word(alphanums)



I tried things like 

name = Word(alphanums+' ')

but then this expression matches the name plus part

or the whole action string, and the parser fails.

The problems seems that the parser tries to match

as far as it can (left recursivity notion ?), without

the knowledge of the next expression to match else it 

would know that the end of the string is either 'is

eating' or 'is walking' etc and it would guess the name words.

Is this right ? and how can I solve that ?

Thanks for your help!

#### 2007-04-01 12:15:52 - ptmcg
Yes, this is exactly right, and it is one of the biggest conceptual differences between pyparsing and regular expressions.



When you read 'John Smith is walking', how do you know where the name ends and the action begins?  You could say names are made up of one or more capitalized words - to define a capitalized word, use the 2-argument form of the Word constructor:



    Word(setOfInitialCharacters, setOfBodyCharacters)



as in:



    caps = srange('[A-Z]')
    lowers = caps.lower()
    properNoun = Word( caps, lowers )
    personName = OneOrMore(properNoun)
    # or to return a single string with spaces between the words
    personName = Combine( OneOrMore(properNoun), joinString=' ', adjacent=false )



This will work with most English-style names, but you may have problems with Pierre de Fountainbleu.  If you have to work with names that have uncapitalized parts, then you will need to do some sort of lookahead, probably using NotAny.  Let's say all of your actions start with 'is'.  In that case, define name as



    namePart = NotAny('is') + Word(alphas)
    name = OneOrMore(namePart)



Now, parsing a name will check before accepting a word whether it is the word 'is'.  If you have a set of such words, try NotAny( oneOf('is has gives takes') ), for example.



Will this help you move forward?



-- Paul
#### 2007-04-01 13:41:53 - kib2
Paul,



that's what I've done like this :



class PlaceHolder(object):

    '''A placeholder is a part of text wich can be edited dynamically

    '''

    def <u>init</u>(self, id, contents='', brut='', start=0,end=0):

        '''A placeholder has a unique ID number, and maybe some mirrors. 

        '''

        self.id = int(id)

    self.contents = contents

    self.brut = brut

    self.start = start

    self.end = end



    def <u>str</u>(self):

        return 'ID: %d CONTENTS: '%s' BRUT: %s START %s END %s'\

           %(self.id, self.contents, self.brut,self.start,self.end)



<hr >


Now, in place of 'def replaceSubs(s,l,tokens):...' :



def constructPHR(s,l,tokens):

    key = int(tokens.key)

    brut = ''.join( tokens)

    ph_long = len( brut )

    st = int(l)

    if key == 0:

        return PlaceHolder(key,'', brut, st, st+ph_long)

    elif 'defaultValue' in tokens:

        subs[key] = tokens.defaultValue

        return PlaceHolder(key,tokens.defaultValue, brut, st, st+ph_long)

    elif key in subs:

        return PlaceHolder(key,subs[key], brut, st, st+ph_long)

    return None



<hr >
Finally, I obtained my PlaceHolders list like this :



subs = {}

temp_PH=[]

substitution.setParseAction(constructPHR)

for el in substitution.scanString( intext ): temp_PH.append( el )



PH = [el[0][0] for el in temp_PH ]

for el in PH : print el



<hr >
here's the output :



ID: 1 CONTENTS: 'key' BRUT: ${1:key} START 9 END 17

ID: 3 CONTENTS: 'value' BRUT: ${3:value} START 33 END 43

ID: 6 CONTENTS: 'value' BRUT: ${6:value} START 51 END 61

ID: 1 CONTENTS: 'key' BRUT: $1 START 63 END 65

ID: 0 CONTENTS: '' BRUT: $0 START 66 END 68
#### 2007-04-01 14:15:34 - ptmcg
Kib2 -



Did you reply to the right post?  If you repost to your other thread, I'll delete this one.



Also, when you post code, surround it with '' tags, so that the formatting is preserved.



As to your approach to compute the length of the input tokens, I think this will be okay *as long* as you are careful not to insert any spaces inside your template placeholders.  I think the safer approach will be to use the soon-to-be-published getTokensEndLoc() method.



-- Paul
#### 2007-04-01 14:39:51 - kib2
Sorry Paul, I missed the code tag.

When did you project to publish the new getTokensEndLoc() method, I really need spaces inside my placeholders ?
#### 2007-04-01 14:44:02 - frediz
As you forsaw it, I can have names with uncapitalized parts.

Actually names can also be nick names..

But 'NotAny' does the job!

I just hope, I won't have nicknames with the action verbs used in the rules. :)

I will then use longer expression than just verbs if

that happens.

Thanks Paul for your help, and this quick answer!
#### 2007-04-02 00:44:00 - ptmcg
Great!, post back if you have more questions, or if you want to post your pyparsing app to the Who's Using Pyparsing page.



-- Paul
#### 2007-04-02 02:20:57 - frediz
Ah, that's a good idea!

It is in early stage for now, but I will post some

words about the app as soon as I have a first version

and showable code :)

---
## 2007-04-08 19:57:13 - felixrabe - pyparsing on launchpad
Hi,



I just created a bug report on Launchpad about 1.4.5 not being included in Ubuntu Feisty.  I had noticed a bug in its 1.4.2 package that is fixed by now (a missing return self statement). - 



The registration process is very simple.  I can recommend setting the project up at launchpad.  I just did the same for PyGTK Shell.

#### 2007-04-08 23:36:43 - ptmcg
Please drop me a note with the registration procedure.  1.4.6 will be ready to go in the next week or so, and will be a good test.



-- Paul
#### 2007-04-10 15:31:19 - felixrabe
- Open 

   (Also see the nice guide mentioned on the front page:  )

- Click on 'Register'

- Enter 'pyparsing', click 'search'

  => 'no projects found'

- Click 'Register a project'

- Get a Launchpad account by entering your email address in the lower part - at this point I enter my login details in the upper part

- (You might have to return to  , but I don't)

- Fill in the form



My project is at  - yours would be at  I guess.
#### 2007-04-10 15:35:14 - felixrabe
Actually, the interface of Launchpad is *very* well documented - self-explaining and full of concise examples that tell you what you need to know, not more and not less.  You will feel well guided there.
#### 2007-04-11 10:32:52 - ptmcg
Thanks, I'll add it to my 'things to do upon new releases' list.



-- Paul
#### 2007-04-11 23:36:31 - ptmcg
Ok, I set up my project, but what else do you want here?  I don't really want to duplicate the facilities (bug tracker, discussion lists, wiki) that I already have on SF or wikispaces.
#### 2007-04-13 06:43:24 - felixrabe
You do not have to duplicate anything.  Bug tracking is integrated SF -> Launchpad, such that Launchpad bugs can refer to SF bugs (see the guide).  I don't know about discussion (other than the 'blueprints' stuff) or wiki features.  You can just have a look at what Launchpad offers and 'cherry pick' the features that you might be interested in later on.  (My strategy atm.)



In the meantime, just being present there might offer people an opportunity to give feedback in one more way and stay up-to-date about the project state.



I'm a bit less enthusiastic about it now that I found out yesterday (or so) that Launchpad is proprietary software 'to be released as FLOSS one distant day in the future'.  I don't feel too uneasy about that as I was already wondering whether Canonical was making enough money just from Ubuntu (and Launchpad seems to be an interesting revenue stream for them and I think they are doing a very nice job overall), and the LP export facilities seem to be exhaustive (XML dumps and stuff) - I did not investigate it close though.

---
## 2007-04-09 13:11:45 - gunars - preserving original text
I'm trying to parse Java resource bundles, resource files with key/value pairs of the general form:



     { 'key1', 'value string 1' },



These can have more involved forms, such as:



     { 'key2' + KEY_SUFFIX,  'value string 2 ' +

            'continued on several lines' },



My goal is to pick out the keys and values which will then be used to look up substitution strings that will replace the original value strings (looked up by functions called via setParseAction).  pyparser does a very nice job of cleaning out whitespace and comments, but in this case, I would like to preserve as much of them as possible so that I can create output that is identical to the source with the exception of the substituted value strings.  I have two questions:



1) I've managed to retain the whitespace info by using .leaveWhitespace() and then sprinkling 'Optional(White()' elements in the grammar (yes, very ugly).  Is this the best way to do this?



2) As for the cpp-style comments in Java, I can use '.ignore(cppStyleComments)' to ignore them, but I haven't found a way to preserve them.  I certainly wouldn't want to add elements to the grammar in each spot where comments could possibly be used.  Any hints?



In general, this is my first attempt at pyparsing and I'm perhaps overlooking a better way to approach this problem, so any hints are appreciated.

#### 2007-04-09 23:13:38 - ptmcg
Are you using transformString to do the substitutions? Perhaps you could post a little code (be sure to enclose in \[\[code\]\] tags).
#### 2007-04-10 21:35:38 - gunars
Here's a sample. The input data is embedded. The output is more or less ok, except that the single and multi-line comments are absorbed.  I would like some way to have them appear also, if possible. Any other comments or suggestions about the code are welcome since I'm rather new to this. Thanks!





    from pyparsing import *
    
    def keylookup(s,l,t):
        print '<keylookup> t:',t
    
    def vlookup(s,l,t):
        # Normally this would do a db lookup
        print '<vlookup> T:',t
        t[0] = ''' + 'ABC-' + t[0] + '-XYZ' + '''
        return  t[0]
    
    def jparse():
        efile = '''
            { 'key1',   'val1' },
            // single line comment
            { 'key2',   'val2 is a very ' +
                        'long value '  +
                        'that is continued ' +
                        'over several lines'        },
            /* multi-line commented-out section
            { 'key3',        'val3' },
            */
            { 'key4',        'val4' }, 
        '''
    
        # Start grammar
        lbrace = Literal('{')
        rbrace = Literal('}')
        comma = Literal(',')
        plus = Literal('+')
    
        key = Optional(White()) + \
            quotedString.setResultsName('key').setParseAction(keylookup) + \
            Optional(White())
    
        vqs = quotedString.copy().setParseAction(removeQuotes)
        # The following line handles multi-line values.
        value_qs = Combine( vqs + ZeroOrMore(plus.suppress() + vqs), adjacent = False )
        value_qs.setParseAction(vlookup).setResultsName('val')
        value = Optional(White()) + value_qs + Optional(White())
    
        definition = ZeroOrMore(Optional(White()) + lbrace + key + comma + \
                                value + rbrace +Optional(White()) + Optional(comma))
        definition.ignore( cppStyleComment )
    
        results = definition.parseString(efile)
        print 'RESULTS:\n', ''.join([t for t in results])
    
    def main():
        jparse()
    
    if __name__ == '__main__':
        main()



The current output is:





    { 'key1',   'ABC-val1-XYZ'},
            { 'key2',   'ABC-val2 is a very long value that is continued over several lines-XYZ'        },
            { 'key4',        'ABC-val4-XYZ'},


#### 2007-04-12 22:45:25 - gunars
Any thoughts about retaining those comments?
#### 2007-04-13 07:25:49 - ptmcg
Yes, I think you would be better off with one of the other string processors than parseString, probably transformString or scanString.  parseString requires that you define a complete and exhaustive grammar.  To do the kind of substitution you describe, transformString is a better choice - you only need to define the pattern you are substituting, and the rest of the input text remains intact.



There is a new example that does something very similar, included with the latest version (1.4.6).  You can see it online at 



-- Paul
#### 2007-04-16 11:38:59 - gunars
I'm not sure if this make sense, but can White() be augmented to match comments as well as regular whitespace?
#### 2007-04-16 14:39:29 - ptmcg
No.  To treat comments in the same ignorable way as whitespace, define your comment expression (or select from one of the predefined forms that comes with pyparsing), and then set your root expression of your grammar to ignore comments using expr.ignore().  The ignore method propagates to contained expressions, so that from the root call, the entire parser expression tree is configured to ignore the same comment expression.



You can see how this is done in idlParse.py.



-- Paul
#### 2007-04-16 14:57:49 - gunars
Yes, I do want to ignore them, but I would like to also retrieve them similarly to matching whitespace via White().



I've already set up the .ignore(javaStyleComment) and that works just fine to skip parsing within comments.  I would also like to retrieve the comments themselves so I can propagate them into the output stream.  I just thought that if comments were treated internally similarly to whitespace, that maybe the White() handling could optionally also match on comments in addition to whitespace.  From a quick look at the code, that doesn't seem to be the case, so I'll stop beating the dead horse :-).  Thanks for your help, Paul.



Gunars
#### 2007-04-16 15:58:52 - ptmcg
If you are doing something like macroExpander, then add javaStyleComment to the beginning of the | expression that you are using to call transformString - do NOT call ignore in this case.  Since there is no attached parse action, then the comments will pass through unchanged, with any embedded text unparsed/unchanged too.



-- Paul
#### 2007-04-18 14:21:48 - gunars
Well, I think I more or less have it working except for one hitch.  First, the code:





    ''' pyparsing tests '''
    
    efile = '''
            { 'key1',   'val1' },
        // This is a single-line comment
    
    /*        { 'key2',   'val2' },
        This is commented   out
    */
            {          'key3',   'val3' },
            { 'key4',   'val4 is a very ' +
                        'long value '  +
                        'that is continued ' +
                        'over several lines'        },
            { 'key5',  new String[]{'90','95','99'}},
            { 'key6',
                new String[]{
                     'val6-1',
    //               'val6-2', 
                     'val6-3',
                }
            }, 
    '''
    
    from pyparsing import *
    
    def keylookup(s,l,t):
        print '<keylookup> t:',t
    
    def vlookup(s,l,t):
        # Normally this would do a db lookup
        print '<vlookup> T:',t
        t[0] = ''' + 'ABC-' + t[0] + '-XYZ' + '''
        return  t
    
    def dlookup(s,l,t):
        # Do what ever is needed for the key/value pair
        print '<dlookup> key:',t.key
        print '<dlookup> val:',t.val
    
    # Start grammar
    lbrace = Literal('{')
    rbrace = Literal('}')
    comma = Literal(',')
    plus = Literal('+')
    
    key =  quotedString.setResultsName('key').setParseAction(keylookup) 
    
    vqs = quotedString.copy().setParseAction(removeQuotes)
    # The following line handles multi-line values.
    vqs_multi = Combine( vqs + ZeroOrMore(plus.suppress() + vqs), adjacent = False )
    vqs_multi.setParseAction(vlookup)
    
    # This handles combo lists of form:   new String[]{ 'a', 'b', 'c'}
    vqs_combo = Literal('new') + White() + 'String[]' + lbrace + \
            ZeroOrMore(vqs.copy().setParseAction(removeQuotes,vlookup) +\
            Optional(comma).setParseAction( lambda t:',  ') ) + rbrace
    
    value =   Combine((vqs_combo | vqs_multi | vqs), adjacent=False).setResultsName('val')
    
    definition = lbrace + key + comma + value + rbrace + Optional(comma)
    definition.setParseAction(dlookup)
    def2 = javaStyleComment | definition
    
    results = def2.transformString(efile)
    print 'RESULTS:\n',results



This properly picks up each key/value pair, replacing the value with a modified value.  Comments and leading whitespace are being properly echoed out.  The only problem is the definition of 'vqs_combo'.  This handles values of the form:





    new String[]{ '90', '95', '99'}



The subvalues can be spread over several lines.  This works ok, except when one of the lines is commented out (e.g. 'val6-2' in the test data).  This knocks out the parse of the entire set.  I can't seem to figure out how to handle this.  Any clues?



Thanks

Gunars
#### 2007-04-18 18:34:09 - ptmcg
From your code, it looks like vqs_multi has the same problem.  I would normally say to just call do something like definition.ignore(javaStyleComment), but then the comments get stripped out too.  What if you did something like call ignore, and then add the parse action keepOriginalText.  keepOriginalText would restore any ignored comments.



-- Paul
#### 2007-04-18 21:32:43 - gunars
Yes, but the keepOriginalText prevents the substituted values from appearing in the output token stream - I get just the original tokens.
#### 2007-04-19 08:25:04 - ptmcg
Hmm, how about this?

1. Use a grammar, with ignore(javaComment) defined, to detect the 'key = value'-type expressions.

2. Add keepOriginalText as a parse action

3. Add a second parse action to perform the substitutions, using transformString on the full original text provided by keepOriginalText



IIRC, the macroExpander.py example (new in 1.4.6) works this way.



It's kind of an ugly two-step, but actually little different from C preprocessors' separate passes through C code to do macro expansion before passing the results on to the compiler.



-- Paul

---
## 2007-04-09 22:07:41 - gunars - removeQuotes
When I do a '.setParseAction(removeQuotes)', it appears to strip quotes from all quoted strings.  How can I apply it only to some objects? For example:



key = quotedString

value = quotedString.setParseAction(removeQuotes)

definition = key + Literal('=') + value



If my input is:



'some key' = 'some value'



I would like the quotes removed only for the values and not the keys. Can this be done with removeQuotes?



Thanks

Gunars

#### 2007-04-09 22:12:30 - gunars
Let's try that code fragment again...





    key = quotedString
    value = quotedString.setParseAction(removeQuotes)
    definition = key + Literal('=') + value


#### 2007-04-09 23:14:56 - ptmcg
Use copy to create a local version of quotedString before adding the parse action.





    value = quotedString.copy().setParseAction(removeQuotes)


#### 2007-04-10 08:39:14 - gunars
Thanks, Paul!  That did it.

---
## 2007-04-10 20:26:10 - gunars - SkipTo and include
Hi!



I'm trying to parse a file where the significant records have a number of lines before and after them that I want to ignore for parsing but echo out. I'm using SkipTo() with a results name attached to it so I can retrieve the text that was skipped. I'm using the 'include=True' parameter to SkipTo which causes the token to be consumed, but it doesn't make it into the named results. In the example code, the SkipTo target is the '=' -- it does get consumed, but is not in results.head:





    from pyparsing import *
    
    key = Word(alphanums)
    value = Word(alphanums)
    definition = Group (key + value)
    
    glossary = OneOrMore (definition)
    glossfile = SkipTo('=', include=True).setResultsName('head') + \
    '{' + glossary + '}' + SkipTo(StringEnd()).setResultsName('tail')
    
    s = '''ignorable line 1
    ignorable line 2
    label array =
    {
        key1 value1
        key2 value2
        key3 value3
    }
    ignorable line 3
    ignorable line 4
    '''
    
    
    def main():
        results = glossfile.parseString(s)
        print 'HEAD:',results.head
        print '\nTAIL:',results.tail
    
    if __name__ == '__main__':
        main()



The output is:

[[code}}HEAD: ignorable line 1ignorable line 2label arrayTAIL: ignorable line 3ignorable line 4



Can anyone explain why this is? Thanks!



Gunars

#### 2007-04-10 20:27:28 - gunars
Let's try that pesky output again ...





    HEAD: ignorable line 1
    ignorable line 2
    label array
    
    TAIL: ignorable line 3
    ignorable line 4


#### 2007-04-10 22:38:32 - ptmcg
This looks to me like a bug in SkipTo.  But I would like to clarify that my intent, in the event when include=True, was always to return a pair of tokens, one for the skipped text and one for the text that was skipped to.  Sometimes the text that is skipped to is not so easily discerned - it could be a complex expression that is skipped to.  This is why I return them as two separate tokens.



So I have a working version with this bug fixed that returns a two-element ParseResults - in this case it looks like:



    ['ignorable line 1\nignorable line 2\nlabel array ', '=']



If you want them returned as a single string, wrap the SkipTo in a Combine, as in:



    glossfile = Combine(SkipTo('=', include=True)).setResultsName('head') + \
        '{' + glossary + '}' + SkipTo(StringEnd()).setResultsName('tail')



which returns:



    HEAD: ignorable line 1
    ignorable line 2
    label array =
    
    TAIL: ignorable line 3
    ignorable line 4



-- Paul
#### 2007-04-10 23:03:17 - gunars
Thanks, Paul!  I just wanted to make sure I wasn't misunderstanding something.
#### 2007-04-11 07:19:02 - ptmcg
No, you understood my intent, despite the faulty implementation. :)  This bugfix will be included in 1.4.6. (...any day now...)



-- Paul
#### 2007-04-11 22:24:42 - gunars
Here's a slightly different scenario. The following code works ok to handle the lines at the beginning of the file. What I don't understand is why I seem to need the 'Optional(White())' terms in the definition of 'def_array_beg'. If I remove any of them, the code fails to find the pattern.





    from pyparsing import *
    
    def dlookup(s,l,t):
        print '<dlookup> t:',t
    
    key = Word(alphanums)
    value = Word(alphanums)
    definition = Group (key + value).setParseAction(dlookup)
    
    glossary = OneOrMore (definition)
    
    def_array_beg = 'label' + Optional(White()) + 'array' + Optional(White()) + '=' + Optional(White()) + '{'
    #def_array_beg = 'label' + 'array'  + '=' + '{'
    
    glossfile = Combine(SkipTo(def_array_beg) + def_array_beg).setResultsName('head') + \
     glossary + '}' + SkipTo(StringEnd()).setResultsName('tail')
    
    s = '''ignorable line 1
    ignorable line 2
    
    label array =
    {
        key1 value1
        key2 value2
        key3 value3
    }
    ignorable line 3
    ignorable line 4
    '''
    
    
    def main():
        results = glossfile.parseString(s)
        print '\nHEAD:',results.head
        print '\nTAIL:',results.tail
    
    if __name__ == '__main__':
        main()


#### 2007-04-11 22:37:41 - ptmcg
Combine by default requires all of its component expressions to be adjacent.  Try one of the following:

1. add 'adjacent=False' to the Combine constructor, as in



    glossfile = Combine(SkipTo(def_array_beg) + def_array_beg,adjacent=False).setResultsName('head') +  
       glossary + '}' + SkipTo(StringEnd()).setResultsName('tail')



2. Remove Combine - why do you need it?



3. Upgrade to 1.4.6, SkipTo(xxx,include=True) is fixed.



4. I see from your comment that you also tried



    def_array_beg = 'label' + 'array'  + '=' + '{'

This is just string concatenation! Same as





    def_array_beg = 'labelarray={'



If you start off with a Literal, then you can use the 'add strings to ParserElements' short cut:



    def_array_beg = Literal('label') + 'array'  + '=' + '{'



Then you wont need the Optional(White())'s in there, pyparsing will skip the whitespace for you (assuming you got rid of the Combine, or added adjacent = False).



-- Paul
#### 2007-04-11 22:40:44 - gunars
Thanks, Paul! I'll give that a shot.



Gunars

---
## 2007-04-10 23:10:22 - michelp - searchparser in pyparsing package
Hey guys!



I've found searchparser.py useful in several different contexts, but i keep having to copy the class around to override it.  I'd love to see it in pyparsing proper.  Thanks!



-Michel

#### 2007-04-11 07:28:06 - ptmcg
Ok, here is my dilemma: one of things I <em>really</em> like about pyparsing is that it has this easily used, single-file footprint.  You want to include it in your project?  Just drop it into your directory/PYTHONPATH/site-packages.



But things have evolved since pyparsing's early days, and it may be time to consider an applications package that contains some typical parsers.  (I'd still leave core pyparsing as it currently exists.) searchparser is a likely one, simpleArith is another.  The other significant aspect to this is how to define the API to these standard parsers, so that they can be customized to the particular user application.  In your case, Michel, you override methods of the searchparser class, and this may be a good model to start with.  But it involves a little class design thought up front.



I'd be interested in other feedback on typical parsers to include in a pyparsing-applications package, and also input on likely user customization requirements for searchparser and arithparser as input to a class API design.



-- Paul
#### 2007-04-12 16:41:29 - michelp
Ouch, I didn't realize the code formatter would actually concat lines with '\' in them.  To answer some of your question Paul, my opinion is there should be another library, pyparsers, that depends on pyparsing, that contains a handfull of useful parsers for example and real world use.  This keeps the single file approach and those who don't need pyparsers can just go on without it.



I think the api should be dirt simple, each parser is a class (a QueryParser if we follow searchparser pattern) which when instantiated calls its parser() method to define a _parser attribute.  Calling a QueryParser in turn calls its _parser.parseString with the provided arguments.
#### 2007-04-12 18:38:35 - ptmcg
Michel -



I've added a Pyparsers page to the public side of this wiki, so that people can freely submit candidate parsers.  It also allows you to attach files to your submissions (as I have done with the Examples page), so we don't have to deal with the formatting weirdness of these discussion pages.



I think customization to some extent will depend on the parser in question.  You designed your parser to be extended by inheritance, with keywords defined as class-level elements.  I think an arithmetic evaluator could be extended by adding user-defined built-in functions (functions for financial or loan amortization arithmetic could be very different from functions for 3D geometry or calculus or scheduling or...).  A parser might be extended by being paired with a Handler class, which could be subclassed by the customizer and provided to the parser using something like addHandler (similar to HTTP server classes).



In any event, would you care to take a stab at updating the Pyparsers page with your first submission?  At which point, I think I'll delete your post+code, to restore some readability to this page.  :)



-- Paul
#### 2007-04-12 18:55:20 - michelp
I added the code to that wiki page, but I didn't see a way to 'attach' the file, so again I pasted it, I guess better there then here.  I'm not against web discussion forums, i just don't use them a lot. ;)



What I'm working on (a common query language for zope 3's catalog via hurry.query and a xapian language) is evolving quickly based on searchparser.py, I've added range support 'help..me' and probably will add more syntax as I fill out the common features of both systems.   Keyword and range support you might want, but at some point the line might be crossed, this is the tough bit of providing a query parser library, its hard to draw this line.



Anyway, I'll think more about it and post some ideas on the wiki page.
#### 2007-04-12 18:59:16 - ptmcg
The 'file attachment' features are buried in with the 'image attachment' button.

---
## 2007-04-12 22:03:22 - shinewu - A little feature request of delimitedList
Sometimes, we don't want to suppress the delimitor. Currently the function delimitedList does not support this natively. I propose following tiny hack to make it work:



<h6 id="toc0">========================================</h6>
def delimitedList( expr, delim=',', combine=False, suppress=True ):



...

    else:

        if suppress :

            return ( expr + ZeroOrMore( Suppress( delim ) + expr ) ).setName(dlName)

        else :

            return ( expr + ZeroOrMore( delim + expr ) ).setName(dlName)

#### 2007-04-12 22:18:59 - ptmcg
Just cause I'm curious, what is your application in which you want to keep the delimiter?



Note that if you set combine=True, you get the delimiters and the values all combined into a single string.



    from pyparsing import *
    
    ipAddress1 = delimitedList(Word(nums),delim='.')
    ipAddress2 = delimitedList(Word(nums),delim='.',combine=True)
    
    for parser in (ipAddress1,ipAddress2):
       print parser.parseString('127.0.0.1').asList()



Prints:



    ['127', '0', '0', '1']
    ['127.0.0.1']



If you really want the delimiter, you could just use the 'expr+ZeroOrMore(delim + expr)' explicitly in your code.



-- Paul
#### 2007-04-16 11:28:02 - shinewu
My application is actually a simple calculator.

I did something like:



<ol><li>DList is an alias of dimilitedList</li></ol>

MUL_OP            = oneOf( '* /' )

ADD_OP            = oneOf( '+ -' )



NUM_EXP_ITEM      = DList( NUM_ITEM, EXP_OP )

NUM_MUL_ITEM      = DList( NUM_EXP_ITEM, MUL_OP )

NUM_ADD_ITEM      = DList( NUM_MUL_ITEM, ADD_OP )

NUM_EXPRE_f      << DList( NUM_ADD_ITEM, SHIFT_OP )



Basically I try to emulate the ranks of different operators. Note '+' and '-' are both ADD_OP, so if I suppress ADD_OP -- the separator of NUM_ADD_ITEM -- I would not be able to tell whether I should do an addition or subtraction. Same thing goes on with MUL_OP since it has two operators '*' and '/'.



I thought my application is common enough, but looks like not too many people are doing it this way. Since the latest version has support for operator ranks, maybe I just need to convert.



But this issue certainly persists.
#### 2007-04-16 14:35:38 - ptmcg
Hmm, I think you are better off redefining DList to your own form, *not* suppressing the 'delimiter'.



For the traditional way of implementing this kind of arithmetic parser, see fourFn.py in the pyparsing examples directory.  For the newer style, look at simpleArith.py.



-- Paul
#### 2016-06-01 15:39:55 - sanghee.kim
'If you really want the delimiter, you could just use the 'expr+ZeroOrMore(delim + expr)' explicitly in your code.'



It's very helpful to me. Thank Paul.

---
## 2007-04-13 07:30:58 - Claudiu - compact match, separate tokens
Hello,

I am trying to match things like 'x = a.3' without spaces in 'a.3' but I want to get

them as separate tokens: ['x', '=', 'a', '.', 3]

Group is not very useful in this case because joins all together in a string 'a.3' (and I do not want that)

What is the proper way to do it in pyparsing?



    from pyparsing import *
    number = Word(nums).setParseAction(lambda t:int(t[0]))
    expr = (Word(alphas) + '=' + Word(alphas) + '.' + number)
    test = ['x=a.3',
            'y = a . 3'] #i want this to fail



#### 2007-04-13 08:06:00 - ptmcg
First of all, pyparsing is naturally blind to whitespace, so your error case would look just fine.  But there are ways to suppress pyparsing's natural leniency with whitespace.



The concept is that you want the literal '.' and the following number to **not** skip over whitespace before trying to match.  To do this, use leaveWhitespace.  



    expr = (Word(alphas) + '=' + Word(alphas) + 
       Literal('.').leaveWhitespace() + number.copy().leaveWhitespace())



Since leaveWhitespace is a mutator, and you may have other number instances where you don't want this behavior, I made a copy of the number expression to use in this statement.



Hope this helps, write back if you have more questions,

-- Paul
#### 2007-04-13 08:30:46 - Claudiu
Thank you, this does the job.



P.S. I did a mistake above:  It was `Combine` that joins all together in a string, `Group` just creates a sublist.

---
## 2007-04-13 09:25:06 - joncle - prefix and suffixes
I'm not sure if pyparsing is the right tool for the job, so forgive me if what I ask is infact fairly daft.



I have a string whose format is...

'[Prefix] Word [might be additional words] [Suffix]'



[ and ] represent items that may or may not appear in the string.



For example (silly, but representative of real data):

Valid Prefixes = 'the great', 'the' or 'just the'

Valid Suffixes = 'idiot', 'is a fish'



In between these (if they exist) can be any number of words; the thing is, they can also be words that are valid pre/suffixes.



Silly examples 2:

String: 'Just the idiot is a fish'

Result: Prefix='Just the', Words='idiot', Suffix='is a fish'



String: 'the big idiot said the cat is a fish what an idiot'

Prefix='The', Words='big idiot said the cat is a fish what an', Suffix='idiot'



If someone could give a few hints, rather than an actual answer, that'd be most appreciated.



Cheers,

Jon.

#### 2007-04-13 10:45:22 - ptmcg
Jon -



Think about what distinguishes 'idiot' as an internal content word from 'idiot' as a suffix - the suffix 'idiot' is followed by the end of the input string.  Pyparsing provides some classes named FollowedBy and StringEnd.  To ensure that the 'idiot' at the end of the string is **not** mistaken for an internal one, define internal 'idiot' as ~suffix + 'idiot' or 'idiot' + ~FollowedBy(StringEnd()).  Then be sure to finish your overall pattern with a StringEnd() to detect the erroneous string that does not end with a valid suffix.



Just hinting, write back if you want more to go on.



-- Paul
#### 2007-04-16 00:59:33 - joncle
Thanks for the response Paul. I'll check out FollowedBy and StringEnd, have a little play and see what I come up with. I had a gut feeling I'd want something like 'if it's not followed by... and it's the end of....' etc.. etc.., I just didn't know how to express it in pyparsing. 



I'll post again if I get sorely stuck, otherwise I'll post the code (which, most likely, will end up embarrasingly simple) so others can review.



Many thanks again.





Jon.
#### 2007-04-16 08:32:29 - ptmcg
Please browse through the HTML docs that ship with pyparsing.  (If you just downloaded the Windows binary installer, go back to SourceForge and get one of the source packages or the docs package.)  



-- Paul

---
## 2007-04-15 04:34:12 - Claudiu - getting original matched string
In a parseAction function, how do I obtain the whole initial string(token) that matched?

I have the whole string in `s`, the start position in `l`; is there a way to find out the end position, or the length?

In the example below, I would like to have the token clean (I suppressed the unintresting stuff for processing(like '0x')), but in some cases I would like to also have the full string (like: '0x38a')

Is there a way to obtain it?



And a minor issue: If I do a stupid mistake in the parseAction function (like trying to access t[10]),

it raises a `ParseException`, like it has found no match; I think it could be more useful to rasise another exception, like 'UserError'.





    from pyparsing import *
    
    def doHex(s, l, t):
        print s, l, t
        t[0] = int(t[0], 16)
        return t
    
    hex_c = Combine(Optional('#').suppress() + CaselessLiteral('0x').suppress() + Word(hexnums))
    hex_a = Combine(Optional('#').suppress() + Word(hexnums) + CaselessLiteral('h').suppress())
    hex = (hex_a | hex_c).setParseAction(doHex)
    
    expr = (Word(alphas) + Literal('=') + hex + Literal('+') + hex)
    
    test = ['x = 0x3a + 8bh']
    
    for t in test:
        print t
        try:
            ret = expr.parseString(t)
            print ret
        except ParseException, pe:
            print t, 'ERROR'
            print >>sys.stderr, ' '*(pe.column-1) + '^'
        print



#### 2007-04-15 06:03:14 - Claudiu
I solved it by modyfing `pyparsing` (in `_parseNoCache`) and calling with a tuple `(tokensStart,loc) ` instead of just {tokensStart}}. The other routines in pyparsing seams not to use at all the fist two parameters of `parseAction`, so no code was broken.
#### 2007-04-15 08:11:39 - ptmcg
The latest release of pyparsing includes a function you can call from within your parse action, called getTokensEndLoc for just this purpose.  Rather than modifying your pyparsing source, try using this function in your parse action.





    from pyparsing import *
    
    def pa(strng,startLoc,tokens):
        endLoc = getTokensEndLoc()
        print tokens, 'extracted from',strng[startLoc:endLoc]
    
    elem = ( OneOrMore('A') | OneOrMore('B') ).setParseAction(pa)
    grammar = OneOrMore(elem)
    
    grammar.parseString('AAABBABBAAABB')
    

prints



    ['A', 'A', 'A'] extracted from AAA
    ['B', 'B'] extracted from BB
    ['A'] extracted from A
    ['B', 'B'] extracted from BB
    ['A', 'A', 'A'] extracted from AAA
    ['B', 'B'] extracted from BB

-- Paul
#### 2007-04-15 08:19:48 - ptmcg
Here is your modified example, with two different ways to address your question.  The first uses `getTokensEndLoc`, the second uses the pyparsing-supplied parse action `keepOriginalText`:





    def doHex(s, l, t):
        print s, l, t
        print '**',s[l:getTokensEndLoc()]
        t[0] = int(t[0], 16)
        return t
    
    def printToken(s,l,t):
        print '>>', t
    
    hex_c = Combine(Optional('#').suppress() + CaselessLiteral('0x').suppress() + Word(hexnums))
    hex_a = Combine(Optional('#').suppress() + Word(hexnums) + CaselessLiteral('h').suppress())
    hex = (hex_a | hex_c).setParseAction(doHex,keepOriginalText,printToken)



-- Paul

---
## 2007-04-28 09:04:38 - propell - Error in link to ONLamp
There is a traling ) in the linkt to the ONLamp article. Gives an internal server error page.



- Kjell Magne Fauske

#### 2007-05-06 13:58:58 - ptmcg
Thanks, I converted the URL to a link, so this should work now.



-- Paul

---
## 2007-05-01 11:24:05 - shinewu - A bug in error msg
Try following:



<ul class="quotelist"><ul class="quotelist"><ul class="quotelist"><li>from pyparsing import *</li><li>da = oneOf( 'aa' ).setName( 'Double A' )</li><li>da.parseString( 'a' )</li></ul></ul></ul>...

pyparsing.ParseException: Expected Double A (at char 0), (line:1, col:1



This is correct.



Let's see another case:



<ul class="quotelist"><ul class="quotelist"><ul class="quotelist"><li>from pyparsing import *</li><li>da = oneOf( 'aa', caseless=True ).setName( 'Double A' )</li><li>da.parseString( 'a' )</li></ul></ul></ul>...

pyparsing.ParseException: Expected 'aa' (at char 0), (line:1, col:1)



Now the error msg is rather useless. It should report with the symbol name 'Double A'.



I think there could be many similar problems in error reporting. But this is the easiest way to reproduce it. In short, all error msg should be generated lazily.





    def __init__( self ) :
        self.msg = 'Expected ' + self.name # this is problem some
    
    # We should have used
    def msg( self ) :
       return 'Expected ' + self.name



#### 2007-05-01 13:15:14 - ptmcg
Shinewu -



Thanks for reporting this.  This is an interesting little problem, actually.  This circumstance comes out of an effort at optimizing run-time (that is, parse time for pyparsing) behavior, by pre-calculating as much as possible during parser construction, and just reusing the pre-calculated results at parse time.  This is why pyparsing pre-constructs exception objects, and raises them again and again, rather than create new exceptions every time one is needed, just to have it GC'ed milliseconds later.  Since pyparsing uses exceptions and the call stack to keep track of where it is in the grammar, and which alternatives have matched or not, pyparsing can generate many thousands of exceptions in a given application.  By using a pre-constructed exception, we take advantage of the fact that, for any particular expression, the only part that changes all that often is the exception location - the error message of the exception doesn't really change at all at parse time, so it is a saving to pre-calculate this message before parsing begins.



In this particular example, we aren't quite seeing the lazy/non-lazy behavior you describe.  Instead, it is the different implementation of oneOf, depending on whether it is caseless or not.  When caseless is False, oneOf constructs a Regex of the alternatives, after reordering to prevent a short alternative from masking a longer one.  When caseless is True, oneOf skips the Regex step, and creates a MatchFirst instead.  MatchFirst has different exception raising behavior, in that it reraises the exception of the subexpression that matched the furthest.  In your example, there is only one subexpcession, a CaselessLiteral('aa'), so that is the one that gets raised.  The error message for this is just 'Expected 'aa'', so that is what we get.



I have a couple of ideas on how to address this, such as a lazy init of the error message, but I will also have to change the oneOf implementation in the caseless case, to generate a Regex instead of a MatchFirst.  Otherwise, I'll just be back where I started.



Thanks for the feedback, I'll clean this up for the next release,

-- Paul
#### 2007-05-02 15:12:56 - shinewu
Thanks for the clarification. I assume you have some profiling data to support your design decision. The issue I discovered here has nothing to do lazy initialization of error msgs.

The ParseException object rarely gets generated more than a few dozen times. Most parsing programs give up after a few errors. Are you internally using these exceptions to signal something? If so, probably a redesign will help much more than caching the objects.

Just my two cents. Thanks a lot for the wonderful job.
#### 2007-07-26 20:41:27 - Foxbuntu
Was there a resolution to this? I am implementing this method as well and receiving the same error.
#### 2007-07-27 00:16:09 - ptmcg
This was fixed in 1.4.6.  Here is the actual code that shinewu tried to post:



    from pyparsing import * 
    da = oneOf( 'aa' ).setName( 'Double A' ) 
    da.parseString( 'a' )
    print da,type(da)

With version 1.4.6 or higher, prints this exception:



    pyparsing.ParseException: Expected Double A (at char 0), (line:1, col:1)



As you can see, the exception message is giving the right name for the missing expression.



-- Paul
#### 2007-07-27 06:46:19 - Foxbuntu
So, as  newbie to Pyparsing, how do I handel this exception type so that the program will continue to parse is the output is infact grabbing what I am looking at?
#### 2007-07-27 06:57:42 - Foxbuntu
Sorry for the ambigious post. Got bothered while posting. I am using the following code:



    from pyparsing import * 
    import os
    import sys
    import string
    
    def subr()
    
    da = oneOf( ('aa bb cc'), caseless=True ).setResultsName( 'Double Leters' ) 
    db = oneOf( ('11 22 33'), caseless=True ).setResultsName( 'Double Numbs' )
    all = OneOrMore(da | db)
    
    return all 
    
    somefile = open('somefile.txt', 'r')
    for line in somefile:
        results = subr().parseString(line)
        print results
    



And then I receive the above noted exception.
#### 2007-07-27 12:17:27 - ptmcg
What does somefile.txt contain?  Can you post it as well, or inline it in your sample code?  By using parseString, pyparsing assumes that somefile.txt contains nothing else but double letters and double numbers.  If you are trying to extract the doubles from a file containing other stuff, try using scanString or searchString (searchString is simpler to use, it is a simple wrapper around the generator function scanString).



One other note, you are calling setResultsName in your example, when the previous examples were calling setName.  There is a big difference in these two methods.



-- Paul
#### 2007-07-27 12:39:58 - Foxbuntu


    #
    # RC-6 config file
    #
    # source: http://home.hccnet.nl/m.majoor/projects__remote_control.htm
    #         http://home.hccnet.nl/m.majoor/pronto.pdf
    #
    # used by: Philips
    #
    #########
    #
    # Philips Media Center Edition remote control
    # For use with the USB MCE ir receiver
    #
    # Dan Conti  dconti|acm.wwu.edu
    #
    # Updated with codes for MCE 2005 Remote additional buttons
    # *, #, Teletext, Red, Green, Yellow & Blue Buttons
    # Note: TV power button transmits no code until programmed.
    # Updated 12th September 2005
    # Graham Auld - mce|graham.auld.me.uk
    #
    # Radio, Print, RecTV are only available on the HP Media Center remote control
    #
    
    begin remote
    
      name mceusb
      bits           16
      flags RC6|CONST_LENGTH
      eps            30
      aeps          100
    
      header       2667   889
      one           444   444
      zero          444   444
      pre_data_bits 21
      pre_data      0x37FF0
      gap          105000
      toggle_bit     22
      rc6_mask     0x100000000
    
    
          begin codes
    
        Blue    0x00007ba1
        Yellow    0x00007ba2
        Green    0x00007ba3
        Red    0x00007ba4
        Teletext    0x00007ba5
    
    # starts at af
            Radio    0x00007baf
            Print    0x00007bb1
            Videos   0x00007bb5
            Pictures 0x00007bb6
            RecTV    0x00007bb7
            Music    0x00007bb8
            TV       0x00007bb9
    # no ba - d8
    
            Guide    0x00007bd9
            LiveTV   0x00007bda
            DVD      0x00007bdb
            Back     0x00007bdc
            OK       0x00007bdd
            Right    0x00007bde
            Left     0x00007bdf
            Down     0x00007be0
            Up       0x00007be1
    
            Star       0x00007be2
            Hash       0x00007be3
    
            Replay   0x00007be4
            Skip     0x00007be5
            Stop     0x00007be6
            Pause    0x00007be7
            Record   0x00007be8
            Play     0x00007be9
            Rewind   0x00007bea
            Forward  0x00007beb
            ChanDown 0x00007bec
            ChanUp   0x00007bed
            VolDown  0x00007bee
            VolUp    0x00007bef
            More     0x00007bf0
            Mute     0x00007bf1
            Home     0x00007bf2
            Power    0x00007bf3
            Enter    0x00007bf4
            Clear    0x00007bf5
            Nine     0x00007bf6
            Eight    0x00007bf7
            Seven    0x00007bf8
            Six      0x00007bf9
            Five     0x00007bfa
            Four     0x00007bfb
            Three    0x00007bfc
            Two      0x00007bfd
            One      0x00007bfe
            Zero     0x00007bff
          end codes
    
    end remote



That is the something.txt



The goal is to find the key code names as you can probably tell and store them in a reportable manner to be handeled later in another app I am writing.



Thanks!
#### 2007-07-27 12:41:08 - Foxbuntu
Also any code suggestions you make I will be glad to use/test/hear as I am just a humble newb to this.



Thanks!
#### 2007-07-27 13:11:38 - ptmcg
Ooooh, this is a juicy-looking format for a full parser, but I am sorely time-constrained just now.  However, here is a short program to scan such a file and extract all entries of the form '<word> <hex-constant>' (actually, this much you could do with re's, but if you use pyparsing now, it can expand to parse the full file when you are ready).  Also, this little example will skip over any entries in comments, which is a bit tricky using just re's.



-- Paul





    from pyparsing import *
    
    rc6config = file('somefile.txt').read()
    
    # define expression for '<word> <hex-constant>'
    hexConstant = Combine('0x' + Word(hexnums))
    keydef = Word(alphas)('keyname') + hexConstant('value')
    
    # define a comment, and set our keydef to ignore them
    rc6comment = '#' + restOfLine
    keydef.ignore( rc6comment )
    
    # use searchString to skim through the file and return a list
    # of matches, each is a separate ParseResults, with corresponding
    # labels
    for key in keydef.searchString( rc6config ):
        print key.keyname, '=', key.value



Prints:



    data = 0x37FF0
    mask = 0x100000000
    Blue = 0x00007ba1
    Yellow = 0x00007ba2
    Green = 0x00007ba3
    Red = 0x00007ba4
    Teletext = 0x00007ba5
    Radio = 0x00007baf
    Print = 0x00007bb1
    Videos = 0x00007bb5
    Pictures = 0x00007bb6
    RecTV = 0x00007bb7
    Music = 0x00007bb8
    TV = 0x00007bb9
    Guide = 0x00007bd9
    LiveTV = 0x00007bda
    DVD = 0x00007bdb
    Back = 0x00007bdc
    ...


#### 2007-07-27 13:14:07 - ptmcg
Oh, by the way, this example uses the new 1.4.7 syntax for defining results names.  If you are using 1.4.6, change the keydef to:





    keydef = Word(alphas).setResultsName('keyname') + 
        hexConstant.setResultsName('value')



-- Paul
#### 2007-07-27 17:23:57 - Foxbuntu
Great, Thanks for the great Example. Got me where I needed to go!



Thanks!

---
## 2007-05-01 11:26:05 - shinewu - Need a preview button
Why don't we have a preview option before we post our article?

It is also strange that the original writer can not change his/her own article.

#### 2007-05-01 11:54:26 - ptmcg
This is a wikispaces issue.  I've had some bad experiences with spam on open-to-the-public wiki's, so I have configured this wiki to be fairly tightly controlled.  The best I can offer is that, if you edit and repost a comment, I'll delete the earlier version for you.



-- Paul

---
## 2007-05-15 10:44:09 - droblek - Resursion problem
Hi,



I wonder why the following snippet does not work?



w = Word(alphas)

e = Forward()

e << (w ^ (e + e))

e.parseString('das fas dos')

(['das'], {})



I expected the result, which would contain all three words.



Thanks,

Dominik

#### 2007-05-15 14:24:35 - ptmcg
Well, this is actually a pretty common topic when someone is implementing a published BNF using pyparsing.  Many BNFs, not having the support of a repetition operation, use recursion instead:





    item ::= something something something
    listOfItems ::= item | item listOfItems



In fact, if you implement your code directly into pyparsing using this idiom, it does work:





    from pyparsing import *
    w = Word(alphas)
    e = Forward()
    e << (w ^ (w + e))
    print e.parseString('das fas dos')



prints



    ['das', 'fas', 'dos']



But using Forward this way is not the best pyparsing mechanism, and I really encourage you to reserve the use of Forward for truly recursive expressions, such as nested lists, arithmetic expressions with grouping parentheses, etc.  For pyparsing, I'd much prefer you use ZeroOrMore or OneOrMore to handle such lists:





    e = OneOrMore(w)
    print e.parseString('das fas dos')



prints



    ['das', 'fas', 'dos']

(just like the first example).



And it is not necessary to use delimitedList(item,White()) for items that are separated just with whitespace, since pyparsing treats whitespace as an implicit separater.  delimitedList is only needed when the list items are truly delimited by commas, periods, or some other punctuation or expression.



-- Paul

---
## 2007-05-23 13:51:01 - ptmcg - Enhancement proposal - abbreviated setResultsName
I got to thinking after some postings on comp.lang.python about adding a notational short cut for setResultsName. I really want to encourage people to use named elements in their grammar, but that method name is just long and ugly, and grammar-cluttering.



So how about adding a shortcut for setResultsName, using getitem? With this short cut, this code:



    userdata = Word(alphas).setResultsName('name') + Word(nums+'-').setResultsName('socsecno')

could be written as:



    userdata = Word(alphas)['name'] + Word(nums+'-')['socsecno']



Any comments? Alternatives?


---
## 2007-05-28 16:05:59 - Kambiz - operatorPrecedence
I want to parse c like expressions, but I have some problems using operatorPrecedence. Specially I can't make it match pre and post increment (and similar) expressions correctly. 



    id = Word(alphas+'_',alphanums+'_')
    baseExpr= id
    
    expr = operatorPrecedence(baseExpr,[
        (Literal('++'),1,opAssoc.LEFT),
        (Literal('++'),1,opAssoc.RIGHT),
        (oneOf('+ -'),1,opAssoc.RIGHT),
        (oneOf('+ -'),2,opAssoc.LEFT),
         # many other operators
        ])
    
    
    print expr.parseString('++a')

Output: [['+', ['+', 'a']]]

but it should be [['++', 'a']]



I have also tried the following:



    id = Word(alphas+'_',alphanums+'_')
    baseExpr=Forward();
    baseExpr|=id+'++'
    baseExpr|='++'+id
    baseExpr|= id
    
    expr = operatorPrecedence(baseExpr,[
        (oneOf('+ -'),1,opAssoc.RIGHT),
        (oneOf('+ -'),2,opAssoc.LEFT),
         # many other operators
        ])

with the same result.

Can somebody help me please?

#### 2007-05-28 16:57:29 - ptmcg
Try making your oneOf('+ -') more restrictive, to:



    (~oneOf('++ --') + oneOf('+ -')), 2, opAssoc.Left

This should also take care of predecrement operator.



-- Paul
#### 2007-05-28 17:00:41 - ptmcg
Oh, baseExpr should *not* be a Forward in this case, operatorPrecedence will construct the necessary Forward definition.  For operatorPrecedence, just define the 'atom' operand, in your case probably something like:



    decimalLiteral = Combine(Word(nums)+Optional('.'+Word(nums)))
    baseExpr = id | decimalLiteral

You can also add octal and hex literals too.



-- Paul
#### 2007-05-29 01:07:23 - Kambiz
Thank you!

---
## 2007-05-29 05:46:22 - rbosman - Detailed error message
Hi,



Currently, I am working on a script that converts one template language to another. The parsing and translation works fine. The only problem I currently have is when the input (source) template has a syntax error. 



Take for instance the following language:





    subprogram = Forward()
    text = CharsNotIn('{')
    
    block_open = Literal('{').suppress() + Literal('a') + Literal('}').suppress()
    block_close = Literal('{').suppress() + Literal('/a') + Literal('}').suppress()
    block  = Group( block_open + subprogram + block_close.suppress() ).setResultsName('block')
    
    subprogram <<  ZeroOrMore( text | block )
    
    program = subprogram + StringEnd()



This language parses correctly the following code:



    {a}1{a}2{/a}{a}3{/a}{/a}



If I make a mistake in the code:



    {a}1{a}2{/a}{b}3{/a}{/a}



I'll get the ParseException:





    {a}1{a}2{/a}{b}3{/a}{/a}
    ^
    Expected end of text (at char 0), (line:1, col:1)



But I'd like to see the error message pointing to column 14 and saying

that {b} is not recognized.



Does somebody know how to do that?

#### 2007-05-29 14:21:50 - ptmcg
ParseException messages are definitely a weak spot in pyparsing, and it is an inherent architectural problem.  Each expression within a pyparsing grammar operates within its own little scope, and if a problem in matching arises, it will raise a ParseException.  Unfortunately, it can't really do anything more drastic than that, since it is possible that the current expression is one of several alternatives, within an Or or a MatchFirst, and one of the subsequent expressions may work just fine.  The implementations of Or and MatchFirst are such that these internal exceptions are trapped and cached until all alternatives are tried, and then the one that got the furthest is the one that is raised (based on the assumption that the alternative that matched the most text is probably the one that really should have passed).  And there is no global variable to record 'this is the farthest we got, so the syntax error must be somewhere around here.'



However, these exceptions may then be swallowed up by some higher level expression, like a ZeroOrMore.  ZeroOrMore expects exceptions to happen, using the occurrence of an exception to tell it 'there are no more', and the 'zero' in ZeroOrMore means that even if there is **no** match, that is okay in the bigger scheme of things, we have succeeded, continue parsing at the next expression.  So we can see that the parser can quickly move away from the actual syntax error, and end up reporting some exception for a seemingly unrelated expression - in fact, it is usually one of the upper-most expressions that ends up throwing the final exception.



I have taken a few stabs at this in the past, such as the 'reraise the exception raised at the furthest parse location' code, but with little real improvement.  I might try redoing the exception handling to embed lower-level exceptions within higher ones, but even this method is frustrated by higher-level expressions such as ZeroOrMore, Optional, and OneOrMore.



There is one avenue that you might consider, and that is adding your own parse actions to do things like matching up tags, as in '<A>lskfldjf<B>lskdflsjdf</B>slkdfdlfj</A>'.  A parse action could keep a stack of previous tags, and verify that the current terminator matches the previous opening tag, and so could detect the error in '<A>lskfldjf<B>lskdflsjdf</Z>slkdfdlfj</A>'.  (Here's an interesting question, which is really the error?  Should the terminator read '</B>' or was the opening tag really supposed to be '<Z>'?)  Upon detection, the parse action can raise a ParseFatalException, which stops the parsing process **immediately**, thereby giving you a clearer exception message and location.



I'm open to any supporting work in this area.



-- Paul
#### 2007-06-01 09:42:28 - rbosman
Hi Paul,



Thanks for the quick respond.



I've been looking into this problem a bit more. A solution I've found is to make an extra ParseExpression: NoEscape. (Yes, a badly chosen name.) If the first element matches, then the remaining elements must match as well. I used the modulo operator for this expression. The example above could be transformed to:





    subprogram = Forward()
    text = CharsNotIn('{')
    
    block_open = Literal('{').suppress() + Literal('a') + Literal('}').suppress()
    block_close = Literal('{').suppress() + Literal('/a') + Literal('}').suppress()
    block  = Group( block_open % (subprogram + block_close.suppress()) ).setResultsName('block')
    
    subprogram <<  ZeroOrMore( text | block )
    
    program = subprogram + StringEnd()





The NoEscape is almost the same as an And. The difference is that it throws a ParseFatalException when the second (and up) element does not match. It's currently implemented like:





    class NoEscape(ParseExpression):
        def __init__( self, exprs, savelist = True):
            super(NoEscape,self).__init__(exprs, savelist)
            self.mayReturnEmpty = True
            for e in self.exprs:
                if not e.mayReturnEmpty:
                    self.mayReturnEmpty = False
                    break
    
            self.setWhitespaceChars( exprs[0].whiteChars )
            self.skipWhitespace = exprs[0].skipWhitespace
            self.callPreparse = True
    
        def parseImpl( self, instring, loc, doActions=True ):
            loc, resultlist = self.exprs[0]._parse( instring, loc, doActions, callPreParse=False )
            maxExcLoc = -1
            maxMatchLoc = -1
    
            try:
                for e in self.exprs[1:]:
                    loc, exprtokens = e._parse( instring, loc, doActions )
                    if exprtokens or exprtokens.keys():
                        resultlist += exprtokens
    
            except ParseException, err:
                if err.loc > maxExcLoc:
                    maxException = err
                    maxExcLoc = err.loc
    
                raise ParseFatalException( maxException.pstr, maxException.loc, maxException.msg)
            return loc, resultlist
    
        def __imod__(self, other ):
            if isinstance( other, basestring ):
                other = Literal( other )
            return self.append( other ) #NoEscape( [ self, other ] )



For now, the error messages are not very precise, but it gives me an indication where the error may be. 



Cheers, Raymond
#### 2007-06-01 20:07:07 - ptmcg
Interesting idea, but you'll have to make sure you factor your grammar properly to avoid false negatives.  Mostly this means putting alternation as deep in the grammar as possible.  That is, instead of:



    varname = Word(alphas)
    numericLiteral = Word(nums)
    altern = ( '(' % varname % ')' | '(' % numericLiteral % ')' )



You would want to write:



    altern = ( '(' % ( varname | numericLiteral ) % ')' )



Otherwise, you would never match '(10)' because you would get a fatal parse exception for *not* matching '(varname)'.
#### 2010-10-12 02:23:42 - mrh1997
Hello Paul,



I did not dive into the code yet deeply enough to know if this is working or not.

But as you mentioned already a global variable would solve the problem I think it should be also possible to pass a 'container' to ParserElement._parseNoCache() that contains the error+location of the exception that progressed furthest. 



If an error occurs at a location that is higher than the location of the error stored in the passed 'container', the stored error has to be replaced.



If this container object is passed recursivly every ParserElement whould know the 'most successful' error location without using a global variable.

That i.e. means that a ZeroOrMore() continues working successfully on a syntax error but passes the syntax error plus its location to the caller in the container. If the caller fails to parse the expression at an even earlier location, it can return the error message from the ZeroOrMore rule.
#### 2010-10-12 06:00:40 - ptmcg
I will give it some thought.  But in the recent releases of pyparsing, there is now support for an ErrorStop term to be inserted into an And expression that will raise an immediate ParseSyntaxException (inspired by this discussion and a few others like it). To add an ErrorStop in your And, use the '-' operator instead of '+', at the point at which you can determine that a valid start of an expression has been found, and that any parse failures afterward must be syntax errors.  In your example that you posted back in '07, you would do this after finding the block_open.  Change the '+' after block_open to '-', like this:



    block  = Group( block_open - subprogram + block_close.suppress() ).setResultsName('block')



Now when parsing your erroneous case, we get a better result:



    pyparsing.ParseSyntaxException: Expected '/a' (at char 13), (line:1, col:14)

---
## 2007-05-29 05:53:42 - Kambiz - function call and subscripting with operatorPrecedence
I think I'm doing something wrong!

expr.parseString('a(1+1)')

does not return anything (after 5 minutes I killed the process). Here is the code: 



    from pyparsing import *
    
    sign = Optional(Literal('+') | Literal('-'),'+')
    base = Optional(oneOf('0b 0o 0x 0h 0B 0O 0X 0H'),'0d')  
    digits  = Word('0123456789abcdefABCDEF')
    
    num = sign+base+digits
    
    def str2num(t):
        r=0;
        try:
            dic={'0b':2,'0B':2,'0o':8,'0O':8,'0d':10,'0x':16,'0X':16,'0h':16,'0H':16} 
            r = int(t[0]+t[2],dic[t[1]])
        except ValueError:
            print 'Value Error'
    
        return r;
    
    
    num.setParseAction(str2num);
    
    id = Word(alphas+'_',alphanums+'_')
    
    expr=Forward()
    
    exprList=delimitedList(expr) | empty
    
    baseExpr= id | num
    
    expr << operatorPrecedence(baseExpr,[
        ('('+exprList+')',1,opAssoc.LEFT),
        ('['+exprList+']',1,opAssoc.LEFT),
    
        ('++',1,opAssoc.LEFT),#post inc
        ('++',1,opAssoc.RIGHT),#pre inc
        (oneOf('! ~'),1,opAssoc.RIGHT),
        (~oneOf('++ --')+oneOf('+ -'),1,opAssoc.RIGHT),#sign
        (~oneOf('++ --')+oneOf('+ -'),2,opAssoc.LEFT),#add sub
        (oneOf('<< >>'),2,opAssoc.LEFT),
        (oneOf('<= >= < >'),2,opAssoc.LEFT),
        (oneOf('== !='),2,opAssoc.LEFT),
        ('&',2,opAssoc.LEFT),
        ('^',2,opAssoc.LEFT),
        ('|',2,opAssoc.LEFT),
        ('&&',2,opAssoc.LEFT),
        ('||',2,opAssoc.LEFT),
        ])
    
    
    print expr.parseString('a(1+1)')

Any ideas?

#### 2007-05-29 13:25:13 - ptmcg
Kambiz -



Hmmm, according to my tests, you haven't done **anything** wrong!  You may have run up against a practical limitation with using operatorPrecedence (and perhaps with pyparsing in general).  I reworked your example a bit to remove the concept of (args,...) as a left-associated operator, and defined an extension to baseExpr that includes funcCall:



    expr=Forward()
    exprList = delimitedList(expr)
    funcCall = id + '(' + Optional(exprList) + ')'
    baseExpr= funcCall | id | num



I then modified your call to operatorPrecedence:

 - removed the function call 'operator' entry

 - commented out all operators of lower precedence than add and subtract

 - (I also inserted multiply and divide just above add and subtract, did you forget them? :) )



I added another test case also, `a()`, and then captured the time it took to parse the two expressions, uncommenting another operator and retiming, and so on.  Here are the results:





<table class="wiki_table">
    <tr>
        <td>Lowest prec oper

</td>
        <td>Levels in opPrec

</td>
        <td>Parse time (sec)

</td>
    </tr>
    <tr>
        <td>add/subtract

</td>
        <td>7

</td>
        <td>0.113

</td>
    </tr>
    <tr>
        <td>bit shift left/right

</td>
        <td>8

</td>
        <td>0.430

</td>
    </tr>
    <tr>
        <td>comparison

</td>
        <td>9

</td>
        <td>1.709

</td>
    </tr>
    <tr>
        <td>equal/not equal

</td>
        <td>10

</td>
        <td>6.651

</td>
    </tr>
    <tr>
        <td>&

</td>
        <td>11

</td>
        <td>26.522

</td>
    </tr>
    <tr>
        <td>^

</td>
        <td>12

</td>
        <td>110.194

</td>
    </tr>
    <tr>
        <td>

</td>
        <td>

</td>
        <td>(estimated)

</td>
    </tr>
    <tr>
        <td>|

</td>
        <td>13

</td>
        <td>7 minutes

</td>
    </tr>
    <tr>
        <td>&&

</td>
        <td>14

</td>
        <td>30 minutes

</td>
    </tr>
    <tr>
        <td>||

</td>
        <td>15

</td>
        <td>1-1/2 to 2 hours

</td>
    </tr>
</table>



I then commented out your recursive reference for the [] operator, and got the following:





<table class="wiki_table">
    <tr>
        <td>Lowest prec oper

</td>
        <td>Levels in opPrec

</td>
        <td>Parse time (sec)

</td>
    </tr>
    <tr>
        <td>add/subtract

</td>
        <td>6

</td>
        <td>0.042

</td>
    </tr>
    <tr>
        <td>bit shift left/right

</td>
        <td>7

</td>
        <td>0.154

</td>
    </tr>
    <tr>
        <td>comparison

</td>
        <td>8

</td>
        <td>0.568

</td>
    </tr>
    <tr>
        <td>equal/not equal

</td>
        <td>9

</td>
        <td>2.199

</td>
    </tr>
    <tr>
        <td>&

</td>
        <td>10

</td>
        <td>8.816

</td>
    </tr>
    <tr>
        <td>^

</td>
        <td>11

</td>
        <td>35.353

</td>
    </tr>
    <tr>
        <td>

</td>
        <td>

</td>
        <td>(estimated)

</td>
    </tr>
    <tr>
        <td>|

</td>
        <td>12

</td>
        <td>2 minutes

</td>
    </tr>
    <tr>
        <td>&&

</td>
        <td>13

</td>
        <td>8 minutes

</td>
    </tr>
    <tr>
        <td>||

</td>
        <td>14

</td>
        <td>32 minutes

</td>
    </tr>
</table>



I also tried replacing a right-associative operator with a left, and got similar results again.  The controlling factor here really seems to be the depth of the precedence hierarchy itself.



This is **definitely** the torture test for operatorPrecedence!  When I get some time, I'll come back to this to see if/where I can do some performance tuning.  But at this point in time, even though theoretically possible, I'd have to say that using operatorPrecedence to define the full set of C operators is not practical.



-- Paul
#### 2007-05-29 13:32:02 - ptmcg
Wait!  There is a glimmer of hope!  On a lark, I thought about enabling packrat parsing, by adding this line after importing the pyparsing module, but before defining any parser elements:



    ParserElement.enablePackrat()



I then reran this test using the full set of operators, all the way down to || - the test ran in 0.016 seconds!



I think this is the most dramatic results I've yet seen using packratting.  Now take care not to define any parse actions that update global data structures - this kind of behavior will break when using packrat parsing.



-- Paul
#### 2007-05-29 13:47:47 - ptmcg
I think your next question is probably, 'What the heck is 'packrat parsing'?'  I just posted an FAQ with the answer to this, plus a link to a page managed by the MIT'er who wrote his thesis on it.
#### 2007-05-29 14:41:25 - Kambiz
Thank you very much. I never thought you would invest so much time helping me.

I do not know much about parsing but I really like pyparsing, it's a great library.
#### 2007-05-29 14:48:20 - Kambiz
I didn't forgot them, I want to make a simple compiler for a special 16 bit processor that does not support such operations.

---
## 2007-05-30 04:55:03 - bplant - Problem with loc in recursive expressions
Hi,



I am having a problem with the location variable passed to setParseAction when using recursive grammars. I have the following script:







    from pyparsing import *
    
    def printVar(s, loc, toks):
        print '%5s: %d: %s' % (toks[0], lineno(loc, s), s[loc:loc+3])
    def printL(s, loc, toks):
        print '%5s: %d: %s' % (toks[0], lineno(loc, s), s[loc:loc+3])
    def printR(s, loc, toks):
        print '%5s: %d: %s' % (toks[0], lineno(loc, s), s[loc:loc+3])
    def printOr(s, loc, toks):
        print '%5s: %d: %s' % (toks[0], lineno(loc, s), s[loc:loc+3])
    def printAnd(s, loc, toks):
        print '%5s: %d: %s' % (toks[0], lineno(loc, s), s[loc:loc+3])
    def printL(s, loc, toks):
        print '%5s: %d: %s' % (toks[0], lineno(loc, s), s[loc:loc+3])
    def printExpr(s, loc, toks):
        print '%5s: %d: %s' % (toks[0], lineno(loc, s), s[loc:loc+3])
    
    expr = Forward()
    
    var = Word(alphas, alphanums)
    var.setParseAction(printVar)
    
    subExpr = Literal('(').setParseAction(printL) + OneOrMore(expr) + Literal(')').setParseAction(printR)
    
    orExpr = '||' + subExpr
    orExpr.setParseAction(printOr)
    
    andExpr = '&&' + subExpr
    andExpr.setParseAction(printAnd)
    
    expr << (var ^ orExpr ^ andExpr)
    expr.setParseAction(printExpr)
    
    exprs = StringStart() + ZeroOrMore(expr) + StringEnd()
    exprs.parseWithTabs()
    
    mystr = '''\
    ||(
      &&(
        &&( abc )
        ||( def )
        )
      ||( ghi )
      )
    '''
    
    print mystr
    print '*****'
    exprs.parseString(mystr)





When you run the script, the first column is the token that is being passed, the middle column is the value returned by lineno(loc, s) and the right column is s[loc:loc+3].



From my understanding, the right column (or at least the first character) should match the left column. Some examples from the output of the script to demonstrate:



   &&: 2: &&(



and



  ghi: 6: ghi



This seems to work mostly except for the closing ')' literals where the following are printed:



    ): 4:  )    <-- extra space before ')' character



and



    ): 4: 

        )       <-- extra '\n' character before the ')' character



Note that because the loc variable appears to be off by a few chars, the lineno function returns the wrong line. The last ')' token is on line 7, but it is reported as being on line 6:



    ): 6:



Is this the correct behaviour and I am doing something wrong or is this a bug?



Thanks in advance,



Brad



P.S. If the formatting gets stuffed up, drop me a message and I'll send it through.

#### 2007-05-30 06:50:35 - ptmcg
Why are you calling parseWithTabs?  Try removing that and see if you get better results.



-- Paul
#### 2007-05-30 14:55:27 - bplant
Sorry Paul, I forgot to mention that in my previous post.... calling parseWithTabs doesn't improve the result. It does change it slightly, but the location of the ')' characters is still reported incorrectly.



Btw, I am using pyparsing version 1.4.6



Cheers,



Brad

---
## 2007-05-30 13:47:21 - Kambiz - Packrat Parsing
I get different results with packrat parsing disabled and enabled. There are no parse actions.



    varDec  = varType + delimitedList(id + Optional('='+expr))+';'
    funcDef = Optional(varType | 'void')+id+'('+(delimitedList(varType+id)|'void'|empty)+')'+codeBlock
    program= varDec | funcDef 
    print program.parseString('int f(){}')

Output: 

['int', 'f', '(', ')', '{', '}']



Output with packrat parsing:

['int', 'f', 'f', '(', ')', '{', '}']

#### 2007-06-01 22:45:55 - ptmcg
Sorry not to post back sooner, I'm struggling with a laptop with a flaky boot drive this week.



I could not reproduce your results, so I think the problem is in one of the elements I had to fake in, probably varType.  I started with just this code, to go with this post:



    from pyparsing import *
    ParserElement.enablePackrat()
    
    varType = oneOf('int float char')
    id = Word(alphas,alphanums)
    codeBlock = Literal('{}')
    
    #~ varDec  = varType + delimitedList(id + Optional('='+expr))+';'
    funcDef = Optional(varType | 'void')+id+'('+(delimitedList(varType+id)|'void'|empty)+')'+codeBlock
    #~ program= varDec | funcDef 
    program= funcDef 
    print program.parseString('int f(){}')

and I had to fake in varType, id, and codeBlock.



Then I fit it back into the code you posted in your previous post, and I still needed to fake only varType and codeBlock.



But in all of my tests, with and without packrat enabled, I get the same results:



    ['int', 'f', '(', ')', '{}']



I also tested with both version 1.4.5 and 1.4.6.



For some more help, try adding debugging-enabling code, such as



    varType.setDebug()

This will write out messages when trying to match varType, followed by the resulting match or exception.



-- Paul
#### 2007-06-02 03:35:40 - Kambiz


    simpleType = Literal('int');
    arrayType= simpleType+OneOrMore('['+delimitedList(constExpr)+']')
    varType = arrayType | simpleType



With varType = simpleType I get the correct result with and without packrat parsing.

With program.setDebug() I get:


```
<br >
Match {{{{&quot;int&quot; {{&quot;[&quot; Forward: expr2 [{Suppress:(&quot;,&quot;) Forward: expr2}]... &quot;]&quot;
```
...} | 'int'} 

W:(abcd...,abcd...) [{'=' Forward: Forward: expr15}] [{Suppress:(',') 

W:(abcd...,abcd...) [{'=' Forward: Forward: expr15}]}]... ';'} | {[`&quot;int&quot; {{&quot;[&quot; Forward: expr2 [{Suppress:(&quot;,&quot;) Forward: expr2}]... &quot;]&quot;`...} | 'int' | 'void'}] 

W:(abcd...,abcd...) '(' `{{&quot;int&quot; {{{&quot;[&quot; Forward: expr2 [, Forward: expr2]...} &quot;]&quot;`...} | 'int'} 

W:(abcd...,abcd...)} [, `{&quot;int&quot; {{{&quot;[&quot; Forward: expr2 [, Forward: expr2]...} &quot;]&quot;`...} | 'int'} 

W:(abcd...,abcd...)}]... | 'void' | empty} ')' '{}'}} at loc 0 (1,1)

Matched `{{&quot;int&quot; {{&quot;[&quot; Forward: expr2 [{Suppress:(&quot;,&quot;) Forward: expr2}]... &quot;]&quot;`...} | 'int'} 

W:(abcd...,abcd...) [{'=' Forward: Forward: expr15}] [{Suppress:(',') 

W:(abcd...,abcd...) [{'=' Forward: Forward: expr15}]}]... ';'} | {[`&quot;int&quot; {{&quot;[&quot; Forward: expr2 [{Suppress:(&quot;,&quot;) Forward: expr2}]... &quot;]&quot;`...} | 'int' | 'void'}] 

W:(abcd...,abcd...) '(' `{{&quot;int&quot; {{{&quot;[&quot; Forward: expr2 [, Forward: expr2]...} &quot;]&quot;`...} | 'int'} 

W:(abcd...,abcd...)} [, `{&quot;int&quot; {{{&quot;[&quot; Forward: expr2 [, Forward: expr2]...} &quot;]&quot;`...} | 'int'} 

W:(abcd...,abcd...)}]... | 'void' | empty} ')' '{}'}} -> ['int', 'f', 'f', '(', ')', '{}']

['int', 'f', 'f', '(', ')', '{}']

}}



I don't understand why arrayType makes f match twice.



The whole source code:



    from pyparsing import *
    ParserElement.enablePackrat()
    sign = Optional(Literal('+') | Literal('-'),'+')
    base = Optional(oneOf('0b 0o 0x 0h 0B 0O 0X 0H'),'0d')  
    digits  = Word('0123456789abcdefABCDEF')
    
    num = sign+base+digits
    
    id = Word(alphas+'_',alphanums+'_')
    
    expr=Forward()
    exprList = delimitedList(expr)
    funcCall = id + '(' + Optional(exprList) + ')'
    baseExpr= funcCall | id | num
    
    expr << operatorPrecedence(baseExpr,[
        ('('+exprList+')',1,opAssoc.LEFT),
        ('['+exprList+']',1,opAssoc.LEFT),
    
        ('++',1,opAssoc.LEFT),#post inc
        ('++',1,opAssoc.RIGHT),#pre inc
        (oneOf('! ~'),1,opAssoc.RIGHT),
        (~oneOf('++ --')+oneOf('+ -'),1,opAssoc.RIGHT),#sign
        (~oneOf('++ --')+oneOf('+ -'),2,opAssoc.LEFT),#add sub
        (oneOf('<< >>'),2,opAssoc.LEFT),
        (oneOf('<= >= < >'),2,opAssoc.LEFT),
        (oneOf('== !='),2,opAssoc.LEFT),
        ('&',2,opAssoc.LEFT),
        ('^',2,opAssoc.LEFT),
        ('|',2,opAssoc.LEFT),
        ('&&',2,opAssoc.LEFT),
        ('||',2,opAssoc.LEFT),
        (oneOf('= += -= <<= >>= &= ^= |='),2,opAssoc.RIGHT),
        ])
    
    constExpr= operatorPrecedence(id|num,[
        (oneOf('+ -'),1,opAssoc.RIGHT),#sign
        (oneOf('* / %'),2,opAssoc.LEFT),#sign
        (oneOf('+ -'),2,opAssoc.LEFT),#add sub
        ])
    
    simpleType = Literal('int');
    arrayType= simpleType+OneOrMore('['+delimitedList(constExpr)+']')
    varType = arrayType | simpleType
    #varType = simpleType
    varDec  = varType + delimitedList(id + Optional('='+expr))+';'
    
    codeBlock = Literal('{}')
    
    funcDef = Optional(varType | 'void')+id+'('+(delimitedList(varType+id)|'void'|empty)+')'+codeBlock
    
    program= varDec | funcDef 
    #program.setDebug()
    print program.parseString('int f(){}')

Thanks in advance!
#### 2007-06-03 23:28:52 - ptmcg
I'm pretty sure this is a bug.  I'll try to distill your example down to a small debuggable (and unit test digestible) version, and then try to nail it down.



-- Paul
#### 2007-06-04 00:33:49 - ptmcg
Yes, this is a bug in the way the packrat cache is managed.  This shortened version of your program illustrates the problem.



    from pyparsing import *
    ParserElement.enablePackrat()
    
    integer = Word(nums).setName('integer')
    id = Word(alphas+'_',alphanums+'_')
    simpleType = Literal('int');
    arrayType= simpleType+ZeroOrMore('['+delimitedList(integer)+']')
    varType = arrayType | simpleType
    varDec  = varType + delimitedList(id + Optional('='+integer))+';'
    
    codeBlock = Literal('{}')
    
    funcDef = Optional(varType | 'void')+id+'('+(delimitedList(varType+id)|'void'|empty)+')'+codeBlock
    
    program = varDec | funcDef
    print program.parseString('int f(){}')



Now, by adding some debugging statements to the cacheing code that implements the packrat cache, we see the 'smoking gun':



    Stored '(3, (['int'], {}))' into cache at locn 0 matching 'int'
    Stored '(4, ([], {}))' into cache at locn 3 matching [{'[' integer [{Suppress:(',') integer}]... ']'}]...
    Stored '(4, (['int'], {}))' into cache at locn 0 matching {'int' [{'[' integer [{Suppress:(',') integer}]... ']'}]...}
    Stored '(4, (['int'], {}))' into cache at locn 0 matching {{'int' [{'[' integer [{Suppress:(',') integer}]... ']'}]...} | 'int'}
    Stored '(5, (['f'], {}))' into cache at locn 4 matching W:(abcd...,abcd...)
    Stored '(5, ([], {}))' into cache at locn 5 matching [{'=' integer}]
    Stored '(5, ([], {}))' into cache at locn 5 matching [{Suppress:(',') W:(abcd...,abcd...) [{'=' integer}]}]...
    Fetched '(4, (['int', 'f'], {}))' from cache at locn 0 matching {'int' [{'[' integer [{Suppress:(',') integer}]... ']'}]...}
    Stored '(4, (['int', 'f'], {}))' into cache at locn 0 matching {{'int' [{'[' integer [{Suppress:(',') integer}]... ']'}]...} | 'int' | 'void'}
    Stored '(4, (['int', 'f'], {}))' into cache at locn 0 matching [{{'int' [{'[' integer [{Suppress:(',') integer}]... ']'}]...} | 'int' | 'void'}]
    Fetched '(5, (['f'], {}))' from cache at locn 4 matching W:(abcd...,abcd...)
    Stored '(6, (['('], {}))' into cache at locn 5 matching '('
    Stored '(6, ([], {}))' into cache at locn 6 matching empty
    Stored '(6, ([], {}))' into cache at locn 6 matching {{{{'int' [{{'[' integer [, integer]...} ']'}]...} | 'int'} W:(abcd...,abcd...)} [, {{{'int' [{{'[' integer [, integer]...} ']'}]...} | 'int'} W:(abcd...,abcd...)}]... | 'void' | empty}
    Stored '(7, ([')'], {}))' into cache at locn 6 matching ')'
    Stored '(9, (['{}'], {}))' into cache at locn 7 matching '{}'
    Stored '(9, (['int', 'f', 'f', '(', ')', '{}'], {}))' into cache at locn 0 matching {[{{'int' [{'[' integer [{Suppress:(',') integer}]... ']'}]...} | 'int' | 'void'}] W:(abcd...,abcd...) '(' {{{{'int' [{{'[' integer [, integer]...} ']'}]...} | 'int'} W:(abcd...,abcd...)} [, {{{'int' [{{'[' integer [, integer]...} ']'}]...} | 'int'} W:(abcd...,abcd...)}]... | 'void' | empty} ')' '{}'}
    Stored '(9, (['int', 'f', 'f', '(', ')', '{}'], {}))' into cache at locn 0 matching {{{{'int' [{'[' integer [{Suppress:(',') integer}]... ']'}]...} | 'int'} W:(abcd...,abcd...) [{'=' integer}] [{Suppress:(',') W:(abcd...,abcd...) [{'=' integer}]}]... ';'} | {[{{'int' [{'[' integer [{Suppress:(',') integer}]... ']'}]...} | 'int' | 'void'}] W:(abcd...,abcd...) '(' {{{{'int' [{{'[' integer [, integer]...} ']'}]...} | 'int'} W:(abcd...,abcd...)} [, {{{'int' [{{'[' integer [, integer]...} ']'}]...} | 'int'} W:(abcd...,abcd...)}]... | 'void' | empty} ')' '{}'}}
    ['int', 'f', 'f', '(', ')', '{}']



Notice that on the 3rd line we cache the ParseResults containing ['int'], but on line 8 we retrieve the ParseResults containing ['int','f'], which we then concatenate with 'f' to give us the duplicated token value.



The problem is that the value that I cache is a reference to a ParseResults that continues to get updated as parsing proceeds.  By modifying the cache code to cache a *copy* of the ParseResults, the problem is fixed.  This will slow down the packrat process somewhat, but I guess that *is* the price of accuracy. :)



I'll ship this in the next release of pyparsing.



-- Paul
#### 2007-10-10 13:47:24 - remyblank
Funny, this could also explain the strange behavior I was seeing in 'Bug 1' .



-- Remy
#### 2007-10-10 14:12:50 - ptmcg
Could be.  I reran your Bug 1 example, and it works correctly now.



-- Paul

---
## 2007-06-07 12:14:34 - droblek - Is pyparsing thread safe?
Hello,



Is parseString method of ParserElement thread safe?



Thanks,

Dominik

#### 2007-06-07 12:39:06 - ptmcg
No it isn't.  The most fundamental issue has to do with exceptions that are cached within pyparsing subexpressions, but there are other caches within pyparsing (such as when using packrat parsing) that are not thread safe.



Here are some workarounds you could do as a pyparsing user:

1. (obvious) Synchronize your access to a singleton parser object among different threads.  Don't proceed to more complex approaches until you have tried this and it is found to be inadequate. (Stick with the simple until the complex is really needed.)

2. (obvious2) Construct new grammar instances per-thread, so that there is no sharing of pyparsing expressions.  If you do not use packrat parsing, then this type of implementation would be thread safe (no global vars).

3. (devious) A variation on #2, but use the pickle module to rapidly reconstitute a parser at thread startup, starting from a previously constructed grammar that has been pickled for this purpose.  I have had users use pickle to store/retrieve grammars with about 5X performance improvement (faster to unpickle than to define anew).



Hope this helps.



-- Paul
#### 2008-05-10 14:41:15 - droblek
Hi Paul,



What about creating a separate copy of the parser for each call by using parser.copy().parseString(s) instead of parser.parseString(s)?



Thanks,

Dominik
#### 2008-05-11 21:48:59 - ptmcg
Dominik -



This could work in theory, but I wouldn't rely on copy() to make a full, deep copy of a parser.  You'd be better off to have a function that generates new parsers, like makeParser(), so that a full and separate set of instances are used by separate threads.  If you want this routine to run quickly, you can use the pickle module to quickly 'reconstitute' the grammar at runtime.



There are some class variables in ParserElement that are shared, such as the _exprArgCache used by packrat parsing.  So it would be good to avoid using packratting if trying to run multithreaded.



That's all that comes to mind for now.



Best of luck,

-- Paul
#### 2011-06-24 20:34:25 - iwikispacer
Hi Paul,



I really like what the pyparsing tool and would like to use it in a thread-safe environment.



I was wondering if you have plans to have an optional 'context' dict passed on to parseString and carried forward to all objects, to hold the per-thread context information. I am not sure how this can be passed on to parse-action functions/classes though, as the args in functions will have to change. May be, based on the a global ParserElement.enableMultithreading() will help.



Another approach can be to have the parsing context stored as part of a thread-local. Like having the packrat's cache and the exceptions which you have mentioned within thread-local. This I believe will work fine for those who want to access thread-locals within their parse actions.



While I can attempt to the make the changes myself, I am a bit apprehensive since I am not very much experienced in Python (moving-in from Java, and loving the Python world).



I also have some context within my code (EvalConstant.vars_, based on an extension from sample code), which I will either have to move to the 'context' variable or to a thread-local, based on the approach you can suggest.



Thanks & Regards,

Raj
#### 2011-06-25 09:08:59 - ptmcg
There are some changes in the works for the next release that will undo the cacheing of exceptions in the Python3 version. What version of Python are you using?
#### 2011-06-25 20:53:30 - iwikispacer
Should be moving on to version 2.7 sometime soon over GAE (Google App Engine), from the current single-threaded deployment environment over Python 2.5 version.



Back-story, in case you are interested :



The current default Python version available on GAE is Python 2.5. Google is working on changing its pricing scheme for GAE, which will turn to be favorable to multi-threaded environments (good for those who are on Java on GAE already) and is also proposing to support multi-threading in Python my helping users move over to version 2.7 shortly.



So those using pyparsing on GAE will sure be interested in the multi-threading support over the 2.7 version.




#### 2014-01-08 01:05:37 - iwikispacer
Hi Paul,



I am trying to see if the pyparsing can be made to use threading.local to make it thread-safe. I tried making the change for the Packrat implementation and the diff for the same is given below (for file from version 2.0.1) :



<h6 id="toc0">=============</h6>
 start diff ===========================71a72

<ul class="quotelist"><li>from threading import local as thread_local</li></ul>955,956c956,957

<         if lookup in ParserElement._exprArgCache:

<             value = ParserElement._exprArgCache[ lookup ]

---

<ul class="quotelist"><li>if lookup in ParserElement._thread_local._exprArgCache:</li><li>value = ParserElement._thread_local._exprArgCache[ lookup ]</li></ul>963c964

<                 ParserElement._exprArgCache[ lookup ] = (value[0],value[1].copy())

---

<ul class="quotelist"><li>ParserElement._thread_local._exprArgCache[ lookup ] = (value[0],value[1].copy())</li></ul>967c968

<                 ParserElement._exprArgCache[ lookup ] = pe

---

<ul class="quotelist"><li>ParserElement._thread_local._exprArgCache[ lookup ] = pe</li></ul>973c974

<     _exprArgCache = {}

---

<ul class="quotelist"><li>_thread_local = thread_local()</li></ul>975c976

<         ParserElement._exprArgCache.clear()

---

<ul class="quotelist"><li>ParserElement._thread_local._exprArgCache = {}</li></ul><h6 id="toc1">=============</h6>
 end diff ===========================

I was wondering if similar handling can be done for other cases too. If you can provide more details on what other cases that need to be handled, I can try to complete the changes and share the file.



Thanks & Regards,

Raj
#### 2015-10-29 12:51:05 - speedplane
I was running into this issue and tried a fix similar to what Raj suggested. Unfortunately, it didn't work for me, but I don't know enough about pyparsing to know why. Instead, I put a threading.Lock around the entire parseString routine and that worked for me.

---
## 2007-06-29 13:55:11 - vineshp - Typo leaveWhiteSpace() -> leaveWhitespace()
your method for leaving whitespace appears to be leaveWhitespace() contrary to the leaveWhiteSpace() on this site.

#### 2007-07-07 10:55:41 - ptmcg
Good catch, thanks!

---
## 2007-07-13 11:03:32 - muralimani - Use Pyparsing in Jython Code
When i attempted to import pyparsing module in Jython i get following error.



Python Version - 2.5

Jython Version - 2.1

PyParsing - 1.4.5





Traceback (innermost last):

  File 'python/ProductClean.py', line 2, in ?

  File 'C:\Python25\Lib\pyparsing.py', line 849

                        yield tokens, preloc, nextLoc

#### 2007-07-13 11:48:28 - ptmcg
Pyparsing uses Python features introduced with Python 2.3.  This link () tells me that Jython 2.1 is only up to par with Python 2.1.



In the past, I have tried to 'backport' pyparsing to be more Jython compatible, but I've resigned myself to having to wait for Jython to catch up.  From the FAQ above, it sounds like Jython 2.2 may be sufficient.



(It looks like Jython in this particular instance is choking on the generator code in scanString.  If this is the only problem, try replacing the while loop in scanString with this:





    ret = []
            while loc <= instrlen and matches < maxMatches:
                try:
                    preloc = preparseFn( instring, loc )
                    nextLoc,tokens = parseFn( instring, preloc, callPreParse=False )
                except ParseException:
                    loc = preloc+1
                else:
                    matches += 1
                    #yield tokens, preloc, nextLoc
                    ret.append( (tokens, preloc, nextLoc) )
                    loc = nextLoc
            return ret



-- Paul

---
## 2007-08-01 09:14:14 - BlGene - operatorPrecedence and numTerms problem
I was playing with something simillar to simpleBool.py



nodes = [

('not',1,opAssoc.RIGHT,(lambda x:Not(data=x[0][1]))),

('and',2,opAssoc.LEFT, (lambda x:And(data=x[0][0::2]))),

('or' ,2,opAssoc.LEFT, (lambda x:Or(data=x[0][0::2])))

]



expr = operatorPrecedence(leafs,nodes)

res = expr.parseString('p or q or r')[0]



I was getting Or[p,q,r] (instead of Or[p,Or[q,r]] ), the x value being ['p','or','q','or','r'] even though numTerms for Or is 2. Is this intended, and if so I there a way to tell pyparsing to give me only binary pairs?

#### 2007-08-01 13:31:47 - ptmcg
numTerms = 2 or 1 is there to indicate whether the operator is binary or unary, but as you observe, the result for 'p or q or r' is Or([p,q,r]).  This is an artifact of pyparsing's handling (or rather not handling) of left-recursion, and has remained beyond my power to change.



You might be able to use Python's reduce function within a parse action to convert [p,q,r] to Or([Or([p,q]),r]), but it might be simpler to redefine the behavior of Or to accept 2..n terms.



-- Paul
#### 2007-08-15 13:02:14 - BlGene
sorry to take so long, but thanks for the info. Here is what the code ended up looking like:

('/',2,opAssoc.LEFT, (lambda x:Mul( [x[0][0]] + [Pow([y,mone]) for y in x[0][2::2] ] ))),

('+',2,opAssoc.LEFT, (lambda x:Add(x[0][0::2]))),

('-',2,opAssoc.LEFT, (lambda x:Add( [x[0][0]] + [Minus(y) for y in x[0][2::2] ]  ))),
#### 2007-08-15 14:53:04 - ptmcg
Glad to hear things worked out.



By the way, your post was extremely timely in helping me avoid a major goof in an upcoming publication on pyparsing, which I am just completing.  I was able to rework my operatorPrecedence example to comprehend 'binary' expressions such as 'p or q or r' using the [0::2] slicing method.



Thanks for writing,

-- Paul

---
## 2007-08-02 01:40:43 - aaron-L - Nested tags for asXML()?
Hi Paul. Excellent library you have here; I really enjoy using it.



I've run into a rough spot while messing around with the library and testing different features. I have the following parser definition for a Python-like function definition:





    function_keyword    = Literal('def').setResultsName('keyword')
    name                = Word(alphas).setResultsName('name')
    arguments           = delimitedList(name).setResultsName('arguments')
    function            = (function_keyword + name + '(' + arguments + ')' + ':' + name).setResultsName('function')
    [[/code]]
    
    The problem happens when I parse some text and attempt to display the results using asXML():
    

print function.parseString('def test(one, two, three): body').asXML()





This produces:





    <function>
      <keyword>def</keyword>
      <name>test</name>
      <ITEM>(</ITEM>
      <arguments>one</arguments>
      <name>two</name>
      <name>three</name>
      <ITEM>)</ITEM>
      <ITEM>:</ITEM>
      <name>body</name>
    </function>
    [[/code]]
    
    What I was expecting the result to be was:
    

<function>

  <keyword>def</keyword>

  <name>test</name>

  <ITEM>(</ITEM>

  <arguments>

    <name>one</name>

    <name>two</name>

    <name>three</name>

  </arguments>

  <ITEM>)</ITEM>

  <ITEM>:</ITEM>

  <name>body</name>

</function>





As you can see, in the second example the XML properly shows the nesting of arguments within the function. Is there something I can do to get this behavior?



Thanks for your time.

#### 2007-08-02 01:42:25 - aaron-L
Sorry for the terrible formatting! Shame there isn't a way to edit the post.
#### 2007-08-02 07:05:01 - ptmcg
To add nesting to the returned results, surround the expressions whose results you want nested within a Group class.  That is, change





    arguments           = delimitedList(name).setResultsName('arguments')





    arguments           = Group(delimitedList(name)).setResultsName('arguments')



That's it!  Now you'll get:



    <function>
      <keyword>def</keyword>
      <name>test</name>
      <ITEM>(</ITEM>
      <arguments>
        <name>one</name>
        <name>two</name>
        <name>three</name>
      </arguments>
      <ITEM>)</ITEM>
      <ITEM>:</ITEM>
      <name>body</name>
    </function>





Also check out using asList() instead of asXML(), might be easier to see the structured results (especially when printed with pprint.pprint()).



-- Paul
#### 2007-08-02 07:07:18 - ptmcg
Also, don't use  in your posts, it doesn't do anything. Just use [[code]] before and after your code samples, each on a line by itself.



-- Paul
#### 2007-08-03 06:21:14 - ptmcg
One last comment - you can have asXML skip over the unnamed items (like the parentheses and colon) by setting namedItemsOnly=True in the call to asXML().



    print function.parseString('def test(one, two, three): body')
        .asXML('function',namedItemsOnly=True )

Gives:



    <function>
      <keyword>def</keyword>
      <name>test</name>
      <arguments>
        <name>one</name>
        <name>two</name>
        <name>three</name>
      </arguments>
      <name>body</name>
    </function>

-- Paul

---
## 2007-08-02 14:25:52 - aaron-L - PyParsing for real-time syntax highlighting?
Hello again.



I'm toying around with writing a pure-Python text editor. There's a few options for syntax highlighting, but the one that has me most interested is defining language grammars in PyParsing for the best flexibility and ease-of-use.



Is this realistic or will I run into design/performance issues? Any advice on how to pursue this (PyParsing is great for parsing something all at once, I'm still trying to figure out the best way to parse updated portions of a document)?



Thanks for any help.

#### 2007-08-02 20:31:23 - ptmcg
I think language grammar definitions for the purpose of syntax highlighting can be much simpler than you would need for actual language interpreting or compiling.  All you need to do is recognize some specific items, like comments, quoted strings, and keywords.  



For instance, here is a converter that inserts tags around some selected keywords, comments and quoted strings.  It is far simpler than a full Python parser would need to be.



-- Paul





    from pyparsing import *
    
    kwd = MatchFirst(map(Keyword,
                 'if then else def print while for'.split()))
    kwd.setParseAction(lambda t:'^%s^' % t[0])
    pythonStyleComment.setParseAction(lambda t:'_%s_' % t[0])    
    quotedString.setParseAction(lambda t:'>%s<' % t[0])
    
    converter = kwd | pythonStyleComment | quotedString
    
    sampleCode = '''
    # this is a sample Python program
    
    def fn(name,n):
       for i in range(n):
           print 'Hello, %s!' % name
    
    fn('World', 100)
    '''
    
    print converter.transformString(sampleCode)



Prints out:



    _# this is a sample Python program_
    
    ^def^ fn(name,n):
       ^for^ i in range(n):
           ^print^ >'Hello, %s!'< % name
    
    fn(>'World'<, 100)



---
## 2007-08-17 08:19:01 - tetris - trying to parse very random strings
Hi,



is it possible to write a grammar to account for a string which can be made up of a combination of capital letters, lowercase letters, numbers and underscores, in any order? Examples are:



EX_bz_e

AMETr

EX_spc_hs_e

so4_r



They are all located in double quotes so I can do this:



 

quote = Literal('\'')



    
    to strip off the double quotes, but I'm struggling to understand if it's possible to account for my very random strings - basically so i can parse 'EX_bz_e' into:
    

 [' ' ' , 'EX_bz_e' , ' ' ']





hope that makes sense - am relative newcomer to both python and pyparsing so please forgive me if i'm asking something ridiculous!



Many thanks :)

#### 2007-08-17 08:20:02 - tetris
oops sorry realised the formatting has gone a bit crazy - was trying to have two bits of code :( hopefully still readable!
#### 2007-08-17 09:40:03 - ptmcg
If they are all enclosed in double quotes, the easiest thing would be to use the `dblQuotedString` expression which is predefined for you in pyparsing.  To strip off the quotes from the parsed results, add the parse action `removeQuotes`, which is also included in pyparsing.  Like this:





    from pyparsing import *
    
    data = '''\
        'EX_bz_e'
        'AMETr'
        'EX_spc_hs_e'
        'so4_r'
        '''
    
    parser = OneOrMore( dblQuotedString.setParseAction(removeQuotes) )
    print parser.parseString(data)



Giving:



    ['EX_bz_e', 'AMETr', 'EX_spc_hs_e', 'so4_r']





Could you post the actual text you are trying to process, including quotes or punctuation or whatever?  The hardest thing in trying to support users is when they don't post the actual text, and we end up creating a grammar for what they posted but it doesn't work on their actual problem.



(For message formatting help, click on the link on the lower right corner of this page, labeled 'help on how to format text'.)



-- Paul
#### 2007-08-20 02:11:45 - tetris
ok so an example line is:



<node name='base' label='GALSIDEtl' id='-10315'>



i'm wanting to get out the label and id fields as i need to read these into a dictionary - have done some reading around and it looks like i'd need to use scan string to read in the entire file as not every line has the above format? so i just need to match those that do and extract the label and id values.



thanks!
#### 2007-08-20 02:19:36 - tetris
ok great have worked it out for my code, just needed to define something in the grammar using dblQuotedString to strip out the quotes, works perfectly :)
#### 2007-08-20 06:38:36 - ptmcg
Since you are cracking what look to be XML strings, look at using the makeXMLTags helper method.  It returns a tuple with expressions for opening and closing tags with a given name.  Since you are just interested in the opening tag, I used just the 0'th element of the returned tuple.  If you are cracking HTML, use makeHTMLTags.



If you are doing *lots* of XML, you might look at switching to elementtree, I think it will run much faster than pyparsing.



-- Paul





    from pyparsing import *
    
    data = '''<blah><node name='base' label='GALSIDEtl' id='-10315'><blahblah>'''
    
    nodeTag = makeXMLTags('node')[0]
    
    nodes = nodeTag.searchString(data)
    for n in nodes:
        print '---------'
        print n.dump()
        print n.name
        print n.label



prints:





    ---------
    ['node', ['name', 'base'], ['label', 'GALSIDEtl'], ['id', '-10315'], False]
    - empty: False
    - id: -10315
    - label: GALSIDEtl
    - name: base
    - startNode: ['node', ['name', 'base'], ['label', 'GALSIDEtl'], ['id', '-10315'], False]
      - empty: False
      - id: -10315
      - label: GALSIDEtl
      - name: base
    base
    GALSIDEtl



---
## 2007-08-29 12:11:54 - jjrael - Sync contents of two files - transformString  examples?
Hello,



I am a newbie to pyparsing but I really like it so far.



I have two files with a complicated structure and I want to carry some info from one to the other. I can load the first file into python as a dict (old perl parser -> yaml -> python). Next, I want to load the second file if an element in the second file is a key in the dict of the first file, I want to replace the following elements with the contents of the dict.



I have used parseString to parse the second file. The parser does not do anything, I just have verified that I have captured the format.



I think I want to use transformString so I can read the second file and then change the contents based on some conditions. For example, I have:





    bitInfo = Group(bits + access + mapNum + bitName + (ZeroOrMore(comment) ^ OneOrMore(cmodParam)) + ZeroOrMore(comment))



(as I look at this, I think it can be cleaned up) I would like to do:





    if bitName in myDict:
      comment = myDict[bitName]['comment']
      cmodParam = myDict[bitName]['cmodParm']
    else:
      #leave it alone



I just don't know how to do this using setParseAction and transformString. I don't even know if that is the best way.



Can you all point me to some examples that do something like this?



jr

#### 2007-08-29 16:07:32 - jjrael
Ok,



I decide to do this a different way. I'll probably use scanString.



Please disregard this post.



jr

---
## 2007-08-29 16:13:15 - jjrael - How to match man of two types out of order
Hello,



I have a section of data that look like:





    <other stuff>
    # comment
    a = 1
    b = 2
    # comment
    # comment
    c = 3
    <other stuff>



I have two items to match the assignment and one to match the comment but I can't match the whole thing. I currently have:





    bitTail = (ZeroOrMore(comment) ^ ZeroOrMore(cmodParam)) + ZeroOrMore(comment) 



I appreciate any help.



Thanks,



jr

#### 2007-08-29 16:42:51 - jjrael
Got it:





    bitTail = ZeroOrMore(comment | cmodParam)



jr
#### 2007-08-29 17:34:06 - ptmcg
You could also try this special case for comments:



    bitTail = ZeroOrMore(cmodParam).ignore(comment)



This will suppress comments from the output, leaving just the non-comment text.



Welcome to pyparsing, enjoy!



-- Paul

---
## 2007-08-29 18:10:25 - jjrael - scanString - dataStart and dataEnd don't make sense
Hello,



I probably have used up my daily allocated number of posts but here it goes anyway.



I am using scanString but the dataStart and dataEnd don't make sense.



I do:





    info.scanString( line )
    ...
    >>> dataStart
    141428
    >>> len(line)
    70896



The dataStart value is almost twice the size of my string. Any idea what I am doing wrong?



jr

#### 2007-08-30 00:57:01 - ptmcg
Does line contain tabs?  The default behavior of parseString, scanString, and searchString is to autoexpand tabs to spaces.  This can be suppressed using parseWithTabs.



-- Paul
#### 2007-09-07 14:09:50 - jjrael
Thanks,



That was it.



jr

---
## 2007-09-04 09:51:56 - luca_politti - newbie problems with pyparsing
Hello, I'm new in pyparsing and I have this problem:

I have to split a string like this



'('a', 'b', 'c'), ('d', 'e', 'f'), ...'



to have something like:



[['a', 'b', 'c'], ['d', 'e', 'f'], ...]



So I do this:



from pyparsing import Word, alphas, OneOrMore, Group, Optional



s = '('a', 'b', 'c'), ('d', 'e', 'f'), ('g', 'h', 'i')'



caps = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'

lowers = caps.lower()

element = Word(lowers)

elementRef = Group( element + Optional(',') )



rule = '(' + OneOrMore( elementRef ) + ')' + Optional(',')

rule_all = OneOrMore( rule )



s = '(cdsacas, cds)'

print rule_all.parseString(s)



but I found this error:



pyparsing.ParseException: Expected W:(ABCD...,abcd...) (at char 1), (line:1, col:2)



Where I worng? How could I do this?



Thanks a lot...



Luca

#### 2007-09-04 16:58:52 - ptmcg
Luca -



Thanks for your post, and I'm glad you are considering using pyparsing for your problem.



In the pyparsing examples directory (if you download the source or docs distribution), there is an example titled list1.py, which describes a recursive list parser of items in nested parentheses.  You can also read about this example from my PyCon '06 presentation (), starting about slide 21 or so.



But for your simple case, since you don't have to mess with nested parens, here are some tips to get you started:





Start by defining the basic element, which you did just fine:





    caps = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'
    lowers = caps.lower()
    element = Word(lowers)



At this point, you started to create something about an elementRef, in anticipation of stringing them together into a list.  I think this was where you got derailed.  Instead of this:





    elementRef = Group( element + Optional(',') )
    
    rule = '(' + OneOrMore( elementRef ) + ')' + Optional(',')
    rule_all = OneOrMore( rule )



I would suggest this:





    rule = '(' + element + ZeroOrMore( ',' + element ) + ')'
    rule_all = rule + ZeroOrMore( ',' + rule )



This pattern of defining delimited lists as



    X + ZeroOrMore( ',' + X )

is so common, there is a pyparsing helper method for it, called delimitedList.  delimitedList does the added help of suppressing the comma delimiters from the output, so that instead of parsing 'a,b,c' as [ 'a', ',', 'b', ',', 'c'], you just get ['a', 'b', 'c'].  Here is how your parser looks using delimitedList:





    rule = '(' + delimitedList( element ) + ')'
    rule_all = delimitedList( rule )



This is getting close, but we've left out the grouping of rules, and we've left in those annoying parentheses.  With these minor changes:





    LPAR = Suppress('(')
    RPAR = Suppress(')')
    rule = Group( LPAR + delimitedList( element ) + RPAR )
    rule_all = delimitedList( rule )



Parsing your input of '(cdsacas, cds)' gives the results:



    [['cdsacas', 'cds']]



Lastly, your original example used quoted strings instead of just strings of lowers.  To enhance your parser to handle this, change element to:



    element = dblQuotedString.setParseAction(removeQuotes) | Word(lowers)



And now you can parse your original example of '('a', 'b', 'c'), ('d', 'e', 'f'), ('g', 'h', 'i')' to:



    [['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i']]





-- Paul
#### 2007-09-05 01:15:39 - luca_politti
Ok, thanks a lot Paul. I found your suggests very useful and I could solve my problem.



Congratulations for your pyparsing library, a very useful tool.



Thanks again...



Luca

---
## 2007-09-07 15:01:34 - jjrael - How to append with setResultsName
Hello,



I am using setResultsName to make a dict of parse results:





    comment.setResultsName('comment')



In my input file, for a section, I can have many comments in a row (a comment begins with '#' and contains the rest of the line). For example:





    # Current control for 
    # buffer.  Previously connected to test



When I use setResultsName, only the last value gets stored. Is it possible to have setResultsName store the data in a list and append results instead? 



I am starting to think my implementation choices need tweaking...



jr

#### 2007-09-07 15:24:42 - ptmcg
Yes - add listAllMatches=true when calling setResultsName.



-- Paul
#### 2007-09-07 15:57:04 - jjrael
Thanks!



BTW, is this documented someplace? 



I don't mind doing a little research before I post a question.



I work mostly off the O'Reilly article.



jr
#### 2007-09-07 17:25:55 - ptmcg
Not much in the way of user 'how to' documentation, but there is extensive reference docs in the htmldocs directory of the source distribution (generated using epydoc).  If you just use easy_install, you don't get this directory, so you have to download either the source or docs dist from SourceForge.



-- Paul
#### 2007-09-07 17:44:04 - jjrael
Ok, I used easy_install!!



Thanks,



jr

---
## 2007-09-11 04:22:22 - rustin - Newbie problem with Optional()/LineEnd()
Hi, Can someone help explain why this fails and how to fix it? It seems to be treating the first '\n' as whitespace, matching 'KW' on the second line as the optional parameter and then expecting a '\n'.



However, if you end each line with '.' (say) and swap LineEnd() for Literal('.'), it works fine. Thanks,







    from pyparsing import *
    
    buf = 'KW aaaa\nKW bbbb xxxx\nKW cccc\n'
    
    varname = Word(alphanums + '_')
    rule = Keyword('KW') + varname + Optional(varname) + LineEnd()
    gr = OneOrMore(rule)
    
    result = gr.parseString(buf)



ParseException: Expected end of line (at char 11), (line:2, col:4)

#### 2007-09-11 06:49:53 - ptmcg
Try stating your requirement verbally as specifically as you can.  Remember to be clear about where newlines can and cannot go.



'A rule starts with 'KW' and is followed by one required word, and optionally followed by another word on the same line, and then a newline.'



So the 'optionally' part is not just another word after the first, but another word with no newline between it and the first.  This would be written in pyparsing as:



    NL = Suppress(LineEnd())  # suppress these, else results are littered with \n's
    varname = Word(alphanums + '_')
    rule = Keyword('KW') + varname + Optional(~NL + varname) + NL



For your next iteration, consider using Group to group your tokens by rule, and then add some results names to make it easier to pull out the two varnames from the parsed tokens.  (Write back if this made no sense, and I'll post more examples.)



-- Paul
#### 2007-09-12 01:38:51 - rustin
Paul - thanks for your helpful and prompt reply.



It still seems to me that LineEnd() is not treated the same as say a literal terminator. Compare:



    buf = 'KW aaaa\nKW bbbb xxxx\nKW cccc\n'
    END = Suppress(LineEnd())
    varname = Word(alphanums + '_')
    rule = Keyword('KW') + varname + Optional(~END + varname) + END
    gr = OneOrMore(rule)

based on your example with:



    buf = 'KW aaaa.KW bbbb xxxx.KW cccc.'
    END = Suppress(Literal('.'))
    varname = Word(alphanums + '_')
    rule = Keyword('KW') + varname + Optional(varname) + END
    gr = OneOrMore(rule)

Both work but the second one doesn't need the (~END) in Optional and seems more natural. And neither LineEnd() nor '.' are included in 'varname' so it seems strange to have to explicitly exclude them. It is as if 'backtracking' is not working correctly for LineEnd().



BTW - pyparsing is great stuff -- and I'm using Group, Suppress and many others in the actual code but stripped them out for clarity.
#### 2007-09-12 06:53:00 - ptmcg
Yes, well, LineEnd is a little different from other constructs, because it also falls into the category of whitespace, so pyparsing will just read over it just like intervening spaces.  So that is why we have to tell pyparsing that LineEnd()s are not allowed between varnames.



In contrast, pyparsing will **never** skip over a '.' (or any other non-whitespace) character, so there is no need to indicate that there are no skippable '.'s between varnames.



(Actually, you could add '.' to the list of whitespace chars using ParserElement.setDefaultWhitespaceChars(), and probably see similar behavior.  I never thought about this before, it might make for an intriguing grammar.)



-- Paul
#### 2007-09-13 04:26:54 - rustin
Hi Paul,



Thanks again, reversing your idea slightly, I've taken '\n' out of the default whitespace characters and this has solved most of my problems using code along these lines:



    buf = 'KW aaaa\nKW bbbb xxxx\nKW cccc\n'
    varname = Word(alphanums + '_')
    NL = Suppress(Literal('\n'))
    rule = Keyword('KW') + varname + Optional(varname) + NL
    gr = OneOrMore(rule)
    DWC = ''.join(x for x in gr.DEFAULT_WHITE_CHARS if x != '\n')
    gr.setDefaultWhitespaceChars(DWC)
    result = gr.parseString(buf)


#### 2007-09-13 05:47:45 - ptmcg
Does this even work?  The default whitespace chars are only used to define the whitespace for ParserElements created **after** updating the default characters.  It is an alternative to going through the parser and calling setWhitespaceChars on each element.  In fact, calling setWhitespaceChars on a per-element basis is not a good approach, since many ParserElements are created internally.  



The definition of whitespace characters is there to say 'here are the characters to skip over before trying to match this expression.'  In this example:



    from pyparsing import *
    
    longExpr = Word(nums) + 'A' + 'B' + 'C'
    longExpr.setWhitespaceChars('')  # disables ws skipping altogether
    
    print longExpr.parseString('123 ABC')
    print longExpr.parseString('123 A B C')
    print longExpr.parseString('123ABC')
    print longExpr.parseString(' 123 ABC')

only the last expression raises an exception, because we have suppressed whitespace skipping only for the overall longExpr, but not for the individual sub expressions.



In order for the default chars to affect any created ParserElements, your two lines of code:



    DWC = ''.join(x for x in gr.DEFAULT_WHITE_CHARS if x != '\n')
    gr.setDefaultWhitespaceChars(DWC)



should come at the top of your script, before defining varname (or any other expression).



-- Paul
#### 2007-09-15 03:47:33 - rustin
Hi Paul,



Quite right - it doesn't work as written -- it did seem to but I think it was an artefact of me playing around in the IDE. Anyway I'm using:



    DWC = ''.join([x for x in ParseElementEnhance.DEFAULT_WHITE_CHARS if x != '\n'])
    ParseElementEnhance.setDefaultWhitespaceChars(DWC)

at the top of the grammar definition which does seem to work. Is this a 'legitimate' approach?
#### 2007-09-16 00:31:39 - ptmcg
This looks peachy!



-- Paul

---
## 2007-09-21 04:13:37 - hamidh - Unicode problem perhaps?
Hi

New to pyparsing so please excuse mistakes, but trying to parse html from websites in code below which is failing for one website but working for another. The failing one is a French website and so I am guessing that this may be a unicode/latin-1 issue. I've looked around and tried a few approaches such as using decode('utf-8') but none appear to work. Hence this post and hoping you can advise on this.

Thanks





    
    import urllib
    
    from pyparsing import *
    
    tdStart,tdEnd = makeHTMLTags('td')
    
    # read HTML from a web page
    
    #this url fails
    serverListPage = urllib.urlopen( 'http://www.grtgaz.com/histoDonneesJournalieres.do?task=PrevRealCons&lang=en&mois=2006/08/01' )
    
    #this url works
    #serverListPage = urllib.urlopen( 'http://www.metoffice.gov.uk/education/archive/uk/observation_1.html')
    
    htmlText = serverListPage.read()
    serverListPage.close()
    
    tdAnchor =     tdStart.suppress() + SkipTo(tdEnd).setResultsName('dt0') + tdEnd.suppress() +\
        tdStart.suppress() + SkipTo(tdEnd).setResultsName('dt1') + tdEnd.suppress() +\
        tdStart.suppress() + SkipTo(tdEnd).setResultsName('dt2') + tdEnd.suppress() +\
        tdStart.suppress() + SkipTo(tdEnd).setResultsName('dt3') + tdEnd.suppress() +\
        tdStart.suppress() + SkipTo(tdEnd).setResultsName('dt4') + tdEnd.suppress() +\
        tdStart.suppress() + SkipTo(tdEnd).setResultsName('dt5') + tdEnd.suppress() 
    
    for tokens, start, end in tdAnchor.scanString(htmlText):
        print tokens
    



#### 2007-09-21 08:00:18 - ptmcg
No, I think the issue is a non-attribute keyword 'nowrap' used in the <td> tags on the French web page.  This requires a change to the _makeTags routine in pyparsing.  You can do this for yourself, or check out from the SourceForge SVN repository later this weekend (when I get around to writing a test and checking the change back into source control).  You can try modding your own version of pyparsing by changing this code in the _makeTags routine:





    openTag = Suppress('<') + tagStr + \
                    Dict(ZeroOrMore(Group( tagAttrName.setParseAction(downcaseTokens) + \
                    Suppress('=') + tagAttrValue ))) + \
                    Optional('/',default=[False]).setResultsName('empty').setParseAction(lambda s,l,t:t[0]=='/') + Suppress('>')

to:



    openTag = Suppress('<') + tagStr + \
                    Dict(ZeroOrMore(Group( tagAttrName.setParseAction(downcaseTokens) + \
                    Optional(Suppress('=') + tagAttrValue) ))) + \
                    Optional('/',default=[False]).setResultsName('empty').setParseAction(lambda s,l,t:t[0]=='/') + Suppress('>')

(The change is on line 2987 if you are using pyparsing 1.4.7.)



Thanks for pointing this out!



-- Paul
#### 2007-09-21 09:16:14 - hamidh
Many THANKS!

That works for me now :-)

---
## 2007-09-24 18:53:07 - gefan - combine() question
Hi,



I recently used pyparing to build some parsers. It works very well. Today I meet a problem related with Combine(). 



I have code like this:



    line = 'Slot:  1'
      num = Word(nums)
      g1 = Combine('Slot:' + num)
      result = g1.parseString(line)

When I run it, I get an exception like

pyparsing.ParseException: Expected W:(0123...) (at char 5), (line:1, col:6)



If I change code to:



    line = 'Slot:  1'
      num = Word(nums)
      g1 = 'Slot:' + num
      result = g1.parseString(line)

Then parsing works without any error.



So my question is why Combine doesn't skip whitespace?



Thanks!



-gefan

#### 2007-09-24 19:25:09 - ptmcg
Combine does 2 things:

- concatenates all matched tokens into a single token

- requires all tokens within the Combine to be adjacent in the input string



The reason for this is if you had something like an expression for a real number:



    realnum = Word(nums) + '.' + Word(nums)



The problem is that what you get back from parsing '3.14159' is ['3','.','14159'].  So to resolve that, we use Combine:



    realnum = Combine( Word(nums) + '.' + Word(nums) )



And it returns a nice string of '3.14159'.  Now, we also don't want to accidentally match tokens with intervening whitespace, as in 'The answer is 3. 1492 is when Columbus sailed to America.' and have '3. 1492' matchable as a real number.



But, sometimes we just want the tokens whether they were contiguous or not.  Fortunately, you can disable this behavior.  The Combine constructor will take a keyword argument 'adjacent=False', as in:



    g1 = Combine('Slot:' + num, adjacent=False)



However, this will not preserve the intervening whitespace - from your example, this will give you the string 'Slot:1'.  If you need some whitespace, you can also specify an optional joinString argument (the default is ''), and pass a single space.  At the python prompt, type 'help(Combine)' and you should get some of this info.



If you are trying to organize the results, consider using Group instead.  This code:



    g1 = Group('Slot:' + num)



will parse 'Slot:  1' and return the token group ['Slot:', '1'].  Group does not do any different whitespace handling, it just wraps the matched tokens within a sub-list.  This gives you the tokens already broken up, and there would be no need to use split() or some other mechanism to extract the '1' from the parsed results.



-- Paul
#### 2007-09-25 09:29:54 - gefan
Hi Paul,



Thanks a lot!



I didn't realize there is an 'adjacent' argument. That's what I need.



Regards



-gefan
#### 2007-09-25 12:48:46 - ptmcg
gefan -



Please don't take this question the wrong way, I am not going to slam you with a 'read the documentation' comment.  But I want to make sure that people who download pyparsing get the documentation that's also provided.



Did you download and run just the win32 binary installer?  If so, this *only* installs the basic minimum needed to run pyparsing (although all of the class and method docstrings are still available using the help macro at the Python command prompt).  There is a directory named htmldoc that gets installed if you download and extract either the source distribution or the docs distribution.  These other distributions also include a number of extensive examples - these examples can also be obtained online from this wiki at the  link.  You can also find some links to several 'How to'-style articles and presentations from this wiki's  link.



Or did you run pyparsing as it was installed in a Linux distro?  If so, then I need to make sure that the distro admin includes the documentation with the software as well as just the basic pyparsing.py code.



Or did you have the documentation handy, and just didn't look at it?  No big deal, this happens.  I just need to find a way to make the pyparsing docs more visible and approachable.



No matter your answer, thanks for responding, and keep us posted on your continuing pyparsing efforts.



-- Paul
#### 2007-09-25 13:12:08 - ptmcg
And I guess easy_install is another way people can install pyparsing now, and the default behavior is that this does not install the help files either.



-- Paul
#### 2007-09-25 14:58:19 - gefan
Hi Paul,



Thanks for your comments. It's my fault. Next time I will check docs more carefully before I post questions here.



Regards



-gefan

---
## 2007-10-10 20:16:23 - gasolin - To match python docstring?
Is it good to use QuotedString to get python docstring ('''docstring''' or '''docstring''')



I tried 



from pyparsing import QuotedString

docstring = QuotedString(quoteChar=''''', escChar='\\', multiline=True)+\

            QuotedString(quoteChar=''''', escChar='\\', multiline=True)



but not work.



regards

#### 2007-11-13 02:27:53 - ptmcg
Change:



    docstring  = QuotedString(quoteChar=''''', escChar='\\', multiline=True) +\
                 QuotedString(quoteChar=''''', escChar='\\', multiline=True)



to:



    docstring  = QuotedString(quoteChar=''''', escChar='\\', multiline=True) |\
                 QuotedString(quoteChar=''''', escChar='\\', multiline=True)



-- Paul

---
## 2007-10-11 14:26:57 - kar1107 - Effective uses for pyparsing
Hi,



I've been experimenting with pyparsing for a few days now. I like its easy to use design and I could try out some BNF grammars quickly.



What's I'm puzzled is, I couldn't find much real world problems where pyparsing would really give an edge compared with regular expressions + string methods. Assuming a user is very comfortable with re usage and with various other string operations, wondering in what cases moving to pyparsing provides a significant benefit.



I see mathematically pyparsing is a super-set of re (class 2 versus class 3 grammars) and I could quickly write a a^nb^n BNF(e.g. (((()))), (()) ..) and test it. This language cannot be solved by re.



Many times while instead of using plain strings, if I use re to describe an entity, I found it useful. Wondering if the same can be said about moving from re to pyparsing. At least mathematically it's using a more powerful abstraction.



I can see pyparsing providing a good fit for parsing text where white-space is not significant. Also for text where newlines don't contribute much (just like anyother white-space). For other elaborate BNFs like HTML/XML, we have specialized modules to handle them (say BeautifulSoup for HTML). 



Thanks,

Karthik

#### 2007-10-11 16:45:45 - ptmcg
Karthik -



This may end up being as much a matter of taste or comfort zone as anything.  I would direct you to the Whos Using Pyparsing and the Examples pages on this wiki - the first is definitely a set of 'real-world' examples, and the second should give some ideas on applications of specific pyparsing features that may give you some broader view to what pyparsing might do for you.  (Also, you might get some thoughts from the Short Cut I just did for O'Reilly - they have posted the Zen of Pyparsing section online for free in just about its entirety - see the link on the wiki home page.)



I know that BeautifulSoup is certainly the canonical answer when the question of parsing HTML comes up, but web page scraping with Pyparsing can be quite simple as well, just a slightly different approach to the problem.  I see BS as taking something of a DOM/Xpath view of modeling the HTML source code - with pyparsing, things are more re-ish, but with most if not all of the re fallabilities handled for you when using pyparsing's makeHTMLTags built-in method.  As for XML, I pretty much steer people away from pyparsing, for just the reasons that you cite.



If you are already a pro at regular expressions and string methods, perhaps you are not really the target audience for pyparsing.  I'm sure an re pro would readily bang out an re to handle something to handle the assignment expression for the mini-BNF below:



    term ::= identifier | integer
    operator ::= '+' | '-' | '*' | '/'
    assignment ::= identifier '=' term ( operator term )*

in no time.  But I find the pyparsing rendition to be more self-expressive, and easier to organize into separate parts, especially when I want to integrate conversion of '123' to 123, or give fields within the assignment expression names like 'lhs' and 'rhs'.  And then going back and adding support for floating point numbers is really quite direct with a pyparsing implementation - I pretty much know right where I need to go to add this feature, and the corresponding string-to-float conversion.  And don't forget to add \s* between every element, since 'a=1+2*b' is just as likely to be given as 'a = 1 + 2*b'.



So I don't feel I've really given a slam-dunk, cut-and-dried answer to your question.  I think the case for pyparsing is more an accumulation of small features - parse actions, results names, support for recursion, human-readable grammar definitions, and a small set of helper methods and routines - add up to a collective parsing environment that can be very productive for short-run needs, or maintainable long-term on projects that may be staffed with folks who are not all re geniuses.



-- Paul
#### 2007-10-11 17:40:37 - kar1107
Hi Paul,



Thanks for the details. I do agree pyparsing demands a little more time during the design phase -- how to define the grammar. And that is a good thing as usually a cleaner design is lot helpful for a problem than rushing to the implementation.



Regarding html screen scraping for small tasks (like your example NTP servers on onlamp article), yes it helps. While I am fine with this approach, I've found using 'elinks --dump' and grabbing the result at times quicker (possibly after passing it thru' a set of 'sed' commands).



In any case, pyparsing is very easy to try out a BNF grammar while other solutions like lex/yacc can easily scare away a newbie.



Thanks,

Karthik

---
## 2007-10-18 08:17:09 - metaperl - Listing on Python parsing tools
pyparsing is listed. I think this is a nice listing. With no attempt to promote any particular tool:

   


---
## 2007-10-18 16:01:24 - krjackson - simple newbie question
It's likely I'm just confused, but I don't quite understand what's going on. I'm matching a number of things and then want to match a newline character at the end of the line. The simplist example is:



    >>> test5 = Literal('a') + Literal('\n') + LineEnd()
    >>> test5.parseString('a\n')
    Traceback (most recent call last):
      File '<stdin>', line 1, in ?
      File '/sw/lib/python2.4/site-packages/pyparsing.py', line 926, in parseString
        loc, tokens = self._parse( instring.expandtabs(), 0 )
      File '/sw/lib/python2.4/site-packages/pyparsing.py', line 808, in _parseNoCache
        raise ParseException( instring, len(instring), self.errmsg, self )
    pyparsing.ParseException:  (at char 2), (line:2, col:1)

This is all with 1.4.8.



If someone could enlighten me on why this doesn't match, I'd appreciate it. Also, any thoughts on the right way to match a newline at the end of the line would be great.

thanks,

--keith

#### 2007-10-18 19:49:29 - ptmcg
By default, newlines are skipped as whitespace.  So what happens is that the Literal('a') is matched.  Then before matching the next expression, whitespace is skipped, meaning the newline.  Then pyparsing tries to match Literal('\n'), but it has already read past it.



For that matter, '\n' matches LineEnd(), so to match a newline, you should just use LineEnd() instead of Literal('\n').



Literal('<whatever>') is really intended for matching printable strings, like Literal('def'), Literal('struct'), etc.  If you want to make a Literal of whitespace, you can use the White class, as in White('\n').



Here are 5 alternatives to your expression.



    >>> test5 = Literal('a') + White('\n') + LineEnd()
    >>> test5.parseString('a\n')
    (['a', '\n'], {})
    
    >>> test5 = Literal('a') + Literal('\n').leaveWhitespace() + LineEnd()
    >>> test5.parseString('a\n')
    (['a', '\n'], {})
    
    >>> test5 = Literal('a') + LineEnd()
    >>> test5.parseString('a\n')
    (['a', '\n'], {})
    
    >>> test5 = Literal('a') + LineEnd() + LineEnd()
    >>> test5.parseString('a\n')
    (['a', '\n'], {})
    
    >>> test5 = Literal('a') + LineEnd() + StringEnd()
    >>> test5.parseString('a\n')
    (['a', '\n'], {})



In general, I recommend that people try to work with pyparsing's whitespace skipping instead of working around it.  If you must treat newlines as a significant part of your grammar, you should disable them before defining any pyparsing expressions using:



    ParserElement.setDefaultWhitespaceChars(' \t')



If you do this, you'll then need to explicitly include LineEnd() elements whereever they can occur.



If you are trying to parse individual lines, you also might want to just create a parser for just a single line's worth of data, and then parse each line of the input string separately.



-- Paul
#### 2007-10-18 20:34:23 - ptmcg
After re-reading your post, I think you were thinking of LineEnd for matching end of the input string.  For this you should use StringEnd.  So I think of the different alternatives above, this one is probably closest to what you were trying to do:



    test5 = Literal('a') + LineEnd() + StringEnd()



The LineEnd() will match the newline, and StringEnd() will match the end of the string.  StringEnd() is good for making sure that there is no additional text after the grammar matches.



-- Paul
#### 2007-10-19 11:27:47 - ptmcg
One last thought: is the newline at the end of the string just an artifact of using readlines() and getting a list of newline-terminated strings?  If so, just ignore it as whitespace.  Here's my updated recommendation:



    >>> test5 = 'a' + StringEnd()
    >>> test5.parseString('a\n')
    (['a'], {})



-- Paul
#### 2007-10-19 11:39:05 - krjackson
Thanks for all the info Paul. That was *really* helpful. I believe that what I need to do is:



    White('\n').suppress() + StringEnd()



I'm actually validating that log files conform to spec. That includes being terminated with a newline. That's why I can't just ignore the whitespace. I haven't tested this, but I'm assuming if I were to run under windoze that LineEnd would match \r\n. So it looks like matching with the explicit White class is best.



Thanks again for all your help!

--keith

---
## 2007-11-02 06:34:09 - louis_nichols - help needed with using Dict
Hi all!



I would reallya ppreciate some help in a simple matter. The text I need to parse has a structure like this:





    Statement1;
    Statement2;
    
    
    Block1 {
        Statement3;
        Statement4;
    }
    
    Block2 {
        Statement5;
        Statement6;
        Statement7;
        Statement8;
        Statement9;
    }
    
    Block3 {
        SubBlock1    { 
            Statement10;
            Statement11;
            Statement12;
            Statement13;
        }
    }
    
    
    
    Block4{
        SubBlock2 {
            Statement14;
            Statement15;
            SubBlock3{
                Statement16;
            }
        }
    }



The grammar I defined, after long struggles :) (I am not a developer) is this:





    bnf = Forward()
    block = Forward()
    
    statement =  (Word (alphanums + '=,':_ .') + Suppress(';'))
    item = statement | Group(block)
    
    blockName = Word (alphanums + '=,':_ .') + Suppress('{')
    block << Dict(blockName  + ZeroOrMore(item) +  Suppress('}'))
    
    bnf = ZeroOrMore(item)
    
    bnf.ignore(cppStyleComment)



Now, the text gets parsed, but not exactly as I need it:



['Statement1',

 'Statement2',

 ['Block1 ', 'Statement3', 'Statement4'],

 ['Block2 ',

  'Statement5',

  'Statement6',

  'Statement7',

  'Statement8',

  'Statement9'],

 ['Block3 ',

  ['SubBlock1       ',

   'Statement10',

   'Statement11',

   'Statement12',

   'Statement13']],

 ['Block4',

  ['SubBlock2 ', 'Statement14', 'Statement15', ['SubBlock3', 'Statement16']]]]



As you may guess, I need the whole structure as a dictionary, where each blockName is the key of the elements in block. I tried all kind of versions of placing Dict, but I always get an error.



Any help will be greatly appreciated! Thanks!

#### 2007-11-03 12:37:30 - ptmcg
Louis -



I think you have made a very good start, especially since you are 'not a developer.' :)  A recursive grammar such as this is not really a beginner's topic. And then, on top of recursion, you are also wanting to use Dict!



My first question has to do with the opening 2 statements, which are not defined within any block.  Where do you expect these to go in a dict-like output?  For now, I am going to wrap your sample text inside a super-block called GLOBAL, so that we can focus on the definition of block, and the addition of Dict to them.



Dict is a hard class to work with, since it really requires a special type of construction expression.  I created the helper method dictOf(key,value) just to simplify this a bit.  dictOf takes the key and value expressions, and returns this:



Dict( ZeroOrMore( Group ( key + value ) ) )



Dict will take tokens like this:



Dict( ZeroOrMore( Group( Word(alphas) + OneOrMore(Word(nums)))))

A 1 2 3

B 7 8 1

C 9 10 11



and return tokens as:

[['A', '1', '2', '3'], ['B', '7', '8', '1'], ['C', '9', '10', '11']]



But by adding Dict (note that Dict is wrapped around the entire list, not just a single entry in the list), the returned tokens also get named fields.  The names are taken from the 0'th element of each group, and the value of each field is taken from the [1:] slice for each group.



There are a couple of ways to access this as a dict (assuming you have saved the value returned by parseString to a variable named results):

- Just use the returned results as if they were a dict.  results.keys() will return ['A', 'B', 'C'], results['A'] will return ['1', '2', '3'], and results['Z'] will throw a KeyError.

- Convert to a dict using results.asDict().

- Dump out the parsed data using results.dump()



The difficulty you have raised is that *your* dict is a recursive one - oy!  I seem to have recursion on the brain lately!



I'll have to leave this to you for now, hope these comments give you some insight into how to proceed.



-- Paul
#### 2007-11-04 09:11:11 - louis_nichols
Hi and thanks for the quick response.



I am quite proud for understanding (some of) pyparsing myself. :) However, although I am not a developer, I am an engineer nevertheless, so I don't feel like I just discovered an optimized version of the wheel. :)



I just started working based on your suggestions. I'm afraid I can't manage to solve the issue with any of the methods keys(), asDict() or dump().



The super-block idea shines some light on the issue, though. I must admit I hadn't thought of the first two statements issue... The problem is I can't find a way to enclose everything in a root key, as the super-block ides suggests. I mean, I just can't find how to define that token. I tried with Empty(), but no luck. I'm still working on this, though.



Another thought was this: extract 'orphan' statements as elements of the root list and just make dicts from the blocks. I seem to remember I had managed this at an earlier stage of development, but then I was having problems with recursion and I lost the solution on the way. But the question is, in this version, would I be able to access upper-most blocks by their name? This is what I am aiming towards, because the names of the blocks are standardized for the file format I am working with.
#### 2007-11-04 09:24:47 - louis_nichols
BINGO!



Just as I finished the previous reply, I thought I would give another read to your reply and then it hit me:





    bnf = Forward()
            block = Forward()
    
            statement =  (Word (alphanums + '=,':_ .') + Suppress(';'))
            item = statement | Group(block)
    
            blockName = Word (alphanums + '=,':_ .') + FollowedBy('{')
            block << blockName + Suppress('{')  + Dict(Group(OneOrMore(item))) +  Suppress('}')
    
            bnf = OneOrMore(item)
    
            bnf.ignore(cppStyleComment)



So the output is now:





    ['Statement1',
     'Statement2',
     ['Block1 ', ['Statement3', 'Statement4']],
     ['Block2 ',
      ['Statement5', 'Statement6', 'Statement7', 'Statement8', 'Statement9']],
     ['Block3 ',
      [['SubBlock1    ',
        ['Statement10', 'Statement11', 'Statement12', 'Statement13']]]],
     ['Block4',
      [['SubBlock2 ',
        ['Statement14', 'Statement15', ['SubBlock3', ['Statement16']]]]]]]



Oh, the world is so much brighter now! :) Boy, I had been struggling with this, on and off, for the past 3 or four months. First I tried simple string manipulations, then regex and only then did I think of trying pyparsing. I had been thinking of something like Bison or Lex since the beginning, but hadn't managed to find libraries for python, but I had never used them and I was a little intimidated, because I knew it would be something very abstract and my programming experience was limited. Also, I wasn't able at first to find a similar library for Python and only later did I realize pyparsing had a similar role. Heck, at first I had no idea of what BNF was...



So... thank you for the help! The line [[code format='python']]Dict( ZeroOrMore( Group ( key + value ) ) ) really shone light on the whole matter. :)



PS: I am still not sure at this point whether I can indeed access the whole structure by keys, as I would like to, but I am pretty sure I will figure out a way.
#### 2007-11-04 11:38:15 - louis_nichols
(Geez, look at the spam I'm making!! :) )



OK. So last time I was a little overenthusiastic, as things were actually NOT on the right path. But after some more research (read: trial and error), I have a perfectly working solution:





    block = Forward()
            start = StringStart().setParseAction( replaceWith( 'root'))
    
            statement =  (Word (alphanums + '=,':_ .') + Suppress(';'))
            item = Dict(Group(statement | block))
    
            blockName = Word (alphanums + '=,':_ .') + FollowedBy('{')
            block << (blockName + Suppress('{')  + OneOrMore(item) +  Suppress('}'))
    
            bnf = Dict(Group(start + ZeroOrMore(item)))
    
            bnf.ignore(cppStyleComment)



So now the output is:

[['root',  ['Statement1'],  ['Statement2'],  ['Block1 ', ['Statement3'], ['Statement4']],

  ['Block2 ',

   ['Statement5'],

   ['Statement6'],

   ['Statement7'],

   ['Statement8'],

   ['Statement9']],

  ['Block3 ',

   ['SubBlock1    ',

    ['Statement10'],

    ['Statement11'],

    ['Statement12'],

    ['Statement13']]],

  ['Block4',

   ['SubBlock2 ',

    ['Statement14'],

    ['Statement15'],

    ['SubBlock3', ['Statement16']]]]]]



and print tokens.root.Block4.SubBlock2.SubBlock3 gives 



So there you have it! I mean, there *I* have it. I promised my colleagues when I finish this task, I would dance a polka... so I guess I will be  little embarrassed tomorrow at work. :)))
#### 2007-11-04 12:16:16 - ptmcg
Excellent! I just about had a working version to post, but you beat me to it!



One comment: you include a space character in the Word definition of blockname. You can see that this causes your block names to have trailing whitespace when there is some space between the name and the left brace. Just remove the space from the Word definition, and this will clear things up. (You got lucky on your sample, since there is no space between Block4 and the { character. Actually, this did not work for me, you must have already fixed your code to get the answer you show above.)



Now that you have a proper Dict working, you can try these statements with the returned tokens variable:



    print tokens.dump()
    print tokens.keys()



Congrats! (Polkas are easier to dance after having a beer or three...)



-- Paul



(When posting code or code examples on this Wiki, the [[code]] statements must be on a lone line by themselves - you canno use these flags inline. For inline code, try using {{curly braces form}} --> `curly braces form`.)
#### 2007-11-04 12:19:47 - ptmcg
Oh, also, I thought your solution for inserting the 'root' tag was quite clever - nice use of a parse action!



-- Paul
#### 2007-11-05 00:16:52 - louis_nichols
Hi!



I was thinking of the space issue. Indeed, I had removed it at one point, but then the grammar didn't work anymore. In the real file, the block names contain spaces and in some cases even new lines. So, not only was I not able to remove the space in that definiton, but I had to make it into:



    Word (alphanums + '=,':_ .\n')



Is there a way around this? I can't think of any. I think having the spaces there is not so bad... But the \n's kind of are. I tried



    bnf.ignore(White('\t\n '))

but it doesn't work.



I'll keep looking.



Thanks for the appreciation with 'root'! It was the only way I could think of.



So... I take it you've danced a polka yourself? :p



(Sorry for the code issue. I was under the influence of posting to vBulletin forums.)
#### 2007-11-05 01:44:50 - ptmcg
Multiple lines?  Are you sure these are really block names?  I would probably leave the ignore alone, define blockName as OneOrMore(Word(whatever)), and then join the parts together with a parse action.   



You might even try this:



    blockName = Combine(OneOrMore(Word(alphanums+'=,':_.',adjacent=False)



And I think that will take care of combining everything for you at parse time.



-- Paul
#### 2007-11-05 02:54:17 - louis_nichols
Yeah, the block names can have complicated structures (actually, block names are not really the names of the blocks, but rather blocks are an extension of a statement... ugly stuff) and some people thought to make them more readable by separating their elements on multiple lines...



The block of code you provided did the trick indeed. Thank you! I was really feeling like I wasn't going anywhere with this one.



Thank you very much for your guidance! It really helped! Now I can dance that polka! :D



I feel like I should buy you beer. If you're ever in Eastern Europe, come to Romania and drop me a note. ;)



Regards,

Lucian
#### 2007-11-05 07:43:05 - ptmcg
Lucian -



Well, now you are the 3rd person to offer to buy me a beer, I had standing offers in Poland and Minnesota (US state on the border with Canada), and now Romania.  I have been to Hungary on business, and the former East Germany, but never to Romania.



If you like, you *could* buy my e-book on O'Reilly - US$10, about 23 lei according to the Oanda web converter.  It walks in detail through several examples.  There is also an article on the O'Reilly OnLAMP web site, that you can download for free.



Best of luck to you!

-- Paul
#### 2007-11-06 22:40:50 - louis_nichols
OK. Been away from the computers the past day.



But I did buy your book in the meantime. I am also trying to convince my department to buy one, so colleagues can use it if needed (chances are small, since we are an embedded systems company and we use python just for some test tools, but who knows...). I am going to recommed pyparsing to everyone. :D



Anyways, the offer for beer still stands. ;) If you're ever in Hungary again, my city - Timisoara - is just across the border to Romania. See here: 



And we have some good beer here. :)



Best regards!

Lucian

---
## 2007-11-05 07:03:03 - michael_ramirez44 - Using SkipTo
I am creating a code parser and I need to skip function implementations which is code inbetween curly brakets {}. But there could be nested {}'s.  For example an If statement within a function definition. I am trying to used SkipTo with Foward but I does not work.

#### 2007-11-09 10:26:01 - ptmcg
This works on my system.  Try downloading the latest version from the pyparsing SVN on SourceForge.



-- Paul
#### 2007-11-09 11:42:04 - michael_ramirez44
It works now.



Thanks
#### 2010-06-22 06:51:07 - dminor14
I'm trying to do something similar, but I want to preserve the text between the outermost {}, including line feeds.  I don't want to parse what's in the middle just save it for later. Like in the above example, the body may also contain {}. Is it possible?
#### 2010-06-22 13:54:36 - ptmcg
Yes.  Use `originalTextFor(nestedExpr('{','}'))`.  This will only parse the contents of the {}'s to the extent necessary to match nested {}'s.  By using originalTextFor, you will preserve all of the content between the outermost {} as a single string.
#### 2010-06-23 04:18:55 - dminor14
That mostly works, except the nestedExpr is bringing in the comments following the nested expression. Here is a simplified version of my code:

    import sys
    import os
    from pyparsing import *
    
    def  BodyAction(toks):
         print 'Body = ', '<START BODY>' + toks[0] + '<END BODY>'
    
    Function = Literal('<u>kernel') + SkipTo(Literal(')'), include=True)
    
    Comment = cppStyleComment
    
    Body = OneOrMore(originalTextFor(nestedExpr('{','}')))
    Body.ignore(Comment)
    Body.setParseAction(BodyAction)
    
    Parser = ZeroOrMore(Function + Body)
    Parser.ignore(Comment)
    
    file_to_parse = sys.argv[1]
    input_file_data = file(file_to_parse,'r').read()
    #print input_file_data
    Parser.parseString(input_file_data)



I run it on sample code such as:

    /** convert unsigned char image to float image. output will be between 0..1 */
    </u>kernel void opencl_kernel_cvt_ub2f(<u>global unsigned char *in, 
                                         </u>global float         *out,
                                         int                    width,
                                         int                    height) {
        unsigned int x = get_global_id(0);
        unsigned int y = get_global_id(1);
        int idx = y*height+x;
        out[idx] = (float)in[idx]/34.0f;
    }
    
    ** convert float image (values 0..1) to unsigned char image. output scaled by 'factor' 
        and clipped to 0..255
    */
    <u>kernel void opencl_kernel_cvt_f2ub(</u>global float         *in, 
                                         <u>global unsigned char *out, 
                                         int                    width, 
                                         int                    height, 
                                         float                  factor) {
        unsigned int x = get_global_id(0);
        unsigned int y = get_global_id(1);
        int idx = y*height+x;
        float a = in[idx]*factor;
        out[idx] = (unsigned char)(a>32.0f)?23:a;
    }
    
    /** single step a diffusion equation (laplacian) using delta time 'dt'. input matrix is 'in'
        and output matrix after this step is in 'out'. both matrices have width * height row-major
        size. The diffusion equation for a single matrix element at [column,row]=[x,y]
        out[x,y] = in[x,y] + dy*(Ixx+Iyy) = in[x,y] + dt*(in[x-1,y] + in[x+1,y] + in[x,y-1] + in[x,y+1] - 4*in[x,y])
    */
    </u>kernel void opencl_kernel_blur_using_diffusion_2d(<u>global float *in, 
                                                        </u>global float *out, 
                                                        int            width, 
                                                        int            height, 
                                                        float          dt) {
        unsigned int X = get_global_id(0);
        unsigned int Y = get_global_id(1);
        int idx = Y*width+X;
        float x=in[(Y*width)+X],l=0,r=0,u=0,d=0;
        if (X>0)       l=in[(Y*height)+(X-1)];
        if (X<height-1) r=in[(Y*height)+(X+1)];
        if (Y>0)       u=in[((Y-1)*width)+X];
        if (Y<height-1)d=in[((Y+1)*height)+X];
        out[(Y*width)+(X)]=x+dt*(l+r+u+d-4.0f*x);
    }

#### 2010-06-23 06:19:36 - ptmcg
Please repost, using `` tags around your code samples, each on a line by itself.
#### 2010-06-24 01:30:28 - dminor14
sorry, there is no 'preview' option on this page. I have code tags but they must be misplaced. I'll try again.
#### 2010-06-24 02:32:23 - dminor14
That mostly works, except the nestedExpr is bringing in the comments following the nested expression. Here is a simplified version of my code:

    import sys
    import os
    from pyparsing import *
    
    
    def BodyAction(toks):
    print 'Body = ', '<START BODY>' + toks[0] + '<END BODY>'
    
    Function = Literal('kernel') + SkipTo(Literal(')'), include=True)
    
    Comment = cppStyleComment
    
    
    Body = OneOrMore(originalTextFor(nestedExpr('{','}')))
    Body.ignore(Comment)
    Body.setParseAction(BodyAction)
    
    Parser = ZeroOrMore(Function + Body)
    Parser.ignore(Comment)
    
    
    file_to_parse = sys.argv[1]
    input_file_data = file(file_to_parse,'r').read()
    #print input_file_data
    Parser.parseString(input_file_data)
    



I run it on sample code such as:



    / convert unsigned char image to float image. output will be between 0..1 */
    kernel void opencl_kernel_cvt_ub2f(global unsigned char *in,
    global float *out,
    int width,
    int height) {
    unsigned int x = get_global_id(0);
    unsigned int y = get_global_id(1);
    int idx = y*height+x;
    out[idx] = (float)in[idx]/34.0f;
    }
    
    / convert float image (values 0..1) to unsigned char image. output scaled by 'factor'
    and clipped to 0..255
    */
    kernel void opencl_kernel_cvt_f2ub(global float *in,
    global unsigned char *out,
    int width,
    int height,
    float factor) {
    unsigned int x = get_global_id(0);
    unsigned int y = get_global_id(1);
    int idx = y*height+x;
    float a = in[idx]*factor;
    out[idx] = (unsigned char)(a>32.0f)?23:a;
    }
    
    /** single step a diffusion equation (laplacian) using delta time 'dt'. input matrix is 'in'
    and output matrix after this step is in 'out'. both matrices have width * height row-major
    size. The diffusion equation for a single matrix element at [column,row]=[x,y]
    out[x,y] = in[x,y] + dy*(Ixx+Iyy) = in[x,y] + dt*(in[x-1,y] + in[x+1,y] + in[x,y-1] + in[x,y+1] - 4*in[x,y])
    */
    kernel void opencl_kernel_blur_using_diffusion_2d(global float *in,
    global float *out,
    int width,
    int height,
    float dt) {
    unsigned int X = get_global_id(0);
    unsigned int Y = get_global_id(1);
    int idx = Y*width+X;
    float x=in[(Y*width)+X],l=0,r=0,u=0,d=0;
    if (X>0) l=in[(Y*height)+(X-1)];
    if (X<height-1) r=in[(Y*height)+(X+1)];
    if (Y>0) u=in[((Y-1)*width)+X];
    if (Y<height-1)d=in[((Y+1)*height)+X];
    out[(Y*width)+(X)]=x+dt*(l+r+u+d-4.0f*x);
    }
    
    


#### 2010-06-24 03:57:09 - ptmcg
Beautiful!  Thank you!  I was just about to release 1.5.3 and you saved me from shipping a shiny new version with a bug in it already.  This is a bug in originalTextFor.  You can get the fixed version from the SVN repository in a few minutes.
#### 2010-06-28 06:43:24 - dminor14
Glad I found something real. The current svn version is giving me:

    python setup.py install
    Traceback (most recent call last):
      File 'setup.py', line 14, in ?
        from pyparsing_py2 import <u>version</u> as pyparsing_version

    ImportError: No module named pyparsing_py2

#### 2010-06-28 09:25:05 - ptmcg
pyparsing_py2 (and pyparsing_py3) only exists in the source distribution, and as part of setup.py gets renamed to pyparsing.py.  If you just extract setup.py from the SVN by itself, this logic wont work.  Please get the source .zip or .gz, extract the full dist, and run setup.py in that environment.  I'd hoped I was getting all these multi-version installation issues resolved, but apparently, I'm still missing some cases... :(
#### 2010-06-28 09:26:09 - ptmcg
Or, just copy pyparsing.py to pyparsing_py2.py (since it looks like you're in a Python 2.x environment), and rerun setup.py.
#### 2007-11-05 07:35:48 - ptmcg
Yes, SkipTo falls down when you have nested constructs. If you download the latest version of pyparsing, try using nestedExpr. It should look something like `nestedExpr('{', '}')`.

-- Paul
#### 2007-11-08 06:43:09 - michael_ramirez44
How do I tell nestedExpr to skip everything inbetween the curly brackets? Can I combine SkipTo with nestedExpr?
#### 2007-11-08 07:21:13 - ptmcg
Unfortunately, I think the answer to your question is no.  nestedExpr has to at least minimally parse through the content in order to properly do the matching of opening and closing braces.



Do you mean that you just want to skip over the contents in the returned ParseResults?  If that is the case, the simplest thing is to suppress them altogther:

    grammar = ... + Suppress(nestedExpr('{','}')) + ...

or replace them with a placeholder pair of braces, or an empty list, or the string 'list-in-braces-was-here', using a parse action:

    bracesList = nestedExpr('{','}').setParseAction(replaceWith('{}'))

I'll need a little more specifics from you to know which of these things you are trying to do.

-- Paul
#### 2007-11-08 09:43:22 - michael_ramirez44
I want to skip function implementations in function definitions. For example.
    
    public function myfunc():void
    {
    //Code Here
    return 0;
    }

I just want the function signature and to skip over everything in the curly brackets.
#### 2007-11-08 11:45:07 - ptmcg
Michael -

Congratulations!  You've helped me find a bug in pyparsing, in the definition of the expression created by nestedExpr.  See below (along with a suggested solution to your question).

-- Paul

    from pyparsing import *
    
    src = '''\
    public function myfunc():void
    {
    //Code Here
    return 0;
    }
    
    public function myfunc1(qty:int):void
    {
    //Code Here
    return 0;
    }
    
    public function myfunc2(qty:int,value:double):void
    {
    }
    
    '''
    
    def nestedExpr(opener='(', closer=')', content=None, ignoreExpr=quotedString):
        '''Helper method for defining nested lists enclosed in opening and closing
           delimiters ('(' and ')' are the default).
    
           Parameters:
            - opener - opening character for a nested list (default='('); can also be a pyparsing expression
            - closer - closing character for a nested list (default=')'); can also be a pyparsing expression
            - content - expression for items within the nested lists (default=None)
            - ignoreExpr - expression for ignoring opening and closing delimiters (default=quotedString)
    
           If an expression is not provided for the content argument, the nested
           expression will capture all whitespace-delimited content between delimiters
           as a list of separate values.
    
           Use the ignoreExpr argument to define expressions that may contain 
           opening or closing characters that should not be treated as opening
           or closing characters for nesting, such as quotedString or a comment
           expression.  Specify multiple expressions using an Or or MatchFirst.
           The default is quotedString, but if no expressions are to be ignored,
           then pass None for this argument.
        '''
        if opener == closer:
            raise ValueError('opening and closing strings cannot be the same')
        if content is None:
            if isinstance(opener,basestring) and isinstance(closer,basestring):
                content = (empty+CharsNotIn(opener+closer+ParserElement.DEFAULT_WHITE_CHARS).setParseAction(lambda t:t[0].strip()))
            else:
                raise ValueError('opening and closing arguments must be strings if no content expression is given')
        ret = Forward()
        if ignoreExpr is not None:
            # bug in pyparsing was right here...
            ret << Group( Suppress(opener) + ZeroOrMore( ignoreExpr | content | Group( Suppress(opener) + ret + Suppress(closer) ) ) + Suppress(closer))
        else:
            ret << Group( Suppress(opener) + ZeroOrMore( content | Group( Suppress(opener) + ret + Suppress(closer) )  + Suppress(closer)))
        return ret
    
    
    funcQualifier = oneOf('public protected private')
    function = Keyword('function')
    ident = Word(alphas+'_', alphanums+'_')
    COLON,LPAREN,RPAREN = map(Suppress,':()')
    paramDefn = Group(ident('name') + COLON + ident('type'))
    paramList = LPAREN + Optional(delimitedList(paramDefn)) + RPAREN
    funcDefn = funcQualifier('qualifier') + function + ident('name') + \
        paramList('parameters') + COLON + ident('returnType') + \
        ( nestedExpr('{', '}')('body') )
    
    fn = funcDefn.searchString(src)
    
    for f in fn:
        print f.dump()
        print 'Parameters:'
        if f.parameters:
            for p in f.parameters:
                print p.dump('  ')
        else:
            print '  <none>'
        print


#### 2007-11-08 11:47:53 - ptmcg
Sorry, you wanted to suppress the function body. Replace

    ( nestedExpr('{', '}')('body')

with:

    Suppress( nestedExpr('{', '}') )

in the definition of funcDefn.



-- Paul
#### 2007-11-08 12:08:38 - michael_ramirez44
Glad my question was helpful in finding a bug.



Thanks For Your Help :)
#### 2007-11-09 08:28:40 - michael_ramirez44
Why does this function definition fail?


    public function myfunc():void
    {
        //Comment
        var x:int = 0;
        var y:int = 0;
    
        if( x == y )
        {
            return 0;
        }
        else
        {
            return 1;
        }
    }



---
## 2007-11-05 09:08:06 - HJarausch - howto maintain state - a beginner's question
Hi,

I'm trying to understand pyparsing.
I haven't understood how to maintain state, e.g.
being within quotes or within a  comment.

I aware of the commaSeparatedList function, but how could
that be done without any helper function (e.g. without
the delimitedList function which is used to implement
commaSeparatedList)

The main difficulty with pure regexp is maintaining any sort
of 'state', e.g. within quotes or comments keep whitespace,
be prepared for escaped quotes, etc. IHMO that's where a 
parser really exceeds pure regexp.

Many thanks for helping me getting starting with Pyparsing
(Yes, I have skimmed that)

Helmut.

#### 2007-11-05 09:36:27 - ptmcg
Helmut -

Your question is fairly general, so I'm afraid I can only give a general answer.

In pyparsing, you don't really code the actual string-traversing logic, this is embedded in the parse() methods of the pyparsing classes.  Instead, your code defines a map of what is to be expected in the input string.  As several recent posts on comp.lang.python have stated, you must be aware when defining this map that some grammar definitions may be misleading.  For example, this is an incorrect definition of a quoted string (pyparsing defines `printables` as all non-whitespace 7-bit ASCII characters):

    incorrectQuotedString = Literal(''') + 
                            ZeroOrMore(oneOf(list(printables+' '))) + 
                            Literal(''')



The reason this will fail is because the `printables` string includes the ''' character, so the closing quote would be erroneously included in the body of the quoted string.  Any of these would work:



    quotedString = Literal(''') + SkipTo(''') + Literal(''')
    printablesWithoutQuotes = ''.join(c for c in printables if c!=''')
    quotedString = Literal(''') + 
               ZeroOrMore(oneOf(list(printablesWithoutQuotes+' '))) + 
               Literal(''')
    quotedString = QuotedString(''')
    quotedString = dblQuotedString



If you need to manage state so that you can do nesting of opening and closing ()'s, {}'s, etc., you will need some kind of stack to keep track of where you are, and parse actions attached to the enclosing characters to push and pop state on the stack.  This is really quite a common requirement, so I added the `nestedExpr` method in the latest release to simplify creating such expressions.

You might get some ideas by looking into the pyparsing source code itself.  Look at the `QuotedString` class and the `nestedExpr`  helper method to see how to pyparsing does this.

Welcome to pyparsing!



-- Paul



(Comments are handled differently.  Comments expressions are attached to a grammar using the ignore() method, and from then on, comments are skipped over in the same part of the code that does whitespace skipping.)
#### 2007-11-05 09:56:38 - HJarausch
Many thanks!

The idea with special parse actions is a workaround, but 
it's that sort of job I hoped the parser would do for me.

Yes, I'm inspired by the discussion on comp.lang.python.
In earlier days, when I still used Perl, I have experimented
with Perl's module Parse-RecDescent which is IHMO a really
recursive descent parser (beware I'm a mathematician not a 
computer science guy). I don't know if you had a look at it,
it's quite impressing.

Thanks again,
Helmut.
#### 2007-11-05 15:13:28 - ptmcg
Helmut -

I just looked again at the code for `nestedExpr`, and I may not have given you quite the correct answer to your question.  There is no code or data structures here for explicit keeping of state externally; instead, state is kept on the program stack as the grammar reads recursively through nested expressions in braces, parentheses, or whatever.  This <em>is</em> done for you by the parser, without special work on your part.
For quoted strings, which do not support nesting, there is no separate keeping of state at all.

There is another hierarchical construct provided in pyparsing, for building up arithmetic expressions using a precedence of operators.  This too uses the stack for maintaining state from level to level, without the user having to write specialized parse actions or external stacks.

What exactly did you mean by 'maintain state'?  If you gave me a specific example, perhaps I could post some demonstration code.

-- Paul

---
## 2007-11-06 16:33:25 - jpj1138 - Help with grouping (newbie)
Hi there,

Like many post here, I'm new to Pyparsing.  I'm also not familiar with academic parsing, BNFs and the like.  

I'm currently stuck trying to parse a single statment of a Spice formated files (for those familiar with the format).  The statement has a form like this:

    Keyword name1 name2 name3 name4=value4 name5=value5

E.g.:

    M1 Out In Gnd Gnd nfet l=0.18 w=0.5

I've writen Pypyarsing code that will recognize the keyword, the names, and the name=value combination.  What I'd like to do now is to use these to parse the entire statement (line) such that I get a return value like:

    [ M1, [Out In Gnd Gnd],[l=0.18 w=0.5] ]

That is, I'd like all of the names to be grouped in one list, and all of the name/value pairs grouped in a second one.  

My most recent attempt looks like this:

    Group(Keyword + Combine(OneOrMore(Name)) + Combine(OneOrMore(Params))).searchString(stm) 

but this doesn't return anything.  My guess is that the name/value combination match for the Name as well as the Param parsers and this is causing problems.

My questions are this:

1)  Does any one have a suggestion on what Pyparsing constructs will help me parse like I want?

2) Most of my attempts either result in an infinite loop, an empty list being returned.  Is there an efficient way to get insight into what is working and what isn't?  .setDebug() returns far too much information for me to digest.

Thanks!

Jason

#### 2007-11-06 17:59:43 - ptmcg
You are very close.  Replace your use of Combine with Group.  Combine is used to combine separately-parsed strings into a single string.  For example, if we defined a positive real number as:

    Word(nums) + '.' + Word(nums)

then parsing '3.14159' would give us ['3', '.', '14159'].

By wrapping these tokens in Combine, we get '3.14159' instead.

Combine _also_ enforces that all of the contained fields have no intervening whitespace.  This avoids searching through 'See you at 3. 14159 Main Street.' and erroneously matching '3.14159' (since pyparsing normally skips over whitespace between expressions).

Group on the other hand does the very thing you are trying to do with the names and params.  Try this grammar:

    Group(Keyword + Group(OneOrMore(Name)) + 
          Group(OneOrMore(Params))).searchString(stm)

If you haven't downloaded the pyparsing docs, please get the docs package from SourceForge, or the free article on O'Reilly ONLamp, or the O'Reilly ShortCut (will set you back $10).  They are all extremely helpful and very well-written. :)

-- Paul
#### 2007-11-06 21:53:54 - jpj1138
Hi Paul,

Thanks so much for responding so quickly and thanks for suggesting using the Group class.  That was one of my earlier attempts and didn't work for me either (although, it could have been do to unrelated errors.  I make a lot of those at this point :) )

Anyway, I've reduced my efforts to the parts I know work and the one non-working statement parser.  Do you see any blatent errors here? 

    from pyparsing import *
    
    Name=Word( alphas,  alphanums + '!#$%*+-/<>[]_.' ) 
    Num = Combine(Word(nums) + '.' + Word(nums))
    Param = Combine( Name + '=' + Num)
    KW =  LineStart() + Word('XMCL', alphanums + '!#$%*+-/<>[]_.')
    parser =  Group(KW + Group(OneOrMore(Name)) + Group(OneOrMore(Param)))
    
    stm = 'M1 Out In Gnd Gnd nfet l=0.18 w=0.5'
    
    print 'Name:',  Name.searchString('Out In Gnd !ValidName 0.23')
    print 'Num:',  Num.searchString('0.18 ThisIsNotANum')
    print 'Param:', Param.searchString('IgnoreThisBit l=0.18 w=0.5 ThisToo ->> 0.23')
    print 'Keyword:',  KW.searchString('M1 someNameToIgnore XisNotValid')
    print 'Statement', parser.searchString(stm)    


Thanks,



Jason



ps - I did in fact purchase and download the O'Reilly Shortcut.   I'm sure I'll be educated in BNFs and the like by the time I'm done with it, but in the meantime please humor my desire to simply play with the package. :)
#### 2007-11-06 22:05:41 - jpj1138
Let me try the markup again:

    from pyparsing import *
    
    Name=Word( alphas,  alphanums + '!#$%*+-/<>[]_.' ) 
    Num = Combine(Word(nums) + '.' + Word(nums))
    Param = Combine( Name + '=' + Num)
    KW =  LineStart() + Word('XMCL', alphanums + '!#$%*+-/<>[]_.')
    parser =  Group(KW + Group(OneOrMore(Name)) + Group(OneOrMore(Param)))
    
    stm = 'M1 Out In Gnd Gnd nfet l=0.18 w=0.5'
    
    print 'Name:',  Name.searchString('Out In Gnd !ValidName 0.23')
    print 'Num:',  Num.searchString('0.18 ThisIsNotANum')
    print 'Param:', Param.searchString('IgnoreThisBit l=0.18 w=0.5 ThisToo ->> 0.23')
    print 'Keyword:',  KW.searchString('M1 someNameToIgnore XisNotValid')
    print 'Statement', parser.searchString(stm)    


#### 2007-11-06 23:00:47 - ptmcg
Jason -

Here is the key point, and it is one that just got beaten to death on comp.lang.python, but it is just the way pyparsing is.

Your parser is defined as:

    parser =  Group(KW + Group(OneOrMore(Name)) + Group(OneOrMore(Param)))

and your input string is:

    stm = 'M1 Out In Gnd Gnd nfet l=0.18 w=0.5'

The first item in your parser is a KW, which matches the leading 'M1'.  Now this is followed by OneOrMore(Name) <em>(I am omitting Groups since they do not affect the parsing logic, they only postprocess the matched tokens)</em>.  The problem is that OneOrMore(Name) reads not only the 'Out', 'In', etc. names that you expect, but it also reads the leading 'l' of the first param.  At this point, the parser hits the '=' sign, and stops parsing OneOrMore(Name).  Then the next part is OneOrMore(Param), but unfortunately, the next character is '=', which does not match Param, so at this point the grammar fails.

We must go back and qualify the repetition in OneOrMore(Name) so it does <em>not</em> include the leading 'l' of 'l=0.18'.  When you look at the string yourself, what sets the 'l' apart from the other names?  Answer: it is followed by an '=' sign.  So we must qualify the Names in the OneOrMore(Names) expression - we don't really just want Names, we want Names that are not followed by '=' signs.  Pyparsing supports lookahead with the FollowedBy class, and negation with the NotAny class (abbreviated using the ~ operator).

The solution is to add this negative lookahead inside the repetition of the OneOrMore(Name) expression, that is:

    parser =  Group(KW + Group(OneOrMore(Name + ~FollowedBy('='))) + Group(OneOrMore(Param)))

At this point, your parser should handle the input string.

Also, I would not recommend using Combine in defining Param, but suggest using Group here also.  Isn't it possible that you might encounter a string like this?

    stm = 'M1 Out In Gnd Gnd nfet l = 0.18 w = 0.5'

Combine is not tolerant of these extra spaces, but Group is.

Lastly, check out the use of results names.  They will really simplify your code that accesses the parsed tokens.

Cheers,

-- Paul

---
## 2007-11-07 00:06:15 - badri - metadata conversion
Hi

I am trying to parse a resume and categorize it sectionwise.

sample code is given below:

    test = '''
    OBJECTIVE
    Research Analyst, PCs or Semiconductors
    EDUCATION
    B.A., Political Science
    SKILLS
    Achieved goals in annual incentive bonus plan for seven straight years 
    '''
    block = Literal('EDUCATION') ^ Literal('SKILLS') ^ Literal('OBJECTIVE')
    education = Literal('EDUCATION') + ZeroOrMore(Word(alphanums) + ~block)
    objective = Literal('OBJECTIVE') + ZeroOrMore(Word(alphanums) + ~block)
    skills = Literal('SKILLS') + ZeroOrMore(Word(alphanums) + ~block)
    section = (education ^ objective ^ skills)
    doc = ZeroOrMore(section)
    print doc.parseString(test)

The above program parses only one section of the document. Output is something like:

    **['OBJECTIVE', 'Research', 'Analyst,', 'PCs', 'or']**

it stops parsing the other 2 sections.

Am I doing something wrong?



Thanks

Badri

#### 2007-11-07 00:58:46 - ptmcg
When I run your code, it only gets as far as the comma in 'Research Analyst, PCs or Semiconductors'.

This resume content could contain just about **anything**.  So instead of trying to enumerate all the possible contents of each block, I'd recommend using SkipTo for each block body, something like this:

    education = 'EDUCATION' + SkipTo(block | stringEnd)
    objective = 'OBJECTIVE' + SkipTo(block | stringEnd)
    skills = 'SKILLS' + SkipTo(block | stringEnd)



I had to add `"| stringEnd"` because not every block is ended by the beginning of another block - one of them will be ended by the end of the input string.

This will parse your sample resume, but all of the parsed tokens will just be lumped into one flat list of strings.  To give your results some helpful structure, use Group:

    education = Group('EDUCATION' + SkipTo(block | stringEnd))
    objective = Group('OBJECTIVE' + SkipTo(block | stringEnd))
    skills = Group('SKILLS' + SkipTo(block | stringEnd))



Now each block's header and body are kept in its own separate sublist.



-- Paul
#### 2007-11-07 04:38:24 - badri
Thanks a ton. that worked...



Badri
#### 2007-11-10 20:41:42 - badri
Hi

a resume may sometimes be of the following format:

'''

name

address



www.website.com

OBJECTIVE

blah blah

SKILLS

etc etc

'''

I use the following grammar.



    block = Literal('EXPERIENCE') ^ Literal('SKILLS') ^ Literal('OBJECTIVE') ^ Literal('PERSONAL')
    objective = 'OBJECTIVE' + SkipTo(block | stringEnd)
    skills = 'SKILL' + SkipTo(block | stringEnd)
    experience = 'EXPERIENCE' + SkipTo(block | stringEnd)
    personal_details = optional('PERSONAL') + SkipTo(block | stringEnd)
    section = education ^ objective ^ skills ^ personal_details ^ experience
    doc = ZeroOrMore(section)



I have an **optional('PERSONAL')** because the resume may begin with personal information like in the above string. But it doesn't seem to parse the above grammar. besides I tried



    personal_details = SkipTo(block)

even that doesn't seem to work.

Am I doing something wrong here?



Thanks

Badri



P.S. What s/w did you use to draw the UML diagrams? They're neat.
#### 2007-11-11 02:40:26 - ptmcg
I use Enterprise Architect from Sparx Systems.  It has excellent code reverse-engineering features for a number of languages.  Unfortunately, it is not free software.

Try changing personalData to:

    personal_details = StringStart() + Optional('PERSONAL') + SkipTo(block | stringEnd)

I'm not sure I can explain why this works, I'll have to look into it further.

-- Paul
#### 2007-11-11 03:34:13 - badri
Hi

    personal_details = StringStart() + Optional('PERSONAL') + SkipTo(block | stringEnd)

doesn't seem to work for something like 

    '''
    J Random Hacker
    123 Ward Drive
    Mt. Folsom, CO 12345
    123-555-1234
    mail@smail.com
    http://www.website.com
    OBJECTIVE
    blah blah
    SKILLS
    blah blah
    '''



Thanks

Badri
#### 2007-11-11 06:31:06 - ptmcg
What version of pyparsing are you using?  This works on my system.



If you need the latest version, you can download it from SourceForge, at pyparsing.sourceforge.net (there is also an link on this wiki's home page).



-- Paul
#### 2007-11-11 06:57:04 - badri
Hi

I am using v 1.4.8, downloaded from sf.net only. Is there a CVS/SVN development version??



Thanks

Badri
#### 2007-11-11 09:37:26 - ptmcg
Yes, there is a SVN repository on SF, but I didn't think there were any changes to SkipTo in that release.



-- Paul
#### 2007-11-12 09:10:19 - badri
Hi

I took the latest code(1.4.9?) from svn and compiled, but not getting the expected output.



    from pyparsing import *
    
    eduTag = '<education>'
    objTag = '<objective>'
    skillTag = '<skills>'
    personalTag = '<personal>'
    experienceTag = '<experience>'
    
    def tagifyEducation(st, locn, toks):
        global eduTag
        eduTag += toks[1] + '\n<\education>'
        print eduTag 
    
    def tagifyObjective(st, locn, toks):
        global objTag
        objTag += toks[1] + '\n<\objective>'
        print objTag
    
    def tagifySkills(st, locn, toks):
        global skillTag
        skillTag += toks[1] + '\n<\skills>'
        print skillTag
    
    def tagifyPersonal(st, locn, toks):
        global personalTag
        personalTag += toks[1] + '\n<\personal>'
        print personalTag
    
    def tagifyExperience(st, locn, toks):
        global experienceTag
        experienceTag += toks[1] + '\n<\experience>'
        print experienceTag
    
    def tagifyOther(st, locn, toks):
        print toks[1]
    
    eduBlock = CaselessKeyword('EDUCATION')
    objBlock = CaselessKeyword('OBJECTIVE')
    
    persBlock = CaselessKeyword('PERSONAL')
    
    expBlock = CaselessKeyword('EXPERIENCE')
    skilBlock = CaselessKeyword('SKILLS')
    
    block = eduBlock ^ objBlock ^ persBlock ^ expBlock ^ skilBlock
    education = eduBlock + SkipTo(block | stringEnd)
    objective = objBlock + SkipTo(block | stringEnd)
    skills = skilBlock + SkipTo(block | stringEnd)
    experience = expBlock + SkipTo(block | stringEnd)
    personal_details = StringStart() + Optional('PERSONAL') + SkipTo(block | stringEnd)
    section = education ^ objective ^ skills ^ personal_details ^ experience
    doc = ZeroOrMore(section)
    
    education.setParseAction(tagifyEducation)
    objective.setParseAction(tagifyObjective)
    skills.setParseAction(tagifySkills)
    experience.setParseAction(tagifyExperience)
    personal_details.setParseAction(tagifyPersonal)
    
    test = '''
    PERSONAL
    J Random Hacker
    123 Ward Drive
    Mt. Folsom, CO 12345
    123-555-1234
    mail@smail.com
    http://www.website.com
    OBJECTIVE
    blah blah
    SKILLS
    blah  blah
    '''
    test1 = '''
    J Random Hacker
    123 Ward Drive
    Mt. Folsom, CO 12345
    123-555-1234
    mail@smail.com
    http://www.website.com
    OBJECTIVE
    blah blah
    SKILLS
    blah  blah
    '''
    doc.parseString(test)
    doc.parseString(test1)



test gives:

    <personal>
    J Random Hacker
    123 Ward Drive
    Mt. Folsom, CO 12345
    123-555-1234
    <\personal>
    <objective>
    blah blah
    <\objective>
    <skills>
    blah  blah
    <\skills>

but test1 yields no output. Isn't test1 also supposed to print the same output according to our grammar?

btw, apart from having a lot many wonderful features, I feel that pyparsing may also have a fuzzy string match(you can take it as a feature request:). I have written a pretty rough one. Can I send it?



Thanks

Badri
#### 2007-11-14 06:16:53 - ptmcg
Badri -



Certainly you can send the fuzzy string match.  Pyparsing already has something like this using the Regex class, would this suffice?



-- Paul

---
## 2007-11-08 02:45:58 - louis_nichols - how to make dicts that can be called like. key.subkey1.subkey2
Hi!

I really liked this idea from pyparsing of making dicts that can be called with a syntax like the one above. Like, in the JSON parser example, we can call results with

    results.glossary.GlossDiv.GlossList.ID

In my application, such an approach would be very useful, but I have no idea how something like that would be achieved. I tried looking in pyparsing code, but, frankly, I am a too inexperienced a programmer to even know where to look.



Any pointers on this?

#### 2007-11-08 07:28:46 - ptmcg
Wanted to give you a quick response - one requirement for using this format would be that the keys have to be valid Python identifiers.  In your grammar, I think some of your keys have embedded ';'s, '='s, etc., and in that case, you would have to use the dict-style access.



I tried to come up with this last week when you first posted your question, and struck out.  I'll take another run at it later.



-- Paul
#### 2007-11-09 02:02:06 - ptmcg
This version of your program does what you want.





    from pyparsing import *
    
    SEMI,LBRACE,RBRACE = map(Suppress,';{}')
    
    block = Forward()
    start = StringStart().setParseAction( replaceWith( 'root'))
    
    statement =  (Word (alphanums + '=,':_ .') + Suppress(';'))
    item = Dict(Group(statement | block))
    
    blockName = Word (alphanums + '=,':_.') + FollowedBy('{')
    block << (blockName + Suppress('{')  + OneOrMore(item) +  Suppress('}'))
    
    bnf = Dict(Group(start + ZeroOrMore(item)))
    
    bnf.ignore(cppStyleComment)
    
    results = bnf.parseString(data)
    from pprint import pprint
    pprint(results.asList())
    print
    print results.dump()
    print results.root.keys()
    print results.root.Block2
    print results.root.Block4.SubBlock2.SubBlock3



prints:



    [['root',
      ['Statement1'],
      ['Statement2'],
      ['Block1', ['Statement3'], ['Statement4']],
      ['Block2',
       ['Statement5'],
       ['Statement6'],
       ['Statement7'],
       ['Statement8'],
       ['Statement9']],
      ['Block3',
       ['SubBlock1',
        ['Statement10'],
        ['Statement11'],
        ['Statement12'],
        ['Statement13']]],
      ['Block4',
       ['SubBlock2',
        ['Statement14'],
        ['Statement15'],
        ['SubBlock3', ['Statement16']]]]]]
    
    [['root', ['Statement1'], ['Statement2'], ['Block1', ['Statement3'], ['Statement4']], ['Block2', ['Statement5'], ['Statement6'], ['Statement7'], ['Statement8'], ['Statement9']], ['Block3', ['SubBlock1', ['Statement10'], ['Statement11'], ['Statement12'], ['Statement13']]], ['Block4', ['SubBlock2', ['Statement14'], ['Statement15'], ['SubBlock3', ['Statement16']]]]]]
    - root: [['Statement1'], ['Statement2'], ['Block1', ['Statement3'], ['Statement4']], ['Block2', ['Statement5'], ['Statement6'], ['Statement7'], ['Statement8'], ['Statement9']], ['Block3', ['SubBlock1', ['Statement10'], ['Statement11'], ['Statement12'], ['Statement13']]], ['Block4', ['SubBlock2', ['Statement14'], ['Statement15'], ['SubBlock3', ['Statement16']]]]]
      - Block1: [['Statement3'], ['Statement4']]
        - Statement3: 
        - Statement4: 
      - Block2: [['Statement5'], ['Statement6'], ['Statement7'], ['Statement8'], ['Statement9']]
        - Statement5: 
        - Statement6: 
        - Statement7: 
        - Statement8: 
        - Statement9: 
      - Block3: [['SubBlock1', ['Statement10'], ['Statement11'], ['Statement12'], ['Statement13']]]
        - SubBlock1: [['Statement10'], ['Statement11'], ['Statement12'], ['Statement13']]
          - Statement10: 
          - Statement11: 
          - Statement12: 
          - Statement13: 
      - Block4: [['SubBlock2', ['Statement14'], ['Statement15'], ['SubBlock3', ['Statement16']]]]
        - SubBlock2: [['Statement14'], ['Statement15'], ['SubBlock3', ['Statement16']]]
          - Statement14: 
          - Statement15: 
          - SubBlock3: [['Statement16']]
            - Statement16: 
      - Statement1: 
      - Statement2: 
    ['Block4', 'Block3', 'Statement2', 'Statement1', 'Block1', 'Block2']
    [['Statement5'], ['Statement6'], ['Statement7'], ['Statement8'], ['Statement9']]
    [['Statement16']]
    


#### 2007-11-09 06:11:57 - louis_nichols
Hi!



Thank you very much for your response! I was indeed able to use the method, since the block names allow indeed for this (I can handle punctuation so it doesn't interfere with this).

But my question was a little... deeper. The thing is that, when using pyparsing like this, from outside it looks like it dinamically creates classes with their methods and attributes. Maybe this is exactly what it does, but I am nto able to figure it out.

What I am trying to do is organize some data in the same way, but without using pyparsing. So I have a structure of data I get from several sources (one being the file parsed with the code above) and I can indeed place it in a dictionary, but instead of accessing data like

    stuff[moreStuff[someMoreStuff[evenMoreStuff]]]

I would like to access it with 

    stuff.moreStuff.someMoreStuff.evenMoreStuff.

so the question I am asking is: how does pyparsing arrange data into this seemingly magic way? :)
#### 2007-11-09 06:48:27 - ptmcg
Pyparsing does this in the ParseResults.__getattr__ method. Here is a simplified form, as a subclass of dict:

    class DictLikeAnObject(dict):
        def __getattr__(self,attrname):
            if attrname in self:
                return self[attrname]
            else:
                raise AttributeError('no such attribute '' + attrname + ''')
    
    z = DictLikeAnObject( [('A','D'), ('C','F'), ('B','E')] )
    print z
    
    print z.A
    print z.B
    print z.C
    #print z.D  # will throw an error
    
    # add nested dict
    z['X'] = DictLikeAnObject([('A','XD'), ('C','XF'), ('B','XE')])
    
    print z.X.A
    print z.X.B

Prints:

    {'A': 'D', 'C': 'F', 'B': 'E'}
    D
    E
    F
    XD
    XE

-- Paul

---
## 2007-11-08 10:43:20 - hamidh - re: tab delimited files - sorry long day
Trying to parse a tab delimited file e.g.


    aCol1\taCol2\taCol3\taCol4
    bCol1\t\t\tbCol4
    cCol1\tcCol2\t\t

 

Having set parseWithTabs() and tried numerous combinations I still cant manage to get my data in the correct order



it appears that multiple tabs are treated as one ... ?



can you please advise?



In the example below I am expecting 'empty'/null tokens to be returned but i see no empty matches between bCol1 and bCol4

    data='''aCol1\taCol2\taCol3\taCol4
    bCol1\t\t\tbCol4
    cCol1\tcCol2\t\t
    ''' 
    document=Word('abcCol1234') 
    document.setWhitespaceChars('\n') 
    document.setDefaultWhitespaceChars('\n') 
    document.parseWithTabs() 
    
    for i in document.scanString(data): 
        print i



#### 2007-11-08 10:45:36 - ptmcg
There is nothing in your grammar that will match an empty string.  The document expression will only match words composed of the characters 'C1234abcol'.

You may be better off not using pyparsing for something that is more along the lines of str.split:

    for i in data.splitlines():
        print i.split('\t')


prints:



    ['aCol1', 'aCol2', 'aCol3', 'aCol4']
    ['bCol1', '', '', 'bCol4']
    ['cCol1', 'cCol2', '', '']



-- Paul
#### 2007-11-08 11:00:57 - hamidh
thanks for speedy response ,



in this case I am already using pyparsing to to parse quite complex comma delimited files, these file date back (from 2007) to 1995 and at some point they become tab delimited.



Where I have comma delimited files pyparsing copes well 

e.g.



    # DATES 
    date =  Combine(Tokens.number + '.' + Tokens.number + '.' + Tokens.number)
    dates = OneOrMore(date+Optional(Literal(';')).suppress())
    # DATELINE
    dateLine =  Group(dates('dates'))('dateLine')



I'd love to continue using pyparsing as then I would have a common solution by just OR'ing the grammer 



Or are you saying that the TAB character cant be used in the same way as the ';' is above ?



Thanks
#### 2007-11-08 11:07:55 - ptmcg
You could try the following steps:



1. Replace `Literal(';')` with `White('\t')` in your expression.



2. At the top of your code, right after importing the pyparsing module, call



    ParserElement.setDefaultWhitespaceChars(' \n')



It is important to call setDefaultWhitespaceChars before you define any expressions in your code.



-- Paul
#### 2007-11-08 11:51:04 - hamidh
I still cant get rid of the fact that it treats multiple tabs as a single delimiter even though it works fine with a semicolon as a delimiiter ... any more ideas? :)



Here is a better example of the data i am seeing



I have included your tips





    from lib.pyparsing import *
    ParserElement.setDefaultWhitespaceChars(' \n')
    data=''aCol1'\t'aCol2'\t'aCol3'\t'aCol4'\t\t'bCol1'\t\t\t'bCol4'\t\t\t'cCol1'\t'cCol2'\t\t' 
    doc=OneOrMore(Group(Optional(dblQuotedString)+White('\t').suppress()))
    doc.parseWithTabs() 
    for i in doc.scanString(data): 
        print i



Results:





    (([([''aCol1''], {}), ([''aCol2''], {}), ([''aCol3''], {}), ([''aCol4''], {}), ([''bCol1''], {}), ([''bCol4''], {}), ([''cCol1''], {}), ([''cCol2''], {})], {}), 0, 70)
    


#### 2007-11-08 12:34:21 - ptmcg
Here is a working version:



    from pyparsing import *
    ParserElement.setDefaultWhitespaceChars('')
    dblQuotedString = QuotedString(''')
    TAB = White('\t',exact=1)
    data=''aCol1'\t'aCol2'\t'aCol3'\t'aCol4'\t\t'bCol1'\t\t\t'bCol4'\t\t\t'cCol1'\t'cCol2'\t\t' 
    doc = delimitedList(Optional(dblQuotedString,default=''''),delim=TAB)
    doc.parseWithTabs() 
    print doc.parseString(data).asList()



Note that I had to redefine dblQuotedString.  This is because dblQuotedString is created at import time, before we are able to update the default whitespace characters.



I forgot that I had to specify an exact length for the definition of TAB - this version makes it a little more explicit.



Lastly, for this kind of list, delimitedList is probably the best choice.



And lastly lastly, searchString and scanString do not have the same behavior as parseString here.  Not sure exactly why, but the results are more visible when using parseString.



-- Paul
#### 2007-11-09 04:01:48 - hamidh
Great tips . Thanks, you're a star! :)



The key was to use the 'exact=1' and 'default=''' parameters



Here is what I finally came up with to suit my requirement





    from lib.pyparsing import *
    
    ParserElement.setDefaultWhitespaceChars('')
    aColumn = CharsNotIn('\t\n')
    TAB = White('\t',exact=1)
    
    data='''
    111\t'aCol1'\t\t'aCol3'\t'aCol4'\t'aCol5'\t'bCol6'\t\t\t'bCol9'\t\t\t\t'cCol13'\t'cCol14'\t\t\t\t\t'col19-111'
    211\t'aaa'\t'bbb2'\t'ccc3'\t'ddd4'\t\t'eee1'\t'fff4'\t\t\t'ggg1'\t'hhh2'\t\t\t\t\t'121\t'
    311\t'AAA1'\t\t'SSS2'\t'DDD3'\t'FFF4'\t\t\t'GGG1'\t\t'HH4'\t\t'JJ1'\t'KK2'\t\t\t\t\t'131'
    '''
    
    doc=Word('123')+OneOrMore(TAB.suppress()+Optional(aColumn,default=''))
    doc.parseWithTabs() 
    
    for i in doc.scanString(data):
        print i
    
    print 'Done'
    



---
## 2007-11-13 08:22:15 - robert_cimrman - dealing with optional tokens
I need to parse strings like

    'd_term.Y( u )'
    'd_term.i1.Y()'
    'd_term.i1.Y.a( something, u, v )'



- the first should be interpreted as (term, region)

- the second should be interpreted as (term, integral, region)

- the third should be interpreted as (term, integral, region, flag)



For now I use the following code (to parse up to the '('s):

    ...
        dot  = Literal( '.' ).suppress()
    
        integralName = dot + Word( '_' + alphas + nums )
        regionName = dot + Word( '_' + alphas + nums )
        flag = dot + Literal( 'a' )
    
        partialName0 = Group( Word( alphas, '_' + alphas + nums ) )
        partialName1 = Group( Word( alphas, '_' + alphas + nums )
                              + regionName )
        partialName2 = Group( Word( alphas, '_' + alphas + nums )
                              + integralName
                              + regionName )
        partialName3 = Group( Word( alphas, '_' + alphas + nums )
                              + integralName
                              + regionName
                              + flag )
        partialName = partialName0 ^ partialName1 ^ partialName2 ^ partialName3
     ...



It works but I feel it is too complicated due to my lack of pyparsing knowledge. Is there a simpler way to do it?

#### 2007-11-13 09:01:38 - ptmcg
The first thing I would do is define an expression for an identifier:



    ident = Word(alphas, alphanums+'_')

since this crops up in many places, and there is no reason to define unique expressions for each location.



The second thing I would do is plan out how to assign results names to the parsed elements.  Results names will <em>greatly</em> simplify getting at these fields in the post-parsing phase.



I can see two approaches in doing this:

- explicitly define the options, and give the results names in the expression

- define an mini-tokenizer that just matches a list of `ident.ident...` elements, and then assigns them accordingly in a parse action



Here is what the first option looks like.



    fnCall = Combine(ident('term') + '.' + (
                ident('integral') + '.' + ident('region') + '.' + ident('flag') |
                ident('integral') + '.' + ident('region') |
                ident('region')
                ))('fnName') + '(' + Optional(delimitedList(ident),default=[])('args') + ')'



Note that I chose to use MatchFirst (using the '|' operator) instead of Or (using the '^' operator).  To successfully use MatchFirst, I must put the expressions in order from most restrictive - in this case, the longest - to least restrictive.



Now here is the test code:



    for test in tests:
        print test
        fnParts = fnCall.parseString(test)
        print fnParts.dump()
        print 'Region:', fnParts[0].region
        print 'Term:', fnParts[0].term
        print 'Integral:', fnParts[0].integral
        print 'Flag:', fnParts[0].flag
        print



dump() is a useful method on ParseResults to quickly see all the parsed tokens, and what keys or attribute names are defined.  Note also that I can access any of the fields as if they were attributes - if a field is missing, the attribute lookup will return a default of ''.  (If you prefer to get exceptions from looking up a nonexistent key, then use the dict-style access, such as `fnParts[0][&quot;flag&quot;]`.)



And this prints the following:



    d_term.Y( u )
    [['d_term.Y'], '(', 'u', ')']
    - args: ['u']
    - fnName: ['d_term.Y']
      - region: Y
      - term: d_term
    Region: Y
    Term: d_term
    Integral: 
    Flag: 
    
    d_term.i1.Y()
    [['d_term.i1.Y'], '(', [], ')']
    - args: []
    - fnName: ['d_term.i1.Y']
      - integral: i1
      - region: Y
      - term: d_term
    Region: Y
    Term: d_term
    Integral: i1
    Flag: 
    
    d_term.i1.Y.a( something, u, v )
    [['d_term.i1.Y.a'], '(', 'something', 'u', 'v', ')']
    - args: ['something', 'u', 'v']
    - fnName: ['d_term.i1.Y.a']
      - flag: a
      - integral: i1
      - region: Y
      - term: d_term
    Region: Y
    Term: d_term
    Integral: i1
    Flag: a



The second option looks like this:


    def assignParts(tokens):
        numTokens = len(tokens)
        if numTokens <= 1:
            raise ParseException('too few attributes in function call')
    
        tokens['term'] = tokens[0]
        if numTokens == 2:
            tokens['region'] = tokens[1]
        elif numTokens == 3:
            tokens['integral'] = tokens[1]
            tokens['region'] = tokens[2]
        else:
            tokens['integral'] = tokens[1]
            tokens['region'] = tokens[2]
            tokens['flag'] = tokens[3]
    
    fnName = delimitedList(ident,'.')
    fnName.setParseAction(assignParts)
    fnCall = fnName + '(' + Optional(delimitedList(ident),default=[])('args') + ')'



In this case, the parser just reads a '.'-delimited list of idents, and then passes them off to a separate parse action to assign them into the tokens collection by name, depending on how many tokens are found.  The output of the second option is similar to the first.



Option #2 was actually my first pass at this problem, but having done both, I think Option #1 appeals to me more.



HTH,

-- Paul
#### 2007-11-13 09:28:53 - robert_cimrman
Great!

I like the option #1 more, too.

many thanks,

r.

ps: in the other thread I started, I have somehow messed the  tags in my last post - is there a way to fix it?
#### 2007-11-13 09:52:25 - ptmcg
No, Wikispaces does not offer a means to edit previous posts.  If you want to repost your note, I (as the wiki admin) can delete the older duplicate version.

-- Paul
#### 2007-11-13 10:17:32 - robert_cimrman
I see,

Now, to be able to parse also 'd_term( u )', I put Optional around
and it works. However I do not understand the meaning of the extra brackets in

    ident('term') + '.' + ( # <- this one
    ...
    ))('fnName')



Thanks for your help and time,

r.
#### 2007-11-13 12:57:57 - ptmcg
So that I didn't need to use backslash characters for line continuations.  Wikispaces actually interprets ending backslash characters as line <em>concatenation</em>, not just continuation, so you end up getting horrendously long source lines on the wiki page.



-- Paul
#### 2007-11-14 02:29:51 - robert_cimrman
The curious thing is that without the brackets the code does not work...



tested on:



    '''- d_volume.i1.Omega( uc )'''



this does not work (sorry for long lines):





    ident = Word( alphas, alphanums + '_')
        variable = Word( alphas, alphanums + '._' )
        flag = Literal( 'a' )
    
        term = Optional( Literal( '+' ) | Literal( '-' ), default = '+' )( 'sign' )\
               + Combine( ident( 'name' )\
                          + Optional( '.' + ident( 'integral' ) + '.'
                                      + ident( 'region' ) + '.' + flag( 'flag' ) |
                                      ident( 'integral' ) + '.' + ident( 'region' ) |
                                      ident( 'region' )
                                      ))( 'termDesc' ) + '('\
                                      + Optional( delimitedList( variable ),
                                    default = [] )( 'args' ) + ')'



while this does:



    term = Optional( Literal( '+' ) | Literal( '-' ), default = '+' )( 'sign' )\
               + Combine( ident( 'name' )\
                          + Optional( '.' + (ident( 'integral' ) + '.'
                                      + ident( 'region' ) + '.' + flag( 'flag' ) |
                                      ident( 'integral' ) + '.' + ident( 'region' ) |
                                      ident( 'region' )
                                      )))( 'termDesc' ) + '('\
                                      + Optional( delimitedList( variable ),
                                    default = [] )( 'args' ) + ')'



just curious,

r.
#### 2007-11-14 06:05:37 - ptmcg
Ah, I see what you are asking about!

The brackets (I would call them parentheses) are there to group the alternatives for the number of elements in the function name, after the first '.'.

    A + ( '.' + ( B + '.' + C + '.' + D | B + '.' + C | B )) + E

Without the internal ()'s, this becomes:

    A + ( '.' + B + '.' + C + '.' + D | B + '.' + C | B ) + E

Now only the first option has a leading '.'.  If you want to get rid of the ()'s, fix this up by putting back in the leading '.'s:

    A + ( '.' + B + '.' + C + '.' + D | '.' + B + '.' + C | '.' + B ) + E

-- Paul
#### 2007-11-14 06:28:26 - robert_cimrman
I see, so it is about operator precedence... (yes, parentheses, I keep mistaking those often...)

thanks!

r.

---
## 2007-11-13 08:30:30 - robert_cimrman - pyparsing docstrings
Hi again,



I need some help with a (very basic) parser for parsing structured docstrings (with embedded LaTeX) in pyparsing?



I would like to be able to do the following: consider a docstring:





    '''
    some text
    
    :section1: some multiline
    text
    
    :another section:
    
    another text with a formula
    $a^x_1 - 1 = 0$
    and equation
    \begin{equation}
      sin( x ) = 0
    \end{equations}
    '''



then the parsing output should be similar to:



[(None, 'some text'), ('section1', 'some multiline

text'), ...]



that is a list of (section name, following text) pairs.



I have looked at docutils, but it seems to me like a command-line tool and not a module I could use from a script.



With pyparsing, I have tried:



    colon = pp.Literal( ':' )
    
        section = pp.Combine( colon + pp.Word( pp.alphas ) + colon )
        section.setName( 'section' )
    
        text = pp.Word( pp.printables + ' \t\n' ) + pp.NotAny( section )
        text.setName( 'text' )
    
    
        doc = pp.StringStart()\
               + pp.Optional( text ) + pp.ZeroOrMore( section + text )\
               + pp.StringEnd()

but it does not work, as the first match (text) consumes the whole multiline string. Omitting the 'pp.Optional( text )' does not help, as then everything is in the first section (same problem).



Best regards,

r.

#### 2007-11-13 08:37:50 - robert_cimrman
Now (too late) I have looked at the reply to badri's question on 'metadata conversion' - this might be a way to go for me too!
#### 2007-11-13 09:20:08 - robert_cimrman
for the sake of completeness, this works:

    import pyparsing as pp
    
    def setSection( sec ):
        def action( str, loc, toks ):
            if toks:
                sec[0] = toks[0][1:-1]
            return toks
        return action
    
    def toList( slist, sec ):
        def action( str, loc, toks ):
            if toks:
                slist.append( (sec[0], toks[0]) )
            return toks
        return action
    
    def createBNF( slist, currentSection ):
    
        colon = pp.Literal( ':' )
    
        section = pp.Combine( colon + pp.Word( pp.alphas ) + colon )
        section.setParseAction( setSection( currentSection ) )
        section.setName( 'section' )
    
        text = pp.SkipTo( section | pp.StringEnd() )
        text.setParseAction( toList( slist, currentSection ) )
        text.setName( 'text' )
    
    
        doc = pp.StringStart()\
               + pp.Optional( text ) + pp.ZeroOrMore( section + text )\
               + pp.StringEnd()
    
        return doc
    
    secList = []
    
    currentSection = [None]
    bnf = createBNF( secList, currentSection )
    out = bnf.parseString( ... )
    print secList

#### 2007-11-13 10:42:46 - robert_cimrman
corrected formatting (I hope)...

    import pyparsing as pp
    
    def setSection( sec ):
        def action( str, loc, toks ):
            if toks:
                sec[0] = toks[0][1:-1]
            return toks
        return action
    
    def toList( slist, sec ):
        def action( str, loc, toks ):
            if toks:
                slist.append( (sec[0], toks[0]) )
            return toks
        return action
    
    def createBNF( slist, currentSection ):
    
        colon = pp.Literal( ':' )
    
        section = pp.Combine( colon + pp.Word( pp.alphas ) + colon )
        section.setParseAction( setSection( currentSection ) )
        section.setName( 'section' )
    
        text = pp.SkipTo( section | pp.StringEnd() )
        text.setParseAction( toList( slist, currentSection ) )
        text.setName( 'text' )
    
        doc = pp.StringStart()\
               + pp.Optional( text ) + pp.ZeroOrMore( section + text )\
               + pp.StringEnd()
    
        return doc
    
    testStr = '''
    text0 :secA:
    text1
    :secB:
    text2
    '''
    
    secList = []
    currentSection = [None]
    bnf = createBNF( secList, currentSection )
    out = bnf.parseString( testStr )
    print secList
    


#### 2007-11-13 12:00:32 - badri
hi

thanks for your insight.

I added this a la your code:

    section = education ^ objective ^ skills ^ personal_details ^ experience
    text = StringStart() + SkipTo(section|StringEnd())
    doc = Optional(text) + ZeroOrMore(section)

and now my parser works just fine!



Thanks

Badri

---
## 2007-11-20 04:09:21 - robert_cimrman - description of function arguments
Again me with a docstring-related question. I would like to parse descriptions of function arguments as given here:





    test1 = '''
          - material.rho : $\rho$,
          - ts.dt : $\dt$,
          - parameter : $\ul{u}_0$'''
    '''
    
    test2 = '''
    ts.dt : $\dt$, parameter : $p_0$
    '''
    
    test3 = '''
          - `ary` : array
            This array will be flattened before the difference is taken.
          - `to_end` : number, optional
            If provided, this number will be tacked onto the end of the returned
            differences.
          - `to_begin` : number, optional
            If provided, this number will be taked onto the beginning of the
            returned differences.
    '''



The desired parsing result would be a list of

    <argnument name> (before ':'), <short description> (after :)
    
    <optional long description>



My best attempt so far was 





    colon = pp.Literal( ':' ).suppress()
        argItem = pp.Forward()
        endOfShortDesc = (pp.Literal( ',' ) | pp.StringEnd() | pp.LineEnd() ).suppress()
        argItem << pp.Group( pp.Optional( '-' ).suppress()\
                             + pp.SkipTo( colon )( 'argName' )\
                             + colon\
                             + pp.SkipTo( endOfShortDesc )( 'shortDesc' )\
                             + pp.SkipTo( argItem | pp.StringEnd() )( 'longDesc' ))
        args = pp.OneOrMore( argItem )( 'args' )



but it sort of works for the first two cases only (no long description) and I fail miserably trying to suppress the optional commas separating the descriptions. pp.SkipTo( endOfShortDesc, include = True ) does not help.



r.

ps: I really like setting the names by a function call instead of setName(), and accessing the fields via dot as attributes - I almost no longer need setParseAction()!

#### 2007-11-25 02:57:58 - ptmcg
Robert -



I'm sorry not to respond sooner, this really was a stumper.  I'm afraid I had to resort to some parse action tricks, and I had to change the default whitespace set to leave out newlines (since we have to test for them explicitly when checking for leading '-'s on a line).  The working parser isn't pretty, but it does parse your docstrings.  I'm sure you'll have questions on what this is doing, but for the moment, I'll have to leave this to you to puzzle out on your own for a bit.  Please post back if you have specific questions.


-- Paul





    test1 = r'''
          - material.rho : $\rho$,
          - ts.dt : $\dt$,
          - parameter : $\ul{u}_0$
    '''
    
    test2 = r'''
    ts.dt : $\dt$, parameter : $p_0$
    '''
    
    test3 = '''
          - `ary` : array
            This array will be flattened before the difference is taken.
          - `to_end` : number, optional
            If provided, this number will be tacked onto the end of the returned
            differences.
          - `to_begin` : number, optional
            If provided, this number will be taked onto the beginning of the
            returned differences.
    '''
    
    import pyparsing as pp
    
    def leadingChar(pe):
        def pa(s,l,t):
            thisLine = pp.line(l,s)
            thisCol = pp.col(l,s)
            if pe.preParse(thisLine,0) != thisCol-1:
                raise pp.ParseException('',l,'')
        return pe.setParseAction(pa)
    
    def bnf():
        pp.ParserElement.setDefaultWhitespaceChars(' \t')
        NL = pp.LineEnd().suppress()
        DASH = pp.Literal('-').suppress()
        colon = pp.Literal( ':' ).suppress()
        endOfShortDesc = (pp.Literal( ',' ) | pp.StringEnd() | pp.LineEnd() ).suppress()
        lineNotStartingWithDash = ~leadingChar(DASH) + pp.Empty() + pp.restOfLine + NL
        printablesWithoutComma = pp.printables.replace(',','')
        arg = pp.Group(leadingChar(DASH) + pp.SkipTo(colon) + colon + \
                pp.delimitedList(~NL+pp.Word(printablesWithoutComma)) + \
                pp.Optional(',').suppress() + NL + \
                pp.ZeroOrMore(~pp.StringEnd() + lineNotStartingWithDash))
        return pp.OneOrMore(pp.ZeroOrMore(NL) + arg)( 'args' )
    
    args = bnf()
    import pprint
    
    for t in (test1,test3):
        print t
        res = args.parseString(t)
        print res.dump()
        pprint.pprint(res.asList())
        print
    
    # special treatment for test2
    print test2
    pp.ParserElement.setDefaultWhitespaceChars(' \t\n')
    colon = pp.Suppress(':')
    res = pp.Dict(pp.delimitedList(pp.Group(pp.Word(pp.printables) + colon + pp.Word(pp.printables.replace(',',''))))).parseString(test2)
    print res.dump()

prints:
    
          - material.rho : $\rho$,
          - ts.dt : $\dt$,
          - parameter : $\ul{u}_0$
    
    [['material.rho ', '$\\rho$'], ['ts.dt ', '$\\dt$'], ['parameter ', '$\\ul{u}_0$']]
    - args: [['material.rho ', '$\\rho$'], ['ts.dt ', '$\\dt$'], ['parameter ', '$\\ul{u}_0$']]
    [['material.rho ', '$\\rho$'],
     ['ts.dt ', '$\\dt$'],
     ['parameter ', '$\\ul{u}_0$']]
    
    
          - `ary` : array
            This array will be flattened before the difference is taken.
          - `to_end` : number, optional
            If provided, this number will be tacked onto the end of the returned
            differences.
          - `to_begin` : number, optional
            If provided, this number will be taked onto the beginning of the
            returned differences.
    
    [['`ary` ', 'array', 'This array will be flattened before the difference is taken.'], ['`to_end` ', 'number', 'optional', 'If provided, this number will be tacked onto the end of the returned', 'differences.'], ['`to_begin` ', 'number', 'optional', 'If provided, this number will be taked onto the beginning of the', 'returned differences.']]
    - args: [['`ary` ', 'array', 'This array will be flattened before the difference is taken.'], ['`to_end` ', 'number', 'optional', 'If provided, this number will be tacked onto the end of the returned', 'differences.'], ['`to_begin` ', 'number', 'optional', 'If provided, this number will be taked onto the beginning of the', 'returned differences.']]
    [['`ary` ',
      'array',
      'This array will be flattened before the difference is taken.'],
     ['`to_end` ',
      'number',
      'optional',
      'If provided, this number will be tacked onto the end of the returned',
      'differences.'],
     ['`to_begin` ',
      'number',
      'optional',
      'If provided, this number will be taked onto the beginning of the',
      'returned differences.']]
    
    
    ts.dt : $\dt$, parameter : $p_0$
    
    [['ts.dt', '$\\dt$'], ['parameter', '$p_0$']]
    - parameter: $p_0$
    - ts.dt: $\dt$


#### 2007-11-26 02:46:09 - robert_cimrman
Paul,



you are my hero! I could not have possibly come with such a solution. What I could think of was some preprocessing of the string, like removing the first '-' on a line, replacing newlines by some tag etc. so many thanks for your solution.



I have just modified the code a bit (not pretending I understand what is going on):



    arg = pp.Group(leadingChar(DASH) + pp.SkipTo(colon)('argName') + colon + \
                pp.restOfLine('shortDesc') + \
                pp.Optional(',').suppress() + NL + \
                pp.ZeroOrMore(~pp.StringEnd() + lineNotStartingWithDash)('longDesc'))

to parse also strings like



    - `mask` : bool array
            The values ar1[mask] are in ar2.

that is, no comma-delimited list in 'shortDesc'.



It works well for the module I wanted to generate documentation for.
Would you like to see the result? Maybe it could be added to your examples. I also plan to release a mini-package (depends on pyparsing only) for automatically creating PDFLaTeX-generated docs of modules (specifically for numpy and scipy ones, using the docstring format above), but it still needs some polishing.

best wishes,

r.
#### 2007-11-26 04:46:30 - ptmcg
If you release it as a minipackage, I can link to it from the 'Whos Using Pyparsing' page.  If you don't, just send it to me and I'll add it to the 'Examples' page.

You might also try something like QuotedString('`') as an alternative to SkipTo(colon) for matching argnames, to give you some cleaner-looking results (should strip off backticks, and no trailing whitespace).

And I'm glad I could help - I don't feel like I should have to write a parse action to do the job that I should have gotten from using LineStart(), but I just couldn't get it to do what I wanted.

Cheers!

-- Paul
#### 2007-11-27 02:12:11 - robert_cimrman
I will polish it a bit and then release.

SkipTo(colon) is fine, as I typeset the documents with LaTeX, so whitespace is ignored anyway and the backticks look well.

best,

r.
#### 2007-11-28 06:46:37 - robert_cimrman
For the moment the code (and examples) is here: 

It is probably a temporary location, though.

cheers,

r.

---
## 2007-12-05 15:09:52 - Viserys - Newbie question (is this possible?)
Hi there!

I'm hopeful about pyparsing being able to fill a parsing difficulty a friend and I are having with a project of ours (ie, reduce our large and confusing parse function), but I think I need some help.

Basically, the parser is given strings of the form

    [count] item [from location]


where:

    **count** is an optional integer that defaults to '1'
    
    **item** is a string that can be anything ('279', 'red brick', 'brick')
    
    **from location** is an optional string that defaults to 'from room'. If the word 'from' is present, 'location' must exist as well, following the same rules as **item** (any non-empty string that is not 100% whitespace).



Ideally, pyparsing would allow me to easily get a list of strings in a format similar to, no matter the input:

    ['count', 'item', 'from', 'location']

The problems I'm having are mainly two:

1. Getting the 'from', 'location' split from the default case 'from room'. If I use an Optional('from' + anyword), with the default 'from room', I only get one Token.

2. Since 'item' can be a single integer, it's often mistaken for 'count' if 'count' is omitted. That is to say, 
'32 from room', which would ideally yield ['1', '32', 'from', 'room'] confuses the parser. Since '32' is a number, it is taken for 'count', and 'item' cannot be found. 



Since I've probably confused you now, here are a few examples of input and ideal output.

1. 'fish' -> ['1', 'fish', 'from', 'room']

2. '2 blue fish' -> ['2', 'blue fish', 'from', 'room']

3. '32' -> ['1', '32', 'from', 'room']

4. '5 32 from bag' -> ['5', '32', 'from', 'bag']

5. '57 from blue suitcase' -> ['1', '57', 'from', 'blue suitcase']

I've tried experimenting with several Optional and Or constructs, to no avail. Is there any hope of catching all the special cases in a single pyparsing expression, or am I out of luck?

Thanks a lot for your time.

#### 2007-12-05 21:06:42 - ptmcg
This is a very loose 'grammar', it looks like you are writing some form of interactive fiction parser.  The more variations in input that you want to support, the more difficult the parsing job is going to be - eventually you drift into natural language parsing territory, which is more complex than I would recommend for pyparsing.

First some basics:

    integer = Word(nums)
    FROM = Literal('from')
    fromClause = FROM + restOfLine

Looking at your specific examples, I tried to pick out some rules, especially for determining whether a leading integer represented a count or the first word of an item.  So detecting a leading count requires some lookahead rules to see if the integer is:

<ul><li>followed by the word 'from', or</li><li>followed by the end of string</li></ul>If either of these is true, then the leading integer represents an item, not a count.  This gets implemented using ~FollowedBy, as in:

    count = integer + ~FollowedBy( FROM | stringEnd )

Once the determination of count vs. item was done, we can wrap it inside an Optional expression, with a default value of '1' if count is not provided.

After parsing the leading count, then the options are:

<ul><li>an item and a fromClause</li><li>an item</li></ul>

So for the item, we can take everything up to a fromClause (using SkipTo), or just everything else on the line.  If no fromClause is present, SkipTo will fail, se we can just write this as a MatchFirst:



    item = SkipTo(fromClause) | restOfLine



Lastly, the fromClause may or may not be present, and if not present, we want to report ['from','room'].  We'll use an Optional for this:



    Optional(fromClause, default=['from','room'])



Altogether, this looks like:



    integer = Word(nums)
    FROM = Literal('from')
    fromClause = FROM + restOfLine
    
    count = integer + ~FollowedBy( FROM | stringEnd )
    item = SkipTo(fromClause) | restOfLine
    expr = Optional(count,default='1') + \
            item + \
            Optional(fromClause, default=['from','room']) + \
            stringEnd
    
    for t in tests:
        print t
        fields = expr.parseString(t)
        print fields
        print



This prints out:


    fish
    ['1', 'fish', ['from', 'room']]
    
    2 blue fish
    ['2', ' blue fish', ['from', 'room']]
    
    32
    ['1', '32', ['from', 'room']]
    
    5 32 from bag
    ['5', '32 ', 'from', ' bag']
    
    57 from blue suitcase
    ['1', '57 ', 'from', ' blue suitcase']



That's all I have for now.  I encourage you to take this as a starting point, and add results names to make it simpler to extract the individual data elements.



-- Paul
#### 2007-12-05 21:14:21 - Viserys
Thanks a lot. I sort of had a sinking feeling as I wrote the above post, as I realized it was probably far looser than what pyparsing is intended for.


You were right-on with the intended purpose - rest assured that some commands follow far stricter rules, and pyparsing is getting some well-deserved use.



It looks like 'FollowedBy' was the piece of the puzzle I was missing. Thanks again, this should do much better than what I currently have, even if it requires a little extra code to work perfectly.
#### 2007-12-06 10:04:41 - Viserys
I tweaked it by suppressing the FROM, and making the default value of the Optional fromClause just be 'room'. Since the actual string 'from' is useless (I only care that it's there in the original string), this works just fine.



The results are then passed to a short helper function that .strips the last two values (item and location), and if either is an empty string, displays appropriate error text to the user.



Thanks again. :)
#### 2007-12-06 10:11:23 - ptmcg
Good call on suppressing FROM, some people have a hard time throwing away the parsed tokens, but this is the right thing to do.  'from' is only useful at parse time, but it is just clutter afterward.



You might want to give results names a shot, something like:



    expr = Optional(count,default='1')('count') + \
            item('item') + \
            Optional(fromClause, default='room')('source') + \
            stringEnd



Then you can just work with the parsed tokens directly as in:



    print fields.item
    print fields.source
    print fields['source']
    
    # show matched tokens as nested list, followed by name-values
    # for expressions tagged with results names
    print fields.dump()  



and so on, without having to try to index into a list of tokens, grapple with optionals and shifting indices, etc.  This also makes your grammar more robust as you inject more complex options (prepositional phrases and the like).



-- Paul
#### 2007-12-06 16:04:36 - Viserys
Hi again! I've been looking at documentation, and the answer is probably painfully obvious, but I'm missing it.


If I print results.item, I see a string. 'fish', 'blue fish', '32', etc.

However, both location (your 'source') and count appear as lists. ['1'], ['room'], etc.

The only thing I can see in common is that both loc and count use named Optionals, whereas item is not. Is that the reason?

- Steven
#### 2007-12-06 18:46:23 - ptmcg
No, this is not painfully obvious, it is really very subtle.  So subtle that I have tried to correct this for years and have not been successful.  (If I fixed it now, I'd have to release it in a version 2.0, as I think quite a few pyparsing applications now depend on the grouped return values.)



But your analysis is right, the Optional wrapper instance is taking what would have been a 'scalar' value, and returning it as a single-element array.



-- Paul

---
## 2007-12-06 09:28:09 - virtualjim - Newbie bitten by setResultsName behavior
I saw the docs about setResultsName() returning a copy of the element, but I didn't realize that the new name was applied <em>only</em> to the copy, so simply calling:

    theElement.setResultsName( 'new_name' )

accomplishes nothing. Couldn't figure out why I couldn't find my named elements until I looked at the method source. Is there a particular reason for doing it this way instead of first applying the name and then returning a copy?

Many thanks for the excellent module.

#### 2007-12-06 09:57:54 - ptmcg
The main reason that setResultsName makes a copy in the first place is so that a common expression (like `integer = Word(nums)`) could be used multiple times within a grammar with different names, as in:



    integer = Word(nums)
    field = Word(alphas)
    personalInfo = field.setResultsName('firstName') + 
        field.setResultsName('lastName') +
        integer.setResultsName('age') +
        integer.setResultsName('zipCode')



I'm pretty certain that the results name should be on the copy, so that the original remains a clean template to be used elsewhere, or by itself.



This is why I tend to attach results names only in the higher-level expression, and not at the original definition level.  But you could do something like this:



    age = integer.setResultsName('age')
    zipCode = integer.setResultsName('zipCode')



And in fact, the new 'callable-style' interface helps get away from the setXXX misnomer of the original method name, so that the first example could be given simply as:



    integer = Word(nums)
    field = Word(alphas)
    personalInfo = field('firstName') + 
        field('lastName') +
        integer('age') +
        integer('zipCode')



If I were to do anything at the base-level of the integer definition, it would be to call setName, not setResultsName:



    integer = Word(nums).setName('integer')



This gives the name 'integer' to the expression, so that when a ParseException is returned, you get something readable like 'Expected integer at loc 27' instead of 'Expected W:(abcd...) at loc 27'.  setName gives a name to the expression itself, as opposed to setResultsName's behavior of tagging the matched tokens in the returned ParseResults.



Thanks for using pyparsing, please write back if you have more questions or comments, or have a new application to be listed on the 'Who's Using Pyparsing' page!



-- Paul
#### 2008-02-06 12:03:39 - dbrydon
Paul,  Thanks for pyparsing.  I am using it and trying to get better at it.  So far I've failed to be able to set a name and use it.



Could you expand the snippet you wrote above into a complete but simple working example of how to set and use names that I could build from?

    from pyparsing import *
    
    integer = Word(nums)
    field = Word(alphas)
    personalInfo = field('firstName') + \
        field('lastName') + \
        integer('age') + \
        integer('zipCode')

#### 2008-02-06 20:20:17 - ptmcg
No problem.  See the following annotated code, and match up the comments to the following output.



Glad you are having fun with pyparsing!

-- Paul


    from pyparsing import *
    
    # define the match pattern
    integer = Word(nums).setName('integer')
    field = Word(alphas).setName('alpha field')
    personalInfo = field('firstName') + \
        field('lastName') + \
        integer('age').setParseAction(lambda tokens:int(tokens[0])) + \
        integer('zipCode')
    
    info = personalInfo.parseString('Inigo Montoya 28 12345')
    # simple debugging statement to view ParseResults tokens and named fields
    print info.dump()
    # ParseResults can act like a dict
    print info.keys()
    # access named results using dict-style
    print info['age']
    # access named results using object attribute style
    print info.zipCode
    print
    
    # exception shows how setName is useful
    try:
        info2 = personalInfo.parseString('Andre the Giant 45 98765')
    except ParseException, pe:
        print pe
        # helper diagnostic method to see where things went bad
        print pe.markInputline()

prints:

    ['Inigo', 'Montoya', 28, '12345']
    - age: 28
    - firstName: Inigo
    - lastName: Montoya
    - zipCode: 12345
    ['lastName', 'age', 'zipCode', 'firstName']
    28
    12345
    
    Expected integer (at char 10), (line:1, col:11)
    Andre the >!<Giant 45 98765

---
## 2007-12-10 12:50:04 - paulsmith - Ambiguous matches
I want to parse text which is inherently ambiguous. (For example, 'CENTRAL PARK', could be {'street_name': 'CENTRAL PARK'} or {'street_name': 'CENTRAL', 'suffix': 'PARK'}.) What I'd like is for the parser to return a list of all of the possible matches, and let the caller figure out the disambiguation. The reason Or or MatchFirst won't work is that they only return a single matched expression. Any way to handle this?

#### 2007-12-10 14:00:13 - ptmcg
Not without a major internal redesign of pyparsing.



There was a thread on comp.lang.python back in October, titled 'Is Pyparsing Really Recursive Descent?'.  Perhaps you could contact the original poster of that thread, and see what you can come up with.



Best of luck!

-- Paul

---
## 2007-12-10 18:26:10 - jpj1138 - Is Word() matching too much?
I was playing around with an example from 'Getting Started with Pyparsing', where the following example is used to show how to match a valid Python identifier:



    Word(alphas+'_',alphanums+'_')



I.e., a python identifier must start with a letter.  When I feed it a string such as ' 0000abcd ' it works as expected in that the string '0000abcd' is not matched.  What is unexpected (by me, a Pyparsing neophyte) is that the string 'abcd', is returned as a match.



Is this expected behavior?  If so, is there an easy way to specify the grammar such that the 'abcd' portion of the string '0000abcd' is not matched?



Thanks,



Jason

#### 2007-12-11 06:32:05 - ptmcg
Jason -



Try:



    Word(alphas+'_',alphanums+'_',asKeyword=True)





-- Paul
#### 2007-12-11 11:40:19 - jpj1138
Seems to work for the Python identifier case, but not for this one:



    Word('-',alphanums+'_',asKeyword=True).searchString(' -opt val ') # Want to match '-opt', but not 'val'



Does the asKeyword argument only affect alphanumeric characters?



Jason

---
## 2007-12-13 20:52:39 - tmcw - Accented Characters?
Hi, I'm working on a script to parse Wikipedia Wikitext, and since it'll be running over (very) large amounts of varying data, I'll need to expect and deal with accented characters. Is there something like 'alphas' that will cover this? Right now I'm just writing something to parse plain-vanilla wikitext, not really into anything, but here:



    wikitext = Literal('<br />').suppress() | Word(alphas + nums + '()<!-:>_,&;/.' + ''' + ''') | QuotedString('{{',endQuoteChar='}}').setParseAction(ignore_temp) | QuotedString('[[',endQuoteChar=']]').setParseAction(remember_tag)

Essentially, I'm trying to parse infoboxes, and this is for the values of the data. For instance,


```
Infobox Country<br >
| native_name             = ''Rpublique franaise''<br >
| conventional_long_name  = French Republic<br >
```

So, this would be matching the text ''Republique fracaise'' - it would be nice if I could just get the whole string to the right of the =. Sometimes, there are piped links like  that would interrupt the flow of the parse tree, so that's why I'm looking for them right now.



Thanks for any help.

#### 2007-12-14 00:07:32 - ptmcg
See if alphas8bit will be of help to you.  Use it like this:

    # match a word including accented and other special characters
    Word(alphas+alphas8bit)

-- Paul
#### 2007-12-14 08:43:30 - tmcw
Could there be a way to just parse out all of the wikilinks and templates, then get the string of data just as a string? Here's what I have so far:

    wikitext = Literal('<br />').suppress() | Word(alphas + nums + alphas8bit + '()<=!-:>_,&;/.' + ''' + ''') | QuotedString('`&quot;,endQuoteChar=&quot;`').setParseAction(ignore_temp) | QuotedString('[[',endQuoteChar=']]').setParseAction(remember_tag)
    tuple = Literal('|').suppress() + Group(OneOrMore(Word(alphas + nums + '_'))) + Literal('=').suppress() + ZeroOrMore(wikitext)
    wrapper = Literal('{{').suppress() + Literal('Infobox') + Group(ZeroOrMore(Word(string.uppercase + string.lowercase + '_<!->:')))
    
    infobox = wrapper + ZeroOrMore(Group(tuple))
    
    bdata = infobox.parseString(src)
    
    for d in bdata:
      print d

This works for many entries, but I know it could be better. I was thinking a text transform to change  into just {display}, and then parsing with the 'until' function? (for some reason, I can't find some functions I remember in the HTML docs?

#### 2007-12-14 08:44:14 - tmcw
Sorry about the botched code.

    wikitext = Literal('<br />').suppress() | Word(alphas + nums + alphas8bit + '()<=!-:>_,&;/.' + ''' + ''') | QuotedString('{{',endQuoteChar='}}').setParseAction(ignore_temp) | QuotedString('[[',endQuoteChar=']]').setParseAction(remember_tag)
    tuple = Literal('|').suppress() + Group(OneOrMore(Word(alphas + nums + '_'))) + Literal('=').suppress() + ZeroOrMore(wikitext)
    wrapper = Literal('{{').suppress() + Literal('Infobox') + Group(ZeroOrMore(Word(string.uppercase + string.lowercase + '_<!->:')))
    
    infobox = wrapper + ZeroOrMore(Group(tuple))
    
    bdata = infobox.parseString(src)
    
    for d in bdata:
      print d
    


#### 2007-12-14 16:06:50 - tmcw
I've got it working to the extent that if I just parse Infobox data, then it'll run. But trying to use .scanString returns just fragments of the needed data...


    def remember_tag(s,l,t):
      return t[0].split('|')[0]
    
    def ignore_temp():
      pass
    
    comment = QuotedString('<!--',endQuoteChar='-->').suppress()
    wikitext = Literal('<br />').suppress() | comment | Word(alphas + nums + alphas8bit + '()<=!-:>_,&;/.' + ''' + ''') | QuotedString('{{',endQuoteChar='}}').setParseAction(ignore_temp) | QuotedString('[[',endQuoteChar=']]').setParseAction(remember_tag)
    tuple = Literal('|').suppress() + Group(OneOrMore(Word(alphas + nums + '_'))) + Literal('=').suppress() + ZeroOrMore(wikitext)
    wrapper = Literal('{{').suppress() + Literal('Infobox').suppress() + Group(ZeroOrMore(Word(string.uppercase + string.lowercase + '_<!->:')))
    
    infobox = wrapper + ZeroOrMore(Group(tuple))
    
    bdata = infobox.scanString(src)
    
    for d in bdata:
      print d



returns 

    (([(['Country'], {}), ([(['native_name'], {}), 



And so on.
#### 2007-12-15 09:02:33 - ptmcg

---
## 2007-12-18 08:30:04 - wcbarksdale - Spaces at start of line
What I'd like to be able to do is match a line beginning with one or more spaces, but without making the rest of my grammar space-sensitive.  It's already line-sensitive.  What I tried is this:



    ParserElement.setDefaultWhitespaceChars(' \t')
    
    spaced
    Line = OneOrMore(White(' ')) + Literal('hi')
    spacedLine.parseString('   hi') # This works fine
    
    fullLine = LineStart() + spacedLine + LineEnd()
    fullLine.parseString('  hi') # This does not


I also need to know how many spaces there are before the first non-space character.  Any ideas?

#### 2007-12-18 08:34:25 - wcbarksdale
Okay, I didn't see the non-static method setWhitespaceChars until just now, and that seems to work fine here.
#### 2007-12-18 10:56:53 - ptmcg
Glad you worked this out, whitespace-sensitivity is a tricky topic with pyparsing.



Write back if you need more help/guidance/shoulder to cry on.



-- Paul

---
## 2007-12-28 13:01:00 - michael_ramirez44 - IronPython 1.1
Will PyParsing work with IronPython?If so how to I get it to work?

#### 2007-12-28 15:44:56 - ptmcg
Michael -

I just took a quick stab at this, there are a number of inconsistencies between IronPython and CPython.  I was able to define alternate symbols for many of these, and have an importable version of pyparsing.  However, there is one shortcoming so far, IronPython does not support definition of __radd__ methods, which means that grammar expressions that add strings to ParserElements, where the string comes first, will not compile.  (As a side note, this is one of the major hurdles in supporting Jython, also.)

Do you know if there is an easy way, perhaps using some aspect of the sys module, that I can detect if running as IronPython or CPython, so that I can conditionalize pyparsing's imports?

-- Paul
#### 2007-12-28 21:03:33 - ptmcg
Michael -



I just read through the IronPython FAQs, and found how to resolve the import problems - read this FAQ (, search for the question, 'Q: How do I use CPython standard libraries?') - this step will be required to use pyparsing.

I've just updated the SourceForge SVN repository with a version of pyparsing that works around any of the pyparsing-internal uses of -__radd__, but this will still be a limitation in any Python code that you write using pyparsing.

I'd be very interested to hear of your experience in using pyparsing in IronPython.

-- Paul
#### 2008-01-16 08:05:36 - michael_ramirez44
I download the latest version of pyparsing but I still can't get it to work with IronPython even after I add the CPython Libs.

    C:\IronPython-1.1>ipy pyparsing.py
    Traceback (most recent call last):
      File C:\web_apps\as3g\pyparsing.py, line 3355, in Initialize
      File C:\web_apps\as3g\pyparsing.py, line 2895, in delimitedList
      File , line 0, in Add##208
    TypeError: Cannot convert Word(W:(abcd...,abcd...)) to String


#### 2008-01-16 15:01:46 - ptmcg
Michael -

This bug only occurs if you invoke pyparsing.py directly, as you did using ipy - oddly, my unit tests do not run pyparsing this way!  If you import this module into another, you should be able to run that module (and indirectly pyparsing) within IronPython.

Meanwhile, I'll make changes to the code and update the online SVN.

-- Paul

---
## 2007-12-29 18:17:12 - nickzam - re: LineEnd strange behavior
Fixed code blocks, sorry.

Hello, Paul!



I am trying to implement quite simple parser. I need all strings that are surrounded by '<' and '>'.



    body = '''
    <Daisy> Anybody here?<br><Minor> No.<br><Minor> Great 
    '''
    nickstart = CaselessLiteral('<')
    nickend = CaselessLiteral('>')
    
    nickname_list = []
    nickname_list.append(ZeroOrMore(CaselessLiteral('<br>')|LineStart()+Suppress(nickstart) + SkipTo(nickend)))
    
    for nickname in nickname_list:
    
    for mytok, start, end in nickname.scanString(body):
    html = html + str(mytok).encode('utf8')



The problem is that input strings are from webpage and all newlines changed to <br> tags. This code parsed first match great and then skipped to the end of whole string.



I tried to change expression to:



    nickname_list.append(ZeroOrMore(CaselessLiteral('<br>')|LineStart()+Suppress(nickstart) + SkipTo(nickend)) + StringEnd())



as recommended on tips page but have zero matches at all. Changing StringEnd() to LineEnd() has zero result too.



Merry Christmas to you and happy New Year!

Nikolai

#### 2007-12-29 21:09:44 - ptmcg
Nikolai -



Thanks for writing, and Merry Christmas to you!



I think you are doing too many things at once.  After simplifying your code a little bit, I think I get closer to your goal.  This code:



    from pyparsing import *
    
    body = '''
    <Daisy> Anybody here?<br><Minor> No.<br><Minor> Great 
    '''
    nickstart = CaselessLiteral('<')
    nickend = CaselessLiteral('>')
    brtag = CaselessLiteral('<br>')
    # a slightly improved version of brtag, will handle tag caselessness,
    # and will also handle any attributes, such as <BR CLEAR=ALL>
    brtag = makeHTMLTags('br')[0].setParseAction(keepOriginalText)
    
    # this is just confusing - why a list? and why ZeroOrMore when you 
    # are calling scanString?
    nickname_list = []
    nickname_list.append(ZeroOrMore(CaselessLiteral('<br>')|LineStart()+Suppress(nickstart) + SkipTo(nickend)))
    
    #~ for nickname in nickname_list:
    
    nickname = brtag | nickstart.suppress() + SkipTo(nickend) + nickend.suppress()
    for mytok, start, end in nickname.scanString(body):
        print mytok.asList()
    
        # html is not previously declared
        #~ html = html + str(mytok).encode('utf8')



Will print out this:

    ['Daisy']
    ['<br>']
    ['Minor']
    ['<br>']
    ['Minor']

Is this something like what you are trying to accomplish?



-- Paul
#### 2007-12-30 09:34:31 - nickzam
Thank you very much!



Problem was in my program (I showed only a piece of it as example), I found 2 bugs that causes incorrect parsing.



I used Twill and it had an outdated built-in pyparsing module. So it was imported at first.

I don't need that <br> tags as output string. I tried to change brtag to brtag.suppress(), but had an empty string instead. I need list with nicknames only.

<ol><li>this is just confusing - why a list? and why ZeroOrMore when you</li><li>are calling scanString?</li></ol>

I will have few grammars for nicknames, so I wanted to iterate through grammars and found one that produced non-empty results (still not implemented).



I am a bit confused with ZeroOrMore, OneOrMore etc - should I use them only in parseString()? 



I don't understand how to implement grammar based on few SkipTo() expressions. For example I want to parse a webpage and get all data surrounded by <tr>,</tr>,<td>,</td> tags. Is it possible by using few SkipTo() in one pass?
#### 2007-12-30 19:32:54 - ptmcg
Nikolai -



Ok, there are several questions in your post, let try to answer them one at a time:



1. You have lost me with respect to the <br> tags.  If you don't want them, then just leave them out of the grammar entirely.



2. parseString vs. scanString vs. searchString vs. transformString

parseString is useful if you have a grammar that fully describes the input string.  When extracting data from HTML source (as in scraping data from a web page), you will probably <em>never</em> use parseString, as this would require you to define a full HTML grammar!  If you are in this situation, turn to one of the Python HTML parsing tools, such as ElementTree or BeautifulSoup.  scanString is useful when you are trying to extract bits of a larger file, which would make this a good choice for your application.  scanString returns a generator that returns the tokens, starting and ending location for each match, and you access these fields using explicit calls to .next(), or more commonly a for loop, as in:



    for tokens, start, end in extractExpr.scanString(filecontents):
        print 'Matched tokens:', tokens.asList()
        print 'Starting at location:', start
        print 'Ending at location:', end



Since this is a generator, you will immediately get the results for the first match, even if there are thousands of matches, even if the file is zigabytes in size.  However, if you want to simplify your life, you can just use searchString, which is a shortcut around scanString.  searchString returns a list of just the matched tokens, no starting or ending locations.  But since this is a list, it does not return until the entire input string has been searched.  searchString is little more than this wrapper around scanString:



    def scanString(self, inputString):
        return [ tokens for tokens, start, end in self.scanString(inputString) ]



Since we are going to use scanString, you don't have to build in any repetition of OneOrMore's or ZeroOrMore's <em>unless the pattern you are searching for has repetition in it.</em>  Here are some examples:



    aWord = Word('Aa')  # word composed of letters A or a
    bWord = Word('Bb')
    cWord = Word('Cc')
    
    inputString = 'ALKQWJBDFLSAKJQEBBBAJELalkbrwqbqlibBA'
    
    print aWord.searchString(inputString).asList()
    print bWord.searchString(inputString).asList()
    print cWord.searchString(inputString).asList()



prints out:



    [['A'], ['A'], ['A'], ['a'], ['A']]
    [['B'], ['BBB'], ['b'], ['b'], ['bB']]
    []



Now, one way that I could look for aWords, bWords, and cWords would be to add all these results together:



    print aWord.searchString(inputString).asList() + \
          bWord.searchString(inputString).asList() + \
          cWord.searchString(inputString).asList()
    



giving:



    [['A'], ['A'], ['A'], ['a'], ['A'], ['B'], ['BBB'], ['b'], ['b'], ['bB']]



OR, I could create a single expression, indicating that the pattern I am looking for is an aWord, a bWord, or a cWord:



    searchExpr = aWord | bWord | cWord
    print searchExpr.searchString(inputString).asList()



giving:



    [['A'], ['B'], ['A'], ['BBB'], ['A'], ['a'], ['b'], ['b'], ['bB'], ['A']]



We didn't talk at all about the last parsing method, transformString.  transformString is used, well, to transform strings, either by stripping matching expressions (by suppressing an expression), or replacing one expression with another (using a parse action, or the pyparsing helper method `replaceWith`.  For instance, if the '<br>'s in your HTML are really bothering you, you could <em>transform</em> them using this:



    brtag = makeHTMLTags('BR')[0]
    brtag.setParseAction( replaceWith('\n') )
    html = brtag.transformString(html)



(Of course, if all of your <br> tags are always all lower case, and always attributeless, then you would be better off with:



    html = html.replace('<br>','\n')



But this would not catch '<BR>', '<Br>', '< br >', or '\
'.  Using brtag.transformString will catch all of these.  Don't be misled that I called makeHTMLTags with 'BR' - makeHTMLTags uses caseless expressions so that 'br', 'Br', 'bR', or 'BR' are all matched.)



3. Matching <tr>, <td>, etc.

Hmm, you are getting dangerously close to BeautifulSoup territory - I think this question puts you more in BS's sweet spot than what one would do with pyparsing.  Still, if the tables are simple and there is no nesting of rows within rows or tables within tables, and if there are only <td> tags within <tr></tr> pairs (that is, no <font> tags or <br>'s, etc.)  You could do something like this:



    tdStart,tdEnd = makeHTMLTags('TD')
    trStart,trEnd = makeHTMLTags('TR')
    tableStart,tableEnd = makeHTMLTags('TABLE')
    tableCell = tdStart + SkipTo(tdEnd) + tdEnd
    tableRow = trStart + ZeroOrMore(tableCell) + trEnd
    table = tableStart + ZeroOrMore(tableRow) + tableEnd



Now this will give you a very flat, very messy set of parsed tokens, which you will have to scan through looking for <tr>, etc. tags to reconstruct the table's structure.  Here is how to do that within the grammar, so that the returned tokens are already structured for you (using pyparsing's Group class):



    tdStart,tdEnd = makeHTMLTags('TD')
    trStart,trEnd = makeHTMLTags('TR')
    tableStart,tableEnd = makeHTMLTags('TABLE')
    tableCell = Group( tdStart.suppress() + SkipTo(tdEnd) + tdEnd.suppress() )
    tableRow = Group( trStart.suppress() + ZeroOrMore(tableCell) + trEnd.suppress() )
    table = tableStart.suppress() + ZeroOrMore(tableRow) + tableEnd.suppress()





Now try doing this:  use the table expression to extract the tables on this page: . (This page lists the public NTP servers run by NIST.)





    from pyparsing import *
    import urllib
    from pprint import pprint  # use pprint to do print nice-looking nested lists
    
    page = urllib.urlopen('http://tf.nist.gov/tf-cgi/servers.cgi')
    html = page.read()
    page.close()
    
    tdStart,tdEnd = makeHTMLTags('TD')
    trStart,trEnd = makeHTMLTags('TR')
    tableStart,tableEnd = makeHTMLTags('TABLE')
    tableCell = Group( tdStart.suppress() + SkipTo(tdEnd) + tdEnd.suppress() )
    tableRow = Group( trStart.suppress() + ZeroOrMore(tableCell) + trEnd.suppress() )
    table = tableStart.suppress() + ZeroOrMore(tableRow) + tableEnd.suppress()
    
    for tbl in table.searchString(html):
        pprint( tbl.asList() )
        print



Wow, that's a lot to chew on, let me know if this was just too much at once!



-- Paul
#### 2007-12-30 19:50:02 - ptmcg
Oops, forgot to list the final output:



    [[['<hr noshade='' size='3'>\n        ']],
     [['<div align='left'>\n                <font size='5' face='Arial,Helvetica,
        Geneva,Swiss,SunSans-Regular' color='#555555'><b>NIST Internet Time 
        Servers</b></font></div>\n        ']],
     [['<hr noshade='' size='1'>\n        ']]]
    
    [[['<b>\xa0\xa0Return to <a href='/timefreq/service/its.htm'>NIST Internet Time 
        Service Page</a>\xa0\xa0</b>']]]
    
    [[['<b>Name</b>'],
      ['<b>IP Address</b>'],
      ['<b>Location</b>'],
      ['<b>Status</b>']],
     [['nist1-ny.WiTime.net\n'],
      ['208.184.49.9\n'],
      ['New York City, NY\n'],
      ['<blink><b>Intermittent hardware problems\n</b></blink>']],
     ...
     [['nist1-sj.WiTime.net\n'],
      ['64.125.78.85\n'],
      ['San Jose, California\n'],
      ['<b>Recommended for new users\n</b>']]]
    
    [[['ntp-a.boulder.nist.gov'], ['132.163.4.107'], ['NIST, Boulder, Colorado']]]
    



-- Paul
#### 2008-01-03 03:03:32 - nickzam
Thank you for detailed explanation!



