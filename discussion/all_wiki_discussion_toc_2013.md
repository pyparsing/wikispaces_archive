## Pyparsing Wikispaces Discussion - 2013

_[Note: these entries are fairly old, and predate many new features of pyparsing,
and are predominantly coded using Python 2.
They are captured here for historical benefit, but may not contain
the most current practices or features. We will try to add editor
notes to entries to indicate when discussions have been 
overtaken by development events.]_

[2013-01-02 09:24:04 - hetsch - Pyparsing sequence of repeating pattern](all_wiki_discussion_toc_2013.md#2013-01-02-092404---hetsch---pyparsing-sequence-of-repeating-pattern)  
[2013-01-10 11:37:16 - Horus107 - Problem with recursive grammar](all_wiki_discussion_toc_2013.md#2013-01-10-113716---horus107---problem-with-recursive-grammar)  
[2013-01-17 01:14:02 - Horus107 - Complete conversion to dict](all_wiki_discussion_toc_2013.md#2013-01-17-011402---horus107---complete-conversion-to-dict)  
[2013-01-31 07:43:44 - Djeendo - KeyValues to JSON converter](all_wiki_discussion_toc_2013.md#2013-01-31-074344---djeendo---keyvalues-to-json-converter)  
[2013-02-02 07:53:25 - cdegenkolb - keep whitespaces in found expressions](all_wiki_discussion_toc_2013.md#2013-02-02-075325---cdegenkolb---keep-whitespaces-in-found-expressions)  
[2013-02-05 10:15:16 - pulsa82 - lineno() slow for large files](all_wiki_discussion_toc_2013.md#2013-02-05-101516---pulsa82---lineno-slow-for-large-files)  
[2013-02-06 00:07:45 - egabancho - Parse longest match inside "and" expresion](all_wiki_discussion_toc_2013.md#2013-02-06-000745---egabancho---parse-longest-match-inside-"and"-expresion)  
[2013-02-11 00:51:12 - sniper228 - Parsing Basic Block](all_wiki_discussion_toc_2013.md#2013-02-11-005112---sniper228---parsing-basic-block)  
[2013-02-20 05:35:09 - xeo4 - recursion limit exceeded problem with recursive grammar](all_wiki_discussion_toc_2013.md#2013-02-20-053509---xeo4---recursion-limit-exceeded-problem-with-recursive-grammar)  
[2013-02-26 03:01:35 - josandres - Combine elements in an arbitrary order](all_wiki_discussion_toc_2013.md#2013-02-26-030135---josandres---combine-elements-in-an-arbitrary-order)  
[2013-02-27 11:10:35 - lianjunj - pyparsing performance expectation?](all_wiki_discussion_toc_2013.md#2013-02-27-111035---lianjunj---pyparsing-performance-expectation)  
[2013-02-28 07:05:23 - elphono - Keyword that matches ONLY keyword](all_wiki_discussion_toc_2013.md#2013-02-28-070523---elphono---keyword-that-matches-only-keyword)  
[2013-03-04 08:28:07 - veli9999 - need to identify functions and ignore every other thing in a file](all_wiki_discussion_toc_2013.md#2013-03-04-082807---veli9999---need-to-identify-functions-and-ignore-every-other-thing-in-a-file)  
[2013-03-05 00:53:01 - kmbt - setDefaultWhitespaceChars not woring for anyOpenTag\/anyCloseTag?](all_wiki_discussion_toc_2013.md#2013-03-05-005301---kmbt---setdefaultwhitespacechars-not-woring-for-anyopentag\anyclosetag)  
[2013-03-11 05:02:15 - gordonschulz - Usage of Each()\/heterogeneous file](all_wiki_discussion_toc_2013.md#2013-03-11-050215---gordonschulz---usage-of-each\heterogeneous-file)  
[2013-04-04 07:08:58 - peyguin - Odd parser behaviour](all_wiki_discussion_toc_2013.md#2013-04-04-070858---peyguin---odd-parser-behaviour)  
[2013-04-10 01:02:02 - multani0 - How to improve error messages?](all_wiki_discussion_toc_2013.md#2013-04-10-010202---multani0---how-to-improve-error-messages)  
[2013-04-10 07:42:13 - jonotoronto - MIDL Parser](all_wiki_discussion_toc_2013.md#2013-04-10-074213---jonotoronto---midl-parser)  
[2013-04-15 07:27:23 - clintpriest - Include file and parse?](all_wiki_discussion_toc_2013.md#2013-04-15-072723---clintpriest---include-file-and-parse)  
[2013-04-22 06:53:35 - sniper228 - Calculating Variable Dependency](all_wiki_discussion_toc_2013.md#2013-04-22-065335---sniper228---calculating-variable-dependency)  
[2013-04-26 15:38:02 - DZaaaaaa - Problem with PGN example](all_wiki_discussion_toc_2013.md#2013-04-26-153802---dzaaaaaa---problem-with-pgn-example)  
[2013-05-09 07:30:35 - abhijitbhatt2003 - Parsing configuration file](all_wiki_discussion_toc_2013.md#2013-05-09-073035---abhijitbhatt2003---parsing-configuration-file)  
[2013-05-14 07:30:20 - f.wilamo - Leading OneOrMore ](all_wiki_discussion_toc_2013.md#2013-05-14-073020---fwilamo---leading-oneormore-)  
[2013-05-17 00:28:07 - pschwaha - use OrderedDict](all_wiki_discussion_toc_2013.md#2013-05-17-002807---pschwaha---use-ordereddict)  
[2013-05-17 06:26:56 - AlphaPrime - Only Allow One NOT](all_wiki_discussion_toc_2013.md#2013-05-17-062656---alphaprime---only-allow-one-not)  
[2013-05-17 10:47:02 - mirk410 - Losing the tail-end of my expression](all_wiki_discussion_toc_2013.md#2013-05-17-104702---mirk410---losing-the-tail-end-of-my-expression)  
[2013-05-19 13:29:32 - wilo108 - `searchString` works, but `parseString` throws an exception?](all_wiki_discussion_toc_2013.md#2013-05-19-132932---wilo108---searchstring-works-but-parsestring-throws-an-exception)  
[2013-05-21 09:11:53 - mirk410 - Escaping delimiter character](all_wiki_discussion_toc_2013.md#2013-05-21-091153---mirk410---escaping-delimiter-character)  
[2013-05-22 07:29:33 - HumbertMason - Strange warning declaring a simple PyParsing recursive grammar in Python](all_wiki_discussion_toc_2013.md#2013-05-22-072933---humbertmason---strange-warning-declaring-a-simple-pyparsing-recursive-grammar-in-python)  
[2013-05-23 10:07:16 - t1m0 - setResultsName bug or misunderstanding](all_wiki_discussion_toc_2013.md#2013-05-23-100716---t1m0---setresultsname-bug-or-misunderstanding)  
[2013-06-04 01:53:06 - Leevi3 - simple bool operator precedence](all_wiki_discussion_toc_2013.md#2013-06-04-015306---leevi3---simple-bool-operator-precedence)  
[2013-06-13 02:02:38 - termopro - Trying to find an address in text](all_wiki_discussion_toc_2013.md#2013-06-13-020238---termopro---trying-to-find-an-address-in-text)  
[2013-06-24 17:12:37 - ecesurfer - Parsing C Function Implementation](all_wiki_discussion_toc_2013.md#2013-06-24-171237---ecesurfer---parsing-c-function-implementation)  
[2013-06-30 20:00:57 - geoff-99 - Problem with tag names created by asXML](all_wiki_discussion_toc_2013.md#2013-06-30-200057---geoff-99---problem-with-tag-names-created-by-asxml)  
[2013-07-12 17:15:34 - Errglefloot_the_Enormous - What is the best method for parsing large files?](all_wiki_discussion_toc_2013.md#2013-07-12-171534---errglefloot_the_enormous---what-is-the-best-method-for-parsing-large-files)  
[2013-08-03 11:13:51 - EldritchCheese - Unexpected result using setParseAction](all_wiki_discussion_toc_2013.md#2013-08-03-111351---eldritchcheese---unexpected-result-using-setparseaction)  
[2013-08-09 10:52:17 - typesupply - Tips for parsing block with conditional ending?](all_wiki_discussion_toc_2013.md#2013-08-09-105217---typesupply---tips-for-parsing-block-with-conditional-ending)  
[2013-08-29 09:14:50 - usysinc - Install failed with Python2.6 on Windows7](all_wiki_discussion_toc_2013.md#2013-08-29-091450---usysinc---install-failed-with-python26-on-windows7)  
[2013-09-07 07:55:12 - EldritchCheese - defaultdict and setResultsName](all_wiki_discussion_toc_2013.md#2013-09-07-075512---eldritchcheese---defaultdict-and-setresultsname)  
[2013-09-12 00:14:38 - strzmiele - simpleBool](all_wiki_discussion_toc_2013.md#2013-09-12-001438---strzmiele---simplebool)  
[2013-09-12 11:21:10 - bbaldino1 - setResultsName best practices?](all_wiki_discussion_toc_2013.md#2013-09-12-112110---bbaldino1---setresultsname-best-practices)  
[2013-09-15 12:29:55 - sorenh - Problem with select_parser](all_wiki_discussion_toc_2013.md#2013-09-15-122955---sorenh---problem-with-select_parser)  
[2013-09-19 12:22:04 - bbaldino1 - Yet another question 'Group' usage](all_wiki_discussion_toc_2013.md#2013-09-19-122204---bbaldino1---yet-another-question-group-usage)  
[2013-10-23 11:54:25 - pjennings1234 - ParseSyntaxException isn't always fatal](all_wiki_discussion_toc_2013.md#2013-10-23-115425---pjennings1234---parsesyntaxexception-isnt-always-fatal)  
[2013-10-24 02:07:47 - israkir - Parsing nested parenthesis producing multiple parsing results](all_wiki_discussion_toc_2013.md#2013-10-24-020747---israkir---parsing-nested-parenthesis-producing-multiple-parsing-results)  
[2013-12-02 14:40:46 - acomfygeek - pfe.markInputline()](all_wiki_discussion_toc_2013.md#2013-12-02-144046---acomfygeek---pfemarkinputline)  
[2013-12-08 22:39:31 - kwos - Partial parser](all_wiki_discussion_toc_2013.md#2013-12-08-223931---kwos---partial-parser)  


---
## 2013-01-02 09:24:04 - hetsch - Pyparsing sequence of repeating pattern
Hi,

I'm using pyparsing the first time, so sorry for a dumb question.

I have the following code:


    text = '[@a eee, fff fff, ggg @b eee, fff, ggg @c eee eee, fff fff,ggg ggg@d]'
    command_s = Suppress(Optional('[') + Literal('@'))
    command_e = Suppress(Literal('@') | Literal(']'))
    task = Word(alphas)
    arguments = ZeroOrMore(
        Word(alphas) + 
        Suppress(
            Optional(Literal(',') + White()) | Optional(White() + Literal('@'))
        )
    )
    command = Group(OneOrMore(command_s + task + arguments + command_e))
    print command.parseString(text)
    
    # which outputs only the first @a sequence
    # [['a', 'eee', 'fff', 'fff', 'ggg']]
    
    # the structure should be someting like:
    [
         ['a', 'eee', 'fff fff', 'ggg'],
         ['b', 'eee', 'fff', 'ggg'],
         ['c', 'eee eee', 'fff fff', 'ggg ggg'],
         ['d']
    ]





yesterday I've posted a similar question to this one. Python Regex Named Groups This work's pretty nice for simple things.

After some researching I've read about the pyparsing library which seems to be pretty perfect for my tasks.

    text = '[@a eee, fff fff, ggg @b eee, fff, ggg @c eee eee, fff fff,ggg ggg@d]'
    command_s = Suppress(Optional('[') + Literal('@'))
    command_e = Suppress(Literal('@') | Literal(']'))
    task = Word(alphas)
    arguments = ZeroOrMore(
        Word(alphas) + 
        Suppress(
            Optional(Literal(',') + White()) | Optional(White() + Literal('@'))
        )
    )
    command = Group(OneOrMore(command_s + task + arguments + command_e))
    print command.parseString(text)
    
    # which outputs only the first @a sequence
    # the structure should be someting like:
    [
         ['a', 'eee', 'fff fff', 'ggg'],
         ['b', 'eee', 'fff', 'ggg'],
         ['c', 'eee eee', 'fff fff', 'ggg ggg'],
         ['d']
    ]

@ indicates the start of a sequence, the first word is a task (a) followed by optional comma-separated arguments (eee, fff fff, ggg). The problem is, that @b, @c and @d are ignored by the above code. Also 'fff fff' getting treated as two separated arguments, it should only be one. The mess in the sample string is not my idea - this comes from user input.

Hope this illustrates my current problem. Thank you for any help!

Regards,
hetsch

#### 2013-01-02 09:28:56 - hetsch
Sorry for the messed up output. I asked the question also on stackoverflow.com and reused some parts in this one. Seems that I can't edit my post.
#### 2013-01-02 09:48:57 - hetsch
Allready answered in 

---
## 2013-01-10 11:37:16 - Horus107 - Problem with recursive grammar
Hello!

I try to construct a parser for a recursive grammar:


    rootkey1 rootval2;
    dict
    {
      dictkey1 dictval2;
      subdict
      {
        subkey1 subkey2;
      }
    } 


The subdict can contain another subdict and so on....



    ident = Word(alphanums + '.')
    semi = Literal(';').suppress()
    lcb =  Literal('{').suppress()
    rcb =  Literal('}').suppress()
    
    FKeyValue = ident + SkipTo(semi) + semi
    FDictionary = Forward()
    FBlock = lcb + ZeroOrMore(FKeyValue | FDictionary) + rcb
    FDictionary << ident + FBlock 
    ParameterFile = ZeroOrMore(FDictionary | FKeyValue)


I've removed grouping and dict

It mostly works but when it comes to recursion it parses wrong. The text above is parsed to:



    [ 'rootkey1',
      'rootval2',
      'dict',
      'dictkey1',
      'dictkey2',
      'subdict',
      '{\n    subkey1 subkey2']
    


So it somehow catches the { and an newline. 

If I remove all newlines from the text:


    rootkey1 rootval2; dict { dictkey1 dictval1; subdict { subkey1 subkey2; } }

which is valid too, the problem changes:



    [ 'rootkey1',
      'rootval2',
      'dict',
      'dictkey1',
      'dictval1',
      'subdict',
      '{ subkey1 subkey2']


Any suggestions what could be the problem there?

Thanks!

#### 2013-01-10 20:33:31 - ptmcg
Whenever you use SkipTo in a parser, you have to be careful that you aren't skipping over too much. And you should make this expression the last expression among any alternatives, to give more specific expressions a chance to match first.

This is the case here.  In FBlock, change:


    FBlock = lcb + ZeroOrMore(FKeyValue | FDictionary) + rcb

to


    FBlock = lcb + ZeroOrMore(FDictionary | FKeyValue) + rcb

I think things will work better from here.

-- Paul

---
## 2013-01-17 01:14:02 - Horus107 - Complete conversion to dict
Hello,

the ParseResult.asDict() method returns a dict off the parsing results if properly marked using Dict(...). But it converts only the first level of a nested dict. Nested dicts are still of type ParseResults. I want to have a dict with nested dicts. Is there any builtin method to entirely (recursively) convert the results? (writing my own one wouldn't be too hard too ;-)

Thanks!

#### 2013-01-17 03:47:36 - Horus107
Ok, for anyone to comment or copy:



    def _as_dict(self, parse_results):
            ''' Recursively converts the ParseResults object to a dict. '''
            d = parse_results.asDict()
            for i in d:
                if type(d[i]) == ParseResults:
                    d[i] = self._as_dict(d[i])
            return d



---
## 2013-01-31 07:43:44 - Djeendo - KeyValues to JSON converter
Hello. This is the fist time I use pyparsing and I have some troubles.

There is a Valve's data format called KeyValues.  . It is very similar to JSON. JSON could be got from KeyValues by separating keys and values in dictionary with ':' and separating dictionary members with ','. Here is what I've got so far:


    # KeyValues Grammar
    ws = Optional(Word(' \n\t\r'))
    dict_begin = ws + '{' + ws
    dict_end = ws + '}' + ws
    
    kvString = dblQuotedString
    kvDictionary = Forward()
    
    kvKey = kvString.copy()
    kvKey.setParseAction(lambda t: t[0] + ':')
    kvValue = kvString | kvDictionary
    kvMember = ws + kvKey + ws + kvValue + ws
    kvMembers = Dict(Group(OneOrMore(kvMember)))
    kvDictionary <<= (dict_begin + Optional(kvMembers) + dict_end)
    kvDocument = kvMembers  # root
    
    # Deal with whitespace manually.
    kvDocument.leaveWhitespace()
    dict_begin.leaveWhitespace()
    dict_end.leaveWhitespace()
    
    def keyValuesToJSON(instring):
        return kvDocument.transformString(instring)


It adds ':' after each Key. But I have no idea how to separate all members with ','.

P. S: Turned off whitespace handling to make JSON layout the same as in source file.

#### 2013-01-31 15:26:20 - ptmcg
Wow, very nice job at laying out this parser, even using the latest '<<=' operator!

I was also a bit dismayed at how much work you had to go through to not have transformString kill your formatting whitespace - again, my hat is off to you in dealing with this.

This is a place where you can use an Empty() expression to inject some custom behavior where there wasn't any to begin with. I've taken your code, and made a *very* few minor changes, but note the use of insertComma:


    # KeyValues Grammar
    ws = Optional(Word(' \n\t\r'))
    dict_begin = ws + '{' + ws
    dict_end = ws + '}' #+ ws
    
    #~ kvString = dblQuotedString
    kvString = QuotedString(''', unquoteResults=False)
    kvDictionary = Forward()
    
    kvKey = kvString.copy()
    kvKey.setParseAction(lambda t: t[0] + ':')
    kvValue = kvString | kvDictionary
    insertComma = Empty().leaveWhitespace().setParseAction(lambda:',')
    kvMember = ws + kvKey + ws + kvValue + Optional(~dict_end + insertComma) + ws
    kvMembers = Dict(Group(OneOrMore(kvMember)))
    kvDictionary <<= (dict_begin + Optional(kvMembers) + dict_end)
    kvDocument = kvMembers  # root
    
    # Deal with whitespace manually.
    kvDocument.leaveWhitespace()
    dict_begin.leaveWhitespace()
    dict_end.leaveWhitespace()
    
    def keyValuesToJSON(instring):
        return kvDocument.transformString(instring)


This will insert ','s after each kvMember that is not the last one within a dict.

I also had to replace your definition of kvString with a custom QuotedString - a couple of places in your input you had two quoted strings with no separating whitespace, and the default dblQuotedString treated the adjacent ' characters as a single escaped ' character within a larger string.

Hope this helps, and again, very nice job on your parser!

-- Paul
#### 2013-02-01 23:17:32 - Djeendo
Thank you very much. It definetely helps =)

---
## 2013-02-02 07:53:25 - cdegenkolb - keep whitespaces in found expressions
hi

I'm fairly new to pyparsing an stumbled upon a problem i can't solve.

I have the following example text and my expressions


    mystring = '''===List Heading===
    *[[list item with link]]
    * list item without link
    *[[very long list item with linebreak
    and link]]
    *very long list item with
    linebreaks and stuff
    '''
    
    uniword = Word(alphanums)
    
    listingWord = OneOrMore(uniword)
    
    listingLink = ( Literal('[[') + listingWord + Literal(']]') )
    
    listingItem = Group((Literal('*').suppress() + (listingLink ^ listingWord)  )).setResultsName('item',listAllMatches=True)
    
    
    listingHeadDelim = ('=' + ZeroOrMore( '=' ) ).suppress()
    listingHeading = Group(listingHeadDelim + OneOrMore(uniword) + listingHeadDelim).setResultsName('heading')
    
    listing = listingHeading + OneOrMore(listingItem)
    
    result = listing.parseString(mystring)


this gives me a dictionary in the form of


    item
        [['[[', 'list', 'item', 'with', 'link', ']]'], ['list', 'item', 'without', 'link'], ['[[', 'very', 'long', 'list', 'item', 'with', 'linebreak', 'and', 'link', ']]'], ['very', 'long', 'list', 'item', 'with', 'linebreaks', 'and', 'stuff']]
    heading
        ['List', 'Heading']


so far so good. my problem now is i need the link names and the list heady in one array element.
so more something like
 
    item
        [[''], ['list item without  link'], [''], ['very long list item with linebreaks and stuff']]
    heading
        ['List Heading']


i did try different versions of group, combine, leaveWhitespace() and Optional(White()) bot ether this breaks the parsing process or i get broken results.

did i miss an easy solution?

with regards
christian

#### 2013-02-03 09:42:17 - cdegenkolb
Well its me again. 
I had some more time to fiddle with the problem and for now i have some kind of solution



    uniword = Word(alphas)
    
    listingWord = Combine(OneOrMore(uniword), joinString=' ', adjacent=False)
    
    listingLink = Combine( Literal('[[') + listingWord + Literal(']]') , adjacent=False)
    
    listingItem = Group((Literal('*').suppress() + (listingLink ^ listingWord) )).setResultsName('item',listAllMatches=True)
    
    
    listingHeadDelim = ('=' + ZeroOrMore( '=' ) ).suppress()
    listingHeading = Group(listingHeadDelim + OneOrMore(listingWord) + listingHeadDelim).setResultsName('heading')
    
    listing = listingHeading + OneOrMore(listingItem)
    
    erg = listing.parseString(mystring)


gives me the following result



    item
        [['[[list item with link]]'], ['list item without link'], ['[[very long list item with linebreak and link]]'], ['very long list item with linebreaks and stuff']]
    heading
        ['List Heading']


As far as I can say this should match my desired result.

Does anybody have a better solution? 
I will start testing this code on real world data. 
So maybe this will break.

with regards
christian

---
## 2013-02-05 10:15:16 - pulsa82 - lineno() slow for large files
Is there a good way to count newline characters as they go by, wihout using pyparsing.lineno()?

I recently started using pyparsing to compile some ATE pattern files like this one.



    vector ($tset1
     TCK, TDI, TMS, TDO,
     some, more, signals
     )
    {
     > tset1 0000001XXXXXXXXXXXXXXXXXXXX00XXXXXXXXXXXXXXXXXXX ;
     > tset1    // Statements can span multiple lines.
      0000101XXXXXXXXXXXXXXXXXXXX00XXXXXXXXXXXXXXXXXXX ;
    repeat 10   // Commands modify the next statement.
     > tset1 0010011XXXXXXXXXXXXXXXXXXXX00XXXXXXXXXXXXXXXXXXX ;
    
    // ... a million lines of pattern data ...
    
    halt
     > tset1 0000010XXXXXXXXXXXXXXXXXXXX01X0XXXXXXXXXXXXXXXXX ;
    }


Pyparsing is a huge improvement over my old code, except for one problem:
For debugging purposes I need to record the line number of every complete statement, and these files can be rather large (over 100 MB).

My main parse action looks like this:


    class MyParser:
        def onStatement(self, s, loc, toks):
            line = lineno(loc, s)
            # rest of parse action...


pyparsing.lineno() counts '\n' characters in the string parsed so far.
A few thousand lines into the input, time spent in lineno() becomes a major slowdown, and gets worse as we consume more input.
I killed the program after an hour.  After I took out the call to lineno(), it ran in about a minute.

So, my question: Can we count lines without using lineno()?

Are there any other good tips for parsing large input files?

Thank you.

-Adam

#### 2013-02-08 02:03:56 - ptmcg
Here is an alternative to lineno:


    import bisect
    
    _NL = '\n'
    class LineCounter(object):
        def __init__(self, sourcestr, firstLine=0):
            self.sourcestr = sourcestr
            self.sourcestrlen = len(sourcestr)
            self.firstLine = firstLine
            self.cache = [(-1,0)]
    
        def getLineNo(self, loc):
            if loc >= self.sourcestrlen:
                raise ValueError('location greater than string length')
    
            maxloc,maxlineno = self.cache[-1]
            while loc > maxloc:
                nextloc = self.sourcestr.find(_NL, maxloc+1)
                if nextloc == -1:
                    return maxlineno + self.firstLine
                maxlineno += 1
                self.cache.append((nextloc, maxlineno))
                if nextloc >= loc:
                    return maxlineno - 1 + self.firstLine
                maxloc = nextloc
            else:
                # use bisect to do binary search for nearest loc in cache
                cacheloc = bisect.bisect_left(self.cache, (loc,0))
                return self.cache[cacheloc][1] - 1 + self.firstLine
    


Here is a test of LineCounter vs lineno, using pyparsing's own source code as a sample input string:


    from pyparsing import lineno
    
    source = open('pyparsing.py').read()
    numberer = LineCounter(source, firstLine=1)
    for i in range(len(source)):
        l1 = lineno(i,source)
        l2 = numberer.getLineNo(i)
        if l1 != l2:
            print i, l1, l2


LineCounter keeps an internal cache of previously-seen newline locations, and when the cache has not yet reached the target locn, it starts searching for newlines at the last place it left off, instead of starting over again from 0. This should help reduce the quadratic behavior you were seeing. 

Over 100Mb in about a minute? That is pretty good for pyparsing! If you post your parser, I might have some other hints for you, but it sounds like you've already done some pretty good tuning work.

HTH,
-- Paul
#### 2013-02-10 19:34:26 - pulsa82
Thanks, Paul.  That does the trick nicely.

I still haven't quite made the mental switch from lex & yacc -- it never occurred to me
to take advantage of the way pyparsing slurps in the whole file at once.

About the results I reported earlier: I had slightly misremembered my test case.
The largest pattern file I have at the moment is 67MB, 716k lines.
Half of those lines result in a call to lineno() -- 358k, give or take a few.
With my conversion code commented out, the parser alone takes 80 seconds to run.
When I add in the call to your lineno() replacement, it adds another 2 seconds -- quite reasonable.

My entire program now runs as intended in 115 seconds, on my Intel Core i5-3470.

Packrat parsing didn't help me.  It slowed down the program and ate memory.
PyPy gets hung up somehow, for reasons I didn't investigate.

I think for now I will call it good enough.
If I find time and am able to make any significant improvements in the future, I will try to post them here.

Thanks again for all your work on pyparsing.  It's a great tool for Getting Things Done.

Adam

---
## 2013-02-06 00:07:45 - egabancho - Parse longest match inside "and" expresion
Hello I'm using pyparsing with my configuration files and I'm facing one problem quite 'wired', at least for me.

This is my configuration file:


    fields:
      @inherit_from(bar)
      field1
      foo = field2
      field3
    documentation:
      'Some doc string here'


this is the parser that I've made


    indentStack = [1]   
    
    field_def = (Word(alphanums + '_') + Optional(Suppress('=') + Word(alphanums + '_'))).setResultsName('field_definitions', listAllMatches=True)       
    inherit_from = (Suppress('@inherit_from') + originalTextFor(nestedExpr('(', ')')))setResultsName('inherit_from')                                                                          
    fields = Suppress('fields:') + indentedBlock(Optional(inherit_from) + OneOrMore(field_def), indentStack)
    
    doc_string = QuotedString(quoteChar=''''', multiline=True) | quotedString.setParseAction(removeQuotes)
    documentation = (Suppress('documentation:') + indentedBlock(doc_string, indentStack)).setResultsName('documentation')                           
    
    return fields + Optional(documentation) 


The problem here is that pyparsing interprets that the reserved word 'documentation:' is just another field and I'm always getting and indentation error.

Any kind of idea/solution will be more than welcome ;-)

#### 2013-02-08 02:15:34 - ptmcg
I think the use of indentedBlock needs fixing up. You have defined what you expect to find in the total body of the indented block, but pyparsing needs to look at each line by line within the block to see if the indentation has been ended. So the argument to indentedBlock is an expression describing what you expect to see for each element within the indentation.  So try this instead:



    fields = Suppress('fields:') + indentedBlock(inherit_from | field_def, indentStack)


That is, the fields block 'is an indented block made up of inherit_from or field_def expressions'. 

-- Paul
#### 2013-02-11 06:58:56 - egabancho
Thanks Paul!, this works more or less perfectly for me.

---
## 2013-02-11 00:51:12 - sniper228 - Parsing Basic Block
Hi i want to give program visualization to my project
so i want to parse following code into .dot language i.e 


    main ()
    {
      int a, b;
    <bb 2>:
      a = 1;
      b = 20;
      if (a < b)
        goto <bb 3>;
      else
        goto <bb 7>;
    ...


to 


    digraph {
        start [label='Start'];
    
        start -> bb1;
        bb1 [shape = box, style ='filled', fillcolor = '#0000aa', label='a = 1;\nb = 20;']
        bb1 -> decision;
    
        decision [shape=diamond, style='filled', fillcolor = '#0000ff ', label='if (a < b)goto <bb 3>;\nelse goto <bb 7>;\n'];



#### 2013-02-11 18:07:52 - ptmcg
How complex is the language you are trying to parse? Start by writing a BNF for a simple subset of this language. Identify the different types of program statements (declaration, label, assignment, if-then-else, goto, while, print, etc.). Keep these simple - limit assignments to just 'varname '=' integer' for now, limit if conditions to 'varname comparsionOp varname', limit varnames to single alpha characters, etc. - enough so you can parse the snippet you posted above. One of the nice things about pyparsing is that you can start with a simple BNF, and then go back and expand it bit by bit (expanding varnames to alphanumerics, for example).

Then map out how you will convert this parsed output to DOT instructions. For each instruction type, you'll need to decide what shape it will be shown in, what will be in the shape, and how it will link to neighboring shapes. As you parse the code, you'll need to assign unique identifiers to each statement so that you can make these linkages accordingly. Finally, you'll traverse the parsed statements, and generate the shapes and linkages in the appropriate DOT commands.

From the code snippet, I would try to end up with something like this:



    digraph {
    
        start [label='Start'];
        end [label='End'];
    
        # all shapes
        label_bb_2 [shape = box, style ='filled', fillcolor = '#808080', label='bb 2'];
        label_bb_3 [shape = box, style ='filled', fillcolor = '#808080', label='bb 3'];
        label_bb_7 [shape = box, style ='filled', fillcolor = '#808080', label='bb 7'];
        s000 [shape = box, style ='filled', fillcolor = '#0000aa', label='a = 1'];
        s001 [shape = box, style ='filled', fillcolor = '#0000aa', label='b = 20'];
        s002 [shape=diamond, style='filled', fillcolor = '#0000ff ', label='a < b'];
        s003 [shape = box, style ='filled', fillcolor = '#0000aa', 'goto <bb 3>'];
        s004 [shape = box, style ='filled', fillcolor = '#0000aa', 'goto <bb 7>'];
    
        # link shapes
        start -> label_bb_2;
        label_bb_2 -> s000;
        s000 -> s001;
        s001 -> s002;
        s002 -> s003 [label='T'];
        s002 -> s004 [label='F'];
        s003 -> label_bb_3;
        s004 -> label_bb_7;
    
    }


Take a stab at your simplified BNF and corresponding pyparsing parser, then we can see how to take the next step of generating the DOT statements.

-- Paul
#### 2013-02-12 04:55:30 - sniper228
Thank you sir...

---
## 2013-02-20 05:35:09 - xeo4 - recursion limit exceeded problem with recursive grammar
Hello!

I use pyparsing to parse the SMTLib2 grammar described in the appendix of this document:


In general, my parser works fine. But as soon as the nesting level of expressions ('term') gets too deep, no matter what I try I get a 'maximum recursion depth exceeded' error.

The error inducing file in question is this one:


Increasing the recursion limit did not help as this goes beyond the stack size (and I do not want to have to increase the stack size - I'd rather work with as low a recursion depth as possible). I'm now wondering if this is simply impossible to accomplish with pyparsing or if I made a mistake somewhere down the road?

Here's my grammar: 
(It's quite a bit, hence I pasted it to pastebin.)

Any idea as to why this behavior might occur would be of great help! 
Aina

#### 2013-02-20 07:34:18 - xeo4
Just realized that the regex for binary and hexadecimal is incorrect... never mind, they do not occur in the example posted.
#### 2013-02-28 00:02:03 - ptmcg
Your paste doesn't even compile for me:

This is not even valid Python syntax:



    bvalue = 'true' | 'false' 


Try:



    bvalue = oneOf('true false')


You have a similar problem in info_flag.


term has unbalanced parentheses, and is used in var_binding before you define it.


script is misspelled on the last line.


In general, if you are having recursion errors, then you need to look at how Forwards are being used. A quick look at your parser doesn't show me anything wrong. You could try adding 


    term.setName('term').setDebug()


And then watch to see where you start spinning while parsing.

I hope that's enough to get you going again.

-- Paul
#### 2013-02-28 00:33:47 - xeo4
Hi Paul,

thanks for your reply and sorry for the paste screw-up - I had to strip my code from quite some stuff in order to make it more simple and readable and obviously stripped too much. (That's also where the term screw-up and the typo in the last line came from.)

b_value originally was defined as:


    TRUE = Keyword('true')
    FALSE = Keyword('false')
    
    b_value         = \
                    NoMatch().setName(''true' or 'false'') \
                    | TRUE                                            \
                    | FALSE


Same goes for info_flag.

Again, sorry about that. I shouldn't have created the paste when being dead tired :(
Actually I also realized later that for some unknown reason script.parseFile(infile, parseAll = True) does not work without setting my parse actions.

Anyway, I already tried to gather more info by setDebug() before, to no avail (posting here was my last resort....).
Good to know that my parser looks fine in general, though - I was not sure if I maybe overlooked something that might make troubles with pyparsing.

I kinda gave up yesterday and wrote a dedicated recursive decent parser without pyparsing - and it works perfectly fine. Seems that due to the deeply recursive structure of the grammar, pyparsing's own recursive overhead was just too much.

Just fyi, this is the error message I got:


    [snip]
      File '/usr/lib64/python3.2/site-packages/pyparsing.py', line 968, in parseString
        loc, tokens = self._parse( instring, 0 )
      File '/usr/lib64/python3.2/site-packages/pyparsing.py', line 836, in _parseNoCache
        preloc = self.preParse( instring, loc )
      File '/usr/lib64/python3.2/site-packages/pyparsing.py', line 793, in preParse
        loc = self._skipIgnorables( instring, loc )
      File '/usr/lib64/python3.2/site-packages/pyparsing.py', line 785, in _skipIgnorables
        loc,dummy = e._parse( instring, loc )
      File '/usr/lib64/python3.2/site-packages/pyparsing.py', line 842, in _parseNoCache
        loc,tokens = self.parseImpl( instring, preloc, doActions )
      File '/usr/lib64/python3.2/site-packages/pyparsing.py', line 2480, in parseImpl
        return self.expr._parse( instring, loc, doActions, callPreParse=False )
      File '/usr/lib64/python3.2/site-packages/pyparsing.py', line 842, in _parseNoCache
        loc,tokens = self.parseImpl( instring, preloc, doActions )
      File '/usr/lib64/python3.2/site-packages/pyparsing.py', line 2208, in parseImpl
        loc, resultlist = self.exprs[0]._parse( instring, loc, doActions, callPreParse=False )
      File '/usr/lib64/python3.2/site-packages/pyparsing.py', line 846, in _parseNoCache
        loc,tokens = self.parseImpl( instring, preloc, doActions )
      File '/usr/lib64/python3.2/site-packages/pyparsing.py', line 1444, in parseImpl
        raise ParseException(instring, loc, self.errmsg, self)
    RuntimeError: maximum recursion depth exceeded


It's a pity that I had to give up on pyparsing as it is easy to use and makes one's life a lot easier...

Thanks again!

---
## 2013-02-26 03:01:35 - josandres - Combine elements in an arbitrary order
Hi,
I'm trying to define a token for parsing relative time expressions, containing days, hours, minutes and seconds the following way:

    WW days XX hours YY mins ZZ secs

These unit may be optional (for example, a relative time can be defined by just specifying the hours, or by omitting the days).

I'm wondering if pyparsing provides a class like OneOrMore, or Combine, or something like that, but i haven't been able to parse them correctly.

I've tried to use a Combine object where all its elements are optional, but empty strings match this expression.

Can anyone give me a hint about the correct way of doing this?

Thanks in advance!

#### 2013-02-26 05:00:43 - josandres
Hi Again,

The answer is to use the Each class with Optional objects! :-D
#### 2013-02-27 23:36:42 - ptmcg
Using Each with Optional objects will still match an empty string though. I'm not coming up with a good expression to indicate that at least one object must be present, maybe you'll need to add a parse action to ensure you don't match an empty string.
#### 2013-02-27 23:50:51 - josandres
Thanks a lot ptmcg for your answer.
I used a parseAction for processing the matching tokens. It just works for my scenario :-D

---
## 2013-02-27 11:10:35 - lianjunj - pyparsing performance expectation?
Hi,
   I'm trying to parse a 256KB text file, which is broken into multiple sections separated by empty lines.  Before I add more detailed parsing of individual sections, I just evaluated the time needed for parsing the text into individual sections, which took 2.7 secs on my machine with 32GB memory and 3.4Ghz i7. Is it expected?

My parser is the following:


    f = open('test.dat','r')
    origData = f.read();
    f.close();
    
    emptyLine = LineStart() + LineEnd()
    sectionBreak = OneOrMore(emptyLine)
    sectionData = SkipTo(sectionBreak, include=True) 
    
    wholeDoc = OneOrMore(sectionData)
    result = wholeDoc.parseString()
    


I'm not able to paste the orignal files, but I have tried on other text file, which shows similar performance.

Is there anyway to improve its speed to be less than a sec?

Thanks

#### 2013-02-27 11:15:12 - lianjunj
Just to add a little more detail: I was using Python 2.7.2 and the 256KB file has 6162 lines and about 220 sections.
#### 2013-02-27 23:32:28 - ptmcg
LineStart() does a fair amount of work, since it tries to match logical start-of-line after taking into account whitespace or comment skipping. What if you change sectionBreak to LineEnd() * (2,None)? That is, 2 or more consecutive end-of-lines. But less than 1 second? Doubtful.

In general, runtime speed is not a strength of pyparsing. Its emphasis is more on simplified grammar development and maintenance.

If you are really focused on runtime speed, you might prototype up a parser in pyparsing, and then convert it to one of the other Python parsing libraries, PLY, spark, yapps, etc.

-- Paul
#### 2013-02-28 10:12:57 - lianjunj
Looks like the new sectionBreak definition gives about 15% improvement and after using pypy, the overall improvement is about 30%. So, the total time dropped from 2.8 sec to about 1.8 sec.

We already have a perl script that did similar parsing, and the reason we want to move to pyparsing is to make the script more maintainable. I'll take a look at other libraries and hope that they are not as cryptic as the original perl one.

Thanks

---
## 2013-02-28 07:05:23 - elphono - Keyword that matches ONLY keyword
Hello,
Im in the midst of an application using pyparsing and i got a simple problem that i am sure is pretty easy to solve for someone who is used with pyparsing.

Here i go, let's assume i want to parse:

Must match is mykeyword
must double match is ~ mykeyword and &mykeyword
must not match is BLA3mykeyword and huaeh34apioejhMyKeyWord
must still not match is apga'35mykeyword3513hh

my goal is to catch all mykeyword occurences when relevant (example: i want to match mykeyword but not allmykeyword, i want to match =mykeyword or 'mykeyword.

I got HALF the code i want with:

    authorizedPrefixes=(oneOf('& ~ # ' { ( [ - | ` _ \ ^ @ ) ] = + } $ % * ! : / ; . , ? < >') | Literal('''))
    expr=Combine(authorizedPrefixes+CaselessKeyword('mykeyword'))


but when i try to include the fact that i want White()+Keyword('mykeyword') is a valid entry is when i can't get things to work.

#### 2013-02-28 07:26:18 - elphono
By re-reading what i just post i realize i am not very clear with what my real problem is.

In fact i am a bit confused by how the Keyword class does half the work. I mean 


    Keyword('keyword')

will match keyword and not keywordAnythingBehind

that is OK with me, but why that expression matches anythingAheadKeyword ?
i would expect that Keyword('keyword') only matches 'keyword' surrounded by whitespaces. And then i could precise my need by adding that i want also match somethings like =Keyword or :Keyword or 'keyword' and stuff.

My need comes from the fact that i need to parse for example 'ng0999' and not for example 'indexing0999' and with only using Keyword('ng0999') this obviously doesn't work.

I Hope that i am clearer now. 

Thanks you by advance.
#### 2013-02-28 07:39:08 - ptmcg
I cannot reproduce this:


    >>> kw = Keyword('keyword')
    >>> list(kw.scanString('keyword keywordbehind aheadkeyword keyword'))
    [((['keyword'], {}), 0, 7), ((['keyword'], {}), 35, 42)]


This shows matches only at position 0 and position 35.

Can you post a more detailed example showing the problem?

-- Paul
#### 2013-02-28 08:38:17 - elphono
Thanks for your answer, here is a bit of code that emphasize my problem:



    from pyparsing import Keyword, oneOf, ZeroOrMore, Word, alphas, Literal, OneOrMore, alphanums, Combine,Optional, CaselessLiteral, White, CaselessKeyword,SkipTo, replaceWith,Regex
    
    stringToParse='''
    Must match is mykeyword
    must double match is ~ mykeyword and &mykeyword
    must not match is BLA3mykeyword and huaeh34apioejhMyKeyWord
    must still not match is apga'35mykeyword3513hh
    '''
    
    mapOfKeywords={
        'mykeyword':'drowyekym',
        'secondly':'yldnoces'
        }
    
    caselessKeywords=None
    for key,value in mapOfKeywords.items():
        expr=CaselessKeyword(key)
        if(caselessKeywords is None):
            caselessKeywords=expr
        else:
            caselessKeywords|=expr
    
    macroDef=OneOrMore(caselessKeywords)
    number=0
    for line in stringToParse.splitlines():
        number+=1
        result=macroDef.scanString(line)
        print('Line nb: '+str(number)+' is ---->'+line)
        for token,s,e in result:
            print('Token=',token)


#### 2013-02-28 08:49:40 - elphono
I removed unnecessary parts, sorry for double posting, can't edit:



    from pyparsing import Keyword, oneOf, ZeroOrMore, Word, alphas, Literal, OneOrMore, alphanums, Combine,Optional, CaselessLiteral, White, CaselessKeyword,SkipTo, replaceWith,Regex
    
    stringToParse='''
    Must match is mykeyword
    must double match is ~ mykeyword and &mykeyword
    must not match is BLA3mykeyword and huaeh34apioejhMyKeyWord
    must still not match is apga'35mykeyword3513hh
    '''
    
    key=CaselessKeyword('mykeyword')
    
    
    macroDef=key
    number=0
    for line in stringToParse.splitlines():
        number+=1
        result=macroDef.scanString(line)
        print('Line nb: '+str(number)+' is ---->'+line)
        for token,s,e in result:
            print('Token=',token)
    


#### 2013-02-28 09:08:46 - elphono
Ok... just nailed the problem !

CaselessKeyword makes the problem happen.
Keyword doesn't.

Is CaselessKeyword bugged or working as intended ?
#### 2013-02-28 09:13:59 - elphono
Proof with:



    from pyparsing import Keyword,CaselessKeyword
    
    stringToParse='''
    Must match is mykeyword
    must double match is ~ mykeyword and &mykeyword
    must not match is BLA3mykeyword and huaeh34apioejhMyKeyWord
    must still not match is apga'35mykeyword3513hh
    '''
    
    key=Keyword('mykeyword')
    caselessKey=CaselessKeyword('mykeyword')
    
    number=0
    for line in stringToParse.splitlines():
        result=key.scanString(line)
        result2=caselessKey.scanString(line)
        print('Line is ---->'+line)
        for token,s,e in result:
            print('Keyword Token=',token)
        for token,s,e in result2:
            print('CaselessKeyword Token=',token)


and output is:



    Line is ---->Must match is mykeyword
    Keyword Token= ['mykeyword']
    CaselessKeyword Token= ['mykeyword']
    Line is ---->must double match is ~ mykeyword and &mykeyword
    Keyword Token= ['mykeyword']
    Keyword Token= ['mykeyword']
    CaselessKeyword Token= ['mykeyword']
    CaselessKeyword Token= ['mykeyword']
    Line is ---->must not match is BLA3mykeyword and huaeh34apioejhMyKeyWord
    CaselessKeyword Token= ['mykeyword']
    CaselessKeyword Token= ['mykeyword']
    Line is ---->must still not match is apga'35mykeyword3513hh


#### 2013-03-01 01:36:18 - elphono
Hello again, 

could you provide a workaround maybe aside from typing all the ways to write a caseless keyword instead of using CaselessKeyword ?
#### 2013-03-04 01:36:13 - ptmcg
Thanks for the updated posts - I'll try to get to this in the next day or two.

-- Paul
#### 2013-03-04 01:42:28 - ptmcg
A workaround to use is to just use Keyword('keyword', caseless=True). I'm not sure why CaselessKeyword overrides parseImpl when Keyword already implements caselessness, but at least this will get you working. I'll add your tests to my unit tests, thanks!
#### 2013-03-04 01:47:30 - elphono
Thanks for the answer, it's hopefully a quite easy workaround =)

It's a pleasure to see you this active to help us.
Have a great day!

---
## 2013-03-04 08:28:07 - veli9999 - need to identify functions and ignore every other thing in a file
Hi,

I'm new to pyparsing and I'm working with to extract function names and function from large number of files.

I have to ignore every other thing in a file other than function name and its body. Is there a way to do this in pyparsing. 

I have used ignore to ignore headers, comments etc. But this approach is not feasible since several files contains several other things which maynot be defined in grammar.

If I normally write grammar for identifying function and run it. Then I get an error saying function expected at line 1. What I really care is only function name and its body. Is there a way in pyparsing to get only this.

Any help would be appreciated.

Thanks!!

#### 2013-03-04 08:52:48 - ptmcg
Just define your parser to match a function name and body, and then instead of calling parseString, call searchString or scanString.  You can get more info on those methods in the help ().

-- Paul
#### 2013-03-04 08:53:31 - ptmcg
There are also some examples on the Examples page on how to use searchString and scanString.

-- Paul
#### 2013-03-04 09:29:44 - veli9999
But, is there a way to get the line no with scan string. With parseFile I'm able to get the line & line no. But I'm not able to get this with scan String.

Thanks,
manu
#### 2013-03-04 10:13:46 - ptmcg
For every match returned by scanString, you get a tuple of (tokens, startloc, endloc).  You can get the line number by using pyparsing's lineno method: 



    lineNumber = lineno(startloc, sourceString)


where sourceString is the original string being parsed.
#### 2013-03-04 10:42:37 - veli9999
Got it sorted .

Thanks!!
#### 2013-03-04 13:16:06 - veli9999
I have written grammar to detect a function. But what I need is function which consist of a specific word. 


    fun( )
    {
    word
    }


I'm only concerned about function containing word. The grammar I wrote can detect word and function separately. But when I combine the grammar I'm not getting any output. I'm searching the word as Literal.
Is there something such as contains so that I can search the body of a function.

Thanks!!
#### 2013-03-05 08:08:40 - veli9999


    returntype = Literal('void')
    function_name= Word(alphas)
    args= Word(alphas)
    function_open = Literal('{')
    function_close = Literal('}')
    function_body = Literal('word')
    function_decl=Optional('Static')+Optional(returntype)+function_name+'('+Optional(args)+')'
    
    grammar = OneOrMore(function_decl+function_open+function_body+function_close)


Now after function_open I need to search for only word in body. I only need functions having word.

Thanks!!

---
## 2013-03-05 00:53:01 - kmbt - setDefaultWhitespaceChars not woring for anyOpenTag\/anyCloseTag?
Here is a test case:



    from pyparsing import *
    ParserElement.setDefaultWhitespaceChars(' \t\r\n_')
    
    s = 'this is my<b>_</b>1234_string of characters'
    ex = anyOpenTag + anyCloseTag
    res = ex.scanString(s)
    print list(res)

gives me an empty list

I am using pyparsing 1.5.6 on python 2.6.6. 

Setting default whitespace characters doesn't seem to work for anyOpenTag and anyCloseTag. Does anyone know if it is fixed in newer versions of pyparsing?

What I really need to do is parse HTML documents which contain non-breaking space characters encoded as unicode character U+00A0. I know I can just run a .replace on a string and replace it with a standard space, but it might be also good if it worked another way round.

#### 2013-03-05 07:09:49 - ptmcg
Yes, this is a flaw/bug in how setDefaultWhitespaceChars works. When you import pyparsing, many of these builtin expressions like anyOpenTag, anyCloseTag, empty, lineEnd, and so on, all get defined using the standard default whitespace characters of ' \t\n'. When you change the whitespace chars, this does not go back and redefine the existing expressions. As a workaround, copy the definitions of anyOpenTag and anyCloseTag and recreate them after calling setDefaultWhitespaceChars.

-- Paul
#### 2013-03-05 07:40:17 - kmbt
Thanks again for your help. I find the pyparsing library increasingly helpful in text processing tasks.

---
## 2013-03-11 05:02:15 - gordonschulz - Usage of Each()\/heterogeneous file
Hello,

first of all Paul thanks for the awesome pyparsing - so far it has made my life a whole lot easier and
made me write more maintable code.

Right now I am tackling the parsing of a rather heterogeneous file that has a lot of different lines with
different grammars in it. What I have been doing is break up the file into the distinct grammars and then
assign an Each() call to match all of them in any order. It has been going ... okay, but I am not really
sure whether I took the right approach.

To clarify the problem, some example from the file:

    KEYWORD1       VALUE1
    KEYWORD2       VALUE2
    KEYWORD3       VALUE3
    KEYWORDX       VALUE4
    !
    DEVICE1.1      INTERFACE1.1           ----- bandwidth,role,IP                 ----- DEVICE1.2 INTERFACE1.2    # OPTIONAL COMMENT
    DEVICE2.1      INTERFACE2.1           ----- bandwidth,role,IP                 ----- DEVICE2.2 INTERFACE2.2    # OPTIONAL COMMENT
    DEVICEX.Y      INTERFACEX.Y           ----- bandwidth,role,IP                 ----- DEVICEX.Z INTERFACEX.Z    # OPTIONAL COMMENT
    !
    !Dialers
    DEVICE1     INTERFACE1   -----          INTERFACE   >---- NUMBER
    DEVICE2     INTERFACE2   -----          INTERFACE   >---- NUMBER
    DEVICE3     INTERFACE3   -----          INTERFACE   <---- NUMBER
    !
    !layer-3
    DEVICE1     INTERFACE1  -----       t=role           ----< IP
    DEVICE2     INTERFACE2  -----       t=role           ----< IP
    DEVICEX     INTERFACEX  -----       t=role           ----< IP
    !
    !LAN
    DEVICE1.1     INTERFACE1.1  -----       cabletype,speed:duplex,trunk:vlan ----- INTERFACE1.2    DEVICE1.2   # OPTIONAL COMMENT
    DEVICE2.1     INTERFACE2.1  -----       cabletype,speed:duplex,trunk:vlan ----- INTERFACE2.2    DEVICE2.2   # OPTIONAL COMMENT
    DEVICEX.Y     INTERFACEX.Y  -----       cabletype,speed:duplex,trunk:vlan ----- INTERFACEX.Z    DEVICEX.Z   # OPTIONAL COMMENT

    
What this basically does is describe a WAN-/LAN-Infrastructure. Parsing works until I approach the layer-3/LAN-Section
of the file. I can grab the layer-3 stuff, but for w/e reason I can never match the LAN stuff using parseString(). 
I assume it's because of the similar lines?

Example of my code for those blocks (quite roughly grabbed from the source code, there might be some errors/typos in
there):
    

    _layer3_lan_left = Or([device_grammar1,device_grammar2,device_grammar3]
                         ).setResultsName('device*') # these are the different namings for devices
    _layer3_laninterface = Or([laninterface,
                               vlaninterface]
                              ).setResultsName('interface*') # interface names, like Gig, Fa, E, ... or VLAN
    _layer3_role = self.lantype.setResultsName('type*') # a type like t=u, t=x
    _layer3_ipaddr = self.ipaddr.setResultsName('ip*') # a basic CIDR ip address
    self.layer3 = (_layer3_lan_left + _layer3_laninterface +
                   Suppress(self.wanused_marker) +
                   _layer3_role + Suppress(self.lanused_marker) +
                   _layer3_ipaddr
                   ).setResultsName('layer3').setName('layer3')
    # we need those often, definitely more than once</li></ol>self.layer3_grammar = OneOrMore(self.layer3)
    
    _lan_laninterface = self.laninterface.setResultsName('interface*') # interface names, like Gig, Fa, E, ...
    _lan_medium = self.lanmedium.setResultsName('medium*') # this is the medium like copper, fibre, ..
    _lan_duplexspeed = Group(self.lanspeed + Optional(Suppress(':') +
                                  self.duplex, default=None)
    ).setResultsName('speedduplex*') # speed and duplex
    _trunkvlan = Or([self.trunk, self.vlanid]).setResultsName(
        'trunkvlan*') # trunk or vlan id
    self.lanstructure = (_layer3_lan_left + _lan_laninterface +
                         Suppress(self.wanused_marker) +
                         _lan_medium +
                         Suppress(',') + _lan_duplexspeed +
                         Suppress(',') + _trunkvlan +
                         Suppress(self.wanused_marker) +
                         _lan_laninterface + _layer3_lan_left +
                         self.description
                         ).setResultsName('lanstructure')\
                          .setName('lanstructure')
    
    self.lanstructure_grammar = OneOrMore(self.lanstructure)


I create those grammars for every logical block in the file, set up a grammar using the Each() subclass and then
call parseString() on it.


    self._grammar = Each([grammar1,grammar2,grammar3,
                          self.layer3_grammar,
                          self.lanstructure_grammar
                         ])
    
    parsed = self._grammar.parseString(tmpblock)
    

Like I mentioned, it works well until I get to the lan block. It never seems to find it (or reads over it).
Furthermore, I cannot seem to read anything from the debug() output. 

What puzzles me is that by using searchString() on the layer3_grammer and/or lanstructure_grammar pyparsing is able 
to find the correct sections just fine. This makes me believe there might be something wrong with my usage of 
Each() or maybe that my whole approach is fundamentally flawed.

I hope you guys can read anything into my semi-pseudocode or might be able to push me just into the right direction.

Thanks in advance,

/Gordon.

#### 2013-03-11 05:03:59 - gordonschulz
Oops, I messed up the formatting big time - and I cannot seem to edit it. :/ I apologize.
#### 2013-03-11 09:12:19 - gordonschulz
Another try to get it right...

Hello,

first of all Paul thanks for the awesome pyparsing - so far it has made my life a whole lot easier and
made me write more maintable code.

Right now I am tackling the parsing of a rather heterogeneous file that has a lot of different lines with
different grammars in it. What I have been doing is break up the file into the distinct grammars and then
assign an Each() call to match all of them in any order. It has been going ... okay, but I am not really
sure whether I took the right approach.

To clarify the problem, some example from the file:



    KEYWORD1       VALUE1
    KEYWORD2       VALUE2
    KEYWORD3       VALUE3
    KEYWORDX       VALUE4
    !
    DEVICE1.1      INTERFACE1.1           ----- bandwidth,role,IP                 ----- DEVICE1.2 INTERFACE1.2    # OPTIONAL COMMENT
    DEVICE2.1      INTERFACE2.1           ----- bandwidth,role,IP                 ----- DEVICE2.2 INTERFACE2.2    # OPTIONAL COMMENT
    DEVICEX.Y      INTERFACEX.Y           ----- bandwidth,role,IP                 ----- DEVICEX.Z INTERFACEX.Z    # OPTIONAL COMMENT
    !
    !Dialers
    DEVICE1     INTERFACE1   -----          INTERFACE   >---- NUMBER
    DEVICE2     INTERFACE2   -----          INTERFACE   >---- NUMBER
    DEVICE3     INTERFACE3   -----          INTERFACE   <---- NUMBER
    !
    !layer-3
    DEVICE1     INTERFACE1  -----       t=role           ----< IP
    DEVICE2     INTERFACE2  -----       t=role           ----< IP
    DEVICEX     INTERFACEX  -----       t=role           ----< IP
    !
    !LAN
    DEVICE1.1     INTERFACE1.1  -----       cabletype,speed:duplex,trunk:vlan ----- INTERFACE1.2    DEVICE1.2   # OPTIONAL COMMENT
    DEVICE2.1     INTERFACE2.1  -----       cabletype,speed:duplex,trunk:vlan ----- INTERFACE2.2    DEVICE2.2   # OPTIONAL COMMENT
    DEVICEX.Y     INTERFACEX.Y  -----       cabletype,speed:duplex,trunk:vlan ----- INTERFACEX.Z    DEVICEX.Z   # OPTIONAL COMMENT


What this basically does is describe a WAN-/LAN-Infrastructure. Parsing works until I approach the layer-3/LAN-Section
of the file. I can grab the layer-3 stuff, but for w/e reason I can never match the LAN stuff using parseString(). 
I assume it's because of the similar lines?

Example of my code for those blocks (quite roughly grabbed from the source code, there might be some errors/typos in
there):



    # these are the different namings for devices
    _layer3_lan_left = Or([device_grammar1,device_grammar2,device_grammar3]
                         ).setResultsName('device*') 
    
    # interface names, like Gig, Fa, E, ... or VLAN
    _layer3_laninterface = Or([laninterface,
                               vlaninterface]
                              ).setResultsName('interface*') 
    _layer3_role = self.lantype.setResultsName('type*') # a type like t=u, t=x
    _layer3_ipaddr = self.ipaddr.setResultsName('ip*') # a basic CIDR ip address
    self.layer3 = (_layer3_lan_left + _layer3_laninterface +
                   Suppress(self.wanused_marker) +
                   _layer3_role + Suppress(self.lanused_marker) +
                   _layer3_ipaddr
                   ).setResultsName('layer3').setName('layer3')
    # we need those often, definitely more than once
    self.layer3_grammar = OneOrMore(self.layer3)
    
    # interface names, like Gig, Fa, E, ...
    _lan_laninterface = self.laninterface.setResultsName('interface*') 
    
    # this is the medium like copper, fibre, ..
    _lan_medium = self.lanmedium.setResultsName('medium*') 
    
    # speed and duplex
    _lan_duplexspeed = Group(self.lanspeed + Optional(Suppress(':') +
                                  self.duplex, default=None)
                            ).setResultsName('speedduplex*')     
    # trunk or vlan id
    _trunkvlan = Or([self.trunk, self.vlanid]).setResultsName('trunkvlan*') 
    self.lanstructure = (_layer3_lan_left + _lan_laninterface +
                         Suppress(self.wanused_marker) +
                         _lan_medium +
                         Suppress(',') + _lan_duplexspeed +
                         Suppress(',') + _trunkvlan +
                         Suppress(self.wanused_marker) +
                         _lan_laninterface + _layer3_lan_left +
                         self.description
                         ).setResultsName('lanstructure')\
                          .setName('lanstructure')
    
    self.lanstructure_grammar = OneOrMore(self.lanstructure)


I create those grammars for every logical block in the file, set up a grammar using the Each() subclass and then
call parseString() on it.



    self._grammar = Each([grammar1,grammar2,grammar3,
                          self.layer3_grammar,
                          self.lanstructure_grammar
                         ])
    
    parsed = self._grammar.parseString(tmpblock)


Like I mentioned, it works well until I get to the lan block. It never seems to find it (or reads over it).
Furthermore, I cannot seem to read anything from the debug() output. 

What puzzles me is that by using searchString() on the layer3_grammer and/or lanstructure_grammar pyparsing is able 
to find the correct sections just fine. This makes me believe there might be something wrong with my usage of 
Each() or maybe that my whole approach is fundamentally flawed.

I hope you guys can read anything into my semi-pseudocode or might be able to push me just into the right direction.

Thanks in advance,

/Gordon.

---
## 2013-04-04 07:08:58 - peyguin - Odd parser behaviour
I've successfully created 2 different parsers using pyparsing. The first parser is made to parse a semicolon terminated configuration file. Let's call this the <strong>semi_parser</strong>. The second parser is made to parse a bash style file which is '\n' terminated. I'm calling this parser the <strong>bash_parser</strong>.
The main difference of the 2 is that I'm using <strong>setWhitespaceChars(' \t\r')</strong> for every line in the bash_parser. Let's ignore SetDefaultWhitespace for the moment. 
When I test each one independently they both work as expected. When I'm trying to use them in the same program, the <strong>bash_parser</strong> is behaving oddly, but only if its called after the <strong>semi_parser</strong>.

So if I do:


    p1 = bash_parser('bash_file')
    p2 = semi_parser('semi_separated_file')

everything is ok.

If I reverse the order:


    p2 = semi_parser('semi_separated_file')
    p1 = bash_parser('bash_file')

the **bash_parser** has issues with comment lines.

To grab the comment lines I've created the following element.


    octo = Literal('#')
    eol = LineEnd()
    
    comment_lines = Combine(
                octo + restOfLine + eol
    ).setWhitespaceChars(' \t\r')


When the <strong>bash_parser</strong> is called after the <strong>semi_parser</strong> and if the file has the following structure:


    # 
    # A comment
    # 
    VAR=VALUE
    # Comment 2
    # Comment 3

only the first comment line is grabbed as ['#\n'] and then, it skips to the VAR line. It seems that lines 2 and 3 are ignored. On the other hand comment lines containing Comment 2 and 3 are parsed correctly.
This does NOT happen if the <strong>bash_parser</strong> is called first. 

Now it seems to me that, when the <strong>semi_parser</strong> is called, something  connected to whitespace is changed in the Class level and then the <strong>bash_parser</strong> fails. I'm not using any Classmethods in both parsers.

Any help you could provide, would be greatly appreciated.


---
## 2013-04-10 01:02:02 - multani0 - How to improve error messages?
Hello,

I'm trying to improve the error messages produced by pyparsing when it fails to parse an expression, so I can present a good error message to my users.
I asked for , and Eike there already helped me to improve my initial parser by pointing out a few mistakes I was doing while using pyparsing, especially the use of `s` and `loc` in the parse actions to pinpoint the place where the parse action failed, and the use of fail actions in some cases.

Here is my current implementation of the parser, which is a very stripped down version of the parser I'm using for real:


```
from pyparsing import *

def validate_number(s, loc, tokens):
    if int(tokens[0]) != 0:
        raise ParseFatalException(s, loc, "number musth be 0")

def fail(s, loc, tokens):
    raise ParseFatalException(s, loc, "Unknown token %s" % tokens[0])

def fail_value(s, loc, expr, err):
    raise ParseFatalException(s, loc, "Wrong value")

number =  Word(nums).setParseAction(validate_number).setFailAction(fail_value)
operator = Literal("=")

error = Word(alphas).setParseAction(fail)
rules = MatchFirst([
    Literal('x') + operator + number,
])

rules = operatorPrecedence(rules | error , [
    (Literal("and"), 2, opAssoc.RIGHT),
])

def try_parse(expression):
    try:
        rules.parseString(expression, parseAll=True)
    except Exception as e:
        msg = str(e)
        print("%s: %s" % (msg, expression))
        print(" " * (len("%s: " % msg) + (e.loc)) + "^^^")

```


It handles well a good quantity of wrong expressions and show the right place where the actual (first) error is, which is cool. But there are still a few cases where it is not very accurate:


```
    try_parse("x = a and x = 0") 
    # This one is actually good!
    Wrong value (at char 4), (line:1, col:5): x = a and x = 0
                                              ^^^
    try_parse("x = 0 and x = a")
    Expected end of text (at char 6), (line:1, col:1): x = 0 and x = a
                                                             ^^^
    try_parse("x = 0 and (x = 0 and (x = 0 and (x = a)))")
    Expected end of text (at char 6), (line:1, col:1): x = 0 and (x = 0 and (x = 0 and (x = a)))
                                                             ^^^
    try_parse("x = 0 and (x = 0 and (x = 0 and (x = 0)))")
    Expected end of text (at char 6), (line:1, col:1): x = 0 and (x = 0 and (x = 0 and (xxxxxxxx = 0)))
                                                         ^^^

```
I've been recommended (still be the same Eike) to use the `-` operator instead of `+` to build my rules, as follow:

```
rules = MatchFirst([
    Literal('x') - operator - number,
])

```

... in order for the parser not to backtrack, but it doesn't really change much.

Actually, it seems that if pyparsing can't <strong>parse</strong> a token after a `and`, it produces the following kind of error messages. (And by <strong>parse</strong>, I mean, it can't parse the `xxxxx` keyword (which is not defined), or the `a` value (which is supposed to be a number).)
I was expecting the <em>fail action</em> to do something about this, but for some reasons, it doesn't: the `ParseFatalException` exception raised inside this fail action is swollen somewhere.

Any tips on how I could improve those error messages?

#### 2013-04-10 02:15:10 - multani0
[hopefully, I'll get the markup right this time]

Hello,

I'm trying to improve the error messages produced by pyparsing when it fails to parse an expression, so I can present a good error message to my users.
I asked for help on Stackoverflow, and Eike there already helped me to improve my initial parser by pointing out a few mistakes I was doing while using pyparsing, especially the use of `s` and `loc` in the parse actions to pinpoint the place where the parse action failed, and the use of fail actions in some cases.

Here is my current implementation of the parser, which is a very stripped down version of the parser I'm using for real:



    from pyparsing import *
    
    def validate_number(s, loc, tokens):
        if int(tokens[0]) != 0:
            raise ParseFatalException(s, loc, 'number musth be 0')
    
    def fail(s, loc, tokens):
        raise ParseFatalException(s, loc, 'Unknown token %s' % tokens[0])
    
    def fail_value(s, loc, expr, err):
        raise ParseFatalException(s, loc, 'Wrong value')
    
    number =  Word(nums).setParseAction(validate_number).setFailAction(fail_value)
    operator = Literal('=')
    
    error = Word(alphas).setParseAction(fail)
    rules = MatchFirst([
        Literal('x') + operator + number,
    ])
    
    rules = operatorPrecedence(rules | error , [
        (Literal('and'), 2, opAssoc.RIGHT),
    ])
    
    def try_parse(expression):
        try:
            rules.parseString(expression, parseAll=True)
        except Exception as e:
            msg = str(e)
            print('%s: %s' % (msg, expression))
            print(' ' * (len('%s: ' % msg) + (e.loc)) + '^^^')


It handles well a good quantity of wrong expressions and show the right place where the actual (first) error is, which is cool. But there are still a few cases where it is not very accurate:



    >>> try_parse('x = a and x = 0') # This one is actually good!
    Wrong value (at char 4), (line:1, col:5): x = a and x = 0
                                                  ^^^
    >>> try_parse('x = 0 and x = a')
    Expected end of text (at char 6), (line:1, col:1): x = 0 and x = a
                                                             ^^^
    >>> try_parse('x = 0 and (x = 0 and (x = 0 and (x = a)))')
    Expected end of text (at char 6), (line:1, col:1): x = 0 and (x = 0 and (x = 0 and (x = a)))
                                                             ^^^
    >>> try_parse('x = 0 and (x = 0 and (x = 0 and (x = 0)))')
    Expected end of text (at char 6), (line:1, col:1): x = 0 and (x = 0 and (x = 0 and (xxxxxxxx = 0)))
                                                             ^^^


I've been recommended (still be the same Eike) to use the `-` operator instead of `+` to build my rules, as follow:



    rules = MatchFirst([
        Literal('x') - operator - number,
    ])


... in order for the parser not to backtrack, but it doesn't really change much.

Actually, it seems that if pyparsing can't <strong>parse</strong> a token after a and, it produces the following kind of error messages. (And by <strong>parse</strong>, I mean, it can't parse the `xxxxx` keyword (which is not defined), or the `a` value (which is supposed to be a number).)
I was expecting the fail action to do something about this, but for some reasons, it doesn't: the `ParseFatalException` exception raised inside this fail action is swollen somewhere.

Any tips on how I could improve those error messages?

---
## 2013-04-10 07:42:13 - jonotoronto - MIDL Parser
Here's a Microsoft IDL to XML parser written with pyparsing - hopefully this is useful to others: 


---
## 2013-04-15 07:27:23 - clintpriest - Include file and parse?
I'd like to have some syntax that lets another file be included anywhere in the grammar, be it standalone or within a statement.

As an example, something like:


    [/etc/tmp.rf]
    
    Allow from 127.0.0.1, [/etc/allowed_ips]


# is there a way to have a 'anywhere' token</li><li>how can I have its results parsed within the context where its found?</li></ol>


---
## 2013-04-22 06:53:35 - sniper228 - Calculating Variable Dependency
Hi,
I want to calculate variable dependency using pyparsing as for c language 

suppose the equation 


    c = a + b

then output should be as
    
c ->a , b, value = 10
a -> value = 5
b- > value = 5



---
## 2013-04-26 15:38:02 - DZaaaaaa - Problem with PGN example
I was trying to adapt the PGN parser example to handle a file with multiple games. So I took the line 

    pgnGrammar = Suppress(ZeroOrMore(tag))  + ZeroOrMore(move) + Suppress(game_terminator)

and changed it to 

    game = Suppress(ZeroOrMore(tag))  + ZeroOrMore(move) + Suppress(game_terminator)
    pgnGrammar = OneOrMore(game)


And instead of parsing all the games in the file it just parses the first two games.

If I try adding 'parseAll=True' to parseString() I get this error at the start of the 3rd game:

    [Event 'Baden-Baden']
    ^
    Expected end of text (at char 1280), (line:39, col:1)
    tokens =  None
    

How can I fix this error and parse an entire file full of games?


---
## 2013-05-09 07:30:35 - abhijitbhatt2003 - Parsing configuration file
I am trying to use pyparsing to parse a configuration file of following form

x = '/user/test'
y = 3

Here is my code snippet



    ParserElement.defaultWhitespaceChars = (' \t')
    END = StringEnd()
    NL = LineEnd().suppress()
    assignment = Literal('=')
    
    key_str = CharsNotIn('=')
    value_str = Group(~assignment + restOfLine)
    
    line = Group(key_str + assignment + value_str)
    lines = ZeroOrMore(line)
    lines.ignore(NL)
    
    text = '''
    y = 3
    x = 2
    '''


The output that I get from parseFile tells me it is parsing the first line only. Can anybody please help me find out what I am doing wrong?

output:

$ ./parse_config.py 
[['y ', '=', [' 3 ']]]

Thanks
Abhijit


---
## 2013-05-14 07:30:20 - f.wilamo - Leading OneOrMore 
If I use a `OneOrMore(Word) + 'end'` the 'end' can not be detected, because 'end' is matched also by Word. If I change the order and put the 'end' in the beginning, of course parseString is working as expected. How can I parse a group of words (leading OneOrmOre(word)), but with a defined end ?

#### 2013-05-14 19:21:03 - ptmcg
Well, think a little harder then about what you want 'one or more' of. You don't really 
want *any* word, you want any word that is not 'end'. So you need to add lookahead for your 
repeated word to make sure that the word you are about to parse inside the OneOrMore is 
not 'end'.  Try `OneOrMore(~Keyword('end') + Word(alphas)) + 'end'`  
(I used Keyword instaed of Literal so that your lookahead wouldn't stop prematurely on 
words like 'endeavor' or 'endear'.)

#### 2013-05-15 07:37:58 - f.wilamo
Thanks a lot for you immediate help. It is working as you proposed.

---
## 2013-05-17 00:28:07 - pschwaha - use OrderedDict
hi!

is there any experience in using a OrderedDict instead of a dict inside of pyparsing?
at the moment I have simply replaced the references to dict with OrderedDict inside pyparsing.py. are there any ill effects to be expected? might there be interest in this in general?

Philipp


---
## 2013-05-17 06:26:56 - AlphaPrime - Only Allow One NOT
How can I structure the grammar to allow only one NOT statement?  I want to allow this:



    simpleExpr NOT simpleExpr


but not



    simpleExpr NOT simpleExpr NOT simpleExpr


I have it so simpleExpr does not contain NOT, but I can't see how to keep NOT from repeating.  Here's the grammar so far:



    searchExpr = operatorPrecedence(term, [
                (imp_op, 2, opAssoc.LEFT, self.SearchImp),
                (and_op, 2, opAssoc.LEFT, self.SearchAnd),
                (or_op, 2, opAssoc.LEFT, self.SearchOr),
            ])
    
            self.complexExpr = operatorPrecedence(searchExpr, [
                (not_op, 2, opAssoc.LEFT, self.SearchNot),
            ]) + stringEnd


Any thoughts on how to accomplish this?

...Thanks,
...Ken


---
## 2013-05-17 10:47:02 - mirk410 - Losing the tail-end of my expression
I'm new to pyparsing so this is probably something I'm just missing the boat on' I am creating a script that will parse an expression and build an evaluation graph representing the expression.  Once generated, the graph will be evaluated against several data sets to determine if they meet the specified criteria.  I have reduced my grammar down to just Boolean operands:



    AND_OP    = CaselessKeyword('and')('AND_OP')
    AND_OP.setDebug()
    OR_OP     = CaselessKeyword('or')('OR_OP')
    NOT_OP    = CaselessKeyword('not')('NOT_OP')
    
    BOOL_VAL  = ( CaselessKeyword('true') |
                  CaselessKeyword('false'))('BVAL')
    BOOL_VAL.setParseAction(CEBooleanNode)
    
    BOOL_OPRD = ( BOOL_VAL )('EXO')
    BOOL_OPRD.setDebug()
    
    EXPR      = operatorPrecedence( BOOL_OPRD,
                        [
                        (NOT_OP, 1, opAssoc.RIGHT, CENotOpNode),
                        (AND_OP, 2, opAssoc.LEFT,  CEBoolOpNode),
                        (OR_OP,  2, opAssoc.LEFT,  CEBoolOpNode),
                         ])('EXPR')


When I parse the string 'True and True and False' my graph is missing the 'and False' (the result is true when evaluating the graph). When I print the resulting  graph I get 'True and True'.  When I run this through the debugger with a break point in the CEBoolOpNode <u>init</u> method, I am only seeing this invoked once.  I am passing 'parseAll=True' when calling parseString.  

With setDebug() on AND_OP and BOOL_OPRD I get the following output:



    Match {'true' | 'false'} at loc 0(1,1)
    Matched {'true' | 'false'} -> ['true']
    Match 'and' at loc 4(1,5)
    Matched 'and' -> ['and']
    Match {'true' | 'false'} at loc 9(1,10)
    Matched {'true' | 'false'} -> ['true']
    Match {'true' | 'false'} at loc 0(1,1)
    Matched {'true' | 'false'} -> ['true']
    Match 'and' at loc 5(1,6)
    Matched 'and' -> ['and']
    Match {'true' | 'false'} at loc 9(1,10)
    Matched {'true' | 'false'} -> ['true']
    Match 'and' at loc 14(1,15)
    Matched 'and' -> ['and']
    Match {'true' | 'false'} at loc 18(1,19)
    Matched {'true' | 'false'} -> ['false']
    Match 'and' at loc 23(1,24)
    Exception raised:Expected 'and' (at char 23), (line:1, col:24)
    Match {'true' | 'false'} at loc 0(1,1)
    Matched {'true' | 'false'} -> ['true']
    Match 'and' at loc 4(1,5)
    Matched 'and' -> ['and']
    Match {'true' | 'false'} at loc 9(1,10)
    Matched {'true' | 'false'} -> ['true']
    Match {'true' | 'false'} at loc 0(1,1)
    Matched {'true' | 'false'} -> [<__main__.CEBooleanNode object at 0xf7f0674c>]
    Match 'and' at loc 5(1,6)
    Matched 'and' -> ['and']
    Match {'true' | 'false'} at loc 9(1,10)
    Matched {'true' | 'false'} -> [<__main__.CEBooleanNode object at 0xf7f06b8c>]
    Match 'and' at loc 14(1,15)
    Matched 'and' -> ['and']
    Match {'true' | 'false'} at loc 18(1,19)
    Matched {'true' | 'false'} -> [<__main__.CEBooleanNode object at 0xf7f06c8c>]
    Match 'and' at loc 23(1,24)
    Exception raised:Expected 'and' (at char 23), (line:1, col:24)


It appears to be seeing both 'and' operators but the CEBoolOpNode <u>init</u> method appears to be only called once.  Does anything above look off?  The exception being raised may be a clue but I'm not sure what to make of it.  What I'm doing seems similar to the simpleBool example except that script works :^).

One last data point, If I use parentheses it works: True and (True and False).  What am I missing? (This is already long so I did not include code but I can if needed)

Any help is greatly appreciated.

-Andy

#### 2013-05-17 11:29:28 - ptmcg
When you parse a binary operation with operatorPrecedence such as 'a op b op c', you don't get this parsed as 2 binary groups 'a op b' and then the '<result of a op b> op c'.  You just get ['a', op, 'b', op, 'c'].  I suspect that your initializer for CEBoolOpNode just looks at the 0'th and 2nd terms of the parsed input - you have to look at all the alternating terms, as you would using the Python slice [0::2].  Look to see how this is done in the SimpleBool online example: .  (I made this same mistake in an early draft of my 'Getting Started with Pyparsing' book for O'Reilly, and only just caught and fixed it before final publication!)

If I guessed wrong on this, try taking out the parse actions of your operatorPrecedence call and dump out the results - if they still look incomplete, post this info back and we can look at it further.

-- Paul
#### 2013-05-17 12:50:46 - mirk410
Thank you for the prompt reply!  You are correct, I was making that assumption and only looking at terms 0 and 2.  I will try building with the list that is returned.  Thanks again Paul!
#### 2013-05-17 16:40:27 - mirk410
I thought I'd follow up on how I resolved this in case anyone runs into this.  Since I already had my binary boolean operator object ready to receive ['a', op, 'b'], I just created a layer that takes the  ['a', op, 'b', op, 'c', op, ...] and instantiates objects accordingly:


    def processBoolOPList(tokens):
        #| We are intersted in the token list which is the first element
        #| in the provided tokens list
        tokens = tokens[0]
    
        #| Initialize the result to the first token in the list
        result   = tokens[0]
        curOpIdx = 1
        maxOpIdx = len(tokens) - 2
        #| Process binary groups using the result of the current operator
        #| as the first operand to the next operator
        while curOpIdx <= maxOpIdx:
           newTokenList = [result, tokens[curOpIdx], tokens[curOpIdx + 1]]
           result = CEBoolOpNode(newTokenList)
           curOpIdx += 2
        return result

I'm relatively new to python so I'm sure there are some efficiencies to be had here but using this function in operatorPrecedence for AND_OP and OR_OP did the trick.  Thanks again for the guidance.

---
## 2013-05-19 13:29:32 - wilo108 - `searchString` works, but `parseString` throws an exception?

I'm testing pyparsing for use with Chinese text.  I have constructed a very simple expression, of the form:


    expr = Word(... list of Chinese characters ...)
    expr.searchString(test_string)

works as expected, but


    expr.parseString(test_string)

throws


    Traceback (most recent call last):
      File './pyparse_test.py', line 135, in <module>
        main()
      File './pyparse_test.py', line 100, in parse_test
        results = expr.parseString(test_string)
      File '/usr/lib/python2.7/dist-packages/pyparsing.py', line 1032, in parseString
        raise exc
    pyparsing.ParseException


can this be right?  does anyone know what I'm doing wrong?

thanks!

#### 2013-05-19 13:44:23 - wilo108
hmm -- okay, I see the problem now.  My grammar wasn't correct, but it seems there's a bug whereby I didn't see the


    Expected W:(<something>...) (at char 4), (line:1, col:5)

which I should have.  could it be because of the non-ascii stuff?

---
## 2013-05-21 09:11:53 - mirk410 - Escaping delimiter character
My grammar includes regular expressions of the form 'foo =~ #myRegEx#'.  I want to allow the '#' character to be included in the regex via escape char ('\#').  I have read several posts on escaping as well as Paul's detailed description on looking forward (I'm trying to be the parser :^)) but I have not been able to get this to work.

Here is what I have at the moment:


    REGEX_OP  = Literal('=~')
    REGEX_ESC = '\\' + ~FollowedBy('#')
    REGEX_ESC.setParseAction(lambda t:t[1])
    REGEX_DEL = Literal('#').suppress()
    REGEX_BOD = printables.replace('#','')
    REGEX_PAT = REGEX_DEL + \
                Combine(OneOrMore(REGEX_ESC | Word(REGEX_BOD))) + \
                REGEX_DEL
    REGEX_PAT.setParseAction(TQLRegExNode)

When I run 'myStr =~ #t\#N#' through it, I get 'bogus escape (end of line)'. How do I get it to include the escaped '#' as part of the regex and not match on the final delimiter?

#### 2013-05-22 07:55:20 - mirk410
Still no luck, just an update on a thought I had that also did not seem to work.  The problem appears to be that the escaped '#' within the regex is being interpreted as the end delimiter (REGEX_DEL). My thought was to define the delimiter as a non-escaped '#':


    REGEX_DEL = ~Literal('\\') + Literal('#').suppress()

I also tried 'NotAny('\\')...' but got the same error, ''t\': bogus escape (end of line)'.  I feel like there is a basic concept that I am missing here. Any help would be greatly appreciated.

Thanks,
-Andy
#### 2013-05-23 08:16:41 - mirk410
Okay, I got this to work.  One of my challenges that I failed to mention above is that while I want to escape the regex delimiter ('#'), the escape character ('\') must be passed through (left in) the regular expression for all other characters to allow regex special character to be escaped.  For example:


    parsing '#g\(w#' should result in the string 'g\(w'   (escape char left in)
    parsing '#t\#N#' should result in the string 't#N'   (escape char removed)

With the basic concepts I was ale to get it to parse one successfully but not the other and visa versa.  Here is what I ended up with that worked for both:


    rDelimChr = '#'
    ESC_CHAR  =  Literal('\\')
    REGEX_OP  = Literal('=~')
    REGEX_ESC = Literal('\\') + Literal(rDelimChr)
    REGEX_ESC.setParseAction(lambda t:t[1])
    REGEX_DEL = Literal(rDelimChr).suppress()
    REGEX_BOD = printables.replace(rDelimChr,'').replace('\\','')
    REGEX_PAT = REGEX_DEL + \
              Combine(OneOrMore(REGEX_ESC | ESC_CHAR | Word(REGEX_BOD))) + \
              REGEX_DEL
    
    REGEX_PAT.setParseAction(TQLRegExNode)
    
    REGEX     = (VARIABLE + REGEX_OP + REGEX_PAT)('REGEX')
    REGEX.setParseAction(TQLRegExOpNode)

 Having written this, I realized that I do not have to remove the escape character from the '#' chars within the regex - they are not needed but it does not hurt to leave them in.  I'll have a look to see if that will simplify things.

---
## 2013-05-22 07:29:33 - HumbertMason - Strange warning declaring a simple PyParsing recursive grammar in Python
Hello, I am trying to parse a CLIPS-like grammar in Python using PyParsing.

The piece of code I am having problem with is:

    import pyparsing as pp
    
    ...other pyparsing tokens definitions
    
    CONNECTED_CONSTRAINT = pp.Forward()
    
    TERM = CONSTANT | SINGLEFIELD_VARIABLE | MULTIFIELD_VARIABLE | pp.Literal(':') + FUNCTION_CALL | pp.Literal('=') + FUNCTION_CALL
    
    SINGLE_CONSTRAINT = TERM | pp.Literal('~') + TERM
    
    CONNECTED_CONSTRAINT << SINGLE_CONSTRAINT | SINGLE_CONSTRAINT + pp.Literal('&') + CONNECTED_CONSTRAINT
    
    CONSTRAINT = pp.Literal('?') | pp.Literal('$?') | CONNECTED_CONSTRAINT
    
    ORDERED_PATTERN_CE = OB + SYMBOL + pp.ZeroOrMore(CONSTRAINT) + CB
    
    PATTERN_CE = ORDERED_PATTERN_CE
    
    CONDITIONAL_ELEMENT = PATTERN_CE

I have omitted the definition of some parts of the grammar because they are too long.

The problem is that the interpreter gives me this strange error:

    SyntaxWarning: Cannot combine element of type with ParserElement CONNECTED_CONSTRAINT << SINGLE_CONSTRAINT | SINGLE_CONSTRAINT + pp.Literal('&') + CONNECTED_CONSTRAINT

I have noticed that if I write:

    CONNECTED_CONSTRAINT << SINGLE_CONSTRAINT

instead of:

    CONNECTED_CONSTRAINT << SINGLE_CONSTRAINT | SINGLE_CONSTRAINT + pp.Literal('&') + CONNECTED_CONSTRAINT

It works without problems.

However even if I write something like this:

    CONNECTED_CONSTRAINT << SINGLE_CONSTRAINT | pp.Literal('test')

It does not work.

So I suppose the problem is in having an OR, given by the '|' symbol, in a token declared with pp.Forward().

Could you help me please?

Thank you.

#### 2013-05-22 07:47:28 - HumbertMason
Problem solved!

I had to write something like 

    CONNECTED_CONSTRAINT << (SINGLE_CONSTRAINT | SINGLE_CONSTRAINT + pp.Literal('&') + CONNECTED_CONSTRAINT).

This is because the operator '|' has lower precedence than the operator '<<'.

I thought it was a problem of my grammar.

Thank you anyway

P.S. Sorry if I didn't use the code tag. I was looking for something like that when writing this post, and then I noticed the tag to use was explained in a page here. I don't know how to edit the post now...

---
## 2013-05-23 10:07:16 - t1m0 - setResultsName bug or misunderstanding
Hi, 
Here is my example code


    import pyparsing as pp
    a = pp.Keyword('two words').setResultsName('specific two')
    b = (pp.Word(pp.alphas) + pp.Word(pp.alphas)).setResultsName('any two').setResultsName('anywords')
    
    test = (a | b).setResultsName('should always be two words')
    
    print 'This result makes sense to me:'
    print test.parseString('two words').asDict()
    
    print 'But this result doesn't'
    print 'I would assume the key 'should always be two words' would equal 'howabot these''
    print test.parseString('howabout these').asDict()


I am confused by the results of this code.  Here is the output



    This result makes sense to me:
    {'should always be two words': 'two words', 'specific two': 'two words'}
    But this result doesn't
    I would assume the key 'should always be two words' would equal 'howabot these'
    {'should always be two words': 'howabout', 'anywords': (['howabout', 'these'], {})}


Am I misunderstanding something, or is this a bug?   I would expect the outter setResultName to apply to the entire enclosed results, not just the first word.

thanks!
Tim

#### 2013-06-08 22:27:51 - ptmcg
You have a lot of calls to setResultsName in this code, and pyparsing won't necessarily keep them from stepping on each other. For instance, if you want the results name assigned to (a | b) to contain results names for a or b, then you will need to Group the outer expression. Also, you will probably get more readable output using dump() instead of asDict(). See below:



    import pyparsing as pp
    a = pp.Keyword('two words').setResultsName('specific two')
    b = (pp.Word(pp.alphas) + pp.Word(pp.alphas)).setResultsName('any two').setResultsName('anywords')
    
    test = pp.Group(a | b).setResultsName('should always be two words')
    
    print 'This result makes sense to me:'
    print test.parseString('two words').dump()
    
    print 'But this result doesn't'
    print 'I would assume the key 'should always be two words' would equal 'howabot these''
    print test.parseString('howabout these').dump()


prints



    This result makes sense to me:
    [['two words']]
    - should always be two words: ['two words']
      - specific two: two words
    But this result doesn't
    I would assume the key 'should always be two words' would equal 'howabot these'
    [['howabout', 'these']]
    - should always be two words: ['howabout', 'these']
      - anywords: ['howabout', 'these']


HTH,
-- Paul

---
## 2013-06-04 01:53:06 - Leevi3 - simple bool operator precedence
it seems to me that the binding power of the operators in the example simplebool.py is in the wrong order. AND should bind stronger than OR, right ? 

Instead of 

    [[boolExpr = operatorPrecedence( boolOperand,    [    ('not', 1, opAssoc.RIGHT, BoolNot),    ('or',  2, opAssoc.LEFT,  BoolOr),    ('and', 2, opAssoc.LEFT,  BoolAnd),    ])]]

it should be

    [[boolExpr = operatorPrecedence( boolOperand,    [    ('not', 1, opAssoc.RIGHT, BoolNot),    ('and', 2, opAssoc.LEFT,  BoolAnd),    ('or',  2, opAssoc.LEFT,  BoolOr),    ])]]

best,
L

#### 2013-06-04 01:55:02 - Leevi3
oops, wrong format..
Instead of

    [[boolExpr = operatorPrecedence( boolOperand, [ ('not', 1, opAssoc.RIGHT, BoolNot), ('or', 2, opAssoc.LEFT, BoolOr), ('and', 2, opAssoc.LEFT, BoolAnd), ])]]

it should be

    [[boolExpr = operatorPrecedence( boolOperand, [ ('not', 1, opAssoc.RIGHT, BoolNot), ('and', 2, opAssoc.LEFT, BoolAnd), ('or', 2, opAssoc.LEFT, BoolOr), ])]]

#### 2013-06-04 01:55:33 - Leevi3
urg, no preview..
#### 2013-06-04 01:58:43 - Leevi3
Instead of

    [[boolExpr = operatorPrecedence( boolOperand,    [ ('not', 1, opAssoc.RIGHT, BoolNot),      ('or', 2, opAssoc.LEFT, BoolOr),      ('and', 2, opAssoc.LEFT, BoolAnd), ])]]

it should be

    [[boolExpr = operatorPrecedence( boolOperand,     [ ('not', 1, opAssoc.RIGHT, BoolNot),       ('and', 2, opAssoc.LEFT, BoolAnd),       ('or', 2, opAssoc.LEFT, BoolOr), ])]]

#### 2013-06-04 01:58:50 - Leevi3
i give up..

#### 2013-07-17 00:14:09 - ptmcg
Thanks for sticking with this - you need to surround your code samples with the tag [[code]] on a line of its own, before and after your code sample - like this:



    boolExpr = operatorPrecedence( boolOperand, [ ('not', 1, opAssoc.RIGHT, BoolNot), ('and', 2, opAssoc.LEFT, BoolAnd), ('or', 2, opAssoc.LEFT, BoolOr), ])


Thanks for the note - this was already fixed in the source distribution, I just updated the online example.

-- Paul

---
## 2013-06-13 02:02:38 - termopro - Trying to find an address in text
Hi,

I have text which may contain physical address mixed with abracadabra. I would like to detect physical address only.

I did some tests with a grammar defined here: 

The problem is that it can parse exact matches while the same grammar with 'scanString' method returns unpredicted results (the tokens it returns doesn't seem to form the whole address string).

What am i doing wrong and what is the recommended way of solving my problem ?

p.s. - i know how to solve this using regular expressions, but i thought that pyparsing is just made for this kind of tasks

#### 2013-06-17 02:34:19 - termopro
Oh, the almighty God of all the programmers! I beg you turn your eyes and see this topic. Please make pyparsing programmers forget all their duties and respond to my humble question. Let their code become messy, buggy and undocumented until then. Amen.
#### 2013-06-17 17:15:19 - ptmcg
Well, I'm no Almighty, but I'll try to answer your question anyway.  First of all, street address parsing is a pretty sticky problem.  Look at all the different forms in the test cases in the parser example you cite. That parser is written from the standpoint that the string that is to be parsed has already been extracted from some database or larger body of text *somehow*. When you mix the valid street address in with other surrounding text, *you* can detect the extra junk text but it pyparsing is trying very hard to find a street address there. scanString just works through the input text a character at a time, repeatedly trying to match the expression you've given it - it the expression is aggressive in trying to match text, then it is very likely that you will slurp in some unwanted junk in addition to or in place of the real address.  I'm curious though - how would you go about doing this with regex? If you are using regex, then you probably already have a simplified address format in mind. If that's that case, then implement this reduced form street address parser in pyparsing, and then try using that with your junk text.  But one of the constant battles of pyparsing is in choosing a sufficiently complex target data set - if you oversimplify, you run the risk of later having to add back complicating features that will make the original problem near intractable.

Good luck!
-- Paul
#### 2013-06-18 02:16:51 - termopro
Paul, thank you for your reply ! I guess pyparsing is not the right tool for my task (although it is awesome for another task).
#### 2015-04-16 00:46:23 - termopro
Re: 'I'm curious though - how would you go about doing this with regex? '
Below is a library for detecting addresses using regexps:


---
## 2013-06-24 17:12:37 - ecesurfer - Parsing C Function Implementation
I essentially need to extract all the data from a C function implementation and separate it looking for keywords in the right places, i.e look for args in the functions with the function, etc. I have the following code so far but when I pass in a pretty generic C function implementation it doesn't parse anything. Please help.



    from pyparsing import *
    
    LPAREN, RPAREN, LBRACK, RBRACK, EQ, SEMI = map(Suppress, '(){}=;')
    
    #define function and arg components
    ctype = cname = Word(alphanums+'_')
    cvar = Word(alphanums+'_'+'&'+'*'+'->'+'-'+'['+']'+'.') | dblQuotedString
    
    #define function declarations
    func = ctype + Optional('*') + cname 
    argDec = Group(ctype  +Optional('*')+ cname)
    args = Optional(delimitedList(argDec) | OneOrMore(Word(alphas))).setResultsName('args') #include arg list or 'void'
    
    
    cFunctionDeclaration = func + LPAREN + args  + RPAREN
    
    cFuntionCall = func + LPAREN + (Optional(delimitedList(cvar) | cvar) ) + RPAREN
    
    cOperator = oneOf('+ - * / % == != > < >= <= ! && || ~ & | ^ << >>')
    
    cExpression = ((OneOrMore(ctype|cvar) + cOperator + OneOrMore(ctype|cvar|cFuntionCall)) | cFuntionCall) + SEMI
    
    #should take care of generic blocks like IF, While, etc.
    anything = Regex(r'(.*?)')
    cBlock = LBRACK + (anything | Literal('\n')) + RBRACK
    
    cFunctionImplementation = cFunctionDeclaration + LBRACK + OneOrMore(cFunctionDeclaration|cFuntionCall|cBlock|cExpression|anything) + RBRACK



#### 2013-06-25 22:42:31 - ptmcg
For the body of your function, try starting with something simple like nestedExpr('{','}'). What is the simple generic function that you tried? Writing a parser for even a subset of C is not a small task.

-- Paul
#### 2013-06-25 22:44:16 - ptmcg
Look over this C-subset parser () to get some ideas on how to proceed. Perhaps this might even be a sufficient subset for what you are trying to do.

-- Paul
#### 2013-06-26 10:45:42 - ecesurfer
Thank you for the advice on nestedExpr() function, this is very useful, however I actually figured out my own method which allows me to extract a string of a function. Essentially, once I find the function using scanString() I look at the end location index and run the extractor function from there until the function terminates. This allows me to use pyparsing within the function as a separate string. I also realized that 'func' should not be used in cFunctionCall. instead it should be:  cFuntionCall = cname.setResultsName('name') + LPAREN + Group((Optional(delimitedList(cvar)) | Optional(cvar) )).setResultsName('args') + RPAREN + Optional(SEMI) 

Thanks for the help Paul, pyparsing is fantastic!
#### 2013-06-26 10:48:26 - ecesurfer
Sorry for the 'code' typo:

    cFuntionCall = cname.setResultsName('name') + \
                            LPAREN + \
                            Group((Optional(delimitedList(cvar)) | Optional(cvar) )).setResultsName('args') + /       
                            RPAREN + Optional(SEMI)

#### 2013-07-21 08:47:45 - ptmcg
Good deal! A couple other tips:

It is not necessary to write delimitedList(someExpr) | someExpr - it is possible to have a delimitedList with only one element. So you only need to write:



    cFunctionCall = cname('name') + LPAREN + Group(Optional(delimitedList(cvar)))('args') + RPAREN + Optional(SEMI)


Also note the short form of expr.setResultsName('name') as expr('name').

-- Paul

---
## 2013-06-30 20:00:57 - geoff-99 - Problem with tag names created by asXML
Hi.

Just started learning to use pyparsing, and am impressed.  I have tackled problems using brute force and regular expressions in the past and pyparsing looks so much better.

I have struck a problem with using asXML().  I'm using Python 2.7 and pyparsing 1.5.7 on a Windows 7 machine.  I had a look at the pyparsing code and it appears to be arising from the line in asXML() that generates namedItems, but to go further I'd need to understand how ParseResults allocates names when initialising, and that's beyond what I can work out at the moment. It doesn't seem to be a problem with the asXML code, rather something in the tokenlist (or ParseResults instance??) that is sent to asXML.  Well I *think* so anyway. I see from other posts that asXML is not the preferred way of accessing results, but it would fit very easily with the later steps in my project.

Here is an example of code which generates my problem (this is my first post so I hope I don't mess anything up)



    import pyparsing as pprs
    
    #=======================================================================
    # Begin to run the main program
    #=======================================================================
    
    if __name__ == '__main__':
    
        #=======================================================================
        #   pyparsing grammar example with bug?
        #=======================================================================
    
        # define grammar
        LB = pprs.Literal('[').suppress()
        RB = pprs.Literal(']').suppress()
        EQ = pprs.Literal('=').suppress()
    
        tag1 = pprs.Literal('tag1').suppress()
        tag2 = pprs.Literal('tag2').suppress()
        tag3 = pprs.Literal('tag3').suppress()
    
        tag1phrase= (LB + tag1 + EQ + pprs.originalTextFor(pprs.SkipTo(']')) + RB)('tag1_inst')
        tag2phrase= (LB + tag2 + EQ + pprs.originalTextFor(pprs.SkipTo(']')) + RB)('tag2_inst')
        tag3phrase= (LB + tag3 + EQ + pprs.originalTextFor(pprs.SkipTo(']')) + RB)('tag3_inst')
    
        alltags = pprs.Group(tag1phrase*(0,) & tag2phrase*(0,) & tag3phrase*(0,))('alltags_inst')
    
        # test grammar elements
        print '='*80,'\n'
        print 'Possible example of bug in asXML?'
    
        #ts gets the right answer
        print '='*80,'\n'
        ts = '[tag1=tag1 example][tag2=tag2 example][tag3=tag3 example]'
        rs = alltags.parseString(ts)
        rsXML = rs.asXML('Correct')
        print rsXML
    
        #ts2 with repeated components gets the wrong answer, the first component is named incorrectly
        print '='*80,'\n'
        ts2 = ts+ts
        rs2 = alltags.parseString(ts2)
        rs2XML = rs2.asXML('Incorrect - the first tag1 example is named as a tag3_inst instead of as a tag1_inst')
        print rs2XML
    
        print '='*80,'\n'
        print 'I think there is problem with the naming of multiple duplicated components'
    


This is the output it generates :



    Possible example of bug in asXML?
    ================================================================================ 
    
    
    <Correct>
      <alltags_inst>
        <tag1_inst>tag1 example</tag1_inst>
        <tag2_inst>tag2 example</tag2_inst>
        <tag3_inst>tag3 example</tag3_inst>
      </alltags_inst>
    </Correct>
    ================================================================================ 
    
    
    <Incorrect - the first tag1 example is named as a tag3_inst instead of as a tag1_inst>
      <alltags_inst>
        <tag3_inst>tag1 example</tag3_inst>
        <tag2_inst>tag2 example</tag2_inst>
        <tag3_inst>tag3 example</tag3_inst>
        <tag1_inst>tag1 example</tag1_inst>
        <tag2_inst>tag2 example</tag2_inst>
        <tag3_inst>tag3 example</tag3_inst>
      </alltags_inst>
    </Incorrect - the first tag1 example is named as a tag3_inst instead of as a tag1_inst>
    ================================================================================ 
    
    I think there is problem with the naming of multiple duplicated components
    


Any help anyone can offer would be much appreciated. 

And thanks for producing such a neat package in the first place !

Geoff

#### 2013-07-04 16:49:40 - geoff-99
Hi again,

I am getting closer to understanding what's happening, and how, but not why.  I'll try and explain in case that helps someone else sometime, but I'm not 100% sure I fully understand, so apologies in advance for any errors or confusion.  I hope it makes enough sense to be helpful.

It is related to the way I have constructed the grammar, in particular the interaction between ZeroOrMore (shown in the code example using the neat condensed form '*(0,)' ) and Each (shown in the example using the neat '&' form).

First the three ZeroOrMores appearing in the definition of the alltags component are sort of doing what I intended, namely saying that the phrase they are attached to is optional, meaning that nothing goes wrong if they aren't there ( or put another way, the empty string will match them - I think).  So far so good.

But then I put them all into an Each phrase.  That says they all have to be there to constitute at successful match.  The reason I did that was that I wanted to allow them to be in any order, and that's what Each does, it looks for what has to be there but allows them to be in any order.

What is happening when I test my grammar in the second example is that alltags is finding all the matches that I expected - namely the individual tokens [tagx = tagx example] and [tagx=tagx example2].  but then it is adding in a few extra tokens that I didn't expect - but are quite legal in the grammar I have defined.  Because Each looks for alternative orders of the component phrases it has been asked to evaluate, it also (in effect - I haven't checked the code to see if this is how it really happens) rearranges the component tokens in the string it is evaluating to see if putting them in a different order makes a legitimate match.  And in the case of this example, it does.  If I (in my mind) reorder the input string to become '[tag1=tag1 example][tag1=tag1 example2] ...(rest of string)' then the 'OrMore' part of ZeroOrMore on the tag1phrase says bingo, I have a match as well, and returns the two tokens together as a match.  Likewise, with a different conceptual order of the input string, to whit '[tag2=tag2 example][tag2=tag2 example2], the 'OrMore' part of the tag2phrase finds a Match.  Ditto ZeroOrMore(tag3phrase).

After this I get a bit lost, but it seems that the way these duplicated tokens get added to the internal dictionary when setResultNames runs results in the all ending up with the same key in namedItems in asXML.  An example may help, but I may miscount brackets etc.  When namedItems is being built in asXML, it gets a list of tuples with entries like (tag1=tag1 example,0), (tag2=tag2 example,1), (tag3=tag3 example,2).  The second entry in the tuple seems to index the resultsName in namedItems, so namedItems[0] becomes tag1_inst, namedItems[1] becomes tag2_inst etc.  The problem is that the tuples for the paired token entries ALL get the same second entry (viz 0) eg ((tag1=tag1 example1,tag1=tag1 example2)),0), ((tag2=tag2 example1,tag2=tag2 example2),0) etc
So when namedItems is being constructed, whatever the zeroth element of namedItems is keeps on getting overwritten with the resultName of the latest paired token.  In my example the final paired token that Each finds is the [tag3=tag3 example],[tag3=tag3 example2], whose resultName is tag3_inst, which is why the first element in the generated XML does not have the XML tag name I expected, instead it gets tag3_inst

Not sure whether or not that is WHAT pyparsing meant to happen, but absolutely sure it's too far embedded in how the parser works for me to be able to figure out a hack to pyparsing to make it do what I was expecting (presuming of course that it is supposed to).

I think I have figured out a fix to my grammar which will give me what I want.  I'll test that next, and IF it works post that as well.

Having poked around a little bit in the innards of pyparsing, I am even more impressed.  There's a lot that happens under the hood, and it is well and truly beyond my capabilities to have created something so neat.  I can't even follow all of it - but it's elegant and it works well.  Thanks Paul!
#### 2013-07-04 17:30:21 - geoff-99
Hi

For what it's worth, the following changes to my example give me (almost) what I expect.

I removed


    alltags = pprs.Group(tag1phrase*(0,) & tag2phrase*(0,) & tag3phrase*(0,))('alltags_inst')


and replaced it with


    nexttag = (tag1phrase|tag2phrase|tag3phrase)
        alltags = pprs.Each(nexttag*(0,),savelist=True)('alltags_inst')


In effect I have moved the optionality of the ZeroOrMore closer to the Each - which grammar wise is actually a more precise statement of what I meant anyway - and so I no longer have pyparser working overtime to check all orders of something which I have said could be nothing!  Instead nexttag tells pyparser it MUST find one of tag1phrase, tag2phrase or tag3phrase (otherwise go back and look for something else), and nexttag returns whichever of the three it comes across first. THEN, in the alltags phrase pyparser is told to look for ZeroOrMore of the nexttags (which now MUST exist!)  but to accept them in any order when they are found.

and I get the XML tag names I anticipate.

The full 'working' code example is now


    import pyparsing as pprs
    
    #=======================================================================
    # Begin to run the main program
    #=======================================================================
    
    if __name__ == '__main__':
    
        #=======================================================================
        #   pyparsing grammar example with bug?
        #=======================================================================
    
        # define grammar
        LB = pprs.Literal('[').suppress()
        RB = pprs.Literal(']').suppress()
        EQ = pprs.Literal('=').suppress()
    
        tag1 = pprs.Literal('tag1').suppress()
        tag2 = pprs.Literal('tag2').suppress()
        tag3 = pprs.Literal('tag3').suppress()
    
        tag1phrase= (LB + tag1 + EQ + pprs.originalTextFor(pprs.SkipTo(']')) + RB)('tag1_inst*').setName('T1')
        tag2phrase= (LB + tag2 + EQ + pprs.originalTextFor(pprs.SkipTo(']')) + RB)('tag2_inst*').setName('T2')
        tag3phrase= (LB + tag3 + EQ + pprs.originalTextFor(pprs.SkipTo(']')) + RB)('tag3_inst*').setName('T3')
    
        nexttag = (tag1phrase|tag2phrase|tag3phrase)
        alltags = pprs.Each(nexttag*(0,),savelist=True)('alltags_inst')
    
        alltags.setDebug(False)
        tag1phrase.setDebug(False)
        tag2phrase.setDebug(False)
        tag3phrase.setDebug(False)
    
        # test grammar elements
        print '='*80,'\n'
        print 'Possible example of 'fixed' bug in asXML - as it turns the problem was caused by the logic of my grammar'
    
        #ts gets the right answer
        print '='*80,'\n'
        ts = '[tag1=tag1 example][tag2=tag2 example][tag3=tag3 example]'
        rs = alltags.parseString(ts)
        rsXML = rs.asXML('Correct')
        print rsXML
    
        #ts2 with repeated components used to give the 'wrong' answer with the first component named incorrectly
        #But now it gets the 'right' answer (where right is defined as what I expect!)
        print '='*80,'\n'
        ts2 = ts+'[tag1=tag1 example2][tag2=tag2 example2][tag3=tag3 example2]'
    
        rs2 = alltags.parseString(ts2)
        rs2XML = rs2.asXML('Also correct now')
        print rs2XML
    
        print '='*80,'\n'


The output becomes



    ================================================================================ 
    
    Possible example of 'fixed' bug in asXML - as it turns the problem was caused by the logic of my grammar
    ================================================================================ 
    
    
    <Correct>
      <tag1_inst>tag1 example</tag1_inst>
      <tag2_inst>tag2 example</tag2_inst>
      <tag3_inst>tag3 example</tag3_inst>
    </Correct>
    ================================================================================ 
    
    
    <Also correct now>
      <tag1_inst>tag1 example</tag1_inst>
      <tag2_inst>tag2 example</tag2_inst>
      <tag3_inst>tag3 example</tag3_inst>
      <tag1_inst>tag1 example2</tag1_inst>
      <tag2_inst>tag2 example2</tag2_inst>
      <tag3_inst>tag3 example2</tag3_inst>
    </Also correct now>
    ================================================================================ 


Earlier on I noted that it gives me almost what I expect.
The almost is that I seem to lose the XML tag name alltags_inst that I expected.
Hmmm perhaps I need to Group the answer, or something ....

---
## 2013-07-12 17:15:34 - Errglefloot_the_Enormous - What is the best method for parsing large files?
Is it possible to parse line by line of a large file to prevent having to read the entire file into memory?

#### 2013-07-21 08:37:55 - ptmcg
If the grammar is such that you will need the whole file to match, then you will need to redefine it so that you can read the file in in pieces.

Is each line parseable by itself? If so, then you can just do:



    parser = ... pyparsing definition ...
    
    with open(filename) as infile:
       for line in infile:
           results = parser.parseString(line)
    
           # do something with results



If your content spans multiple lines, but does not span linebreaks, then just keep reading lines until you get a successful parse:



    with open(filename) as infile:
       accum = ''
       for line in infile:
           accum += line
           try:
               results = parser.parseString(accum)
           except ParseException as pe:
               continue
           else:
               # do something with results
    
               # reset accum to start accumulating next parseable block
               accum = ''



If your content might end in the middle of a line, then you'll need to save the end location of the parse, and use everything from there to the end of the accum buffer when you reset after a successful parse:



    # add an end-of-parse marker to your grammar
    grammar = grammar + Empty()('endloc').setParseAction(lambda s,l,t:l)
    
    with open(filename) as infile:
       accum = ''
       for line in infile:
           accum += line
           try:
               results = parser.parseString(accum)
           except ParseException as pe:
               continue
           else:
               # do something with results
    
               # reset accum to start accumulating next parseable block
               accum = accum[results.endloc:]


Hope this helps,

-- Paul
#### 2013-07-22 09:34:51 - Errglefloot_the_Enormous
Hi Paul,

Thanks for the clear answer.

I think I misunderstood the basics of pyParsing, or was overthinking it (or both).

So, the correct way to think about it is that parseString() returns matches of low-level sections of the grammar which you then use externally to build up a useable data structure (parse tree/AST/other)? pyParsing itself does not do this for you?

Thanks
 E the Enormous
#### 2013-07-22 10:55:47 - ptmcg
Actually, it is pretty typical for pyparsing to build this all for you, provided you can read in the complete source string at once. You were the one to introduce the question of reading only part of the input at a time, so then pyparsing will only see bits of the input at a time.  Just how 'big' is this input file anyway?

In summary, pyparsing works with parsing an entire string of data, not any kind of streamed input. If you can provide a full string of input, then pyparsing can parse it into as complex a structure as you have defined in your grammar. If you have to break up the input into pieces, then have pyparsing parse the pieces, but you'll have to assemble them into a larger structure yourself (which may be no more difficult than using 'sum', but it depends on your input and how you have broken it up).

-- Paul

---
## 2013-08-03 11:13:51 - EldritchCheese - Unexpected result using setParseAction
First, the intended grammar.  I want to be able to have a list of options, each contained within 
a given list.  

For example, I would like to be able to specify `'[15, 16] {Name, OtherName} [42]'` and receive 
out a dictionary of `'[]'->[15,16,42]` and `'{}'->['Name','OtherName']`.  My goal is for each 
of the lists to be of arbitrary items in the least.  In addition, items in separate lists 
with the same delimiter should be combined together into a single list.

I have this working in most cases as follows.


    #!/usr/bin/env python                                                                                                                                    
    
    from pyparsing import *
    from collections import defaultdict
    
    #Match a comma-separated list of items,                                                                                                                  
    # enclosed by a specified separator                                                                                                                      
    def itemList(item,sep):
        itemSep = item.copy()
        itemSep = itemSep.setParseAction(lambda t:(sep,t[0]))
        output = (Literal(sep[0]).suppress() +
                  itemSep + ZeroOrMore(Literal(',').suppress() + itemSep) +
                  Literal(sep[1]).suppress())
        return output
    #Merge the lists into a dictionary of results.                                                                                                           
    def mergeLists(inputList):
        output = defaultdict(list)
        for key,item in inputList:
            output[key].append(item)
        return output
    #Match a series of separated lists,                                                                                                                      
    #  then merge into a dictionary of lists.                                                                                                                
    def itemOptions(*args):
        options = zip(args[0::2],args[1::2])
        fullList = [itemList(item,sep) for item,sep in options]
        fullList = reduce(lambda a,b:a^b,fullList[1:],fullList[0])
        fullList = ZeroOrMore(fullList)
        fullList.setParseAction(mergeLists)
        return fullList


My tests for this are as follows.



    name = Word(alphas)
    num = Word(nums).setParseAction(lambda t:int(t[0]))
    pair = (name + num).setParseAction(lambda t:(t[0],t[1]) )
    
    itemListing = itemOptions(name,'{}',
                              num,'[]',
                              pair,'<>')
    
    print itemListing.parseString(
        '{NameOne, NameTwo} [42, 1971] {NameThree}')
    #Expected { '{}': ['NameOne','NameTwo','NameThree'],                                                                                                     
    #           '[]': [42, 1971] }                                                                                                                           
    #Received expected result.                                                                                                                               
    
    print itemListing.parseString(
        '<NameOne 1, NameTwo 2>')
    #Expected { '<>': [('NameOne', 1), ('NameTwo', 2)] }                                                                                                     
    #Received { '<>': ['NameOne', 'NameTwo'] }                                                                                                               
    
    print pair.parseString('NameOne 1')
    #Expected [('NameOne', 1)]                                                                                                                               
    #Received expected result 


I don't know why it is that this parsing works for some cases, but not for others.  I had thought it to be related to the returning of a tuple from the parseAction, leading it to be interpreted as two separate tokens, instead of the tuple as a whole being a single token.  However, the last test shows that the 'pair' parser does indeed return a tuple.

If there is an easier way to achieve this type of parsing, or a way to fix my current implementation, I would very much appreciate any help.

#### 2013-08-03 11:55:07 - EldritchCheese
Ah, I found my mistake.  In itemList(), I need to use addParseAction() instead of setParseAction().  The setParseAction() overwrote the older tuple-building parseAction, resulting in the tokens being remembered instead of the adjusted tokens.
#### 2013-08-07 05:45:26 - ptmcg
Good catch - welcome to pyparsing!

---
## 2013-08-09 10:52:17 - typesupply - Tips for parsing block with conditional ending?
I'm starting to experiment with Pyparsing and I've run into something that I can't figure out. The syntax that I'm trying to parse has blocks defined like this:



    feature demo {
    
    } demo;


I'm able to parse this just fine with this:



    from pyparsing import *
    
    tTerminator = Literal(';')
    tTerminator.suppress()
    
    tBlockStart = Literal('{')
    tBlockStart.suppress()
    
    tBlockEnd = Literal('}')
    tBlockEnd.suppress()
    
    kwFeature = Word('feature')
    kwFeature.suppress()
    
    vFeatureName = Word(alphas, exact=4)
    vFeatureNameStart = vFeatureName.setResultsName('featureNameStart')
    vFeatureNameEnd = vFeatureName.setResultsName('featureNameEnd')
    
    featureBlock = kwFeature + vFeatureNameStart + tBlockStart + tBlockEnd + vFeatureNameEnd + tTerminator
    
    test = '''
    feature demo {
    
    } demo;
    '''
    
    result = featureBlock.parseString(test)


The thing that I'm not sure how to solve is that the blocks can be nested. For example:



    feature demo {
        lookup BlahBlah {
    
        } BlahBlah;
    } demo;


In my featureBlock definition above, is there a way to make vFeatureNameEnd dependent on the string matched by vFeatureNameStart? Or, is there an easier way to do this?

I apologize in advance if this is adumb question.

#### 2013-09-07 09:32:56 - ptmcg
This is not a dumb question, and *I* apologize for being so slow in responding. To define a parser that can have nested elements (a recursive parser), you will need to use the Forward type. I'll work up a more detailed example for you, and post it shortly.
#### 2013-09-07 09:50:44 - ptmcg
Here is a recursive parser for your nested example.  I had to redefine things a bit, since your nested block did not start with 'feature', did not have a 4-character name, etc. Note some of the other idioms I've added, such as the short-hand for defining suppressed punctuation, and the addition of results names.

Lastly, I've added some sample code to show how the results names can be used in nested form.

HTH,
-- Paul



    from pyparsing import *
    
    
    '''BNF
    
        block :: blocktype blockname '{' [block] '}' ';'
        blocktype :: ident
        blockname :: ident
        ident :: word of alphabetic characters
    
    '''
    
    tBlockStart,tBlockEnd,tTerminator = map(Suppress,'{};')
    ident = Word(alphas)
    
    featureBlock = Forward()
    
    featureBlock << Group(ident('type') + ident('name') + 
                            tBlockStart + Optional(featureBlock, [])('content') + tBlockEnd + 
                            ident('name2') + tTerminator)
    
    test = '''\
    feature demo {
        lookup BlahBlah {
    
        } BlahBlah;
    } demo;
    '''
    
    result = featureBlock.parseString(test)[0]
    print result.dump()
    print result.name
    print result.type
    print result.content.name

prints


    ['feature', 'demo', ['lookup', 'BlahBlah', [], 'BlahBlah'], 'demo']
    - content: ['lookup', 'BlahBlah', [], 'BlahBlah']
      - content: []
      - name: BlahBlah
      - name2: BlahBlah
      - type: lookup
    - name: demo
    - name2: demo
    - type: feature
    demo
    feature
    BlahBlah



---
## 2013-08-29 09:14:50 - usysinc - Install failed with Python2.6 on Windows7
Install failed with Python2.6 on Windows7

#### 2013-08-31 18:24:07 - ptmcg
Can you post some more details? pyparsing 2.0.0 would not install with Python 2.6, but the latest version 2.0.1 should install just fine.

---
## 2013-09-07 07:55:12 - EldritchCheese - defaultdict and setResultsName
I have a parsing action that returns a collections.defaultdict as its output.  However, I am getting strange results when I try to name this parsing action.  A proof of concept code is shown below.


    #!/usr/bin/env python
    
    from pyparsing import *
    from collections import defaultdict
    
    parser = Word(alphanums).setParseAction(lambda m: defaultdict(int,key=m[0]))
    result = parser.parseString('hello')
    print result[0]
    
    parser = Word(alphanums).setParseAction(lambda m: defaultdict(int,key=m[0])).setResultsName('named')
    result = parser.parseString('hello')
    print result[0]
    print result['named']


I would expect each of the three print statements to produce the same output.  However, each returns a different output entirely.  The first is the expected result, of a defaultdict with a single key-value pair, ('key', 'hello').  The second form shows a defaultdict with an additional key-value pair, (0, 0).  The third returns only the values 0.  As far as I can tell, when building the dictionary of names, pyparsing checks accesses the dictionary with key 0, creating the additional entry.  The result of calling this in then placed into the named dictionary.

Is there some way to have the expected behavior?  I know that I could just avoid using setResultsName(), but I would rather name my parameters for better readability.

#### 2013-09-07 09:29:50 - ptmcg
Your return of a defaultdict is similar to cases where people want to return None from a parse action. A None return from a parse action usually indicates 'use the ParseResults object that was passed to the parse action', so programs that actually want to return None do so by returning [None]. In your case, the defaultdict gets mutated as part of the construction of the ParseResults instance after the parse action is called. The workaround is the same, instead of returning your defaultdict, return your defaultdict wrapped in a list:



    parser = Word(alphanums).setParseAction(lambda m: [defaultdict(int,key=m[0])])
    parser = Word(alphanums).setParseAction(lambda m: [defaultdict(int,key=m[0])])('named')


Now both of these parsers will return the same value.
#### 2013-09-07 14:33:18 - EldritchCheese
Ah, okay.  Thank you.  A bit of a followup question, though, since another issue shown up.



    #!/usr/bin/env python
    
    from pyparsing import *
    from collections import defaultdict
    
    parser = OneOrMore(Word(alphanums)).setParseAction(lambda m: [defaultdict(list,key=m[:])])('named')
    result = parser.parseString('hello goodbye')
    
    print result[0]
    print result['named']


The same parser as before, except that now having a series of words instead of a single word.  Here, printing `result[0]` gives back the defaultdict, but `result['named']` gives a list of length 1 containing the defaultdict, and I'm not really sure what is causing the difference.
#### 2013-09-07 19:44:56 - ptmcg
Well it feels like a band-aid on top of a hack, but once you've returned the [defaultdict] from the parse action, you can unpack it using ungroup:


    parser = ungroup(OneOrMore(Word(alphanums))).setParseAction(lambda m: [defaultdict(list,key=m[:])])('named')


#### 2013-09-08 17:03:40 - EldritchCheese
Ah, thank you.  This is part of the macro that I asked about in the earlier question that I posted on  .  Since it isn't something that I will need to be doing on the fly, and am just adding it into an existing function, a band-aid is exactly what is needed.  Thank you.

---
## 2013-09-12 00:14:38 - strzmiele - simpleBool
Hallo, i have problems with simpleBool.py. When itry to solve equation wwchich is build only by one variable and it is set to False I get result True. For example:

    p = True q = False
    ---
    p
    p = True
    ---
    q
    q = True
    ---
    not p
    ~p = False
    ---
    p and q
    (p & q) = False
    ---
    p and not q
    (p & ~q) = True
    ---
    not not p
    ~~p = True
    ---
    not(p and p)
    ~(p & p) = False
    ---
 
How can I correct this bug ?

Piotr

#### 2013-09-12 00:16:52 - strzmiele
And this is an example:


    test = ['p',
            'q',
            'not p',
            'p and q',
            'p and not q',
            'not not p',
            'not(p and p)',
            'q or not p and r',
            'q or not p or not r',
            'q or not (p and r)',
            'p or q or r',
            'p or q or r and False',
            '(p or q or r) and False',
            ]
    
    p = True
    q = False
    r = False
    print 'p =', p
    print 'q =', q
    print '<br>'
    for t in test:
        res = boolExpr.parseString(t)[0]
        print t,'<br>', res, '=', bool(res),'<br>'
        #print 'res', res, '<br>'
        print '----------------<br>'


#### 2013-09-14 04:49:10 - ptmcg
This is a bug in SimpleBool.py, which I converted to Python 3 compatibility, but in the process, broke for Python 2.x.

I'll post a corrected version shortly, and include a corrected version in the next 2.x release.

Thanks for pointing this out!

-- Paul

---
## 2013-09-12 11:21:10 - bbaldino1 - setResultsName best practices?
I'm trying to build up a grammar piece by piece but running into something that feels a bit awkward and I'm wondering if there's a better way to do it or if I'm just thinking about it wrong.  A simplified example would be as follows:



    sub_type_1 = Word(nums)('sub_type_1')
    sub_type_2 = Word(alphanums)('sub_type_2')
    
    sub_section_a = (sub_type_1 + sub_type_2)('sub_section_a')
    
    sub_section_b = (sub_type_1)('sub_section_b')
    
    overall_text = (ZeroOrMore(sub_section_a) + sub_section_b)('overall_text')


The principle I was trying to stick with here is this: when defining a 'term', the term knows what it is so it defines its own name (i.e. the definition for sub_type_1 names itself 'sub_type_1').  Coming from OOP, this 'felt' right.  For example, it allows me to re-use 'sub_type_1' multiple times without having to rename it each time.  It also 'encapsulates' the definition of a 'term' so that the outer layer doesn't necessarily have to know/care how it is defined/named.

Where I run into problems is when using something like 'ZeroOrMore' with a 'self-named' term (as seen with 'sub_section_a').  Since I want to have all matches listed, I need to use 'listAllMatches=True' when calling setResultsName.  However, when I defined 'sub_section_a' (and named it), I had no idea whether or not it would be used with something like 'ZeroOrMore'--and it doesn't feel like I should have to--I like being able to define the term independently with 'peeking' about future usage (nor should I necessarily want that behavior in all usages of 'sub_section_a').  So this suggests that I need to leave the naming to the 'user' of a term rather than having the term 'name itself' which feels like it breaks my idea of 'encapsulation' (and would then lose the benefits I mentioned above).  And the top-most section will still end up naming itself.

So I'd get definitions like this:



    sub_type_1 = Word(nums)
    sub_type_2 = Word(alphanums)
    
    sub_section_a = sub_type_1('sub_type_1') + sub_type_2('sub_type_2')
    
    sub_section_b = sub_type_1('sub_type_1')
    
    overall_text = (ZeroOrMore(sub_section_a.setResultsName('sub_section_a', listAllMatches=True)) + sub_section_b('sub_section_b'))('overall_text')


Is there a way to do this that looks more like my first example?  Or perhaps my perspective on the benefits of 'encapsulation' when defining the terms is misguided?

Thanks!

#### 2013-09-15 15:08:13 - ptmcg
Don't confuse what a term *is* versus how it is *used*. For instance, let's say you were parsing data with several integers, like a timestamp. You could define an integer simply as:



    integer = Word(nums)
    integer.parseString('x')


When we pass a bad value to the parser, we get this ugly-looking message:



    Expected W:(0123...) (at char 0), (line:1, col:1)


That 'W:(0123...)' is pyparsing's default attempt at describing a Word composed of numeric digits - maybe meaningful to you, but not like to an end-user who will see it in some error log. So we modify integer to add a descriptive name of what it is trying to parse - using setName:



    integer = Word(nums).setName('integer')


Now we get a bit clearer:



    Expected integer (at char 0), (line:1, col:1)


Let's move on to assemble several of these into a timestamp.



    date = integer + '/' + integer + '/' + integer


As I often suggest, we'll add results names, to make it easier to get at the parsed fields:



    date = integer.setResultsName('year') + '/' + integer.setResultsName('month') + '/' + integer.setResultsName('day')


Now we are distinguishing that the different integers should be returned with different labels on them, since they have positional meanings beyond the simple fact that they are integers. And so for this purpose we use setResultsName. 

setResultsName has been a long-standing issue for me, because it does two things: not only does it assign the value for the parsed results to be associated with, but it makes a copy of the given expression, so that the different copies (defined the same, with the same expression name) can have instances that each contain their different results name. So there is more than just 'setting' going on here.  

To help avoid some of this name-induced confusion, I added the short-cut form for setResultsName, which allows us to write:



    date = integer('year') + '/' + integer('month') + '/' + integer('day')


Which I find to be cleaner-looking, and more suggestive of the instantiation of different integer expressions, each playing different roles in the overall expression.

So let's do the same thing with date - assume we are parsing something with 2 dates, say for a library book. One is the checkout date, the other is the expiration date:



    book_reservation = date('checkout_date') + date('exp_date')


Look what happens if we parse a pair of dates:



    result = book_reservation.parseString('2001/1/1 2002/2/2')
        print result


gives us:



    ['2001', '/', '1', '/', '1', '2002', '/', '2', '/', '2']


The tokens are all there, but there is really no distinction of the two different groups. This is an underlying feature of pyparsing, that no matter how you piece together the different bits of a parser, the result is a continuous list of strings.  If we look at the names (using the dump() method), we see the naming hierarchy reflected, but we also see global name values for 'year', 'month', and 'day'. 



    [2001, '/', 1, '/', 1, 2002, '/', 2, '/', 2]
    - checkout_date: [2001, '/', 1, '/', 1]
      - day: 1
      - month: 1
      - year: 2001
    - day: 2
    - exp_date: [2002, '/', 2, '/', 2]
      - day: 2
      - month: 2
      - year: 2002
    - month: 2
    - year: 2002


This is an artifact of the same default 'lump all information about parsing into a single list regardless of how it was built up' behavior.

The easiest way to work around this is to go back to 'date' and wrap it in a pyparsing Group:



    date = Group(integer('year') + '/' + integer('month') + '/' + integer('day'))


This is our way of adding some structure to the parsed data, and keeping one element's names from mixing with another's. Group is there to override the default merging of tokens, and to keep the separate namespaces apart. With this change, our results dump looks like:



    [[2001, '/', 1, '/', 1], [2002, '/', 2, '/', 2]]
    - checkout_date: [2001, '/', 1, '/', 1]
      - day: 1
      - month: 1
      - year: 2001
    - exp_date: [2002, '/', 2, '/', 2]
      - day: 2
      - month: 2
      - year: 2002


This is actually cleaned up in two respects. First, the names 'year', 'month', and 'day' are now just subfields of 'checkout_date' and 'exp_date'. But also note that the list of tokens has been structured into two sublists.

I hope this gives you a little more insight into how names work in pyparsing - good luck!

-- Paul
#### 2013-09-15 15:10:31 - ptmcg
Yikes, that was a long post, and I never got to the 'best practice' part. In general:

- use setName *early* in your definiton of a parser, since your expressions are initially abstract and only know what they parse for

- use setResultsName (or the corresponding shortcut) *late* in the parser definition, because it is then that you as the parser writer can impart what role the expression is playing in the bigger picture.

- use Group to keep names from getting smooshed together.
#### 2013-09-17 16:40:34 - bbaldino1
Hey Paul,
Thanks for another one of your extremely detailed answers.  I wasn't aware of 'setName'--knowing about it helps clarify its usage vs. setResultsName (naming what something is vs how it's used) so I think that answered the crux of my question.

I'll admit, all the tokens being at the top level of the dictionary threw me off a bit, but, programmatically, I end up just 'ignoring' them; the dictionary ends up giving me the hierarchy I expect so that seems to be working well.

I was aware of 'Group' but after reading your response I don't think I'm using it to its full potential so I'll have to keep that in mind.

Thanks for the help!

---
## 2013-09-15 12:29:55 - sorenh - Problem with select_parser
Hi,

I needed an SQL parser and the select_parser is an excellent starting point. However, I'm running into a small problem.. expr is defined using operatorPrecedence with the baseExpr being expr_term, but I don't want e.g. '10' to be a valid where_expr. In other words, I want my where expression to at least have one comparison operator in it, but I'm not sure how to express this without losing the incredibly convenient operatorPrecedence.

Any help would be greatly appreciated.

#### 2013-09-15 12:48:36 - ptmcg
Define where_expr as expr.copy() and then change 'Optional(WHERE + expr('where_expr')) +' in select_core to 'Optional(WHERE + where_expr('where_expr')) +'. Attach a parse action to where_expr to do whatever additional filtering that you want to impose. If the filter criteria fails, have the parse action raise a ParseException.
#### 2013-09-15 21:30:17 - sorenh
Ah, extra filtering in the parseAction handler. Neat. Thanks!

---
## 2013-09-19 12:22:04 - bbaldino1 - Yet another question 'Group' usage
Hey Paul,
I'm trying to learn from the 'Group' suggestions you gave me here () but I'm running into an issue that makes me think I'm still missing someting (despite learning what I could from other posts on 'Group').

I start with a grammar like this:



    number = Word(nums).setParseAction(lambda s, l, t: int(t[0]))
    version_line_prefix = Suppress(Literal('v=').setName('VLINE_PREFIX'))
    version_number = number.setName('VERSION_NUMBER')
    version_line = (version_line_prefix + version_number('VERSION_NUMBER')).setName('VERSION_LINE')


When I run the following unit test, I get exactly what I'm looking for.

The test:


    TEST 1
    str = 'v=0'
    res = version_line.parseString(str)
    print(res.asDict())


Results in:


    {'VERSION_NUMBER': 0}


This is exactly what I wanted...a hierarchy with no terms present at the top level.

I then try and go to the next step an add the following grammar:


    session_section = version_line('VERSION_LINE')


And run the following test:


    TEST 2
    str = 'v=0'
    res = session_section.parseString(str)
    print(res.asDict())


Which gives this result:


    .{'VERSION_NUMBER': 0, 'VERSION_LINE': ([0], {'VERSION_NUMBER': [(0, 0)]})}


This is almost what I want, but I didn't want the 'VERSION_NUMBER' token present at the top level (I'm looking to get a strict hierarchy without anything 'polluting' the top level dictionary other than direct sub-elements).  Taking what you'd instructed me in the previous post, I tried using 'Group' to solve this problem.  If I change the first grammar to this:


    version_line = Group(version_line_prefix + version_number('VERSION_NUMBER')).setName('VERSION_LINE')


Then TEST 2 now gives this result:


    {'VERSION_LINE': ([0], {'VERSION_NUMBER': [(0, 0)]})}


Which is exactly what I want!  But, TEST 1 now gives an empty dict when calling asDict:


    {}


Why does surrounding that line with 'Group' give me an empty dict when parsing?  I still name the 'VERSION_NUMBER' field, so would've expected it to show up.  Is there a way to preserve the hierarchy at each layer?

Thanks again,
Brian

#### 2013-09-19 12:55:54 - ptmcg
Now that you have Group'ed version_line, you now get back a ParseResults containing a list of ParseResults.  Try printing out res[0].asDict().  I also find the repr output of ParseResults to be distracting when calling asDict() - try asList() or dump().

-- Paul
#### 2013-09-24 10:11:31 - bbaldino1
Hey Paul,
Thanks for the response.  I wanted to try things out a bit before responding to see how it was working.  So far things look good...Part of me wishes I didn't have to 'know' something was grouped and therefore use '[0]'--I'm a bit worried I'll run into a case where some code will not know if the result it's dealing with is grouped or not and need to determine so programmatically (any tips there?) but I've not run such a situation yet; will see how things go!

-Brian

---
## 2013-10-23 11:54:25 - pjennings1234 - ParseSyntaxException isn't always fatal
I had some trouble with the ErrorStop functionality where I had several nested And's all part of an Each. One of the And's was raising a ParseSyntaxException, but the ultimate fatal exception reported by pyparsing was that a required part of the Each was missing (indicating that parsing all of the And's had failed, which it had because of a syntax error). 

I noticed that lines ParserElement.tryParse (line 945 of pyparsing 2.0.1) catches ParseFatalException and raises ParseException. This is definitely not what I expected but I don't know enough of the context to know if this is a real bug. I was able to fix my immediate problem by adding:


except ParseSyntaxException:
  raise


in tryParse. I don't know if that breaks anything else.

#### 2013-10-23 13:04:43 - ptmcg
In general tryParse is used as a 'probe' to see if the next element in the grammar would succeed if it were parsed. Each uses tryParse heavily, because there are so many variations of which expression might match next. This is why tryParse traps on the usually fatal ParseSyntaxException. I'll have to give this some thought, tryParse should never *stop* the parsing process, but if a collective expression fails all of its calls to tryParse, then any that raise Fatal exception should be the ones to raise (which is what you are doing with your code change).

Thanks for the note - I'll try to look deeper into this in the next few days or so.

-- Paul

---
## 2013-10-24 02:07:47 - israkir - Parsing nested parenthesis producing multiple parsing results
I'd like to parse a string with nested parenthesis with these conditions:

<ul><li>Elements are delimited by comma , or bar |.</li><li>Nested parenthesis elements might be a single alphanum or another nested parenthesis.</li><li>Each nested parenthesis element connected by bar | literal leads to creation of a new sequence combining previous sequence elements and forward elements connected by comma, outside that nested parenthesis.</li></ul>
In order to clarify, let me give some examples of input strings and the results they should return:

(a, b, c) should return: a, b, c

(a, (b | c)) should return: a, b and a, c

(a, b, (c | (d, e)), f) should return: a, b, c, f and a, b, d, e, f

(a, b, (c | (d, e) | f), g) should return: a, b, c, g and a, b, d, e, g and a, b, f, g

(a, b, c, ((d, (e | f)) | (g, h)), i) should return: a, b, c, d, e, i and a, b, c, d, f, i and a, b, c, g, h, i

((a | b), c) should return: a, c and b, c

#### 2013-10-26 08:43:57 - ptmcg
As I posted on SO when you posted this exact same question, you can get the string parsed using infixNotation (formerly known as operatorPrecedence). Assuming that ',' has precedence over '|', this would look like:



    variable = oneOf(list(alphas.lower()))
    expr = infixNotation(variable, 
                [
                (',', 2, opAssoc.LEFT),
                ('|', 2, opAssoc.LEFT),
                ])


Converting your test cases to a little testing framework, we can at least test the parsing part:



    tests = [
        ('(a, b, c)', ['abc']),
        ('(a, b | c)', ['ab', 'c']),
        ('((a, b) | c)', ['ab', 'c']),
        ('(a, (b | c))', ['ab', 'ac']),
        ('(a, b, (c | (d, e)), f)', ['abcf','abdef']),
        ('(a, b, (c | (d, e) | f), g)', ['abcg', 'abdeg', 'abfg']),
        ('(a, b, c, ((d, (e | f)) | (g, h)), i)',
          ['abcdei', 'abcdfi', 'abcghi']),
        ('((a | b), c)', ['ac', 'bc']),
        ]
    
    for test,expected in tests:
        # if your expected values *must* be lists and not strings, then
        # add this line
        # expected = [list(ex) for ex in expected]
        result = expr.parseString(test)
        print result[0].asList()


which will give you something like this:



    ['a', ',', 'b', ',', 'c']
    [['a', ',', 'b'], '|', 'c']
    [['a', ',', 'b'], '|', 'c']
    ['a', ',', ['b', '|', 'c']]
    ['a', ',', 'b', ',', ['c', '|', ['d', ',', 'e']], ',', 'f']
    ['a', ',', 'b', ',', ['c', '|', ['d', ',', 'e'], '|', 'f'], ',', 'g']
    ['a', ',', 'b', ',', 'c', ',', [['d', ',', ['e', '|', 'f']], '|', ['g', ',', 'h']], ',', 'i']
    [['a', '|', 'b'], ',', 'c']


That is the easy part, parsing the string and getting the operator precedence reflected in the resulting structure. Now if you follow the example from the regex inverter (also as I suggested on SO), you will need to attach objects to each parsed bit, something like this:




    class ParsedItem(object):
        def __init__(self, tokens):
            self.tokens = tokens[0]
    class Var(ParsedItem): 
        ''' TBD '''
    class BinaryOpn(ParsedItem):
        def __init__(self, tokens):
            self.tokens = tokens[0][::2]
    class Sequence(BinaryOpn):
        ''' TBD '''
    class Alternation(BinaryOpn):
        ''' TBD '''
    
    variable = oneOf(list(alphas.lower())).setParseAction(Var)
    expr = infixNotation(variable, 
                [
                (',', 2, opAssoc.LEFT, Sequence),
                ('|', 2, opAssoc.LEFT, Alternation),
                ])


Now you will have to implement the bodies of Var, Sequence, and Alternation. You will not get a list of values directly back from pyparsing, instead you will get one of these object types back. Then, instead of calling asList() as I did in the sample above, you'll call something like generate or makeGenerator to get a generator from that object. Then you'll invoke that generator to have the objects generate all the different results for you.

I leave the rest as an exercise for you.

-- Paul
#### 2013-10-26 08:45:51 - ptmcg
One last hint: you may find itertools.product helpful in implementing Sequence's generator.
#### 2013-10-26 23:07:18 - israkir
Hi Paul, 

Thanks for the answer -- it is way more than I expected. I have been working on this for a while and got a similar solution to yours after seeing your SO answer. Just wanted to get a second opinion regarding to my problem; however obviously this is the easiest and best way to do it. Appreciate your time ;)

---
## 2013-12-02 14:40:46 - acomfygeek - pfe.markInputline()
All -

I was getting an error when calling pfe.markInputline().  It looks like the error is here:

    def markInputline( self, markerString = '>!<' ):
        '''Extracts the exception line from the input string, and marks
           the location of the exception with a special symbol.
        '''
        line_str = self.line
        line_column = self.column - 1
        if markerString:
            line_str = ''.join(line_str[:line_column],
                                markerString, line_str[line_column:])
        return line_str.strip()

.join doesn't have a single argument of a list, but rather three items.  When I changed it to:

            line_str = ''.join([line_str[:line_column],
                                markerString, line_str[line_column:]])

I don't have any problems.  Hope this helps anyone having the same error I had.


---
## 2013-12-08 22:39:31 - kwos - Partial parser
I'm trying to build a partial parser for a programming language that extracts only assignments to variables inside conditional statements. For instance, in 


    IF condition
    assign
    ignore
    assign
    ELSE
    assign
    END-IF
    [[code]],
    
`assign` is an assignment, and `ignore` is some other statement I would like to skip. My 
naive attempt (without nesting for now) is

    ignore = SkipTo(assign | Literal('ELSE') | Literal('END-IF')).suppress()
    stmt = OneOrMore(assign | ignore)
    cond = Literal('IF') + formula + stmt + Literal('ELSE') + stmt + Literal('END-IF')

which results in an infinite loop. I already played around with the include parameter and with setParseAction but I can't get it to work.




