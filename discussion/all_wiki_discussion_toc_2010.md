[2010-01-01 00:28:06 - pgurumur - Parsing perl hash](.#2010-01-01-002806---pgurumur---parsing-perl-hash)  
[2010-01-09 04:56:30 - alito - Traversing parsed results](.#2010-01-09-045630---alito---traversing-parsed-results)  
[2010-01-10 01:03:30 - voltron8 - Algebraic expressions with nonstandard operations](.#2010-01-10-010330---voltron8---algebraic-expressions-with-nonstandard-operations)  
[2010-01-11 11:48:28 - lcampagn - A new C parser](.#2010-01-11-114828---lcampagn---a-new-c-parser)  
[2010-01-15 18:17:25 - hach-que - Arithmetic expression with functions](.#2010-01-15-181725---hach-que---arithmetic-expression-with-functions)  
[2010-01-16 01:40:21 - hach-que - Speeding parsing up](.#2010-01-16-014021---hach-que---speeding-parsing-up)  
[2010-01-17 04:59:52 - voltron8 - Variables and functions](.#2010-01-17-045952---voltron8---variables-and-functions)  
[2010-01-20 00:09:40 - hach-que - Combining results](.#2010-01-20-000940---hach-que---combining-results)  
[2010-01-21 07:36:32 - pmarichal - Can't parse my grammar, please help me](.#2010-01-21-073632---pmarichal---cant-parse-my-grammar-please-help-me)  
[2010-01-22 16:02:08 - Hory - Stumped by recursive elements](.#2010-01-22-160208---hory---stumped-by-recursive-elements)  
[2010-01-26 17:01:33 - filter5 - difficulty with nestedExpr](.#2010-01-26-170133---filter5---difficulty-with-nestedexpr)  
[2010-01-28 04:08:27 - blundeln - Extend pyparsing for bi-directional use](.#2010-01-28-040827---blundeln---extend-pyparsing-for-bi-directional-use)  
[2010-01-30 11:08:45 - filter5 - dynamic xml tags?](.#2010-01-30-110845---filter5---dynamic-xml-tags)  
[2010-02-01 08:40:38 - nemith - storing the original text along with nested parsed values](.#2010-02-01-084038---nemith---storing-the-original-text-along-with-nested-parsed-values)  
[2010-02-12 02:50:51 - marjoj - Comparing different parser generators](.#2010-02-12-025051---marjoj---comparing-different-parser-generators)  
[2010-02-16 11:01:43 - Skipix - Simple arithmetic grammar](.#2010-02-16-110143---skipix---simple-arithmetic-grammar)  
[2010-02-22 02:46:38 - josandres - Logical operators problem](.#2010-02-22-024638---josandres---logical-operators-problem)  
[2010-02-22 08:52:55 - john-l - Selectively disabling `ignoreExprs`](.#2010-02-22-085255---john-l---selectively-disabling-`ignoreexprs`)  
[2010-03-02 02:54:05 - nerochiaro - How to improve error reporting ?](.#2010-03-02-025405---nerochiaro---how-to-improve-error-reporting-)  
[2010-03-02 06:37:55 - dharanitharan - How to parse functions in C language](.#2010-03-02-063755---dharanitharan---how-to-parse-functions-in-c-language)  
[2010-03-05 13:38:36 - bjornjobb - time expression parser](.#2010-03-05-133836---bjornjobb---time-expression-parser)  
[2010-03-09 01:13:54 - josandres - Pyparsing quoted strings](.#2010-03-09-011354---josandres---pyparsing-quoted-strings)  
[2010-03-16 22:59:40 - gobnat - Help understanding Expression subclasses and whitespace treatment](.#2010-03-16-225940---gobnat---help-understanding-expression-subclasses-and-whitespace-treatment)  
[2010-03-17 01:15:48 - elekis - installing pyparsing on python 3.1](.#2010-03-17-011548---elekis---installing-pyparsing-on-python-31)  
[2010-03-20 10:26:22 - Elby - Reading a table by columns](.#2010-03-20-102622---elby---reading-a-table-by-columns)  
[2010-03-22 03:01:43 - josandres - Binary operators](.#2010-03-22-030143---josandres---binary-operators)  
[2010-03-22 16:24:40 - tvn1981 - simple C Expression ](.#2010-03-22-162440---tvn1981---simple-c-expression-)  
[2010-03-23 08:21:20 - lagpoi - paragraphs of text using SkipTo](.#2010-03-23-082120---lagpoi---paragraphs-of-text-using-skipto)  
[2010-03-23 11:16:03 - shankarlingayya - pyparsing Optional](.#2010-03-23-111603---shankarlingayya---pyparsing-optional)  
[2010-03-24 14:21:38 - tvn1981 - Recursive and operatorPrecedence question](.#2010-03-24-142138---tvn1981---recursive-and-operatorprecedence-question)  
[2010-03-26 06:44:07 - elekis - typeerror :'in <string>' requires string as left operand, not int](.#2010-03-26-064407---elekis---typeerror-in-string-requires-string-as-left-operand-not-int)  
[2010-03-27 10:35:28 - shankarlingayya - How to ignore the next line in pyarsing](.#2010-03-27-103528---shankarlingayya---how-to-ignore-the-next-line-in-pyarsing)  
[2010-03-27 11:13:18 - shankarlingayya - How to skip the data between { and } in pyparsing](.#2010-03-27-111318---shankarlingayya---how-to-skip-the-data-between-{-and-}-in-pyparsing)  
[2010-03-27 12:37:47 - Elby - Good usage of setWhitespaceChars](.#2010-03-27-123747---elby---good-usage-of-setwhitespacechars)  
[2010-03-30 09:05:05 - mvilanova - Continue parsing after \r character](.#2010-03-30-090505---mvilanova---continue-parsing-after-\r-character)  
[2010-04-01 14:44:24 - tvn1981 - delimitedList of one or two or three fields](.#2010-04-01-144424---tvn1981---delimitedlist-of-one-or-two-or-three-fields)  
[2010-04-03 11:15:19 - breamoreboy - new regex module](.#2010-04-03-111519---breamoreboy---new-regex-module)  
[2010-04-03 11:46:55 - tvn1981 - match everything between '('   ')'](.#2010-04-03-114655---tvn1981---match-everything-between----)  
[2010-04-09 15:45:40 - edc - problem -- pyparsing results contains extra empty dictionaries (or braces)](.#2010-04-09-154540---edc---problem----pyparsing-results-contains-extra-empty-dictionaries-or-braces)  
[2010-04-16 11:20:06 - JoshuaREnglish - The Time Parser Script](.#2010-04-16-112006---joshuarenglish---the-time-parser-script)  
[2010-04-19 02:12:09 - decay_of_mind - How to parse a different type of SQL queries](.#2010-04-19-021209---decay_of_mind---how-to-parse-a-different-type-of-sql-queries)  
[2010-05-03 03:04:18 - gareth8118 - Passing an object method to setParseAction?](.#2010-05-03-030418---gareth8118---passing-an-object-method-to-setparseaction)  
[2010-05-03 04:22:51 - rgammans - (Easy_)Installing on windows....](.#2010-05-03-042251---rgammans---easy_installing-on-windows)  
[2010-05-04 11:57:59 - gareth8118 - Unexpected list grouping of named result](.#2010-05-04-115759---gareth8118---unexpected-list-grouping-of-named-result)  
[2010-05-04 13:13:00 - skuda - Getting original matched text](.#2010-05-04-131300---skuda---getting-original-matched-text)  
[2010-05-04 22:10:07 - sakana280 - A faster (near-)substitute for operatorPrecedence()](.#2010-05-04-221007---sakana280---a-faster-near-substitute-for-operatorprecedence)  
[2010-05-05 04:45:53 - Hory - Optimisation idea for parsing natural language subsets](.#2010-05-05-044553---hory---optimisation-idea-for-parsing-natural-language-subsets)  
[2010-05-20 10:25:41 - majortal - Bug in cache mechanism?](.#2010-05-20-102541---majortal---bug-in-cache-mechanism)  
[2010-05-30 21:20:45 - flacoflacco - empty ParseResults.asdict(), values() etc???](.#2010-05-30-212045---flacoflacco---empty-parseresultsasdict-values-etc)  
[2010-06-02 10:18:24 - jackdaniels2 - BNF to pyparsing](.#2010-06-02-101824---jackdaniels2---bnf-to-pyparsing)  
[2010-06-02 15:02:58 - etno_thomas - alphas8bit problems](.#2010-06-02-150258---etno_thomas---alphas8bit-problems)  
[2010-06-13 22:54:39 - gobnat - loc driving me loco.](.#2010-06-13-225439---gobnat---loc-driving-me-loco)  
[2010-06-20 21:54:44 - jorgeecardona - Create Balanced expressions.](.#2010-06-20-215444---jorgeecardona---create-balanced-expressions)  
[2010-06-22 06:13:51 - dminor14 - build problem on cent0s5 64-bit](.#2010-06-22-061351---dminor14---build-problem-on-cent0s5-64-bit)  
[2010-06-30 21:26:16 - Torkn - r199 breaks Python 2 compat](.#2010-06-30-212616---torkn---r199-breaks-python-2-compat)  
[2010-06-30 21:56:29 - john_nagle - Address parser missing many street types.](.#2010-06-30-215629---john_nagle---address-parser-missing-many-street-types)  
[2010-07-04 13:11:12 - GregWatson - problem with __builtin__ pyparsing  1.5.3 and Windows Python 3.1.2](.#2010-07-04-131112---gregwatson---problem-with-__builtin__-pyparsing--153-and-windows-python-312)  
[2010-07-06 19:04:52 - GregWatson - newbie problem with addParseAction and Combine](.#2010-07-06-190452---gregwatson---newbie-problem-with-addparseaction-and-combine)  
[2010-07-08 09:48:49 - GregWatson - Questions on setDebug() and And()](.#2010-07-08-094849---gregwatson---questions-on-setdebug-and-and)  
[2010-07-12 03:15:22 - dminor14 - pyparsing and imputil](.#2010-07-12-031522---dminor14---pyparsing-and-imputil)  
[2010-07-22 10:36:47 - avisenna - need urgent help with information nested in special CStyle Comment blocks](.#2010-07-22-103647---avisenna---need-urgent-help-with-information-nested-in-special-cstyle-comment-blocks)  
[2010-07-23 11:47:41 - mlissner - No online documentation? ](.#2010-07-23-114741---mlissner---no-online-documentation-)  
[2010-08-10 15:12:09 - classicgamedev - operatorPrecedence is incredibly slow](.#2010-08-10-151209---classicgamedev---operatorprecedence-is-incredibly-slow)  
[2010-08-12 18:13:10 - scripteaze - Mac Address Grammer and output](.#2010-08-12-181310---scripteaze---mac-address-grammer-and-output)  
[2010-08-16 03:08:24 - avisenna - process nested #ifdef](.#2010-08-16-030824---avisenna---process-nested-ifdef)  
[2010-08-17 04:00:51 - cpuddle - Nested expressions?](.#2010-08-17-040051---cpuddle---nested-expressions)  
[2010-08-19 03:10:26 - milliams - Parsing simple c++ function calls](.#2010-08-19-031026---milliams---parsing-simple-c-function-calls)  
[2010-08-19 15:43:08 - classicgamedev - The proper way to track current line and column?](.#2010-08-19-154308---classicgamedev---the-proper-way-to-track-current-line-and-column)  
[2010-09-03 10:19:54 - classicgamedev - pyparsing masking the built-in struct module](.#2010-09-03-101954---classicgamedev---pyparsing-masking-the-built-in-struct-module)  
[2010-09-06 16:52:48 - foobar4 - match everyline except something](.#2010-09-06-165248---foobar4---match-everyline-except-something)  
[2010-09-08 13:56:28 - avisenna - parsing recursive structureswith optional elements](.#2010-09-08-135628---avisenna---parsing-recursive-structureswith-optional-elements)  
[2010-09-24 19:51:53 - Josh_English - Maybe I don't get groups](.#2010-09-24-195153---josh_english---maybe-i-dont-get-groups)  
[2010-10-05 13:01:45 - panneer - Struck in Infinite Loop while parsing](.#2010-10-05-130145---panneer---struck-in-infinite-loop-while-parsing)  
[2010-10-10 05:24:18 - hadim - Parsing piece of latex file](.#2010-10-10-052418---hadim---parsing-piece-of-latex-file)  
[2010-10-16 16:18:09 - edc - re: cppStyleComment problem](.#2010-10-16-161809---edc---re-cppstylecomment-problem)  
[2010-10-19 17:57:45 - gobnat - How do I get a 10x speed increase?](.#2010-10-19-175745---gobnat---how-do-i-get-a-10x-speed-increase)  
[2010-10-20 16:51:14 - jon.dobson - import error, Python 3.1.2](.#2010-10-20-165114---jondobson---import-error-python-312)  
[2010-10-26 02:09:51 - erastune - pyparsing being "greedy" - I need help](.#2010-10-26-020951---erastune---pyparsing-being-"greedy"---i-need-help)  
[2010-10-27 14:31:12 - johnny-b - pb with begin end type of expressions](.#2010-10-27-143112---johnny-b---pb-with-begin-end-type-of-expressions)  
[2010-10-29 04:43:00 - rxe - update on ambiguous grammars](.#2010-10-29-044300---rxe---update-on-ambiguous-grammars)  
[2010-10-29 04:48:34 - rxe - multisets](.#2010-10-29-044834---rxe---multisets)  
[2010-11-02 08:11:30 - mikasaari - Noob with Groups once more](.#2010-11-02-081130---mikasaari---noob-with-groups-once-more)  
[2010-11-07 05:32:01 - ctctc - version 1.5.5 string bug](.#2010-11-07-053201---ctctc---version-155-string-bug)  
[2010-11-11 12:24:58 - EduardoBarros - Parsing a tag with long text inside](.#2010-11-11-122458---eduardobarros---parsing-a-tag-with-long-text-inside)  
[2010-11-15 03:02:57 - bsder - QuotedString oddness and question about CPP preprocessing numbers](.#2010-11-15-030257---bsder---quotedstring-oddness-and-question-about-cpp-preprocessing-numbers)  
[2010-11-16 07:49:03 - kbriggs4 - wordsToNum.py example doesn't work](.#2010-11-16-074903---kbriggs4---wordstonumpy-example-doesnt-work)  
[2010-11-19 02:36:19 - bsder - Tokens *and* text that produced them](.#2010-11-19-023619---bsder---tokens-and-text-that-produced-them)  
[2010-11-20 12:43:34 - j.f.k. - Fixed width colun table (noob)](.#2010-11-20-124334---jfk---fixed-width-colun-table-noob)  
[2010-11-21 06:00:54 - Begbie00 - Returning an empty string when Optional not found.](.#2010-11-21-060054---begbie00---returning-an-empty-string-when-optional-not-found)  
[2010-11-22 07:52:21 - m3mento - avoiding partial matches?](.#2010-11-22-075221---m3mento---avoiding-partial-matches)  
[2010-11-29 11:52:29 - rick0cm - Parse Exception location](.#2010-11-29-115229---rick0cm---parse-exception-location)  
[2010-11-29 16:19:06 - jcress410 - scanstring and unused text](.#2010-11-29-161906---jcress410---scanstring-and-unused-text)  
[2010-12-02 07:49:34 - openIDTemp-624425161 - Making an array and ignoring some words](.#2010-12-02-074934---openidtemp-624425161---making-an-array-and-ignoring-some-words)  
[2010-12-09 04:09:52 - andyhhp - bug with indentedBlock](.#2010-12-09-040952---andyhhp---bug-with-indentedblock)  
[2010-12-20 15:50:58 - haidong - Pattern search with pyparsing](.#2010-12-20-155058---haidong---pattern-search-with-pyparsing)  
[2010-12-22 09:56:44 - axil - optimization in oneOf code](.#2010-12-22-095644---axil---optimization-in-oneof-code)  
[2010-12-24 20:13:24 - sskye - setWhitespaceChars() enhancement?](.#2010-12-24-201324---sskye---setwhitespacechars-enhancement)  
[2010-12-27 08:31:19 - ptmcg - Flurry of pyparsing](.#2010-12-27-083119---ptmcg---flurry-of-pyparsing)  
[2010-12-27 17:01:00 - bruceba - parsing a proprietary config language (noob)](.#2010-12-27-170100---bruceba---parsing-a-proprietary-config-language-noob)  


---
## 2010-01-01 00:28:06 - pgurumur - Parsing perl hash


    perlattr = suppress('my')
    config = Keyword('$config').suppress()
    identifier = Word(alphanums + '-' + '_')
    assign = suppress('=\>')
    listvar = self._lbk + OneOrMore(self._quoted + suppress(',')) + self._rbk
    dictionary = Forward()
    values = self._quoted | Word(nums) | listvar | dictionary
    
    item = Group(identifier + assign + values + suppress(','))      dictionary \<\< Dict(perlattr + config + suppress('=') + self._lb + \            OneOrMore(item) + self._rb)
    



I am getting an error after the first item, any idea what am I doing wrong?

#### 2010-01-01 00:29:02 - pgurumur
Just so that it is clear, self._lbk and self._rbk are nothing but [ and ]
#### 2010-01-01 08:35:01 - ptmcg
You have a couple of places where it looks like you are trying to parse a comma-delimited list of something as `OneOrMore(something + Suppress(','))`.  You are much better off using `delimitedList(something)`, since the expression you are using will require a comma even after the last item in the list.



I question your definition of dictionary as:



    dictionary \<\< Dict(perlattr + config + suppress('=') + self._lb +     
                   OneOrMore(item) + self._rb)



Does a hash that contains a hash really repeat the `&quot;my $config = {...}&quot;` for the subitem hash?  I haven't worked with Perl since the 20th century, so I may not remember this quite right.  (Perhaps you could post a sample of the source text you are trying to parse.)



I'm guessing a hash looks like `<!-- ws:start:WikiTextRawRule:0:``{a=&amp;gt;1, b=&amp;gt;&amp;quot;a string&amp;quot;, c=&amp;gt;[&amp;quot;a&amp;quot;,&amp;quot;list&amp;quot;,&amp;quot;of&amp;quot;,&amp;quot;values&amp;quot;], d=&amp;gt;{da=&amp;gt;&amp;quot;a&amp;quot;, db=&amp;gt;&amp;quot;nested&amp;quot;, dc=&amp;gt;&amp;quot;hash&amp;quot;}}`` -->{a=&gt;1, b=&gt;&quot;a string&quot;, c=&gt;[&quot;a&quot;,&quot;list&quot;,&quot;of&quot;,&quot;values&quot;], d=&gt;{da=&gt;&quot;a&quot;, db=&gt;&quot;nested&quot;, dc=&gt;&quot;hash&quot;}}<!-- ws:end:WikiTextRawRule:0 -->`.  This may not be exactly right, but I'll go along with this assumption for now.



Is listvar really restricted to only containing quoted strings?  Could it not contain any of your expressions in values?  If so, you probably want to define listvar as a Forward also, and use:



    listvar = Forward()
    dictionary = Forward()
    values = self._quoted | Word(nums) | listvar | dictionary
    listvar \<\< self._lbk + delimitedList(values) + self._rbk
    item = Group(identifier + assign + values)
    dictionary \<\< self._lb + delimitedList(item) + self._rb



Extra points for trying to use Dict, one of the more difficult pyparsing classes to get right.  Now that we have the repetition with delimitedList working, and the correct definition of item (no trailing comma), we can Dict-ify dictionary by just changing dictionary to:



    dictionary \<\< self._lb + Dict(delimitedList(item)) + self._rb





I hope some of these tips get you going again!



-- Paul
#### 2010-01-01 22:03:07 - pgurumur
Thanks paul,



Here is an example:



my $config = {

   FOO =\> 'BAR',

   TEST =\> 1,

   DICT =\> {

      test1 =\> 4,

      test2 =\> [ 1, 2, 3, 4, 5 ],

   }

   test4 =\> [1, 2, 4, 5, ],

}
#### 2010-01-02 21:57:48 - ptmcg
This gets the job done (after adding a missing ',' between test2 and test4):



    from pyparsing import *
    
    _lb,_lbk,_rb,_rbk,_cma = map(Suppress,'{[}],')
    _quoted = quotedString.setParseAction(removeQuotes)
    
    config = Keyword('$config').suppress()
    identifier = Word(alphanums + '-' + '_')
    assign = Suppress('=\>')
    
    listvar = Forward()
    dictionary = Forward()
    values = _quoted | Word(nums) | listvar | dictionary
    listvar \<\< Group(_lbk + delimitedList(values) + Optional(_cma) + _rbk)
    item = Group(identifier + assign + values)
    dictionary \<\< _lb + Dict(delimitedList(item)) + Optional(_cma) + _rb
    
    identref = Combine(Suppress('$') + identifier)
    
    assignStmt = 'my' + identref('var') + '=' + Group(values)('value')
    
    print assignStmt.parseString(hashtext).dump()
    



prints:



    ['my', 'config', '=', [['FOO', 'BAR'], ['TEST', '1'], ['DICT', ['test1', '4'], 
     ['test2', ['1', '2', '3', '4', '5']]], ['test4', ['1', '2', '4', '5']]]]
    - value: [['FOO', 'BAR'], ['TEST', '1'], ['DICT', ['test1', '4'], ['test2', 
               ['1', '2', '3', '4', '5']]], ['test4', ['1', '2', '4', '5']]]
      - DICT: [['test1', '4'], ['test2', ['1', '2', '3', '4', '5']]]
        - test1: 4
        - test2: ['1', '2', '3', '4', '5']
      - FOO: BAR
      - TEST: 1
      - test4: ['1', '2', '4', '5']
    - var: config


#### 2010-01-03 00:48:26 - pgurumur
Dude... you rock! thanks a lot.

---
## 2010-01-09 04:56:30 - alito - Traversing parsed results
I've been using pyparsing for a while now.  While I really like the description language used, I still have problems traversing the resulting parse tree.  



What I usually want to know while traversing is what type of node I'm at so as to branch to the right part of code that handles that type of node.  The combination of setResultsName and getName almost gets me there, except that getName does a bit of magic and doesn't really give me the type of node we are at, but a best-approximation. eg if it's a node with no name and only one child and the child has a name, it returns the child's name, which then leads to broken assumptions.  Would it be possible to add a function that returns the <u>name argument without any trickery, or changing </u>name to just name (ie make it public)?



Thanks

#### 2010-01-09 04:58:25 - alito
The double underscore underlined a chunk of text instead of showing up.  I was referring to the underscore underscore name attribute of the ParseResults objects.
#### 2010-01-09 05:45:03 - alito
(Might as well keep responding to myself)



To clarify, I know that the way that's done in the tutorials is to set a parse action, and use that to create an object hierarchy outside, but that seems like duplicating the structure that's created already by pyparsing.
#### 2010-01-10 07:31:49 - ptmcg
'set a parse action, and use that to create an object hierarchy outside'

That's not quite what happens in the tutorials, at least not the ones I'm thinking of.  In SimpleBool.py, for example, the use of classes as parse actions gives the pyparsing engine a set of classes that are closer to the problem domain than the default ParseResults.  But the object hierarchy that is produced is not outside the pyparsing structure, but within the generated structure.  So I don't see it as 'duplicating' so much as 'redefining' or 'reconstructing'.



-- Paul

---
## 2010-01-10 01:03:30 - voltron8 - Algebraic expressions with nonstandard operations
I need to parse and evaluate algebraic expressions with variables. Variables are simply a variable name in square brackets '[]', but some variables have an optional part after name. For example, simple expression is



    [var1] + [var2]{1} - 120

Optional part {1} is similar with power operator, '^', when variable has this part I need to make special operation with variable and number in figure brackets (number in figure brackets is 1, 2 or 3).

I look at example SimpleCalc.py and seems than is mainly what I need. But I can't understand how extend grammar to parse my expressions.

Also I try to replace {n} with @n and make a rule similar to rule with ^-operator but have no success.

Can any one help me?



Sorry for my bad English.

#### 2010-01-10 07:25:37 - ptmcg
Don't treat '{1}' as an operator, but as an optional element of your variable name.  You can give it a results name, and then test for its existence at variable evaluation time, or you can define a default value of 0 or -1 or None, and then every variable evaluation can branch off the index value to do the right thing.
#### 2010-01-10 11:18:59 - voltron8
ptmcg, thanks for help.

But as I understand this solution is work only when I have one variable with this optional part. In my task all variables can have this part, and each variable can have different or same numbers in figure brackets
#### 2010-01-10 15:16:50 - ptmcg
Here is a small version of what I mean.  Note how I expanded varref to include the optional index in braces, and supplied a default value of 0 if no braces were present.





    arith = '[var1] + [var2]{1} - 120'
    
    from pyparsing import *
    
    LBRACK,RBRACK,LBRACE,RBRACE = map(Suppress,'[]{}')
    
    integer = Combine(Optional('-') + Word(nums))
    integer.setParseAction(lambda t:int(t[0]))
    varref = Group(LBRACK + Word(alphas,alphanums)('varname') + RBRACK +
        Optional(LBRACE + integer('varindex') + RBRACE, default=0))
    
    operand = varref | integer
    arithExpr = operatorPrecedence(operand,
        [
        (oneOf('* /'), 2, opAssoc.LEFT),
        (oneOf('+ -'), 2, opAssoc.LEFT),
        ]
        )
    
    tokens = arithExpr.parseString(arith, parseAll=True)
    for t in tokens:
        if isinstance(t,basestring):
            print t
        else:
            print t.dump()
        print



prints:





    [['var1', 0], '+', ['var2', 1], '-', 120]



-- Paul

---
## 2010-01-11 11:48:28 - lcampagn - A new C parser
I've developed a C parser using pyparsing that I've been using to parse C header files so I can automate a lot of the work involved in using ctypes. I'm posting this here because I've seen a lot of demand for a pure-python C parser and I hope this project will be of use to other people. The parser currently:

 - processes all macros, typedefs, structs, unions, enums, function prototypes, and global variable declarations

 - evaluates typedefs down to their fundamental C types + pointers/arrays/function signatures

 - Allows chaining multiple headers together (manually; does not process #include directives)

 - Caches parse results, since it's pretty slow



The project is currently hosted at launchpad.net/pyclibrary. The parser can be found in CParser.py, while CLibrary.py contains code for automating the use of ctypes by reading C header definitions.

#### 2010-01-15 20:47:16 - ptmcg
Thanks for the link!  This sounds like a really useful utility!
#### 2012-06-07 07:08:43 - somanath
where is the link
#### 2012-06-07 08:31:21 - ptmcg


---
## 2010-01-15 18:17:25 - hach-que - Arithmetic expression with functions
I'm trying to interpret an arithmetic expression, except that the arithmetic expression can also contain function calls.  Those function calls themselves may also have expression in their arguments (which is the problem I'm having).



I currently have this code (slightly modified from the example to include syntax in variables).  It'll correctly handle:



func(a) * 2

func(123, b, 'test') / 6 + abc

def + func234(abc, 123, 'xyz')[4][76] + array2[5]



and pretty much any other variation, except for when there are expressions in the function arguments, like so:



func(a * 2) / 6



because the * is not included as a valid character in the variable definition.  However, if I add * to the valid character list, it then won't correctly parse this expression:



func(a * 2) * 6



because it then includes the ' * 6' as part of the function.  Is there a way I can make it so that expressions can be in function arguments (they should be left as strings and I'll rerun the expression evaluator again since it needs to do functions and everything in there) while also having the rest of the expression interpreted correctly?





    integer = Word(nums).setParseAction(lambda t:int(t[0]))
    variable = Word(alphas + '0123456789_\(., )[]\'')
    operand = integer | variable
    
    expop = Literal('^')
    signop = oneOf('+ -')
    multop = oneOf('* /')
    plusop = oneOf('+ -')
    
    expr = operatorPrecedence( operand,
    [('^', 2, opAssoc.RIGHT),
        (signop, 1, opAssoc.RIGHT),
        (multop, 2, opAssoc.LEFT),
        (plusop, 2, opAssoc.LEFT),]
    )
    
    # Use pyParsing to interpret the expression
    result = expr.parseString(expression)



#### 2010-01-15 20:25:00 - ptmcg
You can't just define a function call by expanding a variable this way, as you learn.  Instead define it as:





    funccall = variable + '(' + Group(Optional(delimitedList(expr))) + ')'
    
    operand = integer | funccall | variable



For this to work, you'll have to have defined expr as a Forward().  Then, when you assign the operatorPrecedence to expr, use '\<\<' instead of '='.



-- Paul
#### 2010-01-15 21:02:00 - hach-que
A little confused here, should I be:



<ul><li>Removing  + '0123456789_\(., )[]\'' entirely from variable?</li><li>Removing only the ( ) characters from variable?</li><li>Or not removing anything from variable?</li></ul>

At the moment I'm just getting the function part as a whole string and then parsing each character in the string manually, though I'm sure there's a better way of interpreting the function calls than that.  I'm just not good at understanding how parsers work :P
#### 2010-01-15 21:14:11 - hach-que
Okay, I removed ( and ), but in the new code (shown below) I can no longer do:



a + func(a * 8)[4]

a + func('test', 8 * 2, 'another argument') / 6

a + func[4]



In the first, I get 'pyparsing.ParseException: Expected ')' (at char 16)', presumably because that Group hasn't been told to allow [ ] on the end.



In the second, the arguments aren't parsed correctly, with ''test', 8 ' being the first parameter, '*' the second and ' 2, 'another argument'' the third.  In this case, it seems that I will also need PyParsing to handle argument seperation by comma and split the arguments as well :/



The third just reports the string as func[4] and while this is the same as before, I presume the error in #1 will mean that I will have to treat [ ] as an operator.
#### 2010-01-15 21:15:29 - hach-que
(forgot to include code in above post)





    expr = Forward()
    
    integer = Word(nums).setParseAction(lambda t:int(t[0]))
    variable = Word(alphas + '0123456789_\., []\'')
    funccall = variable + '(' + Group(Optional(delimitedList(expr))) + ')'
    
    operand = integer | funccall | variable
    
    expop = Literal('^')
    signop = oneOf('+ -')
    multop = oneOf('* /')
    plusop = oneOf('+ -')
    
    expr \<\< operatorPrecedence( operand,
    [('^', 2, opAssoc.RIGHT),
        (signop, 1, opAssoc.RIGHT),
        (multop, 2, opAssoc.LEFT),
        (plusop, 2, opAssoc.LEFT),]
    )
    
    # Use pyParsing to interpret the expression
    result = expr.parseString(expression)


#### 2010-01-15 22:15:12 - hach-que
Okay, I've got it working.  It's not so hard to modify the parser once you start to know how.





expr = Forward()



double = Word(nums + '.').setParseAction(lambda t:float(t[0]))

integer = Word(nums).setParseAction(lambda t:int(t[0]))

variable = Word(alphas)

string = dblQuotedString

funccall = Group(variable + '(' + Group(Optional(delimitedList(expr))) + ')')

array_func = Group(funccall + '[' + Group(delimitedList(expr, '][')) + ']')

array_var = Group(variable + '[' + Group(delimitedList(expr, '][')) + ']')



operand = double | string | array_func | funccall | array_var | variable



expop = Literal('^')

signop = oneOf('+ -')

multop = oneOf('* /')

plusop = oneOf('+ -')



expr \<\< operatorPrecedence( operand,

[('^', 2, opAssoc.RIGHT),

    (signop, 1, opAssoc.RIGHT),

    (multop, 2, opAssoc.LEFT),

    (plusop, 2, opAssoc.LEFT),]

)



<ol><li>Use pyParsing to interpret the expression</li></ol>result = expr.parseString(expression)


#### 2010-01-15 23:14:07 - ptmcg
Yes, you are starting to get it.  You have to start looking for the structure in your input, and not just try to pick out the characters just because they are all together in a row.

---
## 2010-01-16 01:40:21 - hach-que - Speeding parsing up
I currently have this parsing code:





    expr = Forward()
    
    double = Word(nums + '.').setParseAction(lambda t:float(t[0]))
    integer = Word(nums).setParseAction(lambda t:int(t[0]))
    variable = Word(alphas)
    string = dblQuotedString
    funccall = Group(variable + '(' + Group(Optional(delimitedList(expr))) + ')')
    array_func = Group(funccall + '[' + Group(delimitedList(expr, '][')) + ']')
    array_var = Group(variable + '[' + Group(delimitedList(expr, '][')) + ']')
    
    operand = double | string | array_func | funccall | array_var | variable
    
    expop = Literal('^')
    signop = oneOf('+ -')
    multop = oneOf('* /')
    plusop = oneOf('+ -')
    
    expr \<\< operatorPrecedence( operand,
    [('^', 2, opAssoc.RIGHT),
        (signop, 1, opAssoc.RIGHT),
        (multop, 2, opAssoc.LEFT),
        (plusop, 2, opAssoc.LEFT),]
    )
    
    # Use pyParsing to interpret the expression
    result = expr.parseString(expression)



When it parses 'print(func(123) + abc(456))' it takes on average around 4 seconds which is fairly slow.  Is there a way to speed this up (to under 1/10th of a second)?

#### 2010-01-16 19:14:14 - hach-que
My code is now (there's a few Group()s added so that the ParseResults list is easier to sort through):





    expr = Forward()
    
    double = Word(nums + '.').setParseAction(lambda t:float(t[0]))
    integer = Word(nums).setParseAction(lambda t:int(t[0]))
    variable = Word(alphas)
    string = dblQuotedString
    funccall = Group(variable + '(' + Group(Optional(delimitedList(expr))) + ')')
    array_func = Group(funccall + '[' + Group(delimitedList(expr, '][')) + ']')
    array_var = Group(variable + '[' + Group(delimitedList(expr, '][')) + ']')
    
    operand = double | string | array_func | funccall | array_var | variable
    
    expop = Literal('^')
    signop = oneOf('+ -')
    multop = oneOf('* /')
    plusop = oneOf('+ -')
    classop = Literal('.')
    
    expr \<\< operatorPrecedence( operand,
    [(classop, 2, opAssoc.LEFT),
        ('^', 2, opAssoc.RIGHT),
        (signop, 1, opAssoc.RIGHT),
        (multop, 2, opAssoc.LEFT),
        (plusop, 2, opAssoc.LEFT),]
    )
    
    # Use pyParsing to interpret the expression
    result = expr.parseString(expression)



Parsing the string:



print(b.func())



takes around 30 seconds.  Parsing the string:



print(b.func(a.newObj()))



takes over 10 minutes (I don't know exactly how long because I didn't let it run any longer).  There must be some way to considerably speed this parsing up.
#### 2010-01-17 02:23:07 - hach-que
Packrat parsing solved my speed issue.  print(b.func(a.newObj())) now parses in less than a second.
#### 2010-01-17 12:41:33 - ptmcg
Ah, good to hear it!  Packratting is not always a cureall, but it is worth trying, especially with deep operatorPrecedence lists.

---
## 2010-01-17 04:59:52 - voltron8 - Variables and functions
I try to create an calculator with support of variables and functions. Variable names are in square brackets because this name can contain numbers, alphas, and minus ('-') and underscore ('_'). For example [var], [1999-10-12], [11], [var-1], [var_1] 

I need an arifmethics operations (+-*/^@) and some functions (sin, asin, cos...).

Symbol '@' is used as nonstandart operator which applied only for variables. This is similar with power (^) operator: after '@' must be a integer number. Example: [var]@1, [var]@10



Here is my parser.



    # define grammar
    point = Literal( '.' )
    e = CaselessLiteral( 'E' )
    plusorminus = Literal( '+' ) | Literal( '-' )
    number = Word( nums ) 
    integer = Combine( Optional( plusorminus ) + number )
    floatnumber = Combine( integer +
                           Optional( point + Optional( number ) ) +
                           Optional( e + integer )
                         )
    
    ident = Word( '[' + alphas, alphanums + '_-' + ']' )
    
    plus  = Literal( '+' )
    minus = Literal( '-' )
    mult  = Literal( '*' )
    div   = Literal( '/' )
    lpar  = Literal( '(' ).suppress()
    rpar  = Literal( ')' ).suppress()
    addop  = plus | minus
    multop = mult | div
    expop = Literal( '^' )
    assign = Literal( '=' )
    band = Literal( '@' )
    
    expr = Forward()
    atom = ( ( e | floatnumber | integer | ident.setParseAction( assignVar ) ).setParseAction(pushFirst) | 
             ( lpar + expr.suppress() + rpar )
           )
    
    factor = Forward()
    factor \<\< atom + ( ( band + factor ).setParseAction( pushFirst ) | ZeroOrMore( ( expop + factor ).setParseAction( pushFirst ) ) )
    
    term = factor + ZeroOrMore( ( multop + factor ).setParseAction( pushFirst ) )
    expr \<\< term + ZeroOrMore( ( addop + term ).setParseAction( pushFirst ) )
    bnf = expr
    
    pattern =  bnf + StringEnd()
    [[/code]]
    Now it can parse expressions without functions, when I try to add an function support - it fails.

fn = Word( alphas )

atom = ( ( e | floatnumber | integer | ident | ( fn + lpar + expr + rpar ) ).setParseAction(pushFirst) | 

         ( lpar + expr.suppress() + rpar )

       )



Where is my mistake?

Thanks

#### 2010-01-17 12:39:58 - ptmcg
Your ident definition does not properly parse for your desired format.  Compare what you have:



    ident = Word( '[' + alphas, alphanums + '_-' + ']' )

with this:



    ident = '[' + Word(alphas, alphanums + '_-') + ']'



Your original ident expression merely adds '[' to the list of allowed initial characters, and ']' to the list of allowed trailing characters.  What you really want is to *require* a leading '[' and a trailing ']', which is what the second expression does.  You could further narrow this definition to:



    ident = Combine('[' + Word(alphas, alphanums + '_-') + ']')

depending on whether you want to accept '[ a ]' as equally valid as '[a]' (Combine does not permit internal whitespace, and will return the matched text as a single string).
#### 2010-01-17 22:44:22 - voltron8
Thanks, now my understanding was better.



Can you explain me how I can add functions definition?



I try this



fn = Word( alphas )

atom = ( ( e | floatnumber | integer | ident | ( fn + lpar + expr + rpar ) ).setParseAction(pushFirst) |

( lpar + expr.suppress() + rpar )

)



but when run parseString I've get an error
#### 2010-01-18 06:16:08 - ptmcg
Please see this version of your program: 



-- Paul
#### 2010-01-18 08:56:53 - voltron8
Thanks a lot, it's work! If you interested I can give a link to the project, that uses this piece of code. This is raster calculator for GIS

---
## 2010-01-20 00:09:40 - hach-que - Combining results
I have the following code:





    def isAssignmentExpression(self, expression):
        primaryExpr = Forward()
        secondaryExpr = Forward()
    
        double = Word(nums + '.').setParseAction(lambda t:float(t[0]))
        integer = Word(nums).setParseAction(lambda t:int(t[0]))
        variable = Word(alphanums + '_')
        string = dblQuotedString
        funccall = Group(variable + '(' + Group(Optional(delimitedList(secondaryExpr))) + ')')
        array_func = Group(funccall + '[' + Group(delimitedList(secondaryExpr, '][')) + ']')
        array_var = Group(variable + '[' + Group(delimitedList(secondaryExpr, '][')) + ']')
        side = Combine(secondaryExpr, adjacent=False)
    
        primaryOperand = side
        secondaryOperand = double | string | array_func | funccall | array_var | variable
    
        equalityop = oneOf('== \<= \>= \< \> !=')
        assignmentop = Literal('=')
        expop = Literal('^')
        signop = oneOf('+ -')
        multop = oneOf('* /')
        plusop = oneOf('+ -')
        classop = Literal('.')
    
        # Secondary expressions (that is, inside brackets)
        # can use all of the operators.
        secondaryExpr \<\< operatorPrecedence( secondaryOperand,
        [(equalityop, 2, opAssoc.LEFT),
            (classop, 2, opAssoc.LEFT),
            ('^', 2, opAssoc.RIGHT),
            (signop, 1, opAssoc.RIGHT),
            (multop, 2, opAssoc.LEFT),
            (plusop, 2, opAssoc.LEFT),]
        )
    
        # The primary expression can only have the
        # class property operator
        primaryExpr \<\< operatorPrecedence( primaryOperand,
        [(assignmentop, 2, opAssoc.LEFT)]
        )
    
        # Use pyParsing to interpret the expression
        try:
            result = primaryExpr.parseString(expression)
            print result
        except ParseException as e:
            error = str(e)
            self.fatalError(expression, 'Unable to parse line.  Exact error: \n  ' + error)



The problem is that the results come back as:



['print(obj.array[1.0obj.getIndex()])']



if the arrays are involved because Combine doesn't seem to take notice of the delimiter specified in delimitedList.



I'm not sure if there's a better way of doing what I've written above, but I'm trying to seperate the left and right hand sides of an assignment expression by the = operator.  I used to use partition(), but this doesn't work properly if there's a string with an = sign in the middle of it.



Is there a way to fix the delimitedList problem or a better way to write the code above so that it doesn't matter?

#### 2010-01-20 03:10:04 - hach-que
I managed to fix it by changing the definition of array_func to:





    array_func = funccall + '[' + Group(delimitedList(expr, '][')) + ']'


#### 2010-01-20 03:12:23 - hach-que
Oops, wrong solution there :P



The correct solution it to add ', True' to the arguments list of delimitedList, which causes it to combine the string (so it only matches, it doesn't split).

---
## 2010-01-21 07:36:32 - pmarichal - Can't parse my grammar, please help me
Hi all,



TO make it short, I have several files that have this kind of layout:





    Z2-ZZ     LABEL     Update ZZ
    Z2-ZZ     DOMAIN    TM
    Z2-ZZ-01  LABEL     Task id 129
    Z2-ZZ-01  SKEL      RASSA00
    Z2-ZZ-01  PREREQ    Z2-TM
    Z2-ZZ-02  LABEL     Task id 654a(+3) /* this is a comment
    Z2-ZZ-02  PREREQ    Z2-ZZ-01



I need to trap several information, the main one being the label for each services (a service is e.g. Z2-ZZ-02) and the prerequisite (Z2-ZZ-02  PREREQ    Z2-ZZ-01 : means that the service Z2-ZZ-02 has Z2-ZZ-01 as a prerequisite.



Here is the grammar I use (or part of it anyway):







    <span class="kw1">from</span> pyparsing <span class="kw1">import</span> alphanums<span class="sy0">,</span> nums<span class="sy0">,</span> printables
    <span class="kw1">from</span> pyparsing <span class="kw1">import</span> Word<span class="sy0">,</span> Combine<span class="sy0">,</span> OneOrMore<span class="sy0">,</span> Keyword<span class="sy0">,</span> Optional<span class="sy0">,</span> Literal<span class="sy0">,</span> Group<span class="sy0">,</span> LineEnd<span class="sy0">,</span> LineStart
    <span class="kw1">from</span> pyparsing <span class="kw1">import</span> restOfLine<span class="sy0">,</span> Suppress
    
    
    <span class="co1"># basic grammar</span>
    alphaupper <span class="sy0">=</span> <span class="st0">'ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'</span>
    
    service_name <span class="sy0">=</span> Word<span class="br0">&#40;</span>alphaupper<span class="sy0">,</span> exact<span class="sy0">=</span><span class="nu0">2</span><span class="br0">&#41;</span> <span class="co1"># ZZ</span>
    sub_service_name <span class="sy0">=</span> Combine<span class="br0">&#40;</span>service_name + <span class="st0">'-'</span> + service_name<span class="br0">&#41;</span> <span class="co1"># Z2-ZZ</span>
    subsub_service_name <span class="sy0">=</span> Combine<span class="br0">&#40;</span>service_name + <span class="st0">'-'</span> + service_name + <span class="st0">'-'</span> + service_name<span class="br0">&#41;</span> <span class="co1"># Z2-ZZ-01</span>
    service <span class="sy0">=</span> service_name ^ sub_service_name ^ subsub_service_name <span class="co1"># </span>
    services <span class="sy0">=</span> OneOrMore<span class="br0">&#40;</span>service<span class="br0">&#41;</span>
    
    comment <span class="sy0">=</span> <span class="st0">'/*'</span> + restOfLine <span class="co1"># /* comment</span>
    
    <span class="co1"># Label</span>
    label <span class="sy0">=</span> Keyword<span class="br0">&#40;</span><span class="st0">'LABEL'</span><span class="br0">&#41;</span>.<span class="me1">suppress</span><span class="br0">&#40;</span><span class="br0">&#41;</span> + Combine<span class="br0">&#40;</span>OneOrMore<span class="br0">&#40;</span>Word<span class="br0">&#40;</span>printables<span class="br0">&#41;</span><span class="br0">&#41;</span><span class="br0">&#41;</span> + Optional<span class="br0">&#40;</span>comment<span class="br0">&#41;</span>.<span class="me1">suppress</span><span class="br0">&#40;</span><span class="br0">&#41;</span>
    
    label_def <span class="sy0">=</span> service + label <span class="co1"># Z2-ZZ-02  LABEL     Task id 654a(+3) /* this is a comment</span>



When I do the following:





    data <span class="sy0">=</span> <span class="st0">'''
    Z2-TM-22  LABEL     RTD (+900)
    Z2-TM-22  PREREQ    Z2-TM-21+900
    Z2-TM-23  LABEL     RTD (+30 +9)  /* comment, please discard
    Z2-TM-23  PREREQ    Z2-TM-21+30 Z2-TM-22+9
    '''</span>
    <span class="kw1">print</span> <span class="kw2">list</span><span class="br0">&#40;</span>label_def.<span class="me1">scanString</span><span class="br0">&#40;</span>data<span class="br0">&#41;</span><span class="br0">&#41;</span>



I have this as a result:





    
    [ ((['Z2-TM-22', 'RTD'], {}), 0, 24), 
      ((['Z2-TM-23', 'RTD'], {}), 64, 88)]
    



And the label only capture the first word.



And now, I can't see what is happening.



Can someone help me ?

Many thanks in advance.



Phil.

#### 2010-01-21 10:04:08 - ptmcg
In the interests of getting you a quick response, I suggest you *not* use pyparsing for this task.  Your input data is so well-formatted, you can do everything you need with just a vanilla split() command:



    records = [ d.split(None,2) for d in data.splitlines() ]
    
    for r in records:
        print r



prints



    ['Z2-ZZ', 'LABEL', 'Update ZZ']
    ['Z2-ZZ', 'DOMAIN', 'TM']
    ['Z2-ZZ-01', 'LABEL', 'Task id 129']
    ['Z2-ZZ-01', 'SKEL', 'RASSA00']
    ['Z2-ZZ-01', 'PREREQ', 'Z2-TM']
    ['Z2-ZZ-02', 'LABEL', 'Task id 654a(+3) /* this is a comment']
    ['Z2-ZZ-02', 'PREREQ', 'Z2-ZZ-01']



If you absolutely want to use pyparsing (perhaps this is a 'getting your feet wet' exercise), I'll look at this further.



-- Paul
#### 2010-01-21 23:57:04 - pmarichal
Indeed, this is my 'toy' side project here at work to warm up my pyparsing skills.



Anyway, the actual final grammar is much more complete and complex than that and I was just stumbling on this first bit and wanted to see what my mistake was.



So any help would be appreciated.



Phil.
#### 2010-01-22 02:14:03 - ptmcg
Try this:





    # Label
    label = Keyword('LABEL').suppress() + Combine(OneOrMore(~comment + ~LineEnd() + Word(printables)),' ', adjacent=False) + Optional(comment).suppress() 
    label_def = service + label # Z2-ZZ-02  LABEL     Task id 654a(+3) /* this is a comment
    
    # Prereq
    prereq = Combine(service + Optional('+' + Word(nums)))
    prereq_def = service + Suppress('PREREQ') + Group(OneOrMore(~LineEnd() + prereq))
    
    data = '''
    Z2-TM-22  LABEL     RTD (+900)
    Z2-TM-22  PREREQ    Z2-TM-21+900
    Z2-TM-23  LABEL     RTD (+30 +9)  /* comment, please discard
    Z2-TM-23  PREREQ    Z2-TM-21+30 Z2-TM-22+9
    '''
    print [t.asList() for t,_,_ in label_def.scanString(data)]
    print [t.asList() for t,_,_ in prereq_def.scanString(data)]
    


#### 2010-01-28 00:09:19 - pmarichal
Sorry I didn't see your post until today (did not received a notification from wikispaces, odd). 



I'll try that right away and I'll update as soon as I've tested this.



Many thanks anyway.



Phil.

---
## 2010-01-22 16:02:08 - Hory - Stumped by recursive elements
Hi,

pyparsing is a very creative library, like a wonder of programming, but as I was working on a parser for natural language (English), I stumbled upon a problem when parsing clauses with subordinate clauses, such as 'I like to drink water'.



I have a grammar which can be simplified to something like this:



subject = personal_pronoun | noun

predicator = verb

object = preposition + (noun | clause)

clause = Optional(subject) + predicator + Optional(object)



The problem is that the 'object' can be represented not just by a 'noun' but also a 'clause' - which is in this case defined after it. These rules are correct, however, according to the English grammar. Can this limitation be avoided?

#### 2010-01-22 17:02:32 - ptmcg
You can define a recursive element using pyparsing's Forward class.  First, define the expression using an empty Forward:



    clause = Forward()



Then use the expression to build up other expressions:



    object = preposition + (noun | clause)



Finally, insert the expression into the created Forward:



    clause \<\< (Optional(subject) + predicator + Optional(object))



It is important that you *not* use '=' assignment here, as that would rebind the 'clause' variable to a different expression, and you would not achieve your recursive grammar.



That said, I must say, parsing English with pyparsing is a very ambitious goal.  The closest I have come is to define a very narrow set of grammar constructs, such as these from the adventure game example.



    dropCommand = dropVerb + itemRef('item')
    takeCommand = takeVerb + itemRef('item')
    useCommand = useVerb + itemRef('usedObj') + \
        Optional(oneOf('IN ON',caseless=True)) + \
        Optional(itemRef,default=None)('targetObj')
    readCommand = readVerb + itemRef('subjectObj')
    openCommand = openVerb + itemRef('item')
    moveCommand = moveVerb + moveDirection('direction')



If you really want to parse a broad set of English inputs, I would steer you toward the Natural Language Toolkit (NLTK) instead.  You will go crazy trying to disambiguate all of the various constructs that English allows.
#### 2010-01-23 03:02:39 - Hory
Thanks for the good news, Paul. Sorry for missing on Forward in the HowTo.



You're right about parsing English, I will be using pyparsing for a game engine which, similar to your adventure game example, has a limited number of nouns and verbs, tho I did want subclause understanding capabilities. It would allow for commands such as: 'order cashier to give me the money' or 'instruct bodyguard to attack anyone who approaches me'. More flexible input leads to higher interactivity, and there's no interface as flexible as natural language.



After great effort to understand the algorithms, I had actually implemented a bottom-up, shift-reduce parser (LALR(1)), but tho arguably more powerful when functioning, it didn't have the elegance of pyparsing. But it looks like pyparsing is a feasible alternative.
#### 2010-01-24 19:11:18 - ptmcg
This takes me back to my 7th grade English teacher who taught us sentence diagramming.  I suggest you use a similar method to map out the types of sentences and vocabulary you want to support.  Then you can implement that as a BNF.  Good luck!

---
## 2010-01-26 17:01:33 - filter5 - difficulty with nestedExpr
Hi, I'm trying to do something for the first time with pyparsing and am running into some difficulty.



Essentialy I have a structure like this:



group1 (

    color = (blue)

#### 2010-01-26 17:09:07 - filter5
sorry accidentaly submitted too soon!



group1 (

    color=(blue)

    title=('main')

    group2 (

        color=(red)

        title=('sub')

    )

)



I'd like to get back something like

['group 1 color=(blue) title=('main')', [group 2  color=(red) title=('sub')]]



Any thoughts? Thanks!
#### 2010-01-29 00:23:58 - ptmcg
nestedExpr is really targeted at very loosely structured data that happens to contain some nesting with ()'s or {}'s or similar delimiters.  Your format has much more structure to it, allowing a better defined parser approach.  (I took the liberty of inserting '=' after group1 and group2, to make your format more self-consistent.)



    
    data = '''group1=(
    color=(blue)
    title=('main')
    group2=(
    color=(red)
    title=('sub')
    )
    )'''
    
    from pyparsing import *
    
    LPAR,RPAR,EQ = map(Suppress,'()=')
    ident = Word(alphas,alphanums).setName('identifier')
    value = Forward()
    valueDef = Group(ident + EQ + value)
    compound = Group(OneOrMore(valueDef))
    
    value \<\< LPAR + (compound | Word(alphas) | QuotedString(''')) + RPAR
    
    from pprint import pprint
    pprint( valueDef.parseString(data).asList() )



prints:



    
    [['group1',
      [['color', 'blue'],
       ['title', 'main'],
       ['group2', [['color', 'red'], ['title', 'sub']]]]]]
    



This also discards those extraneous '=' tokens, and gives you back a hierarchical structure.



Lastly, with just a few minor insertions of the pyparsing Dict class, you can turn your parser into a deserializer.



Change compound to:



    compound = Dict(OneOrMore(valueDef))



And use this code to extract your data object:



    res = Dict(valueDef).parseString(data)



This gives you a ParseResults object that will give you access to your data fields as if they were attributes of a Python object:



    print res.dump()
    
    print res.keys()
    print res.group1.keys()
    
    print res.group1.title

prints



    [['group1', ['color', 'blue'], ['title', 'main'], 
     ['group2', ['color', 'red'], ['title', 'sub']]]]
    - group1: [['color', 'blue'], ['title', 'main'], ['group2', ['color', 'red'], ['title', 'sub']]]
      - color: blue
      - group2: [['color', 'red'], ['title', 'sub']]
        - color: red
        - title: sub
      - title: main
    ['group1']
    ['color', 'title', 'group2']
    main



If you have an item that would clash with a Python keyword, then you'll have to use dict-style access:



    print res['group1']['title']



HTH,

-- Paul
#### 2010-01-29 00:28:12 - ptmcg
I'm sorry if I jumped straight into the mode of just giving you the answer, in place of letting you have a more extended self-learning experience - your parser uses a recursive grammar, which is a little advanced for a pyparsing first-timer, and I didn't want to leave you hanging until such time as I had time to write up a more detailed or tutorial explanation.  You can read more about a similar parser that I wrote for JSON parsing in the August 2008 issue of Python magazine.
#### 2010-01-29 00:30:21 - ptmcg
You can find the complete copy-pasteable example here: .



-- Paul
#### 2010-01-29 18:20:53 - filter5
Paul, thanks so much for taking the time to help, this really clarified several things.



As for self-learning, not to worry, the actual data I'm looking at is much more complicated, but I'm getting through it by breaking things down step by step.



Python Magazine is down for the moment, but I'll pick up that issue when it comes back.

---
## 2010-01-28 04:08:27 - blundeln - Extend pyparsing for bi-directional use
Hi Paul,



Firstly, pyparsing is an excellent piece of work, and as the result of such fine integration with the python language, it is extremely flexible.



You may or may not be aware of a tool called Augeas (a Red Hat project) which allows the grammars of various config files to be defined in a bi-directional language, such that settings can be changed in the parsed model and then automatically reflected back in the original file.  The nice thing is, that minimal changes are made to the original files, so comments etc. are preserved.



So, to get the point of my message: I'm doing a few experiments with the aim of trying to adapt pyparsing to work also as a bi-directional parser, I feel this will give greater flexibilty over a tool such as Augeas, and will likely allow for some simplification of lens (i.e. a bidirectional grammar) definition over that of Augeas.



See augeas here: 

And the theory here: 



Are you interested in this idea? and do you have any thoughts on it (e.g. potential difficulties)?



I look forward to hearing from you,



Nick Blundell

()

#### 2010-01-28 23:49:40 - ptmcg
Very interesting.  I could envision this implemented either using the DSL approach I describe in the April, 2009 Python Magazine (to generate both directions of the lens in separate pyparsing parser/mappers), or as a set of wrapper classes to augment ordinary Python pyparsing constructs.  Mapping of Literals would be simple enough, but for any expression that can map a variable text, you would need to specify a default value to be used when creating an input to correspond to a newly inserted output.  You would essentially create a matched pair of pyparsing parsers, with clear mapping/correspondence of input element to output element.



I could see something like this code to implement the composer example from the paper:



    COMMA = Literal(',')
    composer = OneOrMore(Word(alphas))
    dates = Word(nums) + '-' + Word(nums)
    nationality = Word(alphas)
    
    inputRec = composer + COMMA + dates + COMMA + nationality
    outputRec = composer + COMMA + nationality
    
    dates.defaultValue = '0000-0000'
    lens = Lens(inputRec, outputRec)
    lens.key = composer



Because the same expressions were used to build up the inputRec and outputRec, the Lens class could use identity matching to find the pieces that are supposed to correspond to each other.



It would be up to the Lens class to match the expressions in the input and output rec, and then to support something like:



    source = '''\
    Jean Sibelius, 1865-1957, Finnish
    Aaron Copland, 1910-1990, American
    Benjamin Britten, 1913-1976, British'''
    
    filtered = lens.enparse(source)
    filtered = filtered + '\nAlexandre Tansman, Polish'
    source = lens.deparse(filtered)



Or if you didn't like having to assign back into source, then create two LensString objects, that would perhaps look like:





    source,filtered = LensString(), LensString()
    
    # not only is the Lens created, but the two lens objects 
    # know which side of the bidirectional parse they correspond to
    lens = Lens(source, filtered)
    
    # body is a property of LensString that sets the value, but 
    # also invokes the Lens parser/deparser
    source.body = 'Jean Sibelius, etc.'
    
    # extract the filtered list of composers, without dates
    print filtered.body
    
    # add a new composer - body property set method invokes 
    # lens's deparser
    filtered.body += '\nAlexandre Tansman, Polish'
    
    print source.body


#### 2010-01-29 01:36:40 - blundeln
Hi Paul,



Your ideas are very interesting and I feel are well worth us exploring.



Actually, though, the example of the composers given in that paper does not truely represent the power of the lens idea, since the most useful application of a lens is to map a source file into a meta model (e.g. say a nested list/dict structure, or a collection of composer objects with variable attributes that can be manipulated) rather than to another string that omits or adds certain elements.  We could think of this a bit like a data object model of structures represented in the source string, much like we now see lots of abstract database modelling frameworks that hide low-level manipulation of SQL from the programmer and let them work on the objects instead (e.g. iteration, deletion of records, alter attributes).



So, often we would like to have a single grammar to define the particular source file/string, but mark it up with mappings to and from the meta model.  This is a good example from the Augeas site, which really explains the essance of the idea: 



I'm still trying to get my head around the pyparsing source at the moment.  I so far get the gist of it, but I've been prodding a few things to see what triggers certain conditions, etc.  What I'm looking for at the moment is a way to print every character (including whitespace) that is parsed, which I can eventually add hooks into so that, when re-writing the source, I'm able to print the original chars or instead modified chars from the meta model.



Just to make that clearer for other readers not familiar with the lens idea: when we parse the file we use the grammar to map parts of the source that we are interested in into some python structure, such as a nested list (e.g. often we skip whitespace, comments, brackets, list delimeters, etc.); now, when we wish to update the source based on a modified parsed structure, we must parse the original file again with the same grammer, but this time outputing what we parse, such as whitespace and all the other things that need preserving but which are not captured in our ealier parsed structure; then the trick is, not blindly to print out those parts in the original string that map to elements of our parsed structure but to replace them with the new modified values, rather like filling the spaces.



Here is my first stab at the syntax, though I stress it is incomplete and I do not think it is the best way (I'm just trying to spark off some other better ideas in my head and other people's), especially since in pyparsing we have nice python objects representing elements, so there are a lot of possibilities for augmenting the grammer, as some of your examples in the previous post show.



The idea is that the elemtent Node() creates a new container object in the parsed tree (like the Augeas square brackets), Store() appends a value to that container (and therefore maps the current value back to the source when writing), and Key() sets the key of the current container to the matched expression, and is therefore used to avoid re-ordering issues when writing back to the source file.





    <span class="co1"># define grammar</span>
    app_entry <span class="sy0">=</span>  Combine<span class="br0">&#40;</span>Word<span class="br0">&#40;</span>alphas<span class="sy0">,</span>exact<span class="sy0">=</span><span class="nu0">1</span><span class="br0">&#41;</span> + Word<span class="br0">&#40;</span>alphanums + <span class="st0">' []()-.\<\>='</span><span class="br0">&#41;</span><span class="br0">&#41;</span>
    app_list <span class="sy0">=</span> Store<span class="br0">&#40;</span>delimitedList<span class="br0">&#40;</span>app_entry<span class="br0">&#41;</span><span class="br0">&#41;</span>
    field_entry <span class="sy0">=</span> Node<span class="br0">&#40;</span> Key<span class="br0">&#40;</span>alphas+<span class="st0">'-'</span><span class="br0">&#41;</span> + <span class="st0">':'</span> + app_list <span class="br0">&#41;</span>
    grammar <span class="sy0">=</span> OneOrMore<span class="br0">&#40;</span>field_entry<span class="br0">&#41;</span>
    
    SAMPLE_TO_BE_PARSED <span class="sy0">=</span> <span class="st0">'''
    Build-Depends-Indep: perl (\>= 5.8.8-12), libcarp-assert-more-perl,
                         libconfig-tiny-perl, libexception-class-perl,
                         libparse-recdescent-perl (\>= 1.90.0),
                         liblog-log4perl-perl (\>= 1.11)
    
                         '''</span>



Appologies if the source formatting was a bit awry - I wasn't sure how it would turn out in the editor.



I'll continue to look at these ideas and I look forward to further discussions with you,



Nick
#### 2010-02-04 04:01:12 - blundeln
Hi Paul,



I've been doing some feasibility testing of this idea and have added some trivial unparsing functionality to pyparsing.  Admittedly, it's trivial at present because you could currently get the same result by flattening and joining the parsed tokens.  But the important point is that it gives a base for building some rich bidirectional parsing funcitonality, since it demonstrates the grammar running in reverse.



The major alterations to pyparsing required to create a functioning bi-directional grammar I see are as follows:



<ul><li>Add an unparse() function to all elements (or base classes), which is the reciprocal of their parseImpl().  It will take a set of tokens, and unparse them according to the grammar, inserting default values for tokens that were not stored in the parsed results (e.g. white space).  If it fails to unparse, then it will raise some sort of exception (e.g. UnparseException).</li></ul>

<ul><li>Add a link from each parsed token to the element that parsed it.  This will give us a lot of flexibility when matching up tokens to the grammar when unparsing, allowing us to insert default text when neccessary and have tokens sets be more sophisticated than simple lists (e.g. dicts, or objects).</li></ul>

<ul><li>You cannot ignore any of the input when bi-parsing since you need to recreate the source, so must explicity capture tokens such as whitespace.  Though the beauty of pyparsing is that we could programmatically introduce White() tokens in the grammer between more interesting tokens, so the grammars would still look uncluttered.</li></ul>

I've made a very rough feasibilty study of this and uploaded it at the following URL.  Simply uncompress it and run 'python test.py' then have a look at the source.







Applogies for this message being a bit rushed - I have to shoot off soon.



I look forward to your comments,



Nick

---
## 2010-01-30 11:08:45 - filter5 - dynamic xml tags?
Does pyparsing have any mechanism to dynamically name xml tags? For example



[[pair = tag.suppress() + val.setResultsName(tagName)]]



where tagName is updated from tag.setParseAction(...)

My problem is that setResultsName() is only evaluated once, is there any way to force it to update for each match?



Thanks!

#### 2010-01-30 15:56:54 - ptmcg
You can assign named results in a parse action instead.  Something like:



    from pyparsing import *
    
    seqNumber = 0
    def setSequentialName(tokens):
        global seqNumber
        seqName = 'item_%02d' % seqNumber
        seqNumber += 1
        tokens[seqName] = tokens[0]
    
    tag = Literal('\<A\>')
    val = Word(alphas)
    pair = tag.suppress() + val.setParseAction(setSequentialName)
    
    data = '''\
        \<A\>red
        \<A\>green
        \<A\>blue'''
    
    results = OneOrMore(pair).parseString(data)
    print results.dump()

prints



    ['red', 'green', 'blue']
    - item_00: red
    - item_01: green
    - item_02: blue



Will that help?
#### 2010-01-30 18:05:47 - filter5
It most certainly will.



Thanks again

---
## 2010-02-01 08:40:38 - nemith - storing the original text along with nested parsed values
Is there a way to store the original text along with the parsed values.  



For example I have this user block





    #-----------------------------------------------------------------------------
    
    Name          :    N5233928
    
    Password      :    0x0020 b5 30 22 22 6f f8 9b 99 71 cb da f5 a3 71 eb 5d 2e b1 50 6e 8c b2 8d 00 2a 7e 70 39 63 d8 17 e4 
    
    Chap password :    0x0020 b5 30 22 22 6f f8 9b 99 02 aa 81 d1 81 55 e4 61 ac ce 44 dd 57 1e 43 63 82 f3 52 13 78 5e 41 53 
    
    State         :    0
    
    S_flags       :    0
    
    Aging policy  :    group115
    
    Good count    :    0
    
    Warning count :    0
    
    Change count  :    0
    
    Last change Lo:    170663072
    
    Last change Hi:    29932368
    
    Last auth Lo  :    0
    
    Last auth Hi  :    0
    
    Rights        :    1
    
    Type          :    2560
    
    EnableType    :    4
    
    Status        :    1
    
    Reset         :    1
    
    Expiry        :    171    108    1    4364    0    5
    
    MaxSession    :    65535
    
    MaxSess2      :    0
    
    Profile       :    115
    
    LogonHrs      :    0x0016 00 ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff 
    
    Alias         :    0    
    
    Value Flags   :    524336
    
    CounterVals_00:    0    0    0    0
    
    CounterRst_00 :    0     0
    
    CounterVals_01:    0    0    0    0
    
    CounterRst_01 :    0     0
    
    ##--- User End
    
    App00    USER_DEFINED_FIELD_0    STRING    Karla Isabel Velazquez
    
    App00    USER_DEFINED_FIELD_1    STRING    
    
    App00    USER_DEFINED_FIELD_2    STRING    GL-1041
    
    App00    IP_ACS_POOLS_LENGTH    INTEGER    1
    
    App00    IP_ACS_POOLS    STRING    
    
    App00    IP_ALLOCATION_METHOD    INTEGER    5
    
    App00    IP_STATIC_ADDR_LENGTH    INTEGER    1
    
    App00    IP_STATIC_ADDR    STRING    
    
    App00    IP_NAS_POOL_LENGTH    INTEGER    1
    
    App00    IP_NAS_POOL    STRING    
    
    App00    user_callback_type    INTEGER    0
    
    App00    user_callback    STRING    
    
    App00    disp_callback    STRING    
    
    App01    Filters\NAS\records    MSTRING    
    
    App01    Filters\NAS\enabled    STRING    0
    
    App01    Filters\NAS\option    STRING    
    
    App01    Filters\Dialup\records    MSTRING    
    
    App01    Filters\Dialup\enabled    STRING    0
    
    App01    Filters\Dialup\option    STRING    
    
    App01    max_priv    STRING    0,0
    
    App01    max_priv_LENGTH    INTEGER    3
    
    App01    PROFILE    STRING    {default = deny default service = deny default cmd = ignore default attribute = deny}===={shell{3}{}{}}
    
    ##--- Values End



I have pyparsing working to parse out all variables and all the values but I require to save the original raw data for that user.  Is that possible?



Here is the code I have so far



    ParserElement.setDefaultWhitespaceChars(' \t\r\n')
    
            COLON = Literal(':').suppress()
            SPACE = Literal(' ').suppress()
            EOL = LineEnd().suppress()
    
            userStart = Word('#','-').suppress()
            userEnd = Literal('##--- User End').suppress()
            valuesEnd = Literal('##--- Values End').suppress()
            dumpEnd = Literal('#End Of Dump').suppress()
    
            def stripWhitespace(val):
                return val[0].strip()
    
            def setFieldNames(tokens):
                ret = ParseResults([])
                for valueline in tokens:
                    if not valueline.key in ret:
                        ret[valueline.key] = ParseResults([])
                    ret[valueline.key][valueline.fieldname] = ParseResults([])
                    ret[valueline.key][valueline.fieldname]['fieldname'] = valueline.fieldname
                    ret[valueline.key][valueline.fieldname]['type'] = valueline.type
                    ret[valueline.key][valueline.fieldname]['value'] = valueline.value
                return ret
    
            #DB dumped on NBBNEN01 at 16:30 August 24 2009
            header_1 = ('#DB dumped on' + Word(printables)('hostname') + Literal('at').suppress() +
                        Combine(Combine(Word(nums) + Literal(':') + Word(nums)) +
                        Word(alphas) + Word(nums) + Word(nums), ' ', adjacent=False)('datetime') + EOL)
            #DB version 10.0
            header_2 = '#DB version' + Word(printables)('db_version')
            #SW version 3.3(2.2)
            header_3 = '#SW version' + Word(printables)('sw_version')    
    
            header = header_1 + header_2 + header_3
    
            variable_chars = alphanums + '_'
    
            variableName = Combine(OneOrMore(Word(variable_chars)), ' ', adjacent=False)
            userVariables = dictOf(variableName + COLON, restOfLine.setParseAction(stripWhitespace)) + EOL
    
            userBlock = userStart  + OneOrMore(userVariables)  + userEnd
    
            valuesTypes = Literal('INTEGER') | Literal('STRING') | Literal('MSTRING') | Literal('ESTRING') 
            valuesLine = Group(Combine(Literal('App') + Word(nums))('key') +  
                               Word(printables)('fieldname') +
                               valuesTypes('type') + 
                               restOfLine('value').setParseAction(stripWhitespace)) + EOL
    
            valuesBlock = Optional(OneOrMore(valuesLine)) + valuesEnd 
            valuesBlock.setParseAction(setFieldNames)
    
            parser = header('header') + OneOrMore(Group(userBlock + valuesBlock('Values')))('users') + userStart + dumpEnd
            return parser.parseFile(filename)



There is a smaller header and a really small footer at the top and bottom of every file.



Header:



    #DB dumped on WSCSAP05 at 01:00 August 24 2009
    
    #DB version 10.0
    
    #SW version 3.3(2.2)



Footer:



    #End Of Dump



Thank a lot in advance.  I couldn't imagine this project without pyparsing!

#### 2010-02-01 08:44:04 - nemith
Sorry for the extra newlines.  Apparently gedit will copy over Windows CRLF format and even though it looks normal they still sneak their way into the post.
#### 2010-02-01 10:08:54 - ptmcg
This comes up every so often, usually with respect to the helpers like makeHTMLTags.  The problem with these helpers is that they read in the raw HTML source, but return a structured list with added results names for the various attributes defined in the HTML.  For example:



    aTag,aEnd = makeHTMLTags('A')
    
    source = 'some text \<a href='sldjflsj'\> sldfjsldj;'
    
    print aTag.searchString(source)[0].dump()



prints:



    ['A', ['href', 'sldjflsj'], False]
    - empty: False
    - href: sldjflsj
    - startA: ['A', ['href', 'sldjflsj'], False]
      - empty: False
      - href: sldjflsj



Which is great if you want the parsed fields like href from inside the tag, but not so good if you want the original HTML text.  So, you could add the originalTextFor helper/wrapper expression:



    print originalTextFor(aTag).searchString(source)[0].dump()



and get back



    ['\<a href='sldjflsj'\>']





But I think in your case, you will want both.  So here is a parse action to add to those expressions that you need to preserve their original text:



    def appendOriginalText(expr):
        def parseAction(s,l,t):
            # insert special results name giving the original text
            t['originalText'] = s[l:t[-1]]
            # remove trailing integer location
            del t[-1]
        ret = expr+Empty().setParseAction(lambda s,l,t:l)
        ret.setParseAction(parseAction)
        return ret
    
    print appendOriginalText(aTag).searchString(source)[0].dump()



This gives:



    ['A', ['href', 'sldjflsj'], False]
    - empty: False
    - href: sldjflsj
    - originalText: \<a href='sldjflsj'\>
    - startA: ['A', ['href', 'sldjflsj'], False]
      - empty: False
      - href: sldjflsj



You still get the parsed data as a structured list, but you can get back the original text by accessing the originalText field of the results.
#### 2010-02-01 10:12:02 - ptmcg
Oops, you need to change this line:



    ret = expr+Empty().setParseAction(lambda s,l,t:l)



to



    ret = expr+Empty().leaveWhitespace().setParseAction(lambda s,l,t:l)



---
## 2010-02-12 02:50:51 - marjoj - Comparing different parser generators
I'm writing my master's thesis. The subject of my thesis is Implementing a Parser Tool to Visualize Trace and Error Log Files



I chose to use python and pyparsing for the tool  because handling parse results and visualizing textual information was much easier in pyparsing than yacc/lex based parsers. I'm not a big fan of regular expressions either, so that is another reason for choosing pyparsing.



However I would need some backup for my decision. Therefore I would be really interested in articles and research papers comparing different parser generators (performance, pros and cons). I would also be interested number of users (especially pyparsing, PLY, Spirit and ANTLR).



Is the pyparsing only parser generator not using regular expressions (or similar style)? 



What I have found so far is:

Andrew Dalke's comparison of PLY, pyparsing and ATNLR

  

and I saw that he will be giving a presentation about PLY and pyparsing next week in PyCon 2010



Ian Kaplan's Why Use ANTLR?





Fourment's comparison of common programming languages 





I would really appreciate any help in this.


---
## 2010-02-16 11:01:43 - Skipix - Simple arithmetic grammar
Hi all,



I'm trying to build my own simple arithmetic expression parser, which does +-*/, and functions (func(1,2,3) style). So far, I've got:





    expr = Forward().setResultsName('expression')
    expr_list = Forward()
    expr_list \<\< expr + ZeroOrMore(comma + expr)
    
    atom = number ^ Group(lparen + expr + rparen) ^ \
       Group(funcname + lparen + Optional(expr_list) + rparen)
    
    factor = Group(atom + ZeroOrMore(ex + atom))
    factor1 = Group(factor + ZeroOrMore(div + factor))
    term = Group(factor1 + ZeroOrMore(mult + factor1))
    expr \<\< Group(term + ZeroOrMore(addop + term)) 



This works so far (in that my program can parse expressions like 1*2+3 and 1+2*3, observing operator precedence, and calculate the right answer). The Group() parts help preserve the operator precedence.



The problem I have with the above, is that it produces unnecessary Group()s. For example, the expression '1' is returned like ParseResults(ParseResults(ParseResults(ParseResults(1)))), because number is an atom, which is part of the factor group, part of the factor1 group, part of the term group, part of the expr group...



        factor = Group(atom + ZeroOrMore(ex + atom))

        factor1 = Group(factor + ZeroOrMore(div + factor))

        term = Group(factor1 + ZeroOrMore(mult + factor1))

        expr \<\< term | Group(term + OneOrMore(addop + term)) 



Does anyone know how I can achieve the same thing, without having lots of nested objects? I think I'd like to ideally do something like:



expr \<\< term | Group(term + OneOrMore(addop + term))



But pyparsing doesn't like that expression.



thanks for any help! :)



sk

#### 2010-02-16 12:03:12 - ptmcg
a. test the more restrictive expression first

b. enclose right-hand side in ()'s ('\<\<' was poor choice of operator for Forward assignment, has lower precedence than '|')



Try:



    expr \<\< (Group(term+OneOrMore(addop+term)) | term)


#### 2010-02-16 12:33:13 - Skipix
that worked nicely, thank you very much! :)
#### 2010-02-16 16:07:01 - ptmcg
Also check out pyparsing's delimitedList construct, which allows you to replace:



    expr_list = Forward()
    expr_list \<\< expr + ZeroOrMore(comma + expr)

with



    expr_list = delimitedList(expr)



It will parse for the commas, but will strip them from the results.  It will also use iteration instead of recursion, which will run faster and with less memory.

---
## 2010-02-22 02:46:38 - josandres - Logical operators problem
Good morning,

I'm trying to make a parser which recognises logical operations with operators such as \<, \>, AND, OR, and \<\> (NOT EQUALS). I have defined my grammar like this:





    class LogicalOperator:
    
        #NE = Literal('\<\>').setParseAction(lambda t : '!=')
        LT = Literal('\<')
        GT = Literal('\>')
        EQ = Literal('=').setParseAction(lambda t : '==')
        AND = Literal('AND').setParseAction(lambda t : 'and')
        OR = Literal('OR').setParseAction(lambda t : 'or')
    
        OPERATOR = LT | GT | EQ | AND | OR# | NE
    
    class FOPCondition(object):
        __identifier = Word(alphanums)
        __OPERAND = Or(__identifier, FOPArithOperation.OP)
        OP = __OPERAND  + LogicalOperator.OPERATOR + __OPERAND



FOPArithOperation can be any arithmetic operation.

Then i have two problems:



1.- \<\> Operator: When parsing an expression like A \<\> B i get this error:





    pyparsing.pyparsing.ParseException: Expected W:(abcd...) (at char 3), (line:1, col:4)



2.- Arithmetic operation inside logical operation. When parsing an expression like A + 5 \< B i get this error:





    pyparsing.pyparsing.ParseException: Expected '\<' (at char 2), (line:1, col:3)



It seems that it gets the identifier and then it expects the logical operator. But i use an Or object, whcih is supposed to get the longer expressions which matches.



Anyone can help me?

#### 2010-02-22 02:58:31 - josandres
The answer to the first question is put the NE operator before the LT and GT operators, as it is using inside a MatchFirst object:





    class LogicalOperator:
    
        NE = Literal('\<\>').setParseAction(lambda t : '!=')
        LT = Literal('\<')
        GT = Literal('\>')
        EQ = Literal('=').setParseAction(lambda t : '==')
        AND = Literal('AND').setParseAction(lambda t : 'and')
        OR = Literal('OR').setParseAction(lambda t : 'or')
    
        OPERATOR = NE | LT | GT | EQ | AND | OR


#### 2010-02-22 08:27:53 - ptmcg
You are trying to define a recursive expression, which will require the use of a Forward() object.  Please look at the example fourFn.py to see an example of a recursive definition of an arithmetic expression.

---
## 2010-02-22 08:52:55 - john-l - Selectively disabling `ignoreExprs`
I have a case where there is (at least) one location in the parser tree that should not first skip over `ignoreExprs`; basically, the syntactic item is allowed to look like a comment.  The way I've currently solved this is to apply the comment expression using `ignore` on the root of the parser tree, and then I override the ignored expressions on the specific component with `ParserComponent.ignoreExprs = None`.  Not surprisingly, this modification to `ParserComponent` happens a long time after `ParserComponent` is first defined, and this seems like something of a hack to me.  Is this considered bad practice?  Is there a better way to approach this kind of situation?

#### 2010-02-22 19:39:15 - ptmcg
I'm hard pressed to come up with a clean way to do this.  You could define the entire grammar, add the ignore expression (which propagates down throughout the grammar), then add your non-ignoring expression afterwards.  But it will be easy to still unintentionally ignore the comments at an ancestor level in the grammar.  So I think, in the absence of seeing more specifics in order to try to come up with an alternative, I'd have to say this sounds like a Bad Idea.



It may be that you are trying to force a filtering or extracting application into using parseString, when scanString, searchString, or transformString might be more appropriate.  For example, if I am scanning for \<a href\> tags, but I want to not match tags within comments, I would do this:



    aTag,aEndTag = makeHTMLTags('A')
    matchingATags = (htmlStyleComment.suppress() | aTag).searchString(htmlSource)

While searching through the HTML, I match and skip over HTML comments first, and only if I'm not in a comment do I test for a matching \<A\> tag.



Hope this gives you some ideas, or write back with more details.



-- Paul
#### 2010-02-23 06:15:48 - john-l
I'm happy to provide an actual example of this problem.  I provide a small sample of a larger grammar below.  (The full grammar can be found .)  Note that in the larger grammar, the call to `ignore` does not happen until much later in the code, and on a much further ancestor of the `IRI` production.  The problem is that `IRI` can look like a comment (in particular, it can be a IRI fragment, like `#fragment`), which looks like the comment syntax.  The first call to `parseString` succeeds, but the second throws an exception.  If you add `IRI.ignoreExprs = None` before the second call to `parseString`, then it succeeds, as desired.



    <span class="kw1">from</span> pyparsing <span class="kw1">import</span> Suppress<span class="sy0">,</span> Regex<span class="sy0">,</span> restOfLine
    
    LT <span class="sy0">=</span> Suppress<span class="br0">&#40;</span><span class="st0">'\<'</span><span class="br0">&#41;</span>
    GT <span class="sy0">=</span> Suppress<span class="br0">&#40;</span><span class="st0">'\>'</span><span class="br0">&#41;</span>
    
    IRI_REF <span class="sy0">=</span> LT + IRI + GT
    IRI_REF.<span class="me1">ignore</span><span class="br0">&#40;</span><span class="st0">'#'</span> + restOfLine<span class="br0">&#41;</span>
    
    res <span class="sy0">=</span> IRI_REF.<span class="me1">parseString</span><span class="br0">&#40;</span><span class="st0">'''# Here we go...
    \<http://pyparsing.wikispaces.com\>'''</span><span class="br0">&#41;</span>
    
    res <span class="sy0">=</span> IRI_REF.<span class="me1">parseString</span><span class="br0">&#40;</span><span class="st0">'''# Here we go...
    \<#fragment\>'''</span><span class="br0">&#41;</span>


#### 2010-02-23 08:32:52 - ptmcg
I think your solution of IRI.ignoreExprs=None looks fine to me.

---
## 2010-03-02 02:54:05 - nerochiaro - How to improve error reporting ?
Hello,

I have been using pyparsing in one of my projects to parse some custom data files with a JSON-like syntax. 

Everything seems to be working pretty well when the data is well-formed.



However these files are meant to be manually edited by humans, and mistakes frequently happen.

When there's a mistake, I noticed that it's very difficult to know where the actual error is, in many cases.



For example, here's a very simple example case.

Obviously my whole grammar is more complex, but this is a typical and simple case that may help you help me figure out what I am doing wrong.





    item = Word(alphanums)
    list = delimitedList(item, Literal(','))
    definition = stringStart + list + stringEnd



Now, with the above, if an user by mistake forget a coma between elements of the list, like 'a, b  c' the parser will complain 'Expected stringEnd (at char 4), (line:1, col:5)'.



Is it possible in any way to have a more helpful message such as 'Expected ',' (at char 4)' ?



Thanks

#### 2010-03-02 17:57:29 - ptmcg
Well, pyparsing is purely a left-to-right processor, and all of the parsing logic is localized within each ParserElement object.  There is very little 'peek into the last object and see if this next token could have been a partial match if only they had included the delimiter' behavior.  So each element is, within its own scope, doing what is expected: the stringStart matches the start of the string, the list (poor variable name choice, BTW) matches the 'a,b' list, and it is not until we get to matching the stringEnd that pyparsing discovers that we aren't really at the end.



The reason each of these expressions works in isolation is that, they don't really know what comes before or after them in the grammar.  It may very well be that your definition expression consists of not only of a delimitedList(item), but with an optional trailing item as well, perhaps as nickname or short identifier of some sort.  So we don't really want to muck with delimitedList, to have it start doing extra looking about to see if maybe there was a missing delimiter.



BUT! You are the programmer, and presumably, you know better than the computer what you want!  There is nothing to stop you from inserting your own expression after the list, something that would match an extra item if one were provided, and emit a suitable error message.  Here is one way you could do such a thing:



    def errorIf(expr,errormsg):
        locmarker = Empty().leaveWhitespace().setParseAction(lambda s,l,t:l)
        match = ( locmarker('matchlocn') + expr.copy())
        def weDontNeedNoStinkingMatches(s,l,t):
            raise ParseException(s,t.matchlocn,errormsg)
        return (match.addParseAction(weDontNeedNoStinkingMatches) | 
                ~expr)
    
    # redefine definition to inlcude a check in case there 
    # is an extra item after the comma-separated list of items
    definition = (stringStart + list + 
        errorIf(item, 'Whoops! you left out a ','!') + 
        stringEnd)



Now I can still parse correct strings normally:



    # parse a proper string
    print definition.parseString('a,b,c')
    
    ['a', 'b', 'c']



But a string with a missing comma gives me your more helpful message, and even the correct location (char location is 0-based, but col is 1-based):



    # parse an erroneous string
    print definition.parseString('a,b c')
    
    Traceback (most recent call last):
      File 'error.py', line 32, in \<module\>
        print definition.parseString('a,b c')
      File 'C:\Python25\lib\site-packages\pyparsing.py', line 1076, in parseString
        raise exc
    pyparsing.ParseSyntaxException: Whoops! you left out a ','! (at char 3), (line:1, col:4)



Best I can do on short notice...



-- Paul

---
## 2010-03-02 06:37:55 - dharanitharan - How to parse functions in C language
Hi friends,

              I'm doing a project on 'Static C source code analyzing' for that i need to parse the functions in C language... How can i do that using pyparsing.. Im really need of this.





Thank you,

Dharanitharan.A



#### 2010-03-05 08:30:30 - sbmlly
Apologies, the previous example should read:



    test = '''\
    variable x = sin(30)
    x = round(3.14159,2) + 7
    gethandle() = 3)''


#### 2010-03-05 19:34:05 - ptmcg
No, the code I posted works just fine in picking out the function calls in your examples.  The problem is that these examples are not just function calls, but have expanded to include variable declarations and assignment statements, which have a bit more to them than just function calls.



You will need to develop further expressions to represent a variable assignment statement.  Something like:





    vardecl = 'variable' + identifier
    varref = identifier
    lhs = vardecl | varref
    rhs = expr
    assignment = Group(lhs) + '=' + Group(rhs)



If you try to parse each of your cases as an assignment, then I think this will work for the first two examples.  I question whether the 3rd example really is a 'real world' example - I don't think I've ever seen code assign a constant to a value returned from evaluating a function.



No special-casing required, you just have to realize when you are changing the rules of the game.  These examples moved beyond just function calls into executable statements, so your grammar needs to do the same.  Fortunately, pyparsing's style and philosophy makes it pretty straightforward to take our previous expression and reuse it to construct a more complex one.



-- Paul
#### 2010-03-05 19:41:28 - ptmcg
My ulterior motive in defining a 'varref' instead of just using identifier, is that eventually, you may need to refer to variable with more than just an identifier.  For example, we might need to assign our value to any of these:





    a[1]
    a[3][4]
    a[x+1]
    a[rownum*15+colnum]



If our language was like C, there may be any number of '*'s before the variable identifier, for pointer dereferencing.



So we might end up with a more general varref expression like:



    varref = identifier + ZeroOrMore('[' + expr + ']')



By reusing expr (which will match any arithmetic expression, from integer constants to multi-term polynomials) instead of just integer, we automagically get support for a whole complicated set of array references, and by wrapping it in a ZeroOrMore, we now also handle scalar, 1, 2, ... n-dimensional array references.
#### 2010-03-06 12:23:34 - sbmlly
Thanks so much Paul, this is definitely a better correction to head in. I'm close to matching every call and your example code has shown how to continue in extending the parser to cover the last few cases.
#### 2010-03-08 05:31:04 - dharanitharan
Hi friends,

            thanks for the reply.... it worked fine but how can make this code to read the c language file and find the function blocks :)
#### 2010-03-08 06:07:09 - dharanitharan
friends,

         Here i give my problem clearly... From a C file i want to extract the function blocks to count the number of lines in each function block.... I have to get the C file as an input and i have to print the number of lines in each function block... Wat i have to do for this :)
#### 2010-03-08 06:54:44 - ptmcg
Dharanitharn -



I think you are misunderstanding the relationship here.  I do not work for you.  I am not a contractor for you.  I try to give guidance to people who are *using* pyparsing, and having questions or struggling with difficult parts.  For beginners, I will sometimes post partial solutions, or sometimes even near-complete solutions.  I've been to your blog and I see that you are in your final year in pursuing your software degree.  It is time to put some of that hard-earned education to use!  I have given you plenty of examples in this thread, plus module documentation, plus a directory rich with examples.  But you have to meet me halfway!



Get started with a simple input example string containing a couple of function blocks.  Don't get wrapped up in reading from files or walking source directories yet.  Just extract a simple example to try your parser on and get that working.  Once that is working, then move up to your larger body of code.



I expect you know what a 'BNF' is - write one (or find one on the web) for a C function that you are trying to match.  Use the BNF to start building up your pyparsing grammar to match a C function.



Come back after you have something to show of your own work.



-- Paul
#### 2010-03-08 07:03:13 - dharanitharan
Sorry Mr.Paul for ur clear understanding only i reveal my problem... Not intentionally want all the coding :) You said 



test = '''\

    sin(30)

    round(3.14159,2)

    int(sin(60))

    draw_line(x1,y1,x2,y2)'''



for t in test.splitlines():

    print function_call.parseString(t).asList()



this know ... from this,is it possible for me to find function definition
#### 2010-03-08 07:26:02 - ptmcg
Unfortunately, there are no function definitions in any of these lines of code.  Nor would I expect to find many on any single line of code in a C source file.  Do you know what a C function definition looks like?  Could you write a simple C function that:

- is named 'testFunc1'

- returns an int

- accepts 2 int parameters, 'a' and 'b'

- consists of the single line 'return a+b;'



Write this function.  Then use it as the test sample for your parser to look for.



Is this homework?  Do you have an instructor who perhaps is paid to give you help on working on this?  I cannot lead you by the hand through this problem.



Until you have a simple C function example, and a first attempt at writing a pyparsing parser to match a C function, I won't be answering any more of your questions. Now get to work! (and I mean that in as encouraging and positive way as I can, you should have all the necessary tools now)
#### 2010-03-08 07:35:50 - dharanitharan
thank u sir :). Let me start to write the function and will surely contact you if i have any queries :)
#### 2012-11-29 19:04:24 - torfat
Hi ptmcg,  I'm trying to match quoted string in function parameter:

'''

print_string('a string', anotherParam)

'''



by changing the 'expr' statement to

expr \<\< (function_call | identifier | real | integer | quotedString)



why is that not working?  TIA
#### 2012-11-29 20:47:50 - torfat
sorry, i found my mistake elsewhere in the code.
#### 2010-03-02 11:02:59 - ptmcg
See this message, it may help you get started.





-- Paul
#### 2010-03-03 00:16:55 - ptmcg
Dharanithran -

First you should try to describe what kind of pattern you are trying to match.  I would advise, if at all possible, AGAINST writing a full C language parser.  Instead, try to spec out what a function declaration looks like. From there, you would assemble your pyparsing expression to match that declaration, using the various pyparsing classes.  Then you can use searchString or scanString to pick the matching function decls out of a larger source file. 



Try this as a start.  Here is a simple pyparsing expression for a C identifier:



    ident = Word(alphas,alphanums+'_')



Here are some typical C types:



    type = oneOf('int float char')



Now heres a very simple declaration:



    SEMI = Suppress(';')
    varDecl = type + delimitedList(ident) + SEMI



Now try to search for any simple variable declarations in your source file, using varDecl.searchString.



How would you expand varDecl to include:

leading '*'s (for pointers)

trailing '[]'s (for arrays)



Now you should be getting the hang of using pyparsing a bit.  Now start thinking about your function problem.  What are the main pieces of a function declaration?  (Hint: one part will be the function name, for which we've already defined `ident`.  There will also be function arguments and a function type. If you get that far, then you will be able to parse an
#### 2010-03-03 08:31:21 - sbmlly
I think the problem is not parse function declarations but to parse function calls;



i.e. float x = sqrt(67*2) - \> sqrt



I'm having trouble with a similar task, in that I can't parse nested function calls in a c-style language.



i.e. sqrt(sum(array)) should parse as two separate calls.
#### 2010-03-03 21:56:54 - ptmcg
This is a pretty typical parsing challenge, to be able to parse an expression that can recursively be composed of pieces that themselves match the parent expression.  Pyparsing handles this recursion using the Forward class.



Here is a simple function expression parser, that will recursively parse functions of the form:



    functionName ( [ arg [, arg]...] )



where arg can be a variable name, an integer, a float, or a function call.  To do this, we declare a base expression expr as:



    expr = Forward()



Now we'll use expr for our argument in defining a function call



    LPAR, RPAR = map(Suppress, '()')
    identifier = Word(alphas+'_', alphanums+'_')
    function_call = identifier + LPAR + Group(Optional(delimitedList(expr))) + RPAR



Finally, we'll define what can go into an expr:



    integer = Regex(r'-?\d+')
    real = Regex(r'-?\d+\.\d*')
    expr \<\< (function_call | identifier | real | integer)



We used the '\<\<' operator to 'inject' the expression's definition, so that we use the existing expr variable, rather than binding some new value to the name 'expr'.



Now we can parse some function calls:



    test = '''\
        sin(30)
        round(3.14159,2)
        int(sin(60))
        draw_line(x1,y1,x2,y2)'''
    
    for t in test.splitlines():
        print function_call.parseString(t).asList()



prints:



    ['sin', ['30']]
    ['round', ['3.14159', '2']]
    ['int', ['sin', ['60']]]
    ['draw_line', ['x1', 'y1', 'x2', 'y2']]



Things get trickier when you add in support for arithmetic expressions, but not too bad.  Pyparsing's operatorPrecedence method can help keep things simple.  Let's just support operations +, -, *, /, and %. Normal precedence of operations says that we do operators in this order:



    * / %
    + -



operatorPrecedence gets called using the form:



    operatorPrecedence( base_operand, list_of_operations_by_precedence_level)



Each element in the list_of_operations_by_precedence_level is a tuple of the operator (or an operator expression), the value 1 or 2 depending if this is a unary or binary operation, and a flag indication whether the operator is right or left associative.  To insert our simple arithmetic expression into our previous function call, we'll replace the 'expr \<\< ...' statement with:



    operand = (function_call | identifier | real | integer)
    expr \<\< operatorPrecedence(operand,
        [
        (oneOf('* / %'), 2, opAssoc.LEFT),
        (oneOf('+ -'), 2, opAssoc.LEFT),
        ])



And that's it.  operatorPrecedence will take care of matching and recursively nesting parentheses, and will group our arithmetic operations by precedence order too.  (If you don't want to bother with precedence of operations, it is simplest to still use operatorPrecedence, just put all the operators into a single level.)



Here is the whole parser:



    from pyparsing import *
    
    expr = Forward()
    
    LPAR, RPAR = map(Suppress, '()')
    identifier = Word(alphas+'_', alphanums+'_')
    function_call = identifier + LPAR + Group(Optional(delimitedList(expr))) + RPAR
    
    integer = Regex(r'-?\d+')
    real = Regex(r'-?\d+\.\d*')
    
    operand = (function_call | identifier | real | integer)
    expr \<\< operatorPrecedence(operand,
        [
        (oneOf('* / %'), 2, opAssoc.LEFT),
        (oneOf('+ -'), 2, opAssoc.LEFT),
        ])



Good luck!
#### 2010-03-05 00:32:28 - dharanitharan
thanks ptmcg :) let me try it out :)
#### 2010-03-05 00:54:50 - dharanitharan
what i need is that i have to get a C language file as an input and i have to identify the functions in that file... this is my aim :) for this what i have to do...





thanks,

Dharanitharan
#### 2010-03-05 04:49:32 - ptmcg
Dharanitharan -



Please go back and read my posting that starts with 'First you should try to describe what kind of pattern you are trying to match.' Are you looking for function declarations, as would be found in a .h file, or the headers of a function implementation as would be found in a .c file?  Both are 'C language file as input', but they are slightly different formats.  Is this going to be a parser that is going to be provided to the world as a general-purpose utility, or is this focused on extracting functions from a given body of source code?  If the former, this will be a significant job, as you will need to fully implement a parser for all forms of functions and types, including pointers and function pointers, which can get to be very complex to parse.  But if this is a utility to deal with a particular body of source code, as if you are going to generate documentation for a personal or work project, then you can look through the source itself and get an idea of the breadth of functions and function types that need to be handled, and hopefully this will be a small subset of all the possible function forms.



Earlier I showed you how to use pyparsing to search for variable declarations.  I will give you one last help on this, and then you must start doing some work of your own.  The header of a C function will look like this:



    type identifier ( [arg [,arg]...] ) {

And an arg will look like:



    type identifier



Now I turn it over to you, try to start developing pyparsing expressions to match a function argument, and a function header.



-- Paul
#### 2010-03-05 08:29:10 - sbmlly
Thanks for your help, but it looks like the code you posted fails in real world examples...



For example:

[code]

test = '''\

    variable x = sin(30)

    x = round(3.14159,2) + 7

    gethandle() = 3)''

[/code]

will fail as the function calls are part of larger expressions. I've tried to convert the code you posted to handle this but I've only had success using simple hacks to try and special-case the above examples, I'm sure a general solution exists.

---
## 2010-03-05 13:38:36 - bjornjobb - time expression parser
Hi, I really liked the time expression parser on the development page. You dont happen to have a version with months/years?



/bjorn

#### 2010-03-05 19:10:56 - ptmcg
What makes months and years more difficult than weeks, days or hours, is that not all months or years can be represented by the same timedelta value - some months are longer than others, and the same for years.  Once you introduce months and years, now you have to refer to a calendar so that 'one month from now' on Feb 15 would presumably mean March 15, only 28 days away (or 29, in every 4th year).  So to add months, you have to construct a date time, update the given field, then back-calculate what the timedelta is.  And if the day happens to be Jan 30, what should I do for 'one month from today'?  There *is* no Feb 30!  Similarly for 'one year from now' for Feb 29, 2004.



So I suppose it could be done with some back-and-forth to a calendar, but you would need some clear resolution rules for the corner cases that would likely to be generally agreeble.  For instance, could you say that 'one month from now' on Jan 28, 29, 30, or 31 would be generally understood to refer to Feb 28 or 29, depending on the year?



For that matter, I still run into people who, on a Thursday will disagree whether 'next Saturday' means the immediately next Saturday (in two days), or really means the following Saturday, and that the Saturday that falls in 2 days should really be called 'this Saturday'.  So this parser is only part about parsing and part about calendar arithmetic, but also a significant part about understanding the cases.  So I'm sad to say, but no, I don't have a month/year version of the time expression parser, and I'm not likely to have one any time soon.  (But please, be my guest to take a stab at one!)
#### 2010-03-08 03:32:00 - bjornjobb
thanks, Ill see if I get around to do it. A module like that would be nice for any calendar software to quickly add entries or jump around.



/bjorn

---
## 2010-03-09 01:13:54 - josandres - Pyparsing quoted strings
Good morning,

I'm trying to parse quoted strings using this expression:



STRING = Suppress(''') + Group(Word(alphanums)) + Suppress(''')



But when trying to parse a expression like 'a' i get the error:



pyparsing.pyparsing.ParseException: Expected ''' (at char 0), (line:1, col:1)



Can anyone help me?

#### 2010-03-09 06:00:30 - ptmcg
Are you sure you put 'a' in quotes?





    STRING.parseString(''a'')



works for me.

---
## 2010-03-16 22:59:40 - gobnat - Help understanding Expression subclasses and whitespace treatment
I'm trying to retain whitespace as a token.  I now seem to be able to do this, but different expression subclasses seem to treat things differently. 



This is my code:



    test = '''   I  want to tokenise     the white space in a string'''
    
    def parseIt(parseDefn,textToParse):
        try:
            spam= parseDefn.parseString(textToParse)
        except:
            spam=None
        return spam
    
    ordWord=Word(alphas,alphanums)
    whiteWord=White(' ')
    
    lines = []
    lines.append(OneOrMore(ordWord & whiteWord))
    lines.append(lines[0] + LineEnd())
    lines.append(OneOrMore(ordWord ^ whiteWord))
    
    for line in lines:
        print line
        print parseIt(line,test)
        print parseIt(line.leaveWhitespace(),test)



Which outputs:



`W:(abcd...,abcd...) &amp; &lt;SPC&gt;`...

['I', '  ', 'want', ' ', 'to', ' ', 'tokenise', '     ', 'the', ' ', 'white', ' ', 'space', ' ', 'in', ' ', 'a', ' ']

['I', '  ', 'want', ' ', 'to', ' ', 'tokenise', '     ', 'the', ' ', 'white', ' ', 'space', ' ', 'in', ' ', 'a', ' ']

`{W:(abcd...,abcd...) &amp; &lt;SPC&gt;`... LineEnd}

None

None

`W:(abcd...,abcd...) ^ &lt;SPC&gt;`...

['I', 'want', 'to', 'tokenise', 'the', 'white', 'space', 'in', 'a', 'string']

['   ', 'I', '  ', 'want', ' ', 'to', ' ', 'tokenise', '     ', 'the', ' ', 'white', ' ', 'space', ' ', 'in', ' ', 'a', ' ', 'string']



How come & preserves whitespace by default but ^ requires leaveWhitespace to work? And what am I doing wrong with LineEnd?



TIA

#### 2010-03-17 21:24:19 - ptmcg
One thing that would help would be to 'turn off' all of the whitespace characters, using ParserElement.setDefaultWhitespaceChars('').  You need to make this call right after importing the pyparsing module.  



Also, your use of the & operator is a bit strange.  In pyparsing's usage, a & b is equivalent to a + b | b + a.  That is, a and b must both be present, but can be in either order.  Note that your first test only parses Words and spaces in pairs, and there is no whitespace after the trailing word string.  You see the difference when you parse using ^ instead of &.  (Also, there is no need to use the performance-sucking ^ operator, you could/should use | instead, as there is no risk of mistaking a ordWord for a whiteWord, or vice versa.)



I'm curious what your application is though.  Each line is defined to have only words like programming variable names - no operators, no punctuation, no numbers.  And what are you going to do with all those whitespace tokens?  You are certainly taking an unusual development path here, and, while I am open to different ways of approaching a problem, I've seen a lot of pyparsing apps by now, and I fear you may be heading down a blind alley.
#### 2010-03-18 17:08:10 - gobnat
Thanks for your explanation and your ParserElement.setDefaultWhitespaceChars('') suggestion.  I will try it.



Application domain: parsing contracts.  At the moment this primarily will involve:

identify leading numbering; and

splitting strings of the form \<anything\> \<one member of (a set of specific set of words in a specific order)\> \<anything\>

some other structures like subparagraphs ': (a) ...; (b) ... ; and (z)  '



Re: blind alley: yes maybe, I'm alive to the possibility.  I'm in a learning stage at the moment.  I got the pyparsing and the nltk ebooks from OReilly and am looking to see how they fit.  Having something other than a regexp as a description is a benefit in itself. 



keeping whitespace: once I've parsed something I want to be able to reconstruct it (eg ''.join).  It may be there are better ways to do this.



Suggestions welcome.

---
## 2010-03-17 01:15:48 - elekis - installing pyparsing on python 3.1
hi, 

I try to install pyparsing for python 3.1

I ve seen in the subversion this is always maintained ( the lasst change date 2 weeks ago).



after downloaded the last subversion, I made in the tarball python setup and pyparsing is installed.



but in a command prompt 

I ve tried this  





    c:\Python31\pyparsinginstall3.1\>python
    Python 3.1.1 (r311:74483, Aug 17 2009, 16:45:59) [MSC v.1500 64 bit (AMD64)] on
    win32
    Type 'help', 'copyright', 'credits' or 'license' for more information.
    \>\>\> import pyparsing;
    Traceback (most recent call last):
      File '\<stdin\>', line 1, in \<module\>
      File 'pyparsing.py', line 2471
        except ParseException, err:
                             ^
    SyntaxError: invalid syntax



any idea??



thanks



a++

#### 2010-03-17 02:03:19 - elekis
ok, I found, 



in python 3.1, it's not pyparsing we have to call but pyparsing_py3.



dunno if it's logical but ...



there is 





a+++

---
## 2010-03-20 10:26:22 - Elby - Reading a table by columns
I have to make a parser for some files which look like ConfigParse files with units and the possibility to enter tables.



Here is a test example and the parser I've created : 





    from    pyparsing   import *
    from    pprint      import      pprint
    
    test = r'''
    [ INFOS ]
    Context        = full
    Temp_ref  (K ) = 298.15   # This is  a comment
    Capacity  (C)  = 125
    Shape  (mm^-1) = 2.3
    
    # This is also a comment
    
    [ FILES ]   # this is another comment
    path_1 = C:\\This\is\a\long\path\with some space in it\data.txt
    path_2 = data2.txt
    
    [ TABLE ]
    STATION         PRECIPITATION   T_MAX_ABS  T_MIN_ABS  T_MAX_MOY  T_MIN_MOY
    (/)                     (mm)    (C)        (C)        (C)        (C)
    Abbeville               72.2    12.6E+0    -7         5.9        0.9
    Ajaccio                 64.8    18.8E+0    -2.6       13.7       5.4
    Auxerre                 49.6    16.9E+0    -8.5       6.9        0.4
    Bastia                  114.2   20.8E+0    -0.9       13.6       4.8
    Beauvais                95      12.7E+0    -6.8       6.1        0.3
    Bergerac                59.4    16.9E+0    -7.8       9.7        0.4
    Biarritz                106.2   22.7E+0    -3.7       11.1       4.3
    Boulogne                83.2    12.8E+0    -3.1       6          1.8
    Bourg-Saint-Maurice     54.8    11.9E+0    -14.6      5.9        -3.9
    '''
    
    keyDef = Word(alphanums+'.-_/').setParseAction(downcaseTokens)
    
    sectionDef = Literal('[').suppress() + keyDef + Literal(']').suppress()
    
    unitDef =  (  Literal('(').suppress()
                + Word(alphanums + '^*/-._')
                + Literal(')').suppress()
               )
    
    tabValueDef = Word(printables)
    
    constDef = (
            keyDef('name')
          + Optional(unitDef('unit'))
          + ('='+empty).suppress()
          + SkipTo('#'|lineEnd)('value')
          )
    
    tableDef = (
            Group(OneOrMore(keyDef))('header')
          + Group(OneOrMore(unitDef))('unit')
          + Group(OneOrMore(tabValueDef))('data')
          )
    
    Comment = '#' + restOfLine
    
    blocDef = Dict( OneOrMore( Group(
                  sectionDef
                + Dict( OneOrMore( Group( constDef ) | tableDef ) )
                ) ) )
    
    blocDef.ignore(Comment)
    
    # test
    r = blocDef.parseString(test)
    pprint(r.asList())
    print '...'
    print 'Stations : '
    print r.table.station



This is almost working, but there are some issues : 



 1) I don't check that the table is well formed and has the same number of columns on each row. Is there a simple way to do that ?



 2) I get the results : 



    Station : 
    ['precipitation', 't_max_abs', 't_min_abs', 't_max_moy', 't_min_moy']



I would like to get this table by column instead of by row, in order to have : 



    Station :
    ['Abbeville', 'Ajaccio', 'Auxerre', 'Bastia', 'Beauvais',
     'Bergerac', 'Biarritz', 'Boulogne', 'Bourg-Saint-Maurice']



Is there a way to fulfill this ? I tried with the following hack : 



    
    def transpose_table(toks):
        ''' Transpose the table defined in toks.data and return a list of columns'''
        nb_col = len(toks.header)
        data = zip( *[toks.data[nb_col*i:nb_col*(i+1)] for i in xrange(len(toks.data)/nb_col)])
        return list( zip(toks.header, toks.unit, data))
    
    tableDef.setParseAction(transpose_table)
    

but this wasn't smart enough...



 2) I don't know what the best way to validate the the data read, check the units and convert values into the best python tuple. I was thinking to add ParseAction to constDef and others, but I'm not sure if this is the good way to go.

Besides, i would like to validate also the name of variables, in order to prevent any character like '/', '.' which would make difficult their use trough python.

What do you think ?



Regards, 

-- 

Elby

#### 2010-03-21 02:50:25 - ptmcg
This is a nice first pass at this parsing problem.  There is an example on the pyparsing wiki that transposes a data table, I think it uses a similar approach to that in the notes below.



The first change I made was to break up your table parse into a collection of lines of data, instead of a single long list of tableValueDef's:





    # instead of reading the table as just a bunch of values, read it
    # by line using end-of-line detection in tableValueLine
    EOL = LineEnd().suppress()
    tabValueDef = Word(printables).setWhitespaceChars(' \t')
    tableValueLine = Group(OneOrMore(~EOL + tabValueDef) + EOL)



I also was a little concerned about your station names, since later on, these names will be used as results names.  So I tweaked tableValueLine to start with a stationName:





    # definition of tableValueLine that handles station names with spaces
    stationName = originalTextFor(OneOrMore(Word(alphas,printables)))
    tableValueLine = Group(stationName + OneOrMore(~EOL + tabValueDef) + EOL)





To use this new tableValueLine expression, I modified tableDef:





    tableDef = (
            Group(OneOrMore(keyDef))('header')
          + Group(OneOrMore(unitDef))('unit')
          #~ + Group(OneOrMore(tabValueDef))('data')
          + EOL
          + Group(OneOrMore(tableValueLine))('data')
          )



Lastly, to do the table transpose, I wrote this parse action:





    def transposeTableWithStations(tableTokens):
        data = ParseResults([])
        for stn in tableTokens.data:
            stnName = stn[0]
            data[stnName] = ParseResults(stn[1:])
            for k,v in zip(tableTokens.header[1:], stn[1:]):
                data[stnName][k] = v
        tableTokens['data'] = data
    # comment out this line to see how data table is parsed by line 
    # before it is tranposed
    tableDef.setParseAction(transposeTableWithStations)



Now you can access the stations within the table as the entries in the table.data field.  View all the key/values using the dump() method:





    print r.table.data.dump()





HTH,

-- Paul
#### 2010-03-21 06:38:31 - Elby
I think I was not really clear about what I meant by 'get a table by column'.

I would like to be able to retrieve information according to the name of each column header through r.table.staion or r.table.precipitation, ...etc.



So I tried to tweak your your ParseAction example : 





    #! /usr/bin/env python
    # -*- coding: utf-8 -*-
    
    from    pyparsing   import *
    from    pprint      import      pprint
    
    test = r'''
    [ INFOS ]
    Context        = full
    Temp_ref  (K ) = 298.15   # This is  a comment
    Capacity  (C)  = 125
    Shape  (mm^-1) = 2.3
    
    # This is also a comment
    
    [ FILES ]   # this is another comment
    path_1 = C:\\This\is\a\long\path\with some space in it\data.txt
    path_2 = data2.txt
    
    [ TABLE ]
    STATION         PRECIPITATION   T_MAX_ABS  T_MIN_ABS  T_MAX_MOY  T_MIN_MOY
    (/)                     (mm)    (C)        (C)        (C)        (C)
    Abbeville               72.2    12.6E+0    -7         5.9        0.9
    Ajaccio                 64.8    18.8E+0    -2.6       13.7       5.4
    Auxerre                 49.6    16.9E+0    -8.5       6.9        0.4
    Bastia                  114.2   20.8E+0    -0.9       13.6       4.8
    Beauvais                95      12.7E+0    -6.8       6.1        0.3
    Bergerac                59.4    16.9E+0    -7.8       9.7        0.4
    Biarritz                106.2   22.7E+0    -3.7       11.1       4.3
    Boulogne                83.2    12.8E+0    -3.1       6          1.8
    Bourg-Saint-Maurice     54.8    11.9E+0    -14.6      5.9        -3.9
    '''
    
    keyDef = Word(alphanums+'.-_/').setParseAction(downcaseTokens)
    
    sectionDef = Suppress('[') + keyDef + Suppress(']')
    
    unitDef =  Suppress('(') + Word(alphanums + '^*/-._') + Suppress(')')
    
    
    constDef = (
            keyDef('name')
          + Optional(unitDef('unit'))
          + Suppress('='+empty)
          + SkipTo('#'|lineEnd)('value')
          )
    
    # instead of reading the table as just a bunch of values, read it
    # by line using end-of-line detection in tableValueLine
    EOL = LineEnd().suppress()
    tabValueDef = Word(printables).setWhitespaceChars(' \t')
    tableValueLine = Group(OneOrMore(~EOL + tabValueDef) + EOL)
    
    # definition of tableValueLine that handles station names with spaces
    stationName = originalTextFor(OneOrMore(Word(alphas,printables)))
    tableValueLine = Group(stationName + OneOrMore(~EOL + tabValueDef) + EOL)
    
    tableDef = (
            Group(OneOrMore(keyDef))('header')
          + Group(OneOrMore(unitDef))('unit')
          #~ + Group(OneOrMore(tabValueDef))('data')
          + EOL
          + Group(OneOrMore(tableValueLine))('data')
          )
    
    def transposeTable(tableTokens):
        # arrange data by columns
        columns = {}
        transposedData = zip(*tableTokens.data)
        for header, unit, data in zip(tableTokens.header, tableTokens.unit, transposedData):
        column = ParseResults([])
        column['unit'] = unit
        column['data'] = data
        columns[header] = column
    
        # delete old results
        del tableTokens['data']
        del tableTokens['unit']
    
        # DEBUG
        print 'columns : '
        pprint(columns)
        print '...'
    
        # adds columns
        for k, v in columns.iteritems() : tableTokens[k] = v
    
    # comment out this line to see how data table is parsed by line 
    # before it is tranposed
    tableDef.setParseAction(transposeTable)
    
    blocDef = Dict( OneOrMore( Group(
                  sectionDef
                + Dict( OneOrMore( Group( constDef ) | tableDef ) )
                ) ) )
    
    blocDef.ignore('#' + restOfLine)
    
    # test
    print '...'
    r = blocDef.parseString(test)
    pprint(r.asList())
    
    for header in r.table.header :
        print '...\n%s :' % header
        print r.table.get(header).dump()



It's almost working, but there are still issues the first column. Here is what I get :



    ...
    station :
    ['precipitation', 't_max_abs', 't_min_abs', 't_max_moy', 't_min_moy']
    ...
    precipitation :
    []
    - data: ('72.2', '64.8', '49.6', '114.2', '95', '59.4', '106.2', '83.2', '54.8')
    - unit: mm
    ...
    t_max_abs :
    []
    - data: ('12.6E+0', '18.8E+0', '16.9E+0', '20.8E+0', '12.7E+0', '16.9E+0', '22.7E+0', '12.8E+0', '11.9E+0')
    - unit: C
    ...



When I print the content of the the column dictionary in transposeTable, I get : 



    ...
    columns : 
    {'precipitation': ([], {'data': [(('72.2', '64.8', '49.6', '114.2', '95', '59.4', '106.2', '83.2', '54.8'), 0)], 'unit': [('mm', 0)]}),
     'station': ([], {'data': [(('Abbeville', 'Ajaccio', 'Auxerre', 'Bastia', 'Beauvais', 'Bergerac', 'Biarritz', 'Boulogne', 'Bourg-Saint-Maurice'), 0)], 'unit': [('/', 0)]}),
     't_max_abs': ([], {'data': [(('12.6E+0', '18.8E+0', '16.9E+0', '20.8E+0', '12.7E+0', '16.9E+0', '22.7E+0', '12.8E+0', '11.9E+0'), 0)], 'unit': [('C', 0)]}),
     't_max_moy': ([], {'data': [(('5.9', '13.7', '6.9', '13.6', '6.1', '9.7', '11.1', '6', '5.9'), 0)], 'unit': [('C', 0)]}),
     't_min_abs': ([], {'data': [(('-7', '-2.6', '-8.5', '-0.9', '-6.8', '-7.8', '-3.7', '-3.1', '-14.6'), 0)], 'unit': [('C', 0)]}),
     't_min_moy': ([], {'data': [(('0.9', '5.4', '0.4', '4.8', '0.3', '0.4', '4.3', '1.8', '-3.9'), 0)], 'unit': [('C', 0)]})



So the problem seems to come when I try to reassign the station field in this function.



Besides, for the moment I just use zip to transpose data into columns and I don't check if all rows have the same number of columns.

Is there a way to consolidate tableDef description in order to prevent pb when you enter non homogeneous rows ?



Thanks a lot for your quick answers



-- Elby
#### 2010-03-21 08:30:13 - ptmcg
See comment changes below:



    def transposeTable(tableTokens):
        # arrange data by columns
    
        # pad data rows with '0.0' values
        print 'len(headers)',len(tableTokens.header)
        numcols = len(tableTokens.header)
        for row in tableTokens.data:
            print len(row)
            if len(row) \< numcols:
                row += ParseResults(['0.0']*(numcols-len(row)))
    
        #~ columns = {}
        columns = ParseResults([])
        transposedData = zip(*tableTokens.data)
        for header, unit, data in zip(tableTokens.header, tableTokens.unit, transposedData):
            column = ParseResults([])
            column['unit'] = unit
            column['data'] = data
            columns[header] = column
    
        # delete old results
        #~ del tableTokens['data']
        del tableTokens['unit']
    
        # DEBUG
        print 'columns : '
        pprint(columns.asList())
        print '...'
    
        # no need for this now, columns were already built as a ParseResults
        # adds columns
        #~ for k, v in columns.iteritems() : tableTokens[k] = v
    
        # just overwrite the entry for 'data' with our new transposed columns
        tableTokens['data'] = columns


#### 2010-03-21 08:33:53 - ptmcg
You could also do this column padding in a parse action attached to tableValueLine, if you know in advance how many columns there are supposed to be.
#### 2010-03-21 11:14:09 - Elby
Is it possible to know how many columns there are when the group 'header' has a match and to add a constraint on the OneOrMore expression defining units and tabValueLine in order to match only line with the same number of columns that the header ?



-- Elby
#### 2010-03-21 11:57:43 - Elby
I've got another question on the transposeTable function.



I would like to replace the tokensTable by the new ParseResult 'columns' 

instead of adding a new one in the field data.



Indeed, with the current version, you have access to two fields named station :



 * r.table.station which has no clear meaning since it give the list of

   the headers



 * r.table.data.station which is the important one but also the most

   difficult to find



I tried to let transposeTable return directly columns but this doesn't seems to work.



Any suggestion ?



-- Elby
#### 2010-03-21 13:16:28 - ptmcg
Parse actions *should* be able to return a value, and this would replace whatever ParseResults had been built from parsing the input string.  What doesn't work?



And yes, you can dynamically structure a number of elements based on a previous token - see how this is done in the countedArray method.



-- Paul
#### 2010-03-22 12:31:57 - Elby
Thanks for the hints :



 * for the first point, when my parseAction was returning columns, r.table was

   a string and not a parseResults, so r.table.dump() crashed.  I changed my

   parseAction to return [columns] instead, and this seems to work.



 * for the second point, I used Forward elements and this did the trick.



Now I've added support for unit conversion : 





    from    pyparsing   import *
    from    pprint      import      pprint
    
    test = r'''
    [ INFOS ]
    Context        = full
    Temp_ref  (K ) = 298.15   # This is  a comment
    Capacity  (C)  = 125
    Shape  (mm^-1) = 2.3
    
    # This is also a comment
    
    [ FILES ]   # this is another comment
    path_1 = C:\\This\is\a\long\path\with some space in it\data.txt
    path_2 = data2.txt
    
    [ TABLE ]
    STATION         PRECIPITATION   T_MAX_ABS  T_MIN_ABS  T_MAX_MOY  T_MIN_MOY
    (/)                     (mm)    (C)        (C)        (C)        (C)
    Abbeville               72.2    12.6E+0    -7         5.9        0.9
    Ajaccio                 64.8    18.8E+0    -2.6       13.7       5.4
    Auxerre                 49.6    16.9E+0    -8.5       6.9        0.4
    Bastia                  114.2   20.8E+0    -0.9       13.6       4.8
    Beauvais                95      12.7E+0    -6.8       6.1        0.3
    Bergerac                59.4    16.9E+0    -7.8       9.7        0.4
    Biarritz                106.2   22.7E+0    -3.7       11.1       4.3
    Boulogne                83.2    12.8E+0    -3.1       6          1.8
    Bourg-Saint-Maurice     54.8    11.9E+0    -14.6      5.9        -3.9
    '''
    
    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
    # Variables to be found, converters and units
    
    variable_table = [
        # Name        # type   # unit 
       ('context',         str,    None    ),
       ('temp_ref',         float,  'C'     ),
       ('capacity',         float,  'C'     ),
       ('shape',         float,  'm^-1'  ),
       ('path_1',         str,    None    ),
       ('path_2',         str,    None    ),
       ('altitude',         float,  'm'     ),
       ('station',         str  ,  None    ),
       ('precipitation', float,  'mm'    ),
       ('t_max_abs',     float,  'C'     ),
       ('t_min_abs',     float,  'C'     ),
       ('t_max_moy',     float,  'C'     ),
       ('t_min_moy',     float,  'C'     ),
    ]
    
    cast_dict      = dict((l[0], l[1]) for l in variable_table if l[1] is not str)
    unit_dict      = dict((l[0], l[2]) for l in variable_table if l[2] is not None)
    unit_converter = {
        'C_to_K'        : lambda x: x+273.15,
        'K_to_C'         : lambda x: x-273.15,
        'mm_to_m'        : lambda x: x*1e-3,
        'm_to_mm'        : lambda x: x*1e+3,
        'mm^-1_to_m^-1' : lambda x: x*1e+3,
        }
    
    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
    # Parse Action definitons
    
    def convertConstant(s, l, toks):
        ''' Convert a constant into the adequat Python type and take care for units '''
        if toks.name in cast_dict :
        value = cast_dict[toks.name](toks.value)
        if toks.name in unit_dict :
            wishUnit = unit_dict[toks.name]
            if hasattr(toks, 'unit') :
            if toks.unit[0] != wishUnit :  
                # TODO : handle exeption where converter is unknown
                value = unit_converter['%s_to_%s' % (toks.unit[0], wishUnit)](value)
            else:
            print '%s ==\> %s supposed to be in %s' % (s, toks.name, wishUnit)
    
        # TODO : delete toks.unit ?
        return [toks.name, value]
    
    
    def convertTable(tableTokens):
        ''' Arrange data by columns and convert into the correct Python type.
        Keep the header line'''
    
        columns = ParseResults([])
    
        # store header names 
        columns['header'] = tableTokens.header
    
        transposedData = zip(*tableTokens.data)
        for header, unit, data in zip(tableTokens.header, tableTokens.unit, transposedData):
        if header in cast_dict : 
            # conversion
            data = map( cast_dict[header], data)
            if header in unit_dict :
            wishUnit = unit_dict[header]
            if unit != wishUnit :
                # TODO : handle exeption where converter is unknown
                data = map(unit_converter['%s_to_%s' % (unit, wishUnit)], data)
    
            columns[header] = data
    
        # just overwrite the entry for 'data' with our new transposed columns
        return [columns]
    
    def defineColNumber(s, l, t):
        ''' define unitLine and tableValueLine to match the same number of row than
        in header'''
        nbcols = len(t.header)
        unitLine \<\< (~EOL + unitDef*nbcols + EOL)
        tableValueLine \<\< Group(~EOL + tabValueDef*nbcols + EOL)
    
    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
    # grammar definition
    
    keyDef = Word(alphanums+'.-_/').setParseAction(downcaseTokens)
    
    sectionDef = Suppress('[') + keyDef + Suppress(']')
    
    unitDef =  Suppress('(') + Word(alphanums + '^*/-._') + Suppress(')')
    
    
    constDef = (
            keyDef('name')
          + Optional(unitDef('unit'))
          + Suppress('='+empty)
          + SkipTo('#'|lineEnd)('value')
          ).setParseAction(convertConstant)
    
    
    EOL = LineEnd().suppress()
    tabValueDef = Word(printables).setWhitespaceChars(' \t')
    
    # unitLine and tableValueLine will be denied later once we know the number of
    # columns in the table
    unitLine = Forward()
    tableValueLine = Forward()
    
    tableDef = (
            Group(OneOrMore(keyDef))('header').setParseAction(defineColNumber)
          + Group(unitLine)('unit')
          + Group(OneOrMore(tableValueLine))('data')
          )
    
    tableDef.setParseAction(convertTable)
    
    blocDef = Dict( OneOrMore( Group(
                  sectionDef
                + Dict( OneOrMore( Group( constDef ) | tableDef ) )
            # TODO : what happen when a section contains constants and a table ?
                ) ) )
    
    blocDef.ignore('#' + restOfLine)
    
    
    # test
    print '...'
    r = blocDef.parseString(test)
    print r.dump()
    



I'm pretty impressed by the simplicity and flexibility I've reach with this

first contact with pyparsing.



I think I'll add an entry in the Scipy cookbook to share this if I have time,

but now, I would like to make this code more robust and add custom exceptions

to warn when conversions failed or when the parser did not reach the end of

file.  Is there a simple way to do that ?



Regards, 



-- Elby

---
## 2010-03-22 03:01:43 - josandres - Binary operators
Good morning,

I'm trying to define a grammar for accepting binary operators, using the following expression (as seen over here):



OPERAND = NUM|IDENTIFIER



SUM_SUB = oneOf('+ -')

ML_DIV = oneOf('* /')



OPERATION = operatorPrecedence(OPERAND,

                               [ (SUM_SUB,2,opAssoc.LEFT),

                               (MUL_DIV,2,opAsoc.LEFT)

                               ]



My problem is that an expression such as 'identifier' is accepted as an operation, when it should not.



Can anyone help me?



Thanks a lot!

#### 2010-03-22 05:46:46 - ptmcg
This looks like you are on the right track.  What does your definition of IDENTIFIER look like?



(When posting code, please put [[code]] tags before and after the code, each on a line of its own, so that your code gets formatted correctly.)

---
## 2010-03-22 16:24:40 - tvn1981 - simple C Expression 
Hi,  



I am attempting to write a simple instrumenter for C code  (not the entire language but only some specific ones) and hope pyparsing can help  



First just to wet my feet  



I try to generate a grammar that recognizes simple Expression  such as

a,      

a \< b   ,  

( a == b)    ,   

( a  &&  ( a == b)  && c==d)   ,   

((foo(a[*i - 1])) && (foo(src[*i + 1])) && (src[*i - 1] \<= src[*i + 1]))



so far I have something like this   



    lp=Literal('(')

    rp=Literal(')')

    lb=Literal('[')

    rb=Literal(']')

    arith=oneOf('+ / - *')

    ptr=Literal('*')





    identifier = Word(alphas,alphanums+'_')

    integer = Word(nums)

    boolops2 = oneOf('&& ||  <h1 id="toc0">= !</h1>
 \< \> \<= \>= ')



    ptr_derf = ptr + identifier



    val = identifier | integer |ptr_derf 

    val1 = Group(val + arith + val) |  val 

    array_idx = Group(identifier+ lb +val1+ rb)



    var = array_idx|val



    funcall = Group(identifier + lp + var +rp)



    exp0= funcall | var  

    exp1= Group(exp0 + boolops2 + exp0)

    exp2 = exp1 | Group(exp0 )

    exp3 = Group(lp + exp2 + rp)   | exp2  



    exps = exp3 + boolops2 + exp3 | exp3  



    exps_f = exps + ZeroOrMore(boolops2 + exps)





The above seems to work OK  (i.e., it recognizes the expressions I gave it)  --- however since I just start with pyparser I think the above code is not right (I am expecting some recursive definition in expression but so far I haven't used any -- so something might be wrong)



I hope you can give comments to the above code so it's more correct  ---



Thanks much !

#### 2010-03-22 16:26:27 - tvn1981
sorry I think the format get messed up when posting  



boolops2 = oneOf('&& ||  \< \> \<= \>= ')  #seems like  the = = and ! = are wiki syntax so I can't put it here
#### 2010-03-22 16:30:41 - ptmcg
Try reposting, and put [[code]] tags around your code, each on a line by itself, one before and one after, and just use [[code]], there is no [[/code]].



Yes, evenutally you will need to either use Forward to define a recursive expression (see fourFn.py on the Examples page) or operatorPrecedence (as in simpleArith.py).
#### 2010-03-23 12:32:59 - tvn1981
a quick question,   



how do I match special characters such as 





'\0

'@'

'*'

'%'

'$'

'?'

'['

']'

'^'

'!'

'c'    

'-'



in other words, I want it to match the character between ' ' 



I have something like  



rules  =  Suppress(''') + Word(alphas+'@#$%^&*()-_[]{}\|!~`') + Suppress(''')  



but it doesn't match things like   '\0'  ,  '\n'

---
## 2010-03-23 08:21:20 - lagpoi - paragraphs of text using SkipTo
I am working on a text documentation processor. PP has been excellent and intuitive to use. The problem I am having is that I cannot match a paragraph that ends the file w/o a newline.





    from pyparsing import *
    
    ParserElement.setDefaultWhitespaceChars('')
    
    sample = '''\
    one line
    two line
    three line
    
    four line
    five line
    
    six
    foo'''
    
    block = LineStart() + \
        MatchFirst( SkipTo(LineEnd() + OneOrMore( LineEnd())),
                    SkipTo(StringEnd()))
    
    def bp( s,l,t):
        return 'block!(' + t[0] + ')'
    
    block.setParseAction(bp)
    
    print block.transformString( sample)



Gives the output




```
<br />
block!(one line<br />
two line<br />
three line)<br />
block!(<br />
four line<br />
five line)<br />
<br />
six<br />
foo<br />

```




If I modify the sample text, adding a trailing newline, I get.




```
<br />
block!(one line<br />
two line<br />
three line)<br />
block!(<br />
four line<br />
five line)<br />
block!(<br />
six<br />
foo)<br />

```




How do I optionally match on one or more blank lines, <strong>OR</strong> the end of the file?



I tried inverting the SkipTo and the MatchFirst but it gave the same behavior.





    block = LineStart() + SkipTo(LineEnd() + MatchFirst( OneOrMore( LineEnd()), StringEnd( )))



#### 2010-03-28 19:16:35 - ptmcg
Give this a try:



    block = LineStart() + SkipTo(LineEnd() + OneOrMore(LineEnd()) | 
                                 LineEnd() + StringEnd() | 
                                 StringEnd())



-- Paul
#### 2010-04-01 14:42:45 - lagpoi
This works wonderfully. Would it be correct to say that your implementation is equivalent to





    block = LineStart() + SkipTo( MatchFirst( [ LineEnd() + OneOrMore( LineEnd()),
                                    LineEnd() + StringEnd(),
                                    StringEnd() ]))
    



I am trying to get my head around how the operators compose.



-- lagpoi
#### 2010-04-01 17:42:12 - ptmcg
Yes, 'a | b | c' is the same as MatchFirst([a,b,c]).  '|' is the lowest precedence operator, so it gets processed last.  So 'a+d | b+e | c' becomes MatchFirst([a+d,b+e,c]).

---
## 2010-03-23 11:16:03 - shankarlingayya - pyparsing Optional
I just started using the pyparsing module for parsing the C++ header file, currently i am facing one problem with Optional() as below



sample Code :

Optional(functionSpecifierName) + Optional(typeDef) + identifier



here i specified 'functionSpecifierName' &  'typeDef' are optional,



when i give the input to the above code as only the identifier data then it is not parsing the data

even though i specified 'functionSpecifierName' & 'typeDef' as optional



-Shankar

#### 2010-03-28 20:10:00 - ptmcg
Leading an expression with several optional pieces runs the risk of accidentally matching the wrong expression with text intended for a later one.  Remember, pyparsing does no lookahead or 'longest match' matching unless you explicitly tell it to.  By default, it is just left-to-right matching.



I am struggling a little because you didn't include the definitions of functionSpecifierName, typeDef, or identifier.  But I'm guessing that you are parsing something C-like, since 'typedef' is a C keyword.  The difficulty that you have to recognize is that the identifier expression will very likely match any of the keywords in your expression.  Pyparsing doesn't know that 'typedef' is not supposed to match as an identifier.



In an expression with so many leading optional parts, I think you'll need to expand the options to force pyparsing to evaluate the parts if they are present.  Instead of:





    Optional(functionSpecifierName) + Optional(typeDef) + identifier



Try this, to force pyparsing to evaluate the expressions in the right order:





    (functionSpecifierName + Optional(typeDef) + identifier |
     typeDef + identifier |
     identifier)



-- Paul
#### 2010-03-29 06:30:01 - shankarlingayya
Thank you...

Now my code is working fine.



Thanks

-Shankar

---
## 2010-03-24 14:21:38 - tvn1981 - Recursive and operatorPrecedence question
Hi,  I tried to generate this simple recursive rule that involves both Forward and operatorPrecedence() and get errors about maximum recursion depth exceeded .  



Thanks,  









    def getRule_test():
        # exp = name | num | name[exp] | exp + exp | exp * exp | 
        name = Word(alphas)
        num = Word(nums)
    
        exp = Forward()
        idx=name + '[' + exp + ']'
    
        arith = operatorPrecedence(
            exp,[('*',2,opAssoc.LEFT),
                 ('+',2,opAssoc.RIGHT)],)
    
        exp \<\< (arith|idx|name|num)  #works ok if take out arith
        return exp
    



#### 2010-03-28 20:30:20 - ptmcg
This looks very close, I think I've done this like this in the past:





    exp = Forward()
    
    idx=name + '[' + exp + ']'
    
    operand = idx|name|num
    
    exp \<\< operatorPrecedence(operand,
         [('*',2,opAssoc.LEFT),
          ('+',2,opAssoc.RIGHT)],)
    



When I've used this, it was to add support for function calls:





    exp = Forward()
    
    idx=name + '[' + exp + ']'
    functionCall = name + '(' + Optional(delimitedList(exp)) + ')'
    
    operand = functionCall|idx|name|num
    
    exp \<\< operatorPrecedence(operand,
         [('*',2,opAssoc.LEFT),
          ('+',2,opAssoc.RIGHT)],)
    



You correctly put the idx expression before name in the definition of operand.  To add function calls, I just defined a similar function call expression and then added it to the operand expression - everything else stays the same.



-- Paul

---
## 2010-03-26 06:44:07 - elekis - typeerror :'in <string>' requires string as left operand, not int
hi, I try to make a parser for csharp (if somwone know it, I take that :D)



in a first time, I just try a simple list of token. There is my parser



[CODE]

from pyparsing_py3 import *







abstractToken    = Keyword('abstract')

asToken          = Keyword('as')

baseToken        = Keyword('base')

boolToken        = Keyword('bool')

breakToken       = Keyword('break')

byteToken        = Keyword('byte')

caseToken        = Keyword('case')

catchToken       = Keyword('catch')

charToken        = Keyword('char')

checkedToken     = Keyword('checked')

classToken       = Keyword('class')

constToken       = Keyword('const')

continueToken    = Keyword('continue')

decimalToken     = Keyword('decimal')

defaultToken     = Keyword('default')

delegateToken    = Keyword('delegate')

doToken          = Keyword('do')

doubleToken      = Keyword('double')

elseToken        = Keyword('else')

enumToken        = Keyword('enum')

eventToken       = Keyword('event')

explicitToken    = Keyword('explicit')

externToken      = Keyword('extern')

falseToken       = Keyword('false')

finallyToken     = Keyword('finally')

fixedToken       = Keyword('fixed')

floatToken       = Keyword('float')

forToken         = Keyword('for')

foreachToken     = Keyword('foreach')

gotoToken        = Keyword('goto')

ifToken          = Keyword('if')

implicitToken    = Keyword('implicit')

inToken          = Keyword('in')

intToken         = Keyword('int')

interfaceToken   = Keyword('interface')

internalToken    = Keyword('internal')

isToken          = Keyword('is')

lockToken        = Keyword('lock')

longToken        = Keyword('long')

namespaceToken   = Keyword('namespace')

newToken         = Keyword('new')

nullToken        = Keyword('null')

objectToken      = Keyword('object')

operatorToken    = Keyword('operator')

outToken         = Keyword('out')

overrideToken    = Keyword('override')

paramsToken      = Keyword('params')

privateToken     = Keyword('private')

protectedToken   = Keyword('protected')

publicToken      = Keyword('public')

readonlyToken    = Keyword('readonly')

refToken         = Keyword('ref')

returnToken      = Keyword('return')

sbyteToken       = Keyword('sbyte')

sealedToken      = Keyword('sealed')

shortToken       = Keyword('short')

sizeofToken      = Keyword('sizeof')

stackallocToken  = Keyword('stackalloc')

staticToken      = Keyword('static')

stringToken      = Keyword('string')

structToken      = Keyword('struct')

switchToken      = Keyword('switch')

thisToken        = Keyword('this')

throwToken       = Keyword('throw')

trueToken        = Keyword('true')

tryToken         = Keyword('try')

typeofToken      = Keyword('typeof')

uintToken        = Keyword('uint')

ulongToken       = Keyword('ulong')

uncheckedToken   = Keyword('unchecked')

unsafeToken      = Keyword('unsafe')

ushortToken      = Keyword('ushort')

usingToken       = Keyword('using')

virtualToken     = Keyword('virtual')

voidToken        = Keyword('void')

volatileToken    = Keyword('volatile')

whileToken       = Keyword('while')

pointvirgule     = Keyword(';')



comment1 = '<em>' + restOfLine



tokens = Forward() 



tokens \<\< ( abstractToken    |asToken          |baseToken        |boolToken        |

            breakToken       |byteToken        |caseToken        |catchToken       |

            charToken        |checkedToken     |classToken       |constToken       |

            continueToken    |decimalToken     |defaultToken     |delegateToken    |

            doToken          |doubleToken      |elseToken        |enumToken        |

            eventToken       |explicitToken    |externToken      |falseToken       |

            finallyToken     |fixedToken       |floatToken       |forToken         |

            foreachToken     |gotoToken        |ifToken          |implicitToken    |

            inToken          |intToken         |interfaceToken   |internalToken    |

            isToken          |lockToken        |longToken        |namespaceToken   |

            newToken         |nullToken        |objectToken      |operatorToken    |

            outToken         |overrideToken    |paramsToken      |privateToken     |

            protectedToken   |publicToken      |readonlyToken    |refToken         |

            returnToken      |sbyteToken       |sealedToken      |shortToken       |

            sizeofToken      |stackallocToken  |staticToken      |stringToken      |

            structToken      |switchToken      |thisToken        |throwToken       |

            trueToken        |tryToken         |typeofToken      |uintToken        |

            ulongToken       |uncheckedToken   |unsafeToken      |ushortToken      |

            usingToken       |virtualToken     |voidToken        |volatileToken    |

            whileToken       |pointvirgule)       





ident = Word( alphas, alphanums ).setName('identifier')



parser = Forward()

parser \<\< ZeroOrMore( tokens | ident )

parser.ignore(comment1)



if <u>name</u> == '<u>main</u>':

  prg = parser.parseFile('E:\CSParser\Test\HelloWorld.cs')

[/CODE]



my hello world looks like

[CODE]

    using System;



public class HelloWorld

{

    public static void Main()

    {

        </em> This is a single line comment

        /* This is a

        multiple

        line comment */

        Console.WriteLine('Hello World! From Softsteel Solutions');

    }

}

[/CODE]



but when I lauch the stuff, I have the following error 

[CODE]

E:\CSParser\Engine\>python CSMain.py

Traceback (most recent call last):

  File 'CSMain.py', line 117, in \<module\>

    prg = parser.parseFile('E:\CSParser\Test\HelloWorld.cs')

  File 'C:\Python31\lib\site-packages\pyparsing_py3.py', line 1409, in parseFile



    return self.parseString(file_contents, parseAll)

  File 'C:\Python31\lib\site-packages\pyparsing_py3.py', line 1067, in parseStri

ng

    loc, tokens = self._parse( instring, 0 )

  File 'C:\Python31\lib\site-packages\pyparsing_py3.py', line 934, in _parseNoCa

che

    preloc = self.preParse( instring, loc )

  File 'C:\Python31\lib\site-packages\pyparsing_py3.py', line 887, in preParse

    loc = self._skipIgnorables( instring, loc )

  File 'C:\Python31\lib\site-packages\pyparsing_py3.py', line 879, in _skipIgnor

ables

    loc,dummy = e._parse( instring, loc )

  File 'C:\Python31\lib\site-packages\pyparsing_py3.py', line 934, in _parseNoCa

che

    preloc = self.preParse( instring, loc )

  File 'C:\Python31\lib\site-packages\pyparsing_py3.py', line 892, in preParse

    while loc \< instrlen and instring[loc] in wt:

TypeError: 'in \<string\>' requires string as left operand, not int



E:\CSParser\Engine\>

[/CODE]



any idea ?



thanks



a+

#### 2010-03-28 20:55:04 - ptmcg
PLEASE use the [[code]] tags before and after your code samples, each on a line of its own.



Can you give this a try with the latest version of pyparsing_py3.py from the pyparsing SVN repository?  You can also get it from this pastebin page: 



-- Paul

---
## 2010-03-27 10:35:28 - shankarlingayya - How to ignore the next line in pyarsing


I want to ignore the next line when i get '\' symbol during my parsing.



currently pyparsing has 'restOfLine' to ignore the rest of the line in the current line itself,



so in a similar way is there any way to skip the next line...



Thanks

-Shankar


---
## 2010-03-27 11:13:18 - shankarlingayya - How to skip the data between { and } in pyparsing
I am writing a parser for C++ header file, and i am facing one problem.



i want to extract only the function signature from a class, but some times class may also contains function definition



is there any way to ignore the data between '{' and '}', then my below code will work:



paramlist = ( delimitedList( Group( (typeName + identifier + Optional( equals + ( identifier ^ Combine(quote+identifier+quote) ^ integer ^ real ) ) ) | (ptrTypeName + identifier) | (typeName + ptrIdentifier)  ) ) )



operationDef = Group( ( Optional(functionSpecifierName) + Optional(typeDef) + identifier + lparen + Optional(paramlist) + rparen + Optional(equals+zero) + semi ) | ( Optional(functionSpecifierName) + identifier + lparen + Optional(paramlist) + rparen + Optional(equals+zero) + Optional(semi) ) | ( Optional(functionSpecifierName) + Optional(ptrTypeDef) + identifier + lparen + Optional(paramlist) + rparen + Optional(equals+zero) + semi ) | ( Optional(functionSpecifierName) + identifier + lparen + Optional(paramlist) + rparen + Optional(equals+zero) + Optional(semi) ) | ( Optional(functionSpecifierName) + Optional(typeDef) + ptrIdentifier + lparen + Optional(paramlist) + rparen + Optional(equals+zero) + semi ) | ( Optional(functionSpecifierName) + identifier + lparen + Optional(paramlist) + rparen + Optional(equals+zero) + Optional(semi) ) | ( Optional(functionSpecifierName) + Optional(typeDef) + identifier + lparen + Optional(paramlist) + rparen + Optional(const_) + semi ) | ( Optional(functionSpecifierName) + identifier + lparen + Optional(paramlist) + rparen + Optional(const_) + Optional(semi) ) | ( Optional(functionSpecifierName) + Optional(ptrTypeDef) + identifier + lparen + Optional(paramlist) + rparen + Optional(const_) + semi ) | ( Optional(functionSpecifierName) + identifier + lparen + Optional(paramlist) + rparen + Optional(const_) + Optional(semi) ) | ( Optional(functionSpecifierName) + Optional(typeDef) + ptrIdentifier + lparen + Optional(paramlist) + rparen + Optional(const_) + semi ) | ( Optional(functionSpecifierName) + identifier + lparen + Optional(paramlist) + rparen + Optional(const_) + Optional(semi) ) | ( enipFunctionExport) ).setResultsName('operation')


---
## 2010-03-27 12:37:47 - Elby - Good usage of setWhitespaceChars
    Hi,



I've made a tentative to detect matrix in a text file:





    from    pyparsing   import *
    from    pprint        import pprint
    from    numpy        import array, NAN
    
    test = '''
    [ matrix ] 
    1   2.  3   4
    2 NaN   6   8
    3   6  12  24    
    
    '''
    
    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
    def convertNumber(t):
        '''Convert a string matching a number to a python number'''
        if t.float : return [float(t[0])]
        else       : return [int(t[0]) ]
    
    # Number defintion
    sign    = oneOf('+ -')
    integer = Word(nums)
    
    number  = Combine(
              Optional(sign)
                     + (   ( integer + Optional('.')('float') + Optional(integer))  # match 0 or 0.02 
                         | (            Literal('.')('float') + Optional(integer))  # match .02
                       )
                     + Optional( CaselessLiteral('E') + Optional(sign) + integer)('float')
            ).setParseAction(convertNumber)
    
    numValue = ( number
           | Keyword('True').setParseAction(replaceWith(True))
           | Keyword('False').setParseAction(replaceWith(False))
           | Keyword('NAN', caseless=True).setParseAction(replaceWith(NAN))
           | Keyword('None').setParseAction(replaceWith(None))
        )
    
    
    SOL     = LineStart().suppress()
    EOL     = LineEnd().suppress()
    rowValue = numValue.setWhitespaceChars(' \t')
    
    matrixLine = Forward()
    
    def defineMatrixCol(t):
        ''' define matrixLine to match the same number of col than t has '''
        print 'First row is : ', t
        nbcols = len(t[0])
        matrixLine \<\< Group( SOL + rowValue*nbcols + EOL )
    
    matrixDef = (
          Group( SOL + OneOrMore(rowValue) + EOL).setParseAction(defineMatrixCol)
        + OneOrMore(matrixLine)
        )
    
    for r , s, t in matrixDef.scanString(test):
        print 'r : '
        print r.dump()
    



This gives this output : 





    First row is :  [[1, 2.0, 3, 4, 2, nan, 6, 8, 3, 6, 12, 24]]
    First row is :  [[2, nan, 6, 8, 3, 6, 12, 24]]
    First row is :  [[3, 6, 12, 24]]



My 'first row' seems to contains the full matrix and it seems that newline was considered as a whitespace, despite the line : 



    rowValue = numValue.setWhitespaceChars(' \t')



I've tried to put this method call on the OneOrMore element, but this doesn't seems to work either.



Could you explain me where/how the setWhitespaceChars method should be called ? 



Thanks a lot



-- Elby

#### 2010-03-29 00:32:03 - ptmcg
Since your input text is line-oriented, then I would set the default set of whitespace characters instead of individually setting them on the low level expressions.  Right after you import pyparsing, add:





    ParserElement.setDefaultWhitespaceChars(' \t')



Then all expressions that you create after that will not automatically skip over end-of-lines.



I think you'll be able to leave SOL's out of your grammar, but the EOL's will definitely be required.  Your definition of EOL=LineEnd() was going to be my next suggestion, but I see that you have done this already.



-- Paul
#### 2010-03-29 10:45:31 - Elby
This code is only a part of a bigger parser, and I need to skip over end-of-lines in some other parts.



If I just set DefaultWhitespaceChars to ' \t' I just get a ParseException later.

So I will have to set  WhitespaceChars on some element to ' \t\r\n', and I don't understand where I should put this.



Could you explain me a bit further how the setWhitespaceChars 'propagate' when you combine elements ?



I've just made a test with the following definition of matrixDef:



    matrixDef = (
          Group( SOL.setWhitespaceChars(' \t')
                  + OneOrMore(rowValue.setWhitespaceChars(' \t')).setWhitespaceChars(' \t') 
                  + EOL.setWhitespaceChars(' \t')
                  ).setWhitespaceChars(' \t').setParseAction(defineMatrixCol)
        + OneOrMore(matrixLine)
        )

and this gives the same results.



As rowValue match only number or python keywords, it should not match any end-of-lines, or do I miss something ?





-- Elby
#### 2010-03-30 16:49:19 - ptmcg
Looking through the pyparsing code, the most common propagation of whitespace characters occurs in:



- And (or the expression created by 'expr + expr') will acquire the whitespace characters of the first expression



- LineStart and LineEnd take the default whitespace characters, minus '\n'



- all subclasses of ParseElementEnhance (typically NotAny, FollowedBy, Optional, Group, One/ZeroOrMore, Forward, Combine, Dict, and Suppress) assume the whitespace characters of the expression they are constructed with



Note that expr1+expr2+expr3 gets evaluated as (expr1+expr2)+expr3, so any sequence of expr+... will assume the whitespace characters of the left-most expression.



On the other hand leaveWhitespace will recursively navigate all sub and comprising expressions and call leaveWhitespace on them as well.



Since you define matrixLine as Group( SOL + rowValue*nbcols + EOL ), it looks like matrixLine should already have the whitespace characters of ' ' and '\t' without your having to do anything.  What you can do is to try interactively debugging some of these expressions - expr.setBreak() will open the pdb debugger when expr is about to be parsed - and look at the contents of self.whiteChars.





One other item - I've found that low-level expressions like floating point numbers really do much better using the Regex class, instead of building one up with Combine, Optional, oneOf, etc.  I know it feels like cheating, but in truth, pyparsing uses a number of re's under the covers.



Instead of:



    sign    = oneOf('+ -')
    integer = Word(nums)
    
    number  = Combine(
              Optional(sign)
                     + (   ( integer + Optional('.')('float') + Optional(integer))  # match 0 or 0.02 
                         | (            Literal('.')('float') + Optional(integer))  # match .02
                       )
                     + Optional( CaselessLiteral('E') + Optional(sign) + integer)('float')
            ).setParseAction(convertNumber)



I'd suggest





    number = Regex(r'[+-]?(\d+(\.\d*)?|\.\d+)([Ee][+-]?\d+)?')
    number.setParseAction(convertNumber)





-- Paul

---
## 2010-03-30 09:05:05 - mvilanova - Continue parsing after \r character
Hi folks,



I am trying to parse an instant messaging log file that has the following format:





    20100329140033,sender@hostname,receiver@hostname,message



I have defined that a message will have two parts: message_id and message_body. Following is my code:





    import sys
    from pyparsing import *
    
    comma = Suppress(',')
    timestamp = Regex('[0-9]{14}')
    sender_email = Regex('([A-Za-z0-9._%+-]+)@([A-Za-z0-9.-]+\.[A-Za-z]{2,4}|([0-9]{1,3}\.){3}[0-9]{1,3})')
    receiver_email = Regex('([A-Za-z0-9._%+-]+)@([A-Za-z0-9.-]+\.[A-Za-z]{2,4}|([0-9]{1,3}\.){3}[0-9]{1,3})')
    
    message_id = Group(timestamp + comma + Optional(sender_email) + comma + Optional(receiver_email) + comma)
    message_body = ?
    message = Group(message_id + message_body)
    transcript = OneOrMore(message)
    
    f = open(sys.argv[1],'r')
    t = transcript.parseString(f.read())
    
    for m in range(len(t)):
            print 'Message #' + str(m) + ' -\> ' + str(t[m])
            f.close()



What I should specify within the message_body for letting the parser know that a message's content will be anything between two messages_id?



Thanks in advance.



Marc

#### 2010-03-30 09:40:02 - mvilanova
Solved!



message_body = SkipTo(message_id,include=False, ignore=None, failOn=None)
#### 2010-03-30 16:24:41 - ptmcg
Glad you found a solution.  You can also try using restOfLine.



-- Paul
#### 2010-03-31 02:37:26 - mvilanova
Hi Paul,



I tried with restOfLine, but I could not get the messages' content when those are contained in multiple lines. I don't know how to let the parser read beyond a '\n' :-)



So, I decided to use SkipTo in the following way:





    message_body = Group(SkipTo(message_id,include=False, ignore=None, failOn=None))



and add a fake 'message_id' at the end of the file for being able to parse the last line. If you know a more elegant way, please let me know.



Thanks in advance.



Marc
#### 2010-03-31 05:19:32 - mvilanova
Hi Paul,



Another question, Do you know how can I remove OneOrMore('\n' | blank_lines) when I am using SkipTo? Here is my code:





    #!/bin/env python
    
    import sys
    from pyparsing import *
    
    #ParserElement.setDefaultWhitespaceChars(' \t\r')
    #SOL = LineStart()
    #EOL = LineEnd().suppress()
    #blank_line = SOL + ZeroOrMore(EOL)
    
    # Message ID
    comma = Suppress(',')
    timestamp = Regex('[0-9]{14}')
    sender_email = Regex('([A-Za-z0-9._%+-]+)@([A-Za-z0-9.-]+\.[A-Za-z]{2,4}|([0-9]{1,3}\.){3}[0-9]{1,3})')
    receiver_email = Regex('([A-Za-z0-9._%+-]+)@([A-Za-z0-9.-]+\.[A-Za-z]{2,4}|([0-9]{1,3}\.){3}[0-9]{1,3})')
    message_id = Group(timestamp + comma + Optional(sender_email) + comma + Optional(receiver_email) + comma)
    
    # Message Body
    message_body = Group(SkipTo(message_id,include=False, ignore=None, failOn=None))
    
    # Message
    message = Group(message_id+message_body)
    
    # Transcript
    transcript = ZeroOrMore(message)
    
    # Ignores
    #transcript.ignore(blank_line)
    
    try:
        f = open(sys.argv[1],'r')
        t = transcript.parseString(f.read())
        f.close()
    
        for m in range(len(t)):
            print t[m]
    except IOError:
        print 'File does not exist!'



---
## 2010-04-01 14:44:24 - tvn1981 - delimitedList of one or two or three fields
Hi, I try to write a rule that parse 

something like  

(something1 something2, something, something3 something4 something 5) or ().  In other words, an delimited List of zeroOrMore fields of [1 to 3] alphas 



The rule I wrote looks like this:  



'(' + ZeroOrMore(delimitedList(al+al+al | al+al | al)) + ')'  where al = Word(alphas)



it works OK but just wondering if there's something nicer built in pyparsing ?   



Thanks ,

#### 2010-04-01 17:38:06 - ptmcg
How about:



    '(' + Optional(delimitedList(al*(0,3))) + ')'



delimitedList will take care of the repetition within the parens, you just need to use Optional to indicate that there might not be any list entries at all.



-- Paul
#### 2010-04-01 17:39:44 - ptmcg
Oops, I guess that should be al*(1,3).  You can multiply by an integer or a tuple.  This is similar to the '{min,max}' repetition operators in re.

---
## 2010-04-03 11:15:19 - breamoreboy - new regex module
Just curious to know if anybody has tried pyparsing with the new version of the regex module as here .  If so would you please be kind enough to share your experiences.  Same applies to the undocumented Scanner class within this module, see  and

 for more data.



Kindest regards.



Mark Lawrence

#### 2010-04-03 13:56:31 - ptmcg
Sorry, installation fail:



    C:\Downloads\Python\regex-0.1.20100331\>python setup.py install
    running install
    running build
    running build_py
    creating build
    creating build\lib.win32-2.5
    copying regex.py -\> build\lib.win32-2.5
    running build_ext
    error: Python was built with Visual Studio 2003;
    extensions must be built with a compiler than can generate compatible binaries.
    Visual Studio 2003 was not found on this system. If you have Cygwin installed,
    you can try compiling with MingW32, by passing '-c mingw32' to setup.py.



I actually have VS2005 installed, too bad this installer doesn't know how to use it.  Any chance you could release a Windows-compatible binary?



Pyparsing doesn't use the Scanner class, so it is unlikely that change will help directly.
#### 2010-04-04 04:11:59 - breamoreboy
If I were the maintainer and/or had the faintest idea what I was doing, I would try to build a Windows compatible binary, however. The bad news is, you must have a compiler that is compatible with your version of Python, see e.g. .  The good news is there is a Python25 directory with the release that has a _regex.pyd file.  Copy this and regex.py to site-packages and you should be in business.
#### 2010-04-04 08:29:14 - ptmcg
Ok, I copied the .pyd file and regex.py now works.  I changed 'import re' to 'import regex as re' to use the new code.  My unit tests all pass, so it would seem that regex.py runs equivalently to re.  My tests don't really exercise performance though - in fact, I'm fairly sure that the re time in pyparsing is dwarfed by all the performance sucked up into function calling.



-- Paul

---
## 2010-04-03 11:46:55 - tvn1981 - match everything between '('   ')'
I try to write a rule to match anything that has '(' ')' around  but not sure what to put for 'anything'



e.g., '('  +  anything  + ')'    should match  

(1,2,3) or ('hi',3)  or (*$32,4*/ ,[])

#### 2010-04-03 13:46:03 - ptmcg
If you know that your parens won't be nested then, this will do:



    '(' + SkipTo(')') + ')'



Otherwise, you should use nestedExpr:



    nestedExpr('(',')')



-- Paul
#### 2010-04-03 14:39:18 - tvn1981
nice ---    I didn't know about these 2 keywords.  Thanks
#### 2010-04-04 01:36:14 - ptmcg
Did you by chance download the Windows binary distribution from SourceForge?  If so, please go back to SF and get the source distribution (just download it, no need to install it).  This distribution includes a copy of all of the examples, plus a doc directory with epydoc-generated HTML documentation, and a class diagram showing all the pyparsing classes and helpers.

---
## 2010-04-09 15:45:40 - edc - problem -- pyparsing results contains extra empty dictionaries (or braces)
Hi all,



First post here -- greetings everyone.



I wrote some code using pyparsing to parse some input file of the following format:





    obj1 {
        attr1a entry1a;
        list1a (item1a item1b);
        list1b (.5 .0);
    }
    obj2 {
        attr2a entry2a;
        attr2b entry2b;
    }





The code I wrote was:



fB = open('someFile.txt','r')

expressions = fB.read();



LBRACE,RBRACE,SEMI = map(Suppress,'{};')

LPAREN,RPAREN = map(Suppress,'()')



intEntry = Combine(Optional('-') + Word(nums)) \

           .setParseAction(lambda t : int(t[0]))

realEntry = Combine(Optional('-') + Optional(Word(nums)) \

                    + '.' + Optional(Word(nums))) \

           .setParseAction(lambda t : float(t[0]))

numEntry = realEntry | intEntry

realOrSciEntry = Combine(  numEntry \

                         + Optional(CaselessLiteral('E') \

                                    + Word('+-'+nums,nums))) \

                 .setParseAction(lambda t : float(t[0]))



<ol><li>define tokens, expressions and entries</li></ol>objEntry = Forward()

listEntry = Forward()



keyToken = Word(alphas+'_', alphanums+'_')

entryToken = ( keyToken | intEntry | realOrSciEntry |

               quotedString.copy().setParseAction(removeQuotes))



<ol><li>define lists</li></ol>numList = (OneOrMore(realOrSciEntry))

intList = (OneOrMore(intEntry))

keyTokenList = (OneOrMore(keyToken))

entryList = intList | numList | keyTokenList



<ol><li>define expressions and entries</li></ol>objExpr = Group(keyToken + objEntry)

expr = Group(keyToken + entryToken + SEMI) \

      | Group(keyToken + LPAREN + entryList + RPAREN + SEMI)



<ol><li>expr needs to come before the list ones</li></ol>mixedExpr = (expr | objExpr)



objEntry \<\< (  LBRACE + Dict(OneOrMore(mixedExpr)) + RBRACE)



dict_t = Dict(OneOrMore(mixedExpr)) \

          .ignore(cStyleComment).ignore('//' + restOfLine) \

          .parseString(expressions,parseAll=True)

fB.close()



print dict_t.dump()



for ki in dict_t.keys():

    print dict_t[ki].values()



    
    
    upon 'dump()' the results look like this:

[['obj1', ['attr1a', 'entry1a'], ['list1a', 'item1a', 'item1b'], ['list1b', 0.5, 0.0]], ['obj2', ['attr2a', 'entry2a'], ['attr2b', 'entry2b']]]

- obj1: [['attr1a', 'entry1a'], ['list1a', 'item1a', 'item1b'], ['list1b', 0.5, 0.0]]

  - attr1a: entry1a

  - list1a: ['item1a', 'item1b']

  - list1b: [0.5, 0.0]

- obj2: [['attr2a', 'entry2a'], ['attr2b', 'entry2b']]

  - attr2a: entry2a

  - attr2b: entry2b



    
    all is fine.  But then when I access the result values as dictionaries (using the for statement in the last part of the code above), I got this:
    

['entry1a', (['item1a', 'item1b'], {}), ([0.5, 0.0], {})]

['entry2a', 'entry2b']



    
    I want to get rid of the extra braces when the input is a list.  I just want a nested dictionary of the input, something like this:
    

{'obj1': [{'attr1a':'entry1a'}, 

          {'list1a':['item1a1', 'item1a2']},

          {'list1b':['item1b1', 'item1b2']},]},

{'obj2': [{'attr2a':'entry2a'},

          {'attr2b':'entry2b'}]}





It's very frustrating.  Any help is appreciated!!

#### 2010-04-09 15:51:20 - edc
Sorry -- fixed the code segment formatting:



I wrote some code using pyparsing to parse some input file of the following format:





    obj1 {
        attr1a entry1a;
        list1a (item1a item1b);
        list1b (.5 .0);
    }
    obj2 {
        attr2a entry2a;
        attr2b entry2b;
    }





The code I wrote was:





    fB = open('someFile.txt','r')
    expressions = fB.read();
    
    LBRACE,RBRACE,SEMI = map(Suppress,'{};')
    LPAREN,RPAREN = map(Suppress,'()')
    
    intEntry = Combine(Optional('-') + Word(nums)) \
              .setParseAction(lambda t : int(t[0]))
    realEntry = Combine(Optional('-') +
                Optional(Word(nums)) \
                + '.' + Optional(Word(nums))) \
               .setParseAction(lambda t : float(t[0]))
    numEntry = realEntry | intEntry
    realOrSciEntry = Combine( numEntry \
                    + Optional(CaselessLiteral('E') \
                    + Word('+-'+nums,nums))) \
                    .setParseAction(lambda t : float(t[0]))
    
    #define tokens, expressions and entries
    objEntry = Forward()
    listEntry = Forward()
    
    keyToken = Word(alphas+'_', alphanums+'_')
    entryToken = ( keyToken | intEntry | realOrSciEntry |
    quotedString.copy().setParseAction(removeQuotes))
    
    #define lists
    numList = (OneOrMore(realOrSciEntry))
    intList = (OneOrMore(intEntry))
    keyTokenList = (OneOrMore(keyToken))
    entryList = intList | numList | keyTokenList
    
    #define expressions and entries
    objExpr = Group(keyToken + objEntry)
    expr = Group(keyToken + entryToken + SEMI) \
    | Group(keyToken + LPAREN + entryList + RPAREN + SEMI)
    
    mixedExpr = (expr | objExpr)
    
    objEntry \<\< ( LBRACE + Dict(OneOrMore(mixedExpr)) + RBRACE)
    
    dict_t = Dict(OneOrMore(mixedExpr)) \
            .ignore(cStyleComment).ignore('//' + restOfLine) \
            .parseString(expressions,parseAll=True)
    fB.close()
    
    print dict_t.dump()
    
    for ki in dict_t.keys():
        print dict_t[ki].values()



upon 'dump()' the results look like this:





    [['obj1', ['attr1a', 'entry1a'], ['list1a', 'item1a', 'item1b'], ['list1b', 0.5, 0.0]], ['obj2', ['attr2a', 'entry2a'], ['attr2b', 'entry2b']]]
    - obj1: [['attr1a', 'entry1a'], ['list1a', 'item1a', 'item1b'], ['list1b', 0.5, 0.0]]
    - attr1a: entry1a
    - list1a: ['item1a', 'item1b']
    - list1b: [0.5, 0.0]
    - obj2: [['attr2a', 'entry2a'], ['attr2b', 'entry2b']]
    - attr2a: entry2a
    - attr2b: entry2b



all is fine.  But then when I access the result values as dictionaries (using the for statement in the last part of the code above), I got this:





    ['entry1a', (['item1a', 'item1b'], {}), ([0.5, 0.0], {})]
    ['entry2a', 'entry2b']



I want to get rid of the extra braces when the input is a list.  I just want a nested dictionary of the input, something like this:





    {'obj1': [{'attr1a':'entry1a'}, {'list1a':['item1a1', 'item1a2']}, {'list1b':['item1b1', 'item1b2']},]}, 
    {'obj2': [{'attr2a':'entry2a'}, {'attr2b':'entry2b'}]}



It's very frustrating. Any help is appreciated!!
#### 2010-04-09 21:09:05 - ptmcg
Pyparsing does not really return lists or dicts, but a custom class called ParseResults.  The 'extra' braces when you print out the results aren't really there, they just show any results names that might be defined for the ParseResults.



Check out the asList and asDict methods on the ParseResults class, and see if these help you in your working with the results.  But really, other than the printed output, you can just treat these objects as if they *are* lists or dicts.  You can str.join() them, you can iterate over them (as you have already found), you can pickle them, you can call len(), get(), pop(), or just about any other list or dict method on them.



-- Paul
#### 2010-04-09 21:09:29 - ptmcg
Oh! and... Welcome to Pyparsing!
#### 2010-04-09 21:13:17 - ptmcg
Looking a little more at your code, this is going to be a problem for you:





    entryToken = ( keyToken | intEntry | realOrSciEntry |
    quotedString.copy().setParseAction(removeQuotes))



By matching for intEntry before realOrSciEntry, you will match the leading whole part of a real number, and then get stuck with the rest of the number not matching anything. '|' operators create MatchFirst, or left-to-right match; '^' operators create Or, or longest-match.  You could replace '|' with '^' in this expression, but if you are careful with the order, you can stick with the more efficient '|' operator.  (Similar issue with the entryList.)



-- Paul

---
## 2010-04-16 11:20:06 - JoshuaREnglish - The Time Parser Script
This is brilliant. Who wrote it so I can give them credit in my code?

#### 2010-06-23 06:21:17 - ptmcg
Glad you liked this!  You can cite the pyparsing wiki as your source, with Paul McGuire as the author.  I've written many of the provided examples on the wiki, and those that have been submitted to me are attributed to their respective authors.

---
## 2010-04-19 02:12:09 - decay_of_mind - How to parse a different type of SQL queries
Hi, I am trying to write an flex SQL SELECT parser. This is my code:



    ####################
    ###### GRAMMAR #####
    ####################
    
    S_QUOTE = Literal(''').suppress()
    D_QUOTE = Literal(''').suppress()
    DOT = Literal('.').suppress()
    COMMA = Literal(',').suppress()
    SEMI = Literal(';').suppress()
    COLON = Literal(':').suppress()
    EQUAL = Literal('=')
    LANGLE = Literal('\<').suppress()
    LBRACE = Literal('[').suppress()
    LPAREN = Literal('(')
    PLUS = Literal('+').suppress()
    RANGLE = Literal('\>').suppress()
    RBRACE = Literal(']').suppress()
    RPAREN = Literal(')')
    
    s_select = CaselessKeyword('select')
    s_from = CaselessKeyword('from')
    s_where = CaselessKeyword('where')
    s_and = CaselessKeyword('and')
    s_as = CaselessKeyword('as').suppress()
    
    t_val = Word(alphanums + '_')
    num = Word(nums + '+-.')
    var = Word(alphas + '%')
    entry = Group(Optional(t_val + DOT).setResultsName('table') + \
            t_val.setResultsName('name')).setResultsName('entry')
    pseudo = Optional(s_as) + (t_val).setResultsName('pseudo')
    table = Group(t_val.setResultsName('name') + Optional(pseudo)).setResultsName('table')
    column = Group(entry + Optional(pseudo)).setResultsName('column')
    proc = entry + LPAREN + Optional(Suppress(SkipTo(RPAREN))) + RPAREN
    
    expr = Group(Optional(LPAREN).suppress() + entry + oneOf('= \> \<').setResultsName('type') + \
           Optional(D_QUOTE) + (proc | entry | var | num) + Optional(D_QUOTE) + \
           Optional(RPAREN).suppress()).setResultsName('expr')
    
    select_q = Group(s_select + Group(delimitedList(column)).setResultsName('what') + \
                s_from + Group(delimitedList(table)).setResultsName('whence') + s_where + \
                Group(delimitedList(expr, s_and)).setResultsName('where')).setResultsName('select')



But unfortunately I have big problem with this test example:



    test1 = '''
    SELECT d2.depo_acc_id, d2.depo_acc_code
                      FROM depo_accounts d1, depo_accounts d2, account_types a
                      WHERE
                      (d1.serve_deposit_id = 100.0)
                      AND (d1.depo_acc_code = ' ed_DepaccP.Text ')
                      AND (a.assets_or_liabilit = 'S')
                      AND (d1.deponent_id = d2.deponent_id)
                      AND (a.account_type = d2.account_type)
    '''

Parser expects FROM after FROM because accepts FROM in 'd2.depo_acc_code FROM' as pseudo attribute of d2.depo_acc_code :(

I tried to use NotAny with 'FROM', but no useful result :(

#### 2010-04-19 09:03:28 - ptmcg
Note that you have no restrictions on what can be a valid 'pseudo'.  'FROM' is being processed as the pseudo for d2.depo_acc_code, then the parser moves forward looking for 'FROM', but it has already gone past.  You'll probably need to preface the definition of pseudo with a ~KEYWORD and define KEYWORD to be any of the keywords you have defined.
#### 2010-04-19 23:56:55 - decay_of_mind
Thanks a lot. It works! Previously, I've tried to use negation after pseudo, with no result of course.



I also write another solution for pseudo. It's big and dirty, but it works:



    column = (delimitedList(entry) ^ delimitedList(entry + pseudo) ^ delimitedList(entry + s_as + pseudo))



---
## 2010-05-03 03:04:18 - gareth8118 - Passing an object method to setParseAction?
Is there a way to pass an instance method as a parse action? I'm trying to wrap 3 parsers into a single class and I'd like each parse action to be a method in this class. Is that doable? I'm not greatly experienced in Python so maybe this is a stupid question.





    class Wrapper(object):
       ...
       def parse_action(self, tok):
          ...
       def create_parser(self):
          parser = blahblah
          parser.setParseAction(parse_action) # \<= like this
          return parser



#### 2010-05-03 06:09:25 - ptmcg
Try:



    parser.setParseAction(self.parse_action)



-- Paul
#### 2010-05-04 08:35:48 - gareth8118
Excellent! thanks

---
## 2010-05-03 04:22:51 - rgammans - (Easy_)Installing on windows....
If pyparsing is a dependency of another then then setuptools can be configured to automatically call 'easy_install pyparsing' if it is not already installed.



Unfortunately it downloads the windows install and tries to run setup.py from inside it. - Which it doesn't contain.

See:-



C:\\>c:\Python26\Scripts\easy_install.exe pyparsing

Searching for pyparsing

Reading 

Reading 

Reading 

Reading 

Reading 

Reading 

Best match: pyparsing 1.5.2

Downloading 

1.5.2/pyparsing-1.5.2.win32.exe/download

Processing download

error: Couldn't find a setup script in c:\docume~1\roger\locals~1\temp\easy_inst

all-heexie\download





The executable is not easy to run either as it requires MSVCR71.dll which is not universally deployed on windows XP.  



Is there any chance of having a window bdist zipfile or something similiar on pypi which will let this work. 



Otherwise I hae to put a seriously complicated set of installation instructions for my app which the non-technical users will shy away from.

#### 2010-05-03 18:25:19 - ptmcg
I have been promising this next release (1.5.3) for months now, but I think I'll actually get some time for it very soon, and I'll try to include a windows bdist zip as one of the options.  As a workaround, you can just include the pyparsing.py file in with your own code.  I intentionally kept pyparsing's code footprint down to a single source file, so it would be easy to include with a project's code.
#### 2010-05-28 04:27:59 - merridus
If you download the pyparse zip file and then easy_install \<location of zip file\> it should work.

---
## 2010-05-04 11:57:59 - gareth8118 - Unexpected list grouping of named result
Another day, another question....



When I run the parser shown below (trying to pick VHDL generics out of source code) I get the following output:





    ['C_FAMILY', 'string', 'spartan6']
    - generic: ['C_FAMILY', 'string', 'spartan6']
      - generic_name: C_FAMILY
      - initialization_value: spartan6
      - type_name: ['string']
    - generic_name: C_FAMILY
    - initialization_value: spartan6
    - type_name: ['string']



I don't understand why, in the named results, the generic_name and initialization_value are bare strings, whereas the type_name is inside a list - especially when you look at the overall ParseResults which shows a simple flat list of the three tokens. Can anyone explain?



Cheers

Gareth





    from pyparsing import *
    
    
    test_string = 'C_FAMILY : string := 'spartan6''
    
    def handle_generic(tok):
        print tok.dump()
    
    LPAREN, RPAREN, COLON, SEMICOLON = map(Suppress, '():;')
    ASSIGN = Suppress(':=')
    identifier = Word(alphas, alphas+nums+'_')
    initialization_value = QuotedString(''').setResultsName('initialization_value');
    initialization = ASSIGN + initialization_value;
    TO = CaselessKeyword('to')
    DOWNTO = CaselessKeyword('downto')
    range_direction = TO | DOWNTO
    range_index = Word(nums).setParseAction( lambda tokens : int(tokens[0]) )
    range_definition = Group( \
        LPAREN \
        + range_index + range_direction + range_index \
        + RPAREN)
    type_name = (identifier + Optional(range_definition))('type_name')
    generic_name = identifier('generic_name')
    generic_formal_declaration = (\
        generic_name + COLON + type_name + \
        Optional(initialization))('generic')
    generic_formal_declaration.setParseAction(handle_generic)
    
    generic_formal_declaration.parseString(test_string)
    
    BNF = '''
            generic_formal_declaration ::=
                identifier : type [ := initialization ]
    
            type_name ::=
                identifier | indexed_type | sliced_type
    
            sliced_type ::=
                identifier '(' range_definition ')'
    
            range_definition ::=
                range_index range_direction range_index
    
            direction ::=
                'to' | 'downto'
    
            initialization ::=
                quoted_string
    '''



#### 2010-05-04 22:17:36 - ptmcg
Your type_name will always return a list structure since it is composed by And'ing several smaller expressions.  type_name in your expression isn't always just a string, sometimes it might also have a range_definition.  What if your source string was 'C_FAMILY : string(1 to 5) := 'spartan6''?  So type_name will always be a list, sometimes with just the name string, sometimes also including the optional range definition.  This will make your post-parsing handling for this particular expression more consistent (although it looks inconsistent compared to parsing of other expressions).



-- Paul
#### 2010-05-05 06:02:03 - gareth8118
OK, that makes sense - but I guess what surprised me is that the list of tokens in 'generic' result <em>isn't</em> grouped this way - should I not expect:





    - generic: ['C_FAMILY', ['string'], 'spartan6']



in this case?

---
## 2010-05-04 13:13:00 - skuda - Getting original matched text
Hello, i am using pyparsing to read any sql filters and group by columns filtered, but i need, apart from parse it, get the original text, if for example i have three groups (three different columns) i need to get the original text that have matched the parser.



Anything like 'parser_object.parseString(str_filter)[0].originalText', i could do a ' '.join(result[0]), but the result it is not pretty because it is not formatted the same way of the original text.

#### 2010-05-04 14:47:26 - skuda
I have found a way to do it here  thanks.
#### 2010-05-04 22:09:36 - ptmcg
Pyparsing 1.5.2 introduced the helper originalTextFor, which should be used in place of the parse action keepOriginalText.  For instance, look at the resulst from parsing the IP address 1.2.3.4 using OneOrMore(Word(nums) + Optional('.')):



    \>\>\> print OneOrMore(Word(nums) + Optional('.')).parseString('1.2.3.4')
    ['1', '.', '2', '.', '3', '.', '4']



But wrap the expression in originalTextFor:



    \>\>\> print originalTextFor(OneOrMore(Word(nums) + Optional('.'))).parseString('1.2.3.4')
    ['1.2.3.4']



Now you get the original input source string, not the separate tokens from the expression.  originalTextFor is also more efficient, not having to walk the call stack using the inspect module.



-- Paul
#### 2010-05-06 07:36:21 - skuda
i am using the parserAction with a function that use getContentsEndLoc and create a list with every group matched, so later i can get with the same index the original text from my list and the tokens from the pyparsing object



I am not having problem with the performance because i am only parsing 3 o 4 conditions. 



I am having other problem, here it is my grammar:





    
            parent_ini = Literal('(')
            parent_fin = Literal(')')
            _in = Literal('IN')
            _notin = Literal('NOT IN')
            _between = Literal('BETWEEN')
            _notbetween = Literal('NOT BETWEEN')
            _and = Literal('AND')
            _or = Literal('OR')
            comilla = Literal(''')
            nombre_campo = Word(alphas + '\.\_')
            valor = Word(nums)
    
            lista_valores = OneOrMore(Optional(comilla) + valor + Optional(comilla) + Optional(Literal(',')))
            lista_valores.setParseAction(lambda tokens: ', '.join((item for item in tokens if item not in (''', ','))))
    
            valor_between = Optional(comilla) + valor + Optional(comilla)
            valor_between.setParseAction(lambda tokens: ''.join((item for item in tokens if item != ''')))
    
            primera = Optional(_and) + Optional(_or) + parent_ini + nombre_campo('nombre_campo') + Or(_in('operador'), _notin('operador')) + parent_ini + lista_valores('valor') + parent_fin + Optional(parent_fin)
    
            between = Optional(_and) + Optional(_or) + Optional(parent_ini) + nombre_campo('nombre_campo') + Or(_between('operador'), _notbetween('operador')) + valor_between('origen') + _and + valor_between('destino') + Optional(parent_fin)
    
            grupo_filtro = Or(primera, between) + ZeroOrMore(between)
            pre_parser = Suppress(parent_ini) + OneOrMore(grupo_filtro) + Suppress(parent_fin)
    



pre_parser fails to parse '((mo.id_temporada BETWEEN '101' AND '110'))' with an infinite loop, and if i use:





    grupo_filtro = Or(Group(primera), Group(between)) + ZeroOrMore(between)



that it is my original try it gives me correct answer if it is 'primera' but not 'between' like the text above because it only tries to match 'primera'. Any help please? :)
#### 2010-05-08 11:49:31 - skuda
I have been able to do what i want using | instead of the Or operator, so i assume i don't know really what it is for the Or operator, sorry for the noise.

---
## 2010-05-04 22:10:07 - sakana280 - A faster (near-)substitute for operatorPrecedence()
FWIW i coded this semi-substitute for operatorPrecedence() recently, and it gave me a 60% boost to parsing speed for a non-trivial grammar (with 4 levels of operator precedence) that im currently using.



It differs from pyparsing's version in that it only supports right-associative single terms, or left-associative double terms, but that should cover common cases like arithmetic/logical (binary) and negation (unary) operators.



Also <em>parse results aren't entirely compatibly with the pyparsing version</em>; matched operands and operators occupy the toks list, rather than a list in the first entry. eg '1+1' will parse giving toks=[1, '+', 1] instead of toks=[[1, '+', 1]]



Anyway, here it is:



    def operatorPrecedence( baseExpr, opList ):
        '''Fast implementation of pyparsing.operatorPrecedence().
    
        opList is a list of tuples (operator expression, number of terms (1 or 2), associativity, parse action)
    
        Associativity is ignored, but remains present for compatibility with the pyparsing version.
        Instead it is assumed right-associative for single terms, and left-associative for double terms.
        '''
        lparen = Literal('(').suppress()
        rparen = Literal(')').suppress()
        expr = Forward()
        lastExpr = (lparen + expr + rparen) | baseExpr
        for op,terms,assoc,pa in opList:
            if terms == 1:
                lastExpr = (op + lastExpr).setParseAction( pa ) | lastExpr
            elif terms == 2:
                lastExpr = (lastExpr + OneOrMore( op + lastExpr )).setParseAction( pa ) | lastExpr
        expr \<\< lastExpr
        return expr



I'm sure there's something important this won't handle that pyparsing's will, but it worked for me, so hopefully this posting might be of use to someone else. Pyparsing has been such a gem in my toolbox, thankyou Paul for sharing your fantastic library with the world!

#### 2010-05-05 04:00:47 - ptmcg
Thanks for sharing this, and for your generous compliments!  opPrec has really filled a niche, but performance of course could be better - I'll look at your submission to see if I can fold your ideas back into the pyparsing version in the next release.
#### 2010-05-05 17:26:35 - sakana280
oops i just noticed my compatibility example above wasn't formatted properly, i'll try again:



eg '1+1' will parse giving toks=[1, '+', 1] instead of toks=[ [1, '+', 1] ]
#### 2010-05-05 19:13:28 - sakana280
I can't remember where i read it, but there was a suggestion to increase parsing speed with opPrec by replacing it with the raw BNFs. Well i started doing that but realised it could be wrapped up neatly into a loop, hence the above function.



Looking at the official implementation of opPrec, it looks like there are a bunch of FollowedBy usages that were introduced in r131 to prevent running parse actions unnecessarily (on intermediate matches). Ive managed to avoid using FollowedBy in my version by setting the parse action on a different part of the grammar.

---
## 2010-05-05 04:45:53 - Hory - Optimisation idea for parsing natural language subsets
While pyparsing isn't intended primarily for natural languages, I would like to argue that it actually parses them more accurately than traditional methods. That is because the process isn't separated into a lexer and a parser. Normally, lexers would have to tag words as certain parts of speech based on probabilities or based on the possible categories of some of the following words. Because the lexer can't check sentence structure, mistakes will be made.



For example, 'open' is both an adjective and a verb. Given the noun 'Sesame', the lexer might tag the phrase 'open Sesame' as if it were a noun phrase, similar to 'closed door'. But the phrase could also represent an imperative clause. With the right grammar, pyparsing will determine that a noun phrase shouldn't exist without a verb, and it will eventually match 'open' as verb. However, if a verb were to follow 'open Sesame', pyparsing will correctly assume that the phrase is the subject. Lexers can do this only in very simple situations.



The problem is that while natural languages have very few types of terminal elements (basically mostly CaselessKeywords), they have a lot of them and using MatchFirst for lists of many words might not be effective, as each CaselessKeyword tries to match separately. What I propose is a new subclass of ParserElement, which holds a list of strings which is simply checked for the inclusion of a certain word using the optimised Python procedures, without having to call the matching methods of each element of the list.



Do you think that this is a feasible addition?


---
## 2010-05-20 10:25:41 - majortal - Bug in cache mechanism?
disclaimer: I did not fully test this yet, but I've been getting some strange results from the caching mechanism and I THINK I found a bug.

The key to ParserElement._exprArgCache is (self, instring, loc, callPreParse, doActions).

Self is a ParserElement so you implemented a hash (the hash of the id of the instance). This is very efficient and very cool, but ids are only unique for the lifetime of the object. I THOUGHT this was ok but I was getting weird collisions.

Grammars are static, right?

So I implemented a destructor for ParserElement and found a neat function called streamline... When this function is run (when calling parseString!) parser elements are being optimized away, enabling new elements to take on old ids???

Fact: I'm getting collisions.

Help?

#### 2010-05-20 16:08:41 - majortal
Maybe Each has the same bug (but it is 2am and I'm checking out)...
#### 2010-05-20 18:10:17 - ptmcg
Thanks for the debugging! I was just about to put out v1.5.3, but I can include this bugfix before shipping. I'll update my unit tests and incorporate your patch - thanks again!



-- Paul
#### 2010-05-20 18:31:39 - ptmcg
Hmm, okay, maybe I don't quite see what the bug is.  I *do* have unit tests of And's, OneOrMore's, etc., both with and without packrat parsing.  Can you post a sample grammar and input data that shows just what bug you are seeing?
#### 2010-05-21 14:07:23 - majortal
Enjoy:



    from pyparsing import ParserElement, Regex, ZeroOrMore, Optional  
    
    ParserElement.enablePackrat()
    AND_ALSO = Optional('z')
    SUPER = Regex(r'super')
    SUPER_DUPER      = ZeroOrMore(AND_ALSO + SUPER)
    ROOM = Regex('room')
    TUPLE = Regex('triple')
    HOTEL_COMPLEX_ROOM   = ZeroOrMore(AND_ALSO + TUPLE) + ROOM
    HOTEL_ATTRIBUTES = AND_ALSO + HOTEL_COMPLEX_ROOM    
    BIG_RULE = SUPER_DUPER + HOTEL_ATTRIBUTES
    
    print ' '.join(BIG_RULE.parseString(u'super triple room'))


#### 2010-05-21 14:13:43 - ptmcg
With and without packrat parsing, I get the same results:



    \>pythonw -u 'majortal1.py'
    super triple room
    \>Exit code: 0
    \>pythonw -u 'majortal1.py'
    super triple room
    \>Exit code: 0

What version of pyparsing is this?
#### 2010-05-21 14:26:49 - majortal
my version? :-)

I've downloaded about a year ago and made some changes. None of them seem major. Can I send / attach the file somewhere?
#### 2010-05-21 14:30:45 - ptmcg
Try using pyparsing.pastebin.com.  I checked your code with both 1.5.2 and the latest 1.5.3 in my dev tree, and both look okay - what are you seeing?
#### 2010-05-21 14:38:55 - majortal
Done (strange to post an entire module there).







This is my version with the fix, but you should see the changes in lines 2386-2409.



I'm seeing 'super triple triple room'...
#### 2010-05-21 14:50:56 - majortal
Paul - as curious as I am to nail this thing, it is way past midnight here (Israel) and my wife will kill me if I don't get some sleep...



Some parting thoughts: the And implementation is definitely buggy, and I fixed it. But this alone does not explain the behavior I am seeing, because when you store a value in the cache you make a copy.



I THINK that because of your parse result <u>new</u> implementation, this copy does not always happen, but I have not yet seen this with my debugger.



My grammar is hanging by a thread - change a small bit and it works, so I'm not surprised it works for you on a slightly different version of pyparsing.



I'll check in tomorrow - thanks!



Tal.
#### 2010-05-21 14:52:19 - ptmcg
With your code and your pyparsing (I verified by adding a print statement in your copy), I *still* see just 'super triple room'.
#### 2010-05-21 14:58:22 - majortal
The pyparsing I pasted was with my fix.

Here it is without the fix:





BTW - Python 2.6.5, windows vista...
#### 2010-05-22 12:10:18 - ptmcg
ok, now I get 'super triple triple room' also, using your post-mod, pre-fix pyparsing version.  I will try to do some debugging to figure out where things go astray, but my time is pretty limited this weekend and coming week.  Thanks for hanging in there!



(Oh, these silly wives of ours, always thinking we need to get our sleep - that is what meetings at work are for!)
#### 2010-05-20 11:24:36 - majortal
I removed the streamline calls and I'm still getting my funky results...

The plot thickens...
#### 2010-05-20 15:40:05 - majortal
YES! I think I found it!!!

The implementation of AND is faulty.

in And.parseImpl():

loc, resultlist = self.exprs[0]._parse( instring, loc, doActions, callPreParse=False )

When caching is on, resultlist is also stored in the cache. But a few lines later we see:

resultlist += exprtokens

So you are also using resultlist as an accumulator and changing it, even though it is in the cache! So when the first expression in the And is parsed again, it will return the accumulation of the entire And!

Solution:

parse_results = ParseResults([])

        loc, resultlist = self.exprs[0]._parse( instring, loc, doActions, callPreParse=False )

        parse_results += resultlist

and

            loc, exprtokens = e._parse( instring, loc, doActions )

            if exprtokens or exprtokens.keys():

                parse_results += exprtokens

return loc, parse_results





I want my 10 hours back...
#### 2010-05-20 16:00:46 - majortal
I THINK ZeroOrMore and OneOrMore have the same bug (but I did not recreate so I did not prove it yet).

I KNOW I do not know how to use the code tags...

2nd try:



    parse_results = ParseResults([])
    loc, resultlist = self.exprs[0]._parse(instring, loc, doActions, callPreParse=False)
    parse_results += resultlist

and



    loc, exprtokens = e._parse(instring, loc, doActions)
    if exprtokens or exprtokens.keys():
        parse_results += exprtokens
    return loc, parse_results



---
## 2010-05-30 21:20:45 - flacoflacco - empty ParseResults.asdict(), values() etc???
Hello! I'm having some problems with the parseResults object- I don't get a non-empty object from asDict(), items() or values(). I ran the code below, and get nothing. I appreciate your help, and sorry if this is a dumb question!



Also, are there any easy AST transformation tools available in pyparsing or do I need to write my own? The next step of this code is to take an AST representing a first-order-logic formula and turn it into CNF.





    from pyparsing import *
    
    def operatorGrammar( baseExpr, opList ):
        ret = Forward()
        lastExpr = baseExpr | ( Suppress('(') + ret + Suppress(')') )
        i = 0
        for opExpr,arity,rightLeftAssoc in opList:
            thisExpr = Forward()
            thisExpr.setName('expr%d' % i)#.setDebug()
            if rightLeftAssoc == 'L':
                if arity == 1:
                    thisExpr \<\< ( Group( lastExpr + opExpr ) | lastExpr )
                else:
                    thisExpr \<\< ( Group( lastExpr + OneOrMore( opExpr + lastExpr) ) | lastExpr )
            else:
                if arity == 1:
                    # try to avoid LR with this extra test
                    if isinstance(opExpr, Optional):
                        thisExpr \<\< (( FollowedBy(opExpr.expr + thisExpr) + \
                        Group( opExpr + thisExpr )) | lastExpr )
                    else:
                        thisExpr \<\< ( opExpr + thisExpr )
                else:
                    thisExpr \<\< ( Group( lastExpr + OneOrMore( opExpr + thisExpr ) ) | lastExpr )
            lastExpr = thisExpr
            thisExpr = Forward()
            i += 1
            ret \<\< lastExpr
        return ret
    
    
    
    lpar  = Literal( '(' ).suppress()
    rpar  = Literal( ')' ).suppress()
    
    exhaustiveop = Optional( Literal('!') )
    perconstantop = Optional( Literal('+') )
    allpossibleop = Optional( Literal('*') )
    
    negop = Optional( Literal('!') )
    andop = Literal('^')
    orop = Literal('v')
    impop = Literal('=\>')
    iffop = Literal('\<=\>')
    faop = Word('FORALL') | Word('ForAll') | Word('forall')
    
    identifier = Word(alphas)
    constant = Group( Optional( perconstantop ).setResultsName('perconstant') + identifier + Optional( exhaustiveop ).setResultsName('exhaustive') )
    atom = Forward()
    atom \<\< Group( identifier.setResultsName('pred') + lpar +  Group( delimitedList( constant ).setResultsName('args') ) + rpar )
    
    expr = operatorGrammar( atom,
    [
    (allpossibleop, 1, 'R'),
    (negop, 1, 'R'),
    (andop, 2, 'L'),
    (orop, 2, 'L'),
    (impop, 2, 'L'),
    (iffop, 2, 'L'),
    (faop, 2, 'L')]
    )
    
    test = ['foo(x)',
    '!foo(x) v smells(x)',
    'foo(x) \<=\> * smells(x)',
    'foo(x) \<=\> * smells(x,y!)',
    'foo(x) \<=\> * smells(+x)',
    'foo(x) =\> foo(x,y) v smells(x)',]
    
    for t in test:
        out = expr.parseString(t)
        print out
        print out.asDict()
        print out.items()
        print out.values()



#### 2010-05-31 08:30:31 - ptmcg
This is an interesting application! Thanks for posting your question.



To start with the answer to your question: the dict-like aspects of a ParseResults are only shown for the outermost ParseResults level.  The nested results for a notation like this usually bury the named elements deep within the final results.  Here is a little variant on the ParseResults.dump() method that might help you to see where your results names are going:





    def operDump(pr, indent='', depth=0):
        out = []
        for el in pr:
            if isinstance(el, ParseResults):
                out.append(operDump(el, indent + '  ', depth+1))
                if el.asDict(): 
                    out.append(el.dump(indent + '  ', depth+1)+'\n')
            else:
                out.append(indent + el + '\n')
        return ''.join(out)





As to your question regarding AST utilities, please look at the simpleBool.py example, to see how I use parse actions and classes to construct an AST-style structure on the fly.  The structure is already apparent at parse time, so why create a redundant state machine to iterate over it?  Your adaptation of operatorPrecedence omits the parse action options in the opList argument, so you may have to restore this or something like it.



Some other suggestions for your code:



1. Word vs. Literal





    faop = Word('FORALL') | Word('ForAll') | Word('forall')



Word uses the provided string as the set of allowable characters with which to parse word groups.  Using this definition, your first alternative would match not only FORALL, but also LOAF, FOR, FORT, ROLL, OFFAL, FAR, FOOFOORA, etc.  If you want to match just variations on FORALL by upper/lower case, then CaselessLiteral is probably what you want. 





    faop = CaselessLiteral('FORALL')



To simplify the post-parse logic (avoid .upper() or .lower() calls), CaselessLiteral *always* returns the token in the case given in your definition, so you can just test for the string 'FORALL' in the parsed results.



If you really want to restrict to just these versions of case, then oneOf will do:





    faop = oneOf('FORALL ForAll forall').setParseAction(replaceWith('FORALL'))





2. setResultsName calls



setResultsName is one of the most important features of pyparsing, but I hate the name I picked for it.  It's really long, and it doesn't really reflect its constructive nature (creating copies of the expr instead of just modifying in place as the 'set' name implies). I encourage you to adopt the short-hand notation of setResultsName as shown below.  I think it's simpler to read, and if you think about it it *kind of* resembles Pythons class construction form.  Here is one of your statements





    atom \<\< Group( identifier.setResultsName('pred') + lpar +  Group( delimitedList( constant ).setResultsName('args') ) + rpar )



And here it is converted to new form:





    atom \<\< Group( identifier('pred') + lpar +  Group( delimitedList( constant ) )('args') + rpar )







3. Optional operators



I see that you use a simplified form of operatorPrecedence in your operatorGrammar method.  But I'm not sure I like that your unary operators all have to be Optional'ized.  What do you think of this instead?





    if isinstance(opExpr, Optional):
        thisExpr \<\< (( FollowedBy(opExpr.expr + thisExpr) + \
        Group( opExpr + thisExpr )) | lastExpr )
    else:
        # change the following line as shown
        #~ thisExpr \<\< ( opExpr + thisExpr )
        thisExpr \<\< ( Group(opExpr + thisExpr) | lastExpr )



This allows you to remove all those Optionals on the operators, which looks weird to me.  Also, by grouping the operator with its associated expr, then this will help maintain your AST hierarchy.





Thanks so much for posting this interesting example!  Write back if you have more questions.



-- Paul
#### 2010-06-02 02:08:50 - flacoflacco
Thanks for helpful reply. The simpleBool.py example is perfect- adding the transform methods of the base ASTnode classes should be super easy from there.



Out of curiosity, one thing I didn't in the examples see was having a non-operator AST type specified. For example, from simpleBool.py we have



[code]

boolOperand = Word(alphas,max=1) | oneOf('True False')

boolExpr = operatorPrecedence( boolOperand,

    [

    ('not', 1, opAssoc.RIGHT, BoolNot),

    ('or',  2, opAssoc.LEFT,  BoolOr),

    ('and', 2, opAssoc.LEFT,  BoolAnd),

    ])

[code]



This is fine for stuff like

[code]

'p and not q'

'not(p and q)'

[code]



But what about when we just have

[code]

'p'

[code]



Is there a way to give a simple boolOperand an AST type, and in general to give all nodes in the AST a default type if not otherwise specified?
#### 2010-06-02 04:53:21 - ptmcg
eval_arith () does this.  See this line:



    operand.setParseAction(EvalConstant)

just before operand is used as the atom expression in operatorPrecedence.



Bearing that in mind, the BoolOperand class in simpleBool would probably be better named BoolASTNode, and then BoolOperand could be another subclass of BoolASTNode, and similarly bound as operand's parse action.

-- Paul

---
## 2010-06-02 10:18:24 - jackdaniels2 - BNF to pyparsing
Hello,



I'm just trying to translate the C grammar at  into pyparsing. But I always get this famous 'maximum recursion depth exceeded'-error. I've already read  but I still don't get it.



Could someone please explain me how I should do this, show me how to do it on an example statement, like this one: additive_exp        : mult_exp

            | additive_exp '+' mult_exp

            | additive_exp '-' mult_exp



Thank you for your patience, but I'd really like to understand pyparsing...



jackdaniels2


---
## 2010-06-02 15:02:58 - etno_thomas - alphas8bit problems
Dear pyparser,



I have some html that looks  like this:



\<td class='listcontentlight_left'\>

\<a href='/members/expert/alphaOrder/view.do;jsessionid=760AD03B767A90E24F20172AAF74BDDB.node2?language=EN&amp;id=96659' title='DER, Jnos'\>DER, Jnos\</a\>

\<br /\>

                            Group of the European People's Party (Christian Democrats)

                            \<br /\>

\</td\>



From this I want to extract the name DER, Jnos. I have written some very simple code to do the job:



name = Word(unicode(alphas + alphas8bit))

begin = Literal('\>')

end = Literal('\<')



names = begin + name + ',' + name + end



out = names.searchString(html)



However this does not extract the name, can anyone tell me what I am doing wrong?



Best, Thomas

#### 2010-06-02 20:17:55 - ptmcg
This works for me, I get a complete match:





    [['\>', '\xc1DER', ',', 'J\xe1nos', '\<']]



If you don't want the angle brackets, use Suppress to keep the out of the parsed results.
#### 2010-06-03 07:22:44 - etno_thomas
Strange, well then i guess it must be something else... I am a beginner with Python, and I dont want to take up to much of your time, but would you have any idea as to why it works for you and not me?



Best, Thomas

---
## 2010-06-13 22:54:39 - gobnat - loc driving me loco.
I am getting the wrong results back from the loc variable in my parse action.  It seems to be off by the leading whitespace.  

The code is:



    #! /usr/bin/python
    # -*- coding: utf-8 -*-
    
    '''
    Trying to work out tokenisation and parseactions within pyparsing
    
    '''
    
    from pyparsing import *
    import string
    
    DASH = Literal('-')
    TESTTEXT1='1. This is some test text to tokenise. \n1.1 It contains: (a) colons; (aa) some    unexplained spaces; (b) semi-colons;\n(c) semicolons preceded by a return; and \n(d) references to 1., 1.1, (a), (i), (XI) within the body of the sentence.'
    TESTTEXT=TESTTEXT1 #[:35]
    PUNCTS= ''':'!@#$%^&*/,?[]{};'''
    KEEPPUNCTS= ''''().'''
    
    class extendedToken:
        def __init__(self,st,loc,tokString):
            self.tokenString = tokString
            self.start =loc
            self.end = self.start + len(tokString)
            print '\>tokstring\<, len, \>extract\<, outcome = ','\>'+tokString+'\<', len(tokString),'\>'+st[self.start:self.end]+'\<',
            if tokString == st[self.start+1:self.end+1]:
                print ' \<- location off by one!'
            elif tokString == st[self.start:self.end]:
                print ' \<- location accurate!!!!'
            else:
                print ' \<-???'
    
        def __str__(self):
            return '%(tokenString)s (locn: %(start)d)' % self.__dict__
    
    def createTokenObject(st, locn, toks):
        print 'in createtokenobject, printing toks = ',toks
        return extendedToken(st,locn,toks[0])
    
    def tokenise(textString):
        if textString is None:
            return None
    
        normWord=Word(alphas)
        punctWord=(Word(PUNCTS) | Word(KEEPPUNCTS))
        normWord.setParseAction( createTokenObject )
        numWord=Word(nums+'.') 
        numWord.setParseAction( createTokenObject )
        t = OneOrMore(numWord | punctWord |  normWord | DASH)#+StringEnd()
    
        spam = t.parseString(textString)
        return spam
    
    if __name__=='__main__':
        print TESTTEXT
        tokenise(TESTTEXT)
    
    
    



And the output is:



1. This is some test text to tokenise. 

1.1 It contains: (a) colons; (aa) some    unexplained spaces; (b) semi-colons;

(c) semicolons preceded by a return; and 

(d) references to 1., 1.1, (a), (i), (XI) within the body of the sentence.

in createtokenobject, printing toks =  ['1.']

\>tokstring\<, len, \>extract\<, outcome =  \>1.\< 2 \>1.\<  \<- location accurate!!!!

in createtokenobject, printing toks =  ['This']

\>tokstring\<, len, \>extract\<, outcome =  \>This\< 4 \> Thi\<  \<- location off by one!

in createtokenobject, printing toks =  ['is']

\>tokstring\<, len, \>extract\<, outcome =  \>is\< 2 \> i\<  \<- location off by one!

in createtokenobject, printing toks =  ['some']

\>tokstring\<, len, \>extract\<, outcome =  \>some\< 4 \> som\<  \<- location off by one!

in createtokenobject, printing toks =  ['test']

\>tokstring\<, len, \>extract\<, outcome =  \>test\< 4 \> tes\<  \<- location off by one!

in createtokenobject, printing toks =  ['text']

\>tokstring\<, len, \>extract\<, outcome =  \>text\< 4 \> tex\<  \<- location off by one!

in createtokenobject, printing toks =  ['to']

\>tokstring\<, len, \>extract\<, outcome =  \>to\< 2 \> t\<  \<- location off by one!

in createtokenobject, printing toks =  ['tokenise']

\>tokstring\<, len, \>extract\<, outcome =  \>tokenise\< 8 \> tokenis\<  \<- location off by one!

in createtokenobject, printing toks =  ['.']

\>tokstring\<, len, \>extract\<, outcome =  \>.\< 1 \>.\<  \<- location accurate!!!!

in createtokenobject, printing toks =  ['1.1']

\>tokstring\<, len, \>extract\<, outcome =  \>1.1\< 3 \> 

1\<  \<-???

in createtokenobject, printing toks =  ['It']

\>tokstring\<, len, \>extract\<, outcome =  \>It\< 2 \> I\<  \<- location off by one!

in createtokenobject, printing toks =  ['contains']

\>tokstring\<, len, \>extract\<, outcome =  \>contains\< 8 \> contain\<  \<- location off by one!

in createtokenobject, printing toks =  ['a']

\>tokstring\<, len, \>extract\<, outcome =  \>a\< 1 \>a\<  \<- location accurate!!!!

in createtokenobject, printing toks =  ['colons']

\>tokstring\<, len, \>extract\<, outcome =  \>colons\< 6 \> colon\<  \<- location off by one!

in createtokenobject, printing toks =  ['aa']

\>tokstring\<, len, \>extract\<, outcome =  \>aa\< 2 \>aa\<  \<- location accurate!!!!

in createtokenobject, printing toks =  ['some']

\>tokstring\<, len, \>extract\<, outcome =  \>some\< 4 \> som\<  \<- location off by one!

in createtokenobject, printing toks =  ['unexplained']

\>tokstring\<, len, \>extract\<, outcome =  \>unexplained\< 11 \>    unexpla\<  \<-???

in createtokenobject, printing toks =  ['spaces']

\>tokstring\<, len, \>extract\<, outcome =  \>spaces\< 6 \> space\<  \<- location off by one!

in createtokenobject, printing toks =  ['b']

\>tokstring\<, len, \>extract\<, outcome =  \>b\< 1 \>b\<  \<- location accurate!!!!

in createtokenobject, printing toks =  ['semi']

[snip]



Ideas about what I'm doing wrong?

#### 2010-06-14 00:44:13 - gobnat
is this the same issue:



?
#### 2010-06-15 00:22:02 - ptmcg
Yes this is the same issue, and I don't know why I didn't fix this before.  I think your tests are excellent, and highlight the problem perfectly.  I'll check in a fixed version to SVN shortly.



In the mean time, you can patch your own version of pyparsing.py by finding the two statements that read:





    tokenStart = loc



and change them to





    tokenStart = preloc



(Be sure to make this change in *both* locations!)



Thanks!

-- Paul
#### 2010-06-15 15:30:54 - gobnat
Thanks for that. At least I know it's not me. 

I used Empty() to get around it.

---
## 2010-06-20 21:54:44 - jorgeecardona - Create Balanced expressions.
Hi,



I'm trying to create a parser for this kind of text:



    class a
    ...
    end a;
    
    class b
    ...
    end b;
    
    class c
    ...
    end d;



That correctly consume the first two classes but fails with a non balanced class construction. 



I was trying to use something similar to makeHTMLTags and makeXMLTags, but they actually consume nonbalanced expressions:





    \>\>\> from pyparsing import *
    \>\>\> a,b = makeXMLTags(Word(alphas))
    \>\>\> c = a + Word(alphas) + b
    \>\>\> c.parseString(r'\<a\>test\</a\>')
    (['a', False, 'test', '\</a\>'], {'endW(Abcd...)': [('\</a\>', 3)], 'startW(Abcd...)': [((['a', False], {'empty': [(False, 1)]}), 0)], 'empty': [(False, 1)]})
    \>\>\> c.parseString(r'\<a\>test\</b\>')
    (['a', False, 'test', '\</b\>'], {'endW(Abcd...)': [('\</b\>', 3)], 'startW(Abcd...)': [((['a', False], {'empty': [(False, 1)]}), 0)], 'empty': [(False, 1)]})
    \>\>\> 



I was  expecting to see a error with \</b\>.



How can I build this kind of balanced expressions?

#### 2010-06-21 06:15:53 - ptmcg
matchPreviousExpr or matchPreviousLiteral will help you here.  You'll need to designate a special token just for the class identifier (in this example, we make a copy by creating a separate token by creating a copy with a results name):





    from pyparsing import *
    
    src = '''
    class a
    ...
    end a;
    
    class b
    ...
    end b;
    
    class c
    ...
    end d;'''
    
    
    identifier = Word(alphas)
    classIdent = identifier('classname')  # note that this also makes a copy of identifier 
    classDefn = 'class' + classIdent + '...' + 'end' + matchPreviousLiteral(classIdent) + ';'
    
    for tokens in classDefn.searchString(src):
        print tokens.classname



This will only match classes a and b, not c.

---
## 2010-06-22 06:13:51 - dminor14 - build problem on cent0s5 64-bit
[root@david-fc6 pyparsing-1.5.2]# python setup.py install

running install

running build

running build_py

running install_lib

byte-compiling /usr/lib/python2.4/site-packages/pyparsing_py3.py to pyparsing_py3.pyc

  File '/usr/lib/python2.4/site-packages/pyparsing_py3.py', line 2470

    except ParseException as err:

                           ^

SyntaxError: invalid syntax

#### 2010-06-22 13:51:00 - ptmcg
Yes, yes, this is a known bug in 1.5.2, stemming from my faulty support for both Python 2 and Python 3.  I will have this fixed in the next day or two.
#### 2010-06-24 02:34:04 - dminor14
No problem, I downloaded from the svn it works fine

---
## 2010-06-30 21:26:16 - Torkn - r199 breaks Python 2 compat
As of r199, setup.py says:

(diff of r198 vs r199)



-# cleaned up for Py2/Py3 compatibility - thanks to Mark Roddy

 if _PY3:

-    from pyparsing_py3 import <u>version</u>

-    modules = ['pyparsing_py3',]

+    from pyparsing_py3 import <u>version</u> as pyparsing_version

 else:

-    from pyparsing import <u>version</u>

-    modules = ['pyparsing',]

+    from pyparsing_py2 import <u>version</u> as pyparsing_version



Unfortunately pyparsing_py2.py does not exist.



Perhaps this patch is missing the following:

svn move src/pyparsing.py src/pyparsing_py2.py

#### 2010-07-01 02:34:20 - ptmcg
My SVN repository on SF is not really a good source for installing pyparsing.  You are better off downloading the source installation (zip or tarball), and running setup.py with that.  The source dists contain the correct pyparsing_pyX.py files for setup.py to do its thing.
#### 2010-07-01 04:25:27 - Torkn
Oh, I see they are very different (different directory structure + many extra files).  I will switch to the tarball.  I was thinking the examples and documentation were a bit sparse :-)



I personally prefer the tarball to be an exact snapshot of the development trunk (for one, it makes it easier to manage version migration on production servers).  Perhaps the homepage of the wiki should be changed to mention the divergence of the svn vs the tarball/zip.

---
## 2010-06-30 21:56:29 - john_nagle - Address parser missing many street types.
The address parser has a useful list of street types, but not a complete one. 

Below are the official USPS lists of street types and unit types.  Including this in the address parser example would move it closer to a working parser.



To use this list, change the parser code line

for 'types' to 



    # types of streets - extend as desired
    type_ = Combine( MatchFirst(map(CaselessKeyword,streettypes)) + Optional('.').suppress())
    [[/code]]
    
    Unfortunately, after doing this, one of the test cases, '1500 Deer Creek Lane', will mis-parse, because 'Creek' is now a known street type.  So this brings out a parser bug where there is a street type word in the name.  Haven't been able to fix that. 
    

<ol><li>Street_Type</li></ol>

<ol><li>Maps lowercased USPS standard street types to their canonical postal</li><li>abbreviations as found in TIGER/Line.  See eg/get_street_abbrev.pl in</li><li>the distrbution for how this map was generated.</li></ol>

Street_Type = {

    'allee'        :    'aly',

    'alley'        :    'aly',

    'ally'        :    'aly',

    'anex'        :    'anx',

    'annex'        :    'anx',

    'annx'        :    'anx',

    'arcade'    :    'arc',

    'av'        :    'ave',

    'aven'        :    'ave',

    'avenu'        :    'ave',

    'avenue'    :    'ave',

    'avn'        :    'ave',

    'avnue'        :    'ave',

    'bayoo'        :    'byu',

    'bayou'        :    'byu',

    'beach'        :    'bch',

    'bend'        :    'bnd',

    'bluf'        :    'blf',

    'bluff'        :    'blf',

    'bluffs'    :    'blfs',

    'bot'        :    'btm',

    'bottm'        :    'btm',

    'bottom'    :    'btm',

    'boul'        :    'blvd',

    'boulevard'    :    'blvd',

    'boulv'        :    'blvd',

    'branch'    :    'br',

    'brdge'        :    'brg',

    'bridge'    :    'brg',

    'brnch'        :    'br',

    'brook'        :    'brk',

    'brooks'    :    'brks',

    'burg'        :    'bg',

    'burgs'        :    'bgs',

    'bypa'        :    'byp',

    'bypas'        :    'byp',

    'bypass'    :    'byp',

    'byps'        :    'byp',

    'camp'        :    'cp',

    'canyn'        :    'cyn',

    'canyon'    :    'cyn',

    'cape'        :    'cpe',

    'causeway'    :    'cswy',

    'causway'    :    'cswy',

    'cen'        :    'ctr',

    'cent'        :    'ctr',

    'center'    :    'ctr',

    'centers'    :    'ctrs',

    'centr'        :    'ctr',

    'centre'    :    'ctr',

    'circ'        :    'cir',

    'circl'        :    'cir',

    'circle'    :    'cir',

    'circles'    :    'cirs',

    'ck'        :    'crk',

    'cliff'        :    'clf',

    'cliffs'    :    'clfs',

    'club'        :    'clb',

    'cmp'        :    'cp',

    'cnter'        :    'ctr',

    'cntr'        :    'ctr',

    'cnyn'        :    'cyn',

    'common'    :    'cmn',

    'corner'    :    'cor',

    'corners'    :    'cors',

    'course'    :    'crse',

    'court'        :    'ct',

    'courts'    :    'cts',

    'cove'        :    'cv',

    'coves'        :    'cvs',

    'cr'        :    'crk',

    'crcl'        :    'cir',

    'crcle'        :    'cir',

    'crecent'    :    'cres',

    'creek'        :    'crk',

    'crescent'    :    'cres',

    'cresent'    :    'cres',

    'crest'        :    'crst',

    'crossing'    :    'xing',

    'crossroad'    :    'xrd',

    'crscnt'    :    'cres',

    'crsent'    :    'cres',

    'crsnt'        :    'cres',

    'crssing'    :    'xing',

    'crssng'    :    'xing',

    'crt'        :    'ct',

    'curve'        :    'curv',

    'dale'        :    'dl',

    'dam'        :    'dm',

    'div'        :    'dv',

    'divide'    :    'dv',

    'driv'        :    'dr',

    'drive'        :    'dr',

    'drives'    :    'drs',

    'drv'        :    'dr',

    'dvd'        :    'dv',

    'estate'    :    'est',

    'estates'   :    'ests',

    'exp'        :    'expy',

    'expr'        :    'expy',

    'express'    :    'expy',

    'expressway':    'expy',

    'expw'        :    'expy',

    'extension' :    'ext',

    'extensions':    'exts',

    'extn'        :    'ext',

    'extnsn'    :    'ext',

    'falls'     :    'fls',

    'ferry'     :    'fry',

    'field'     :    'fld',

    'fields'    :    'flds',

    'flat'        :    'flt',

    'flats'     :    'flts',

    'ford'        :    'frd',

    'fords'     :    'frds',

    'forest'    :    'frst',

    'forests'    :    'frst',

    'forg'        :    'frg',

    'forge'     :    'frg',

    'forges'    :    'frgs',

    'fork'        :    'frk',

    'forks'        :    'frks',

    'fort'        :    'ft',

    'freeway'    :    'fwy',

    'freewy'    :    'fwy',

    'frry'        :    'fry',

    'frt'        :    'ft',

    'frway'     :    'fwy',

    'frwy'        :    'fwy',

    'garden'    :    'gdn',

    'gardens'    :    'gdns',

    'gardn'        :    'gdn',

    'gateway'    :    'gtwy',

    'gatewy'    :    'gtwy',

    'gatway'    :    'gtwy',

    'glen'        :    'gln',

    'glens'     :    'glns',

    'grden'     :    'gdn',

    'grdn'        :    'gdn',

    'grdns'     :    'gdns',

    'green'     :    'grn',

    'greens'    :    'grns',

    'grov'        :    'grv',

    'grove'     :    'grv',

    'groves'    :    'grvs',

    'gtway'     :    'gtwy',

    'harb'        :    'hbr',

    'harbor'    :    'hbr',

    'harbors'    :    'hbrs',

    'harbr'     :    'hbr',

    'haven'     :    'hvn',

    'havn'        :    'hvn',

    'height'    :    'hts',

    'heights'    :    'hts',

    'hgts'        :    'hts',

    'highway'    :    'hwy',

    'highwy'    :    'hwy',

    'hill'        :    'hl',

    'hills'     :    'hls',

    'hiway'     :    'hwy',

    'hiwy'        :    'hwy',

    'hllw'        :    'holw',

    'hollow'    :    'holw',

    'hollows'    :    'holw',

    'holws'     :    'holw',

    'hrbor'     :    'hbr',

    'ht'        :    'hts',

    'hway'        :    'hwy',

    'inlet'     :    'inlt',

    'island'    :    'is',

    'islands'    :    'iss',

    'isles'     :    'isle',

    'islnd'     :    'is',

    'islnds'    :    'iss',

    'jction'    :    'jct',

    'jctn'        :    'jct',

    'jctns'     :    'jcts',

    'junction'    :    'jct',

    'junctions'    :    'jcts',

    'junctn'    :    'jct',

    'juncton'    :    'jct',

    'key'        :    'ky',

    'keys'        :    'kys',

    'knol'        :    'knl',

    'knoll'     :    'knl',

    'knolls'    :    'knls',

    'la'        :    'ln',

    'lake'        :    'lk',

    'lakes'        :    'lks',

    'landing'    :    'lndg',

    'lane'        :    'ln',

    'lanes'     :    'ln',

    'ldge'        :    'ldg',

    'light'     :    'lgt',

    'lights'    :    'lgts',

    'lndng'     :    'lndg',

    'loaf'        :    'lf',

    'lock'        :    'lck',

    'locks'     :    'lcks',

    'lodg'        :    'ldg',

    'lodge'     :    'ldg',

    'loops'     :    'loop',

    'manor'     :    'mnr',

    'manors'    :    'mnrs',

    'meadow'    :    'mdw',

    'meadows'    :    'mdws',

    'medows'    :    'mdws',

    'mill'        :    'ml',

    'mills'        :    'mls',

    'mission'    :    'msn',

    'missn'     :    'msn',

    'mnt'        :    'mt',

    'mntain'    :    'mtn',

    'mntn'        :    'mtn',

    'mntns'     :    'mtns',

    'motorway'    :    'mtwy',

    'mount'     :    'mt',

    'mountain'    :    'mtn',

    'mountains'    :    'mtns',

    'mountin'    :    'mtn',

    'mssn'        :    'msn',

    'mtin'        :    'mtn',

    'neck'        :    'nck',

    'orchard'    :    'orch',

    'orchrd'    :    'orch',

    'overpass'    :    'opas',

    'ovl'        :    'oval',

    'parks'     :    'park',

    'parkway'    :    'pkwy',

    'parkways'    :    'pkwy',

    'parkwy'    :    'pkwy',

    'passage'    :    'psge',

    'paths'     :    'path',

    'pikes'     :    'pike',

    'pine'        :    'pne',

    'pines'     :    'pnes',

    'pk'        :    'park',

    'pkway'     :    'pkwy',

    'pkwys'     :    'pkwy',

    'pky'        :    'pkwy',

    'place'     :    'pl',

    'plain'     :    'pln',

    'plaines'    :    'plns',

    'plains'    :    'plns',

    'plaza'     :    'plz',

    'plza'        :    'plz',

    'point'     :    'pt',

    'points'    :    'pts',

    'port'        :    'prt',

    'ports'     :    'prts',

    'prairie'    :    'pr',

    'prarie'    :    'pr',

    'prk'        :    'park',

    'prr'        :    'pr',

    'rad'        :    'radl',

    'radial'    :    'radl',

    'radiel'    :    'radl',

    'ranch'     :    'rnch',

    'ranches'    :    'rnch',

    'rapid'     :    'rpd',

    'rapids'    :    'rpds',

    'rdge'        :    'rdg',

    'rest'        :    'rst',

    'ridge'     :    'rdg',

    'ridges'    :    'rdgs',

    'river'     :    'riv',

    'rivr'        :    'riv',

    'rnchs'     :    'rnch',

    'road'        :    'rd',

    'roads'     :    'rds',

    'route'     :    'rte',

    'rvr'        :    'riv',

    'shoal'     :    'shl',

    'shoals'    :    'shls',

    'shoar'     :    'shr',

    'shoars'    :    'shrs',

    'shore'     :    'shr',

    'shores'    :    'shrs',

    'skyway'    :    'skwy',

    'spng'        :    'spg',

    'spngs'        :    'spgs',

    'spring'    :    'spg',

    'springs'    :    'spgs',

    'sprng'     :    'spg',

    'sprngs'    :    'spgs',

    'spurs'     :    'spur',

    'sqr'        :    'sq',

    'sqre'        :    'sq',

    'sqrs'        :    'sqs',

    'squ'        :    'sq',

    'square'    :    'sq',

    'squares'    :    'sqs',

    'station'    :    'sta',

    'statn'     :    'sta',

    'stn'        :    'sta',

    'str'        :    'st',

    'strav'     :    'stra',

    'strave'    :    'stra',

    'straven'    :    'stra',

    'stravenue'    :    'stra',

    'stravn'    :    'stra',

    'stream'    :    'strm',

    'street'    :    'st',

    'streets'    :    'sts',

    'streme'    :    'strm',

    'strt'        :    'st',

    'strvn'     :    'stra',

    'strvnue'    :    'stra',

    'sumit'     :    'smt',

    'sumitt'    :    'smt',

    'summit'    :    'smt',

    'terr'        :    'ter',

    'terrace'    :    'ter',

    'throughway':    'trwy',

    'tpk'        :    'tpke',

    'tr'        :    'trl',

    'trace'     :    'trce',

    'traces'    :    'trce',

    'track'     :    'trak',

    'tracks'    :    'trak',

    'trafficway':    'trfy',

    'trail'     :    'trl',

    'trails'    :    'trl',

    'trk'        :    'trak',

    'trks'        :    'trak',

    'trls'        :    'trl',

    'trnpk'        :    'tpke',

    'trpk'        :    'tpke',

    'tunel'     :    'tunl',

    'tunls'     :    'tunl',

    'tunnel'    :    'tunl',

    'tunnels'    :    'tunl',

    'tunnl'     :    'tunl',

    'turnpike'    :    'tpke',

    'turnpk'    :    'tpke',

    'underpass'    :    'upas',

    'union'     :    'un',

    'unions'    :    'uns',

    'valley'    :    'vly',

    'valleys'    :    'vlys',

    'vally'     :    'vly',

    'vdct'        :    'via',

    'viadct'    :    'via',

    'viaduct'    :    'via',

    'view'        :    'vw',

    'views'     :    'vws',

    'vill'        :    'vlg',

    'villag'    :    'vlg',

    'village'    :    'vlg',

    'villages'    :    'vlgs',

    'ville'     :    'vl',

    'villg'     :    'vlg',

    'villiage'    :    'vlg',

    'vist'        :    'vis',

    'vista'     :    'vis',

    'vlly'        :    'vly',

    'vst'        :    'vis',

    'vsta'        :    'vis',

    'walks'     :    'walk',

    'well'        :    'wl',

    'wells'     :    'wls',

    'wy'        :    'way',

}

#

<ol><li>Unit abbreviations, expanded to full word</li></ol>#

Unit_Code = {

    'suite'        :    'suite',

    'ste'        :    'suite',

    'box'        :    'box',

    'mailbox'    :    'box',

    'pobox'        :    'box',

    'pmb'        :    'box',

    'department':    'department',

    'dept'        :    'department',

    'apartment'    :    'apartment',

    'apt'        :    'apartment',

    'room'        :    'room',

    'rm'        :    'room',

    'floor'        :    'floor',

    'fl'        :    'floor',

    'unit'        :    'unit'

}



#### 2010-06-30 21:59:46 - john_nagle
(Aargh.  This forum uses nonstandard formatting primitives.  Trying code snippet again.)





    #    Street_Type
    
    #    Maps lowercased USPS standard street types to their canonical postal
    #    abbreviations as found in TIGER/Line.  See eg/get_street_abbrev.pl in
    #    the distrbution for how this map was generated.
    
    Street_Type = {
        'allee'        :    'aly',
        'alley'        :    'aly',
        'ally'        :    'aly',
        'anex'        :    'anx',
        'annex'        :    'anx',
        'annx'        :    'anx',
        'arcade'    :    'arc',
        'av'        :    'ave',
        'aven'        :    'ave',
        'avenu'        :    'ave',
        'avenue'    :    'ave',
        'avn'        :    'ave',
        'avnue'        :    'ave',
        'bayoo'        :    'byu',
        'bayou'        :    'byu',
        'beach'        :    'bch',
        'bend'        :    'bnd',
        'bluf'        :    'blf',
        'bluff'        :    'blf',
        'bluffs'    :    'blfs',
        'bot'        :    'btm',
        'bottm'        :    'btm',
        'bottom'    :    'btm',
        'boul'        :    'blvd',
        'boulevard'    :    'blvd',
        'boulv'        :    'blvd',
        'branch'    :    'br',
        'brdge'        :    'brg',
        'bridge'    :    'brg',
        'brnch'        :    'br',
        'brook'        :    'brk',
        'brooks'    :    'brks',
        'burg'        :    'bg',
        'burgs'        :    'bgs',
        'bypa'        :    'byp',
        'bypas'        :    'byp',
        'bypass'    :    'byp',
        'byps'        :    'byp',
        'camp'        :    'cp',
        'canyn'        :    'cyn',
        'canyon'    :    'cyn',
        'cape'        :    'cpe',
        'causeway'    :    'cswy',
        'causway'    :    'cswy',
        'cen'        :    'ctr',
        'cent'        :    'ctr',
        'center'    :    'ctr',
        'centers'    :    'ctrs',
        'centr'        :    'ctr',
        'centre'    :    'ctr',
        'circ'        :    'cir',
        'circl'        :    'cir',
        'circle'    :    'cir',
        'circles'    :    'cirs',
        'ck'        :    'crk',
        'cliff'        :    'clf',
        'cliffs'    :    'clfs',
        'club'        :    'clb',
        'cmp'        :    'cp',
        'cnter'        :    'ctr',
        'cntr'        :    'ctr',
        'cnyn'        :    'cyn',
        'common'    :    'cmn',
        'corner'    :    'cor',
        'corners'    :    'cors',
        'course'    :    'crse',
        'court'        :    'ct',
        'courts'    :    'cts',
        'cove'        :    'cv',
        'coves'        :    'cvs',
        'cr'        :    'crk',
        'crcl'        :    'cir',
        'crcle'        :    'cir',
        'crecent'    :    'cres',
        'creek'        :    'crk',
        'crescent'    :    'cres',
        'cresent'    :    'cres',
        'crest'        :    'crst',
        'crossing'    :    'xing',
        'crossroad'    :    'xrd',
        'crscnt'    :    'cres',
        'crsent'    :    'cres',
        'crsnt'        :    'cres',
        'crssing'    :    'xing',
        'crssng'    :    'xing',
        'crt'        :    'ct',
        'curve'        :    'curv',
        'dale'        :    'dl',
        'dam'        :    'dm',
        'div'        :    'dv',
        'divide'    :    'dv',
        'driv'        :    'dr',
        'drive'        :    'dr',
        'drives'    :    'drs',
        'drv'        :    'dr',
        'dvd'        :    'dv',
        'estate'    :    'est',
        'estates'   :    'ests',
        'exp'        :    'expy',
        'expr'        :    'expy',
        'express'    :    'expy',
        'expressway':    'expy',
        'expw'        :    'expy',
        'extension' :    'ext',
        'extensions':    'exts',
        'extn'        :    'ext',
        'extnsn'    :    'ext',
        'falls'     :    'fls',
        'ferry'     :    'fry',
        'field'     :    'fld',
        'fields'    :    'flds',
        'flat'        :    'flt',
        'flats'     :    'flts',
        'ford'        :    'frd',
        'fords'     :    'frds',
        'forest'    :    'frst',
        'forests'    :    'frst',
        'forg'        :    'frg',
        'forge'     :    'frg',
        'forges'    :    'frgs',
        'fork'        :    'frk',
        'forks'        :    'frks',
        'fort'        :    'ft',
        'freeway'    :    'fwy',
        'freewy'    :    'fwy',
        'frry'        :    'fry',
        'frt'        :    'ft',
        'frway'     :    'fwy',
        'frwy'        :    'fwy',
        'garden'    :    'gdn',
        'gardens'    :    'gdns',
        'gardn'        :    'gdn',
        'gateway'    :    'gtwy',
        'gatewy'    :    'gtwy',
        'gatway'    :    'gtwy',
        'glen'        :    'gln',
        'glens'     :    'glns',
        'grden'     :    'gdn',
        'grdn'        :    'gdn',
        'grdns'     :    'gdns',
        'green'     :    'grn',
        'greens'    :    'grns',
        'grov'        :    'grv',
        'grove'     :    'grv',
        'groves'    :    'grvs',
        'gtway'     :    'gtwy',
        'harb'        :    'hbr',
        'harbor'    :    'hbr',
        'harbors'    :    'hbrs',
        'harbr'     :    'hbr',
        'haven'     :    'hvn',
        'havn'        :    'hvn',
        'height'    :    'hts',
        'heights'    :    'hts',
        'hgts'        :    'hts',
        'highway'    :    'hwy',
        'highwy'    :    'hwy',
        'hill'        :    'hl',
        'hills'     :    'hls',
        'hiway'     :    'hwy',
        'hiwy'        :    'hwy',
        'hllw'        :    'holw',
        'hollow'    :    'holw',
        'hollows'    :    'holw',
        'holws'     :    'holw',
        'hrbor'     :    'hbr',
        'ht'        :    'hts',
        'hway'        :    'hwy',
        'inlet'     :    'inlt',
        'island'    :    'is',
        'islands'    :    'iss',
        'isles'     :    'isle',
        'islnd'     :    'is',
        'islnds'    :    'iss',
        'jction'    :    'jct',
        'jctn'        :    'jct',
        'jctns'     :    'jcts',
        'junction'    :    'jct',
        'junctions'    :    'jcts',
        'junctn'    :    'jct',
        'juncton'    :    'jct',
        'key'        :    'ky',
        'keys'        :    'kys',
        'knol'        :    'knl',
        'knoll'     :    'knl',
        'knolls'    :    'knls',
        'la'        :    'ln',
        'lake'        :    'lk',
        'lakes'        :    'lks',
        'landing'    :    'lndg',
        'lane'        :    'ln',
        'lanes'     :    'ln',
        'ldge'        :    'ldg',
        'light'     :    'lgt',
        'lights'    :    'lgts',
        'lndng'     :    'lndg',
        'loaf'        :    'lf',
        'lock'        :    'lck',
        'locks'     :    'lcks',
        'lodg'        :    'ldg',
        'lodge'     :    'ldg',
        'loops'     :    'loop',
        'manor'     :    'mnr',
        'manors'    :    'mnrs',
        'meadow'    :    'mdw',
        'meadows'    :    'mdws',
        'medows'    :    'mdws',
        'mill'        :    'ml',
        'mills'        :    'mls',
        'mission'    :    'msn',
        'missn'     :    'msn',
        'mnt'        :    'mt',
        'mntain'    :    'mtn',
        'mntn'        :    'mtn',
        'mntns'     :    'mtns',
        'motorway'    :    'mtwy',
        'mount'     :    'mt',
        'mountain'    :    'mtn',
        'mountains'    :    'mtns',
        'mountin'    :    'mtn',
        'mssn'        :    'msn',
        'mtin'        :    'mtn',
        'neck'        :    'nck',
        'orchard'    :    'orch',
        'orchrd'    :    'orch',
        'overpass'    :    'opas',
        'ovl'        :    'oval',
        'parks'     :    'park',
        'parkway'    :    'pkwy',
        'parkways'    :    'pkwy',
        'parkwy'    :    'pkwy',
        'passage'    :    'psge',
        'paths'     :    'path',
        'pikes'     :    'pike',
        'pine'        :    'pne',
        'pines'     :    'pnes',
        'pk'        :    'park',
        'pkway'     :    'pkwy',
        'pkwys'     :    'pkwy',
        'pky'        :    'pkwy',
        'place'     :    'pl',
        'plain'     :    'pln',
        'plaines'    :    'plns',
        'plains'    :    'plns',
        'plaza'     :    'plz',
        'plza'        :    'plz',
        'point'     :    'pt',
        'points'    :    'pts',
        'port'        :    'prt',
        'ports'     :    'prts',
        'prairie'    :    'pr',
        'prarie'    :    'pr',
        'prk'        :    'park',
        'prr'        :    'pr',
        'rad'        :    'radl',
        'radial'    :    'radl',
        'radiel'    :    'radl',
        'ranch'     :    'rnch',
        'ranches'    :    'rnch',
        'rapid'     :    'rpd',
        'rapids'    :    'rpds',
        'rdge'        :    'rdg',
        'rest'        :    'rst',
        'ridge'     :    'rdg',
        'ridges'    :    'rdgs',
        'river'     :    'riv',
        'rivr'        :    'riv',
        'rnchs'     :    'rnch',
        'road'        :    'rd',
        'roads'     :    'rds',
        'route'     :    'rte',
        'rvr'        :    'riv',
        'shoal'     :    'shl',
        'shoals'    :    'shls',
        'shoar'     :    'shr',
        'shoars'    :    'shrs',
        'shore'     :    'shr',
        'shores'    :    'shrs',
        'skyway'    :    'skwy',
        'spng'        :    'spg',
        'spngs'        :    'spgs',
        'spring'    :    'spg',
        'springs'    :    'spgs',
        'sprng'     :    'spg',
        'sprngs'    :    'spgs',
        'spurs'     :    'spur',
        'sqr'        :    'sq',
        'sqre'        :    'sq',
        'sqrs'        :    'sqs',
        'squ'        :    'sq',
        'square'    :    'sq',
        'squares'    :    'sqs',
        'station'    :    'sta',
        'statn'     :    'sta',
        'stn'        :    'sta',
        'str'        :    'st',
        'strav'     :    'stra',
        'strave'    :    'stra',
        'straven'    :    'stra',
        'stravenue'    :    'stra',
        'stravn'    :    'stra',
        'stream'    :    'strm',
        'street'    :    'st',
        'streets'    :    'sts',
        'streme'    :    'strm',
        'strt'        :    'st',
        'strvn'     :    'stra',
        'strvnue'    :    'stra',
        'sumit'     :    'smt',
        'sumitt'    :    'smt',
        'summit'    :    'smt',
        'terr'        :    'ter',
        'terrace'    :    'ter',
        'throughway':    'trwy',
        'tpk'        :    'tpke',
        'tr'        :    'trl',
        'trace'     :    'trce',
        'traces'    :    'trce',
        'track'     :    'trak',
        'tracks'    :    'trak',
        'trafficway':    'trfy',
        'trail'     :    'trl',
        'trails'    :    'trl',
        'trk'        :    'trak',
        'trks'        :    'trak',
        'trls'        :    'trl',
        'trnpk'        :    'tpke',
        'trpk'        :    'tpke',
        'tunel'     :    'tunl',
        'tunls'     :    'tunl',
        'tunnel'    :    'tunl',
        'tunnels'    :    'tunl',
        'tunnl'     :    'tunl',
        'turnpike'    :    'tpke',
        'turnpk'    :    'tpke',
        'underpass'    :    'upas',
        'union'     :    'un',
        'unions'    :    'uns',
        'valley'    :    'vly',
        'valleys'    :    'vlys',
        'vally'     :    'vly',
        'vdct'        :    'via',
        'viadct'    :    'via',
        'viaduct'    :    'via',
        'view'        :    'vw',
        'views'     :    'vws',
        'vill'        :    'vlg',
        'villag'    :    'vlg',
        'village'    :    'vlg',
        'villages'    :    'vlgs',
        'ville'     :    'vl',
        'villg'     :    'vlg',
        'villiage'    :    'vlg',
        'vist'        :    'vis',
        'vista'     :    'vis',
        'vlly'        :    'vly',
        'vst'        :    'vis',
        'vsta'        :    'vis',
        'walks'     :    'walk',
        'well'        :    'wl',
        'wells'     :    'wls',
        'wy'        :    'way',
    }
    #
    #    Unit abbreviations, expanded to full word
    #
    Unit_Code = {
        'suite'        :    'suite',
        'ste'        :    'suite',
        'box'        :    'box',
        'mailbox'    :    'box',
        'pobox'        :    'box',
        'pmb'        :    'box',
        'department':    'department',
        'dept'        :    'department',
        'apartment'    :    'apartment',
        'apt'        :    'apartment',
        'room'        :    'room',
        'rm'        :    'room',
        'floor'        :    'floor',
        'fl'        :    'floor',
        'unit'        :    'unit'
    }


#### 2010-07-01 02:48:01 - ptmcg
Try replacing streetName with this:



    streetName = (Combine( Optional(nsew) + streetnumber + 
                            Optional('1/2') + 
                            Optional(numberSuffix), joinString=' ', adjacent=False )
                    ^ Combine(~numberSuffix + OneOrMore(~type_ + Combine(Word(alphas) + Optional('.')) | 
                                                         type_ + ~FollowedBy('and') + FollowedBy(Word(alphas))), 
                              joinString=' ', adjacent=False) 
                    ^ Combine('Avenue' + Word(alphas), joinString=' ', adjacent=False)).setName('streetName')



-- Paul
#### 2010-07-01 02:55:17 - ptmcg
Ah, even better, it is no longer necessary to special-case the leading 'Avenue' street name:



    streetName = (Combine( Optional(nsew) + streetnumber + 
                            Optional('1/2') + 
                            Optional(numberSuffix), joinString=' ', adjacent=False )
                    ^ Combine(~numberSuffix + OneOrMore(~type_ + Combine(Word(alphas) + Optional('.')) | 
                                                         type_ + ~FollowedBy('and') + FollowedBy(Word(alphanums))), 
                              joinString=' ', adjacent=False)).setName('streetName')



I hope this moves you forward in your street parsing quest.
#### 2010-07-01 10:29:00 - john_nagle
That's much better.  '1500 Deer Creek Lane' is now being parsed properly.  So the test cases look good.



I also put a '.suppress()' after each 'Optional('.'), so that 'N. WEBB' and 'N WEBB' both parse the same.  



The next USPS feature needed is to understand both pre-directionals ('N. WEBB') and post-directionals ('5th St. S.').  The parser currently recognizes pre-directionals, although it treats them as part of the street name, and doesn't recognize post-directionals at all.  Those aren't considered part of the street name in the postal world.  



Post-directionals are tough, because '1245 North St'. is a valid street name of 'North'. 



It's worth getting this right.  Many people use street address parsers, and the ones out there are either expensive, proprietary, or unreliable.  Thanks.
#### 2010-07-02 10:46:01 - john_nagle
(Repost for formatting.  Wikispaces lacks a 'Preview' button.)



More issues. The syntax for pre-directionals looks wrong. The current code is:





    # street name 
    nsew = Combine(oneOf('N S E W North South East West NW NE SW SE') + Optional('.').suppress())
    streetName = (Combine( Optional(nsew) + streetnumber + 
                            Optional('1/2') + 
                            Optional(numberSuffix), joinString=' ', adjacent=False )
                    ^ Combine(~numberSuffix + OneOrMore(~type_ + Combine(Word(alphas) + Optional('.').suppress()) | 
                                                         type_ + ~FollowedBy('and') + FollowedBy(Word(alphanums))), 
                              joinString=' ', adjacent=False)).setName('streetName')
    



That grammar has the pre-directional ('nsew') BEFORE the street number. That would match



'N. 1235 Main St.', which is not a standard address, but does not match



'1235 N. Main St.'.



That looks like a bug.
#### 2010-07-07 10:45:46 - john_nagle
I've been making progress on an improved version of the street address parser.  Here are 100 real-world (well, California) addresses I've run through it. The entries marked '<strong>*ERROR</strong>*' were mis-parsed.  The main remaining problems seem to be 1) the need to handle SUITE, APT, etc. on the address line, and

2) street names like STATE HIGHWAY 123.



Anyway, here are some parse results:





    1. 12200 West Olympic Blvd Suite 270 --\> streetnumber: 12200  streetname: OLYMPIC BLVD SUITE  predirectional: WEST
       2. 7828 N. 19th Ave Suite #14 --\> streettype: AVE  streetnumber: 7828  streetname: 19TH  predirectional: N
       3. 10815 Beaver Dam Rd --\> streettype: RD  streetnumber: 10815  streetname: BEAVER DAM
       4. 1205 Bobwhite Lane --\> streettype: LANE  streetnumber: 1205  streetname: BOBWHITE
       5. 3349 Cottage Way, Ste 28 --\> streettype: WAY  streetnumber: 3349  streetname: COTTAGE
       6. 2058 N. Mills Ave. #132 --\> streettype: AVE  streetnumber: 2058  streetname: MILLS  predirectional: N
       7. 38 Copper Cove Drive --\> streettype: DRIVE  streetnumber: 38  streetname: COPPER COVE
       8. 49 Outlook Circle --\> streettype: CIRCLE  streetnumber: 49  streetname: OUTLOOK
       9. P. O. Box 6379 --\> pobox: 6379
      10. PO BOX 368 --\> pobox: 368
      11. 11549 Sheldon --\> streetnumber: 11549  streetname: SHELDON
      12. 19859 Hodge Road --\> streettype: ROAD  streetnumber: 19859  streetname: HODGE
      13. Mission Oaks --\> streetname: MISSION OAKS
      14. 1031 s palmetto #L3 --\> streetnumber: 1031  streetname: PALMETTO  predirectional: S
      15. Po Box 70 --\> streetname: PO BOX
      16. 400 Lenrey Avenue --\> streettype: AVENUE  streetnumber: 400  streetname: LENREY
      17. 1125 Victoria --\> streetnumber: 1125  streetname: VICTORIA
      18. 4738 Palo Verde Av --\> streettype: AV  streetnumber: 4738  streetname: PALO VERDE
      19. 4738 Palo Verde Ave --\> streettype: AVE  streetnumber: 4738  streetname: PALO VERDE
      20. 3951 Development Drive Suite 6 --\> streetnumber: 3951  streetname: DEVELOPMENT DRIVE SUITE
      21. 1416 Westwood Boulevard, Suite 210 --\> streettype: BOULEVARD  streetnumber: 1416  streetname: WESTWOOD
      22. 195 S. Beverly Dr --\> streettype: DR  streetnumber: 195  streetname: BEVERLY  predirectional: S
      23. 10940 Wilshire Boulevard --\> streettype: BOULEVARD  streetnumber: 10940  streetname: WILSHIRE
      24. P.O. Box 1886 --\> pobox: 1886
      25. 2814 Landco Drive --\> streettype: DRIVE  streetnumber: 2814  streetname: LANDCO
      26. P. O. Box 292555 --\> pobox: 292555
      27. 11836 Downey Ave, #104 --\> streettype: AVE  streetnumber: 11836  streetname: DOWNEY
      28. P.O. Box 533 --\> pobox: 533
      29. 5724 Mendocino Boulevard --\> streettype: BOULEVARD  streetnumber: 5724  streetname: MENDOCINO
      30. 3765 S Main St --\> streettype: ST  streetnumber: 3765  streetname: MAIN  predirectional: S
      31. 9223 S Vermont Avenue --\> streettype: AVENUE  streetnumber: 9223  streetname: VERMONT  predirectional: S
      32. 885 Embarcadero Road --\> streettype: ROAD  streetnumber: 885  streetname: EMBARCADERO
      33. P.O. Box 26 --\> pobox: 26
      34. 2902 Del Rosa Ave --\> streettype: AVE  streetnumber: 2902  streetname: DEL ROSA
      35. 2800 Guadalupe Dr --\> streettype: DR  streetnumber: 2800  streetname: GUADALUPE
      36. 777 North Colusa Street --\> streettype: STREET  streetnumber: 777  streetname: COLUSA  predirectional: NORTH
      37. 8100 Lemon Cove Court --\> streettype: COURT  streetnumber: 8100  streetname: LEMON COVE
      38. 1015 Grandview Avenue --\> streettype: AVENUE  streetnumber: 1015  streetname: GRANDVIEW
      39. 1349 B Orange Ave. --\> streettype: AVE  streetnumber: 1349  streetname: B ORANGE
      40. 1950 E. Chapman Ave, Suite 2 --\> streettype: AVE  streetnumber: 1950  streetname: CHAPMAN  predirectional: E
      41. 2440 Sand Creek Rd --\> streettype: RD  streetnumber: 2440  streetname: SAND CREEK
      42. P.O Box 306 --\> pobox: 306
      43. 7522 Park Ave --\> streettype: AVE  streetnumber: 7522  streetname: PARK
      44. 1211 Griffith Avenue --\> streettype: AVENUE  streetnumber: 1211  streetname: GRIFFITH
      45. P.O. Box 0933 --\> pobox: 0933
      46. 11860 La Cienga --\> streetnumber: 11860  streetname: LA CIENGA
      47. 2008 Reed Ave --\> streettype: AVE  streetnumber: 2008  streetname: REED
      48. 466 San Domingo Drive --\> streettype: DRIVE  streetnumber: 466  streetname: SAN DOMINGO
      49. PO Box 3741 --\> pobox: 3741
      50. P.O. Box 5450 --\> pobox: 5450
      51. 2315 Geer Rd --\> streettype: RD  streetnumber: 2315  streetname: GEER
      52. 16199 Avenue 296 --\> streetnumber: 16199  streetname: AVENUE  ***ERROR***
      53. 919 Fourth Avenue --\> streettype: AVENUE  streetnumber: 919  streetname: FOURTH
      54. 16922 Lynn Street Suite B --\> streetnumber: 16922  streetname: LYNN STREET SUITE B
      55. 3770 Hancock St, Suite G --\> streettype: ST  streetnumber: 3770  streetname: HANCOCK
      56. 100 E Patterson Street --\> streettype: STREET  streetnumber: 100  streetname: PATTERSON  predirectional: E
      57. 1788 El Prado --\> streetnumber: 1788  streetname: EL PRADO
      58. P O Box 1053 --\> pobox: 1053
      59. 780 North Euclid, Suite 212c --\> streetnumber: 780  streetname: EUCLID  predirectional: NORTH
      60. 6041 Mission Gorge Rd. --\> streettype: RD  streetnumber: 6041  streetname: MISSION GORGE
      61. 564 Mateo St --\> streettype: ST  streetnumber: 564  streetname: MATEO
      62. 908 North Hollywood Way --\> streettype: WAY  streetnumber: 908  streetname: HOLLYWOOD  predirectional: NORTH
      63. 425 Grant Ave, #22 --\> streettype: AVE  streetnumber: 425  streetname: GRANT
      64. 17875 Sky Park North, # G --\> streetnumber: 17875  streetname: SKY PARK NORTH
      65. 7176 Regional Street --\> streettype: STREET  streetnumber: 7176  streetname: REGIONAL
      66. 1000 Greenly Road --\> streettype: ROAD  streetnumber: 1000  streetname: GREENLY
      67. 5100 N St --\> streettype: ST  streetnumber: 5100  streetname: N
      68. 1954 Hillhurst Avenue PMB 10 --\> streetnumber: 1954  streetname: HILLHURST AVENUE PMB  ***ERROR***
      69. 1626 Wilcox Avenue 418 --\> streetnumber: 1626  streetname: WILCOX AVENUE
      70. 9541 S Santa Fe Springs Rd --\> streettype: RD  streetnumber: 9541  streetname: SANTA FE SPRINGS  predirectional: S
      71. 982 Broadway --\> streetnumber: 982  streetname: BROADWAY
      72. 123 MAIN ST --\> streettype: ST  streetnumber: 123  streetname: MAIN
      73. 2146 East Chevy Chase Drive --\> streettype: DRIVE  streetnumber: 2146  streetname: CHEVY CHASE  predirectional: EAST
      74. 2750 East Imperial Highway --\> streettype: HIGHWAY  streetnumber: 2750  streetname: IMPERIAL  predirectional: EAST
      75. Marlin Way --\> streettype: WAY  streetname: MARLIN
      76. 5434 North Oakbank Avenue --\> streettype: AVENUE  streetnumber: 5434  streetname: OAKBANK  predirectional: NORTH
      77. 8670 Matilija Avenue --\> streettype: AVENUE  streetnumber: 8670  streetname: MATILIJA
      78. 2200 Hermosa Court, # 45 --\> streettype: COURT  streetnumber: 2200  streetname: HERMOSA
      79. 390 Swift Avenue Suite 16 --\> streetnumber: 390  streetname: SWIFT AVENUE SUITE
      80. 164 South Park Street --\> streettype: STREET  streetnumber: 164  streetname: PARK  predirectional: SOUTH
      81. Post Office box 1111 --\> streetname: POST OFFICE BOX  ***ERROR***
      82. 7000 Marina Blvd. --\> streettype: BLVD  streetnumber: 7000  streetname: MARINA
      83. 45535 Northport Loop W, # E --\> streetnumber: 45535  streetname: NORTHPORT LOOP W  ***ERROR***
      84. 1373 Promontory Point Drive --\> streettype: DRIVE  streetnumber: 1373  streetname: PROMONTORY POINT
      85. 20710 Leapwood Ave. --\> streettype: AVE  streetnumber: 20710  streetname: LEAPWOOD
      86. 264 Avalon Harbor --\> streettype: HARBOR  streetnumber: 264  streetname: AVALON
      87. 23133 Hawthorne Blvd --\> streettype: BLVD  streetnumber: 23133  streetname: HAWTHORNE
      88. 969 Edgewater G370 --\> streetnumber: 969  streetname: EDGEWATER G
      89. San Francisco International Airport --\> streetname: SAN FRANCISCO INTERNATIONAL AIRPORT
      90. 2204 El Camino Real, Suite 200 --\> streetnumber: 2204  streetname: EL CAMINO REAL
      91. P.O. Box 336 --\> pobox: 336
      92. 2382 Faraday Avenue --\> streettype: AVENUE  streetnumber: 2382  streetname: FARADAY
      93. 1407 Greenwich Drive, Suite A --\> streettype: DRIVE  streetnumber: 1407  streetname: GREENWICH
      94. 3510 Wilson Rd --\> streettype: RD  streetnumber: 3510  streetname: WILSON
      95. P.O. Box 1259 --\> pobox: 1259
      96. Post Office Box 2108 --\> streetname: POST OFFICE BOX  ***ERROR***
      97. 1112 Ozone Drive --\> streettype: DRIVE  streetnumber: 1112  streetname: OZONE
      98. 2075 Newport Boulevard Suite 101 --\> streetnumber: 2075  streetname: NEWPORT BOULEVARD SUITE
      99. 1555 River Park Drive, Suite 109 --\> streettype: DRIVE  streetnumber: 1555  streetname: RIVER PARK
     100. 307 E Jefferson Blvd, Local 243 --\> streettype: BLVD  streetnumber: 307  streetname: JEFFERSON  predirectional: E


#### 2010-07-08 09:54:27 - john_nagle
My code has diverged enough from the example here that I'll probably have to post it somewhere else.  But I'm now down to 29 errors on 1000 California addresses.  Most of the trouble spots look like this:





    13731 North Highway 88 --\> error: Expected stringEnd (at char 20), (line:1, col:21)
    333 South Hope Street, Sixteenth Floor --\> error: Expected stringEnd (at char 23), (line:1, col:24)
    Highway 190 --\> error: Expected stringEnd (at char 8), (line:1, col:9)
    70813 Highway 111 --\> error: Expected stringEnd (at char 14), (line:1, col:15)
    26517 State Highway 74 --\> error: Expected stringEnd (at char 20), (line:1, col:21)
    80425 US Highway 111 --\> error: Expected stringEnd (at char 17), (line:1, col:18)
    22217 US Highway 18 --\> error: Expected stringEnd (at char 17), (line:1, col:18)
    1567 State Highway 99 --\> error: Expected stringEnd (at char 19), (line:1, col:20)
    32545B Golden Lantern 147 --\> error: Expected stringEnd (at char 22), (line:1, col:23)
    1207 13TH ST STE 1 --\> error: Expected stringEnd (at char 7), (line:1, col:8)
    633 W 5th St 28th Floor --\> error: Expected stringEnd (at char 13), (line:1, col:14)
    2329 US Highway 86 --\> error: Expected stringEnd (at char 16), (line:1, col:17)



The allowed variations on 'STATE HIGHWAY 123' are documented by the USPS at

''.

---
## 2010-07-04 13:11:12 - GregWatson - problem with __builtin__ pyparsing  1.5.3 and Windows Python 3.1.2
FYI



I just installed pyparsing 1.5.3 with the latest Python 3.1.2 (r312:79149) and tried to run the hello world.

It gave an ImportError about not finding <u>builtin</u>



So I just commented out the line 

'import <u>builtin</u>'

and then changed

        singleArgBuiltins.append(getattr(<u>builtin</u>,fname))



to         singleArgBuiltins.append(getattr(<u>builtins</u>,fname))



(i.e. <u>builtin</u> =\> <u>builtins</u>)



Now it works.



If there is a better solution then please post.



Thanks!

#### 2010-07-04 16:45:04 - ptmcg
No, this is a *bug*.  I thought I had tested this installation on Python 3, but apparently I missed a step.  __builtin__ was changed to builtins in Python 3.  I'll give things another week or so to see if any other glaring bugs roll in, then I'll spin out a 1.5.4 with this bug fixed.



Your self-patched version will work fine until then.  Thanks for downloading pyparsing!



-- paul
#### 2010-07-21 13:04:28 - saulspatz
I can't even get pyparsing to install with python 3.1:



c:\Downloads\pyparsing-1.5.3\>c:\Python31\python.exe setup.py -v install

Traceback (most recent call last):

  File 'setup.py', line 12, in \<module\>

    from pyparsing_py3 import <u>version</u> as pyparsing_version

  File 'c:\Downloads\pyparsing-1.5.3\pyparsing_py3.py', line 141, in \<module\>

    import <u>builtin</u>

ImportError: No module named <u>builtin</u>



I tried running the executable installer and that failed too.  It didn't give a very detailed error message.



This is under Windows 7.
#### 2010-07-21 13:42:51 - GregWatson
This looks to be the same problem that I reported.



You could try editing the  c:\Downloads\pyparsing-1.5.3\pyparsing_py3.py file as I described in the original post above and then rerun the setup.



-Greg
#### 2010-07-21 13:46:22 - ptmcg
Once you guys make this one change to pyparsing_py3, is there any other issue with running on Python 3?  If not, then I can turn around a quick 1.5.4 with just this fix, and *then* Python 3 should be a working target.



Thanks,

-- Paul
#### 2010-07-21 13:55:34 - GregWatson
I have not found any other issues, but I have only run on XP SP 3.



Maybe sualspatz can comment once he tries the fix .
#### 2010-07-25 09:46:00 - sminos
I have found another issue !



I am currently working with windows XP SP2.



...\pyparsing-1.5.3\>python setup.py install

Traceback (most recent call last):

  File 'setup.py', line 35, in \<module\>

    copyfile(from_file, 'pyparsing.py')

  File 'setup.py', line 26, in copyfile

    outf = file(toname,'w')

NameError: global name 'file' is not defined



I replace 'file' with 'open' and it works fine now.



thanks for your work.
#### 2010-07-25 19:38:43 - ptmcg
Excellent, thank you!

---
## 2010-07-06 19:04:52 - GregWatson - newbie problem with addParseAction and Combine
Hi



I am trying to use addParseAction with each rule in order to see which rules are matched.



But if I use Combine then the more deeply nested actions do not seem to be invoked.



Here's the code (that works as I expect) without the Combine:



[[from pyparsing import *UnescapedChar = Word(alphanums, exact=1)EscapedChar = Combine( Suppress('\\') + Word(printables,exact=1) )HexChar = Combine( Suppress(r'\x') + Word(hexnums,exact=2) )SingleChar = UnescapedChar ^ HexChar ^ EscapedChar#CharRange = Combine( SingleChar + Suppress('..') + SingleChar )CharRange =  SingleChar + Suppress('..') + SingleChar def doEscapedChar (s,loc,toks):    print('EscapedChar:  saw tokens {0}'.format(toks[0:])) def doUnescapedChar (s,loc,toks):    print('UnEscapedChar:  saw tokens {0}'.format(toks[0:])) def doHexChar (s,loc,toks):    print('hexchar: text  saw tokens {0}'.format(toks[0:]))def doSingleChar (s,loc,toks):    print('SingleChar: saw tokens {0}'.format(toks[0:])) def doCharRange (s,loc,toks):    print('CharRange text was {0}\n   saw tokens {1}'.format(s,toks[0:])) # Define the parser actionsEscapedChar.addParseAction(doEscapedChar)UnescapedChar.addParseAction(doUnescapedChar)HexChar.addParseAction(doHexChar)SingleChar.addParseAction(doSingleChar)CharRange.addParseAction(doCharRange)# testingtext = r'\x20..\x21'print (CharRange.parseString(text))]]



and the output is:



hexchar: text  saw tokens ['20']

SingleChar: saw tokens ['20']

hexchar: text  saw tokens ['21']

SingleChar: saw tokens ['21']

CharRange text was \x20..\x21

   saw tokens ['20', '21']

['20', '21']





i.e. I see the action is invoked for each token.



But, if I add a Combine to the CharRange definition:



[[CharRange = Combine( SingleChar + Suppress('..') + SingleChar )]]



then the output becomes:



CharRange text was \x20..\x21

   saw tokens ['2021']

['2021']





So the hexchar and SingleChar actions seem not to have been invoked, and yet they clearly have been given that the \x have been removed.





Any clarification on this would be much appreciated.



Thanks for a great tool!



Greg

#### 2010-07-06 19:06:49 - GregWatson
Sorry, I messed up the formatting.



Here's the code without Combine:





    from pyparsing import *
    
    UnescapedChar = Word(alphanums, exact=1)
    EscapedChar = Combine( Suppress('\\') + Word(printables,exact=1) )
    HexChar = Combine( Suppress(r'\x') + Word(hexnums,exact=2) )
    
    SingleChar = UnescapedChar ^ HexChar ^ EscapedChar
    
    #CharRange = Combine( SingleChar + Suppress('..') + SingleChar )
    CharRange =  SingleChar + Suppress('..') + SingleChar 
    
    def doEscapedChar (s,loc,toks):
        print('EscapedChar:  saw tokens {0}'.format(toks[0:])) 
    
    def doUnescapedChar (s,loc,toks):
        print('UnEscapedChar:  saw tokens {0}'.format(toks[0:])) 
    
    def doHexChar (s,loc,toks):
        print('hexchar: text  saw tokens {0}'.format(toks[0:]))
    
    def doSingleChar (s,loc,toks):
        print('SingleChar: saw tokens {0}'.format(toks[0:])) 
    
    def doCharRange (s,loc,toks):
        print('CharRange text was {0}\n   saw tokens {1}'.format(s,toks[0:])) 
    
    # Define the parser actions
    EscapedChar.addParseAction(doEscapedChar)
    UnescapedChar.addParseAction(doUnescapedChar)
    HexChar.addParseAction(doHexChar)
    SingleChar.addParseAction(doSingleChar)
    CharRange.addParseAction(doCharRange)
    
    # testing
    
    text = r'\x20..\x21'
    
    print (CharRange.parseString(text))
    



-Greg
#### 2010-07-07 00:02:13 - ptmcg
I've looked into this, and I've identified the issue, but I'm not sure there is a solution. Since Combine has to modify the contained expressions to suppress the default whitespace skipping, it makes a deep copy of the expression used to create it.  Since you don't add the parse actions until after creating the Combine expression, the expressions you add parse actions to are not the ones used in CharRange.  To work around this, move the definition of CharRange until after the subexpressions have parse actions added to them.



-- Paul
#### 2010-07-07 09:16:34 - GregWatson
Excellent - thanks! That would never have occurred to me. 



And thanks for the incredibly prompt support!



-Greg

---
## 2010-07-08 09:48:49 - GregWatson - Questions on setDebug() and And()
Hi, 



I am encountering a couple of problems which I have reduced to a very simple parser:





    from pyparsing import *
    
    SymType = Forward()  # Recursive use
    
    SymSeq = '(' + OneOrMore(SymType) + ')'
    SymSeq.setName('SymSeq').setDebug()
    
    LitA = Literal('A').setName('A').setDebug()
    LitB = Literal('B').setName('B').setDebug()
    
    SymSingle = And(Or(LitA, LitB) ,Literal('END'))
    SymSingle.setName('SymSingle').setDebug()
    
    SymType \<\< Or( SymSingle, SymSeq )
    SymType.setName('SymType').setDebug()
    
    
    # --- testing -------------------------------
    
    #        123456789012345678901234567890
    text = r'A'
    
    print ( SymType.parseString(text) )
    

 

The output from running this (Python 3.1.2 Windows,  pyparsing 1.5.3) is:



Match SymType at loc 0(1,1)

Match A at loc 0(1,1)

Matched A -\> ['A']

Match A at loc 0(1,1)

Matched A -\> ['A']

Matched SymType -\> ['A']

['A']



Two things confuse me:



1. The SymType matches SymSingle (LitA) and yet I dont see a debug line of the form 'Match SymSingle' even though SymSingle has debugging turned on.



2. The definition for SymSingle is ( (LitA | LitB) & 'END') . Since the input is just 'A' and not 'A END' then I would expect the parser to fail, and yet it doesn't. It's as though the And() was ignored.



Any insights to these would be much appreciated.



Cheers,  Greg

#### 2010-07-08 11:20:43 - GregWatson
Duh!



After looking at the code for And() I realized that And and Or expect a single LIST argumant (as it clearly says in the docs!) and not a sequence of **kwargs.



So my code should have been:





    SymSingle = And( [ Or( [LitA, LitB] ),Literal('END') ] )

It fixes both problems.



-Greg
#### 2010-07-08 11:57:01 - ptmcg
I have to translate this to an operator-expression form to see what you are doing:





    SymSingle = (LitA ^ LitB) + 'END'
    



Unless there is some potential for confusion between LitA and LitB, you would be better off with MatchFirsts:





    SymSingle = (LitA | LitB) + 'END'
    



Welcome to pyparsing!



-- Paul

---
## 2010-07-12 03:15:22 - dminor14 - pyparsing and imputil
Hi Paul,

Is it possible for you to re-post your article on pyparsing and imputil?  The Python magazine web site has been down for a while and I'd like to read it. 

Regards,

David


---
## 2010-07-22 10:36:47 - avisenna - need urgent help with information nested in special CStyle Comment blocks
Hi,



I'm trying for a while to extract some information embedded in a 'special' commentblock in C-Header files

Header filesinclding the Comment blocks look like this:



/<strong> @defgroup _MSG_DMC_X1_MSF_OPTIONS_SET_ DMC_X1_MSF_Options_Set

 *  @{

 */



/</strong> Message ID for DMCmedomMSFOptionsSet . */

#define HXH_DMC_X1_MSF_OPTIONS_SET 0x4098u



/<strong> Parameter0: L2 Semi-Auto Entry, Bit 11 .

   (AA only)

   Only effective if the L2 mode has been enabled (with Bit 8). Enables/Disables the

   automatism for semi-auto L2 entry function. It means the supervision of bandwidth

   utilization as configured by PRR_TH and L2_GUARD in DMC_AA_L2_BC0_Configure.

   - Default Value: NX_DISABLE

*/



#define HXH_4098_E7_POS 7

/</strong> Parameter0: Granting of an L3 Request from UTA-R .

   (AA-CO only)

   Enables/Disables the granting of an orderly shutdown request ('L3 request')

   received from the UTA-R.

   - Default Value: NX_DISABLE



   @note

    If disabled, the UTA-C rejects an L3 request in any case. If enabled, the

   UTA-C may grant an L3 request (additional conditions must be fulfilled like

   e.g. Showtime)

*/

#define HXH_4098_E5_MASK 0x0020u

/<strong> Bit offset of @ref HXH_4098_E5_MASK . */

#define HXH_4098_E5_POS 5



typedef struct

{

   /</strong> Parameter0 . */

   uint16 Parameter0;

} T_HXH_DMCmedomMSFOptionsSet;



/** @} */





apart from the C-expressions which are parsed correctly, ignoring the comment blocks

I need to extract the information in the lines starting with '-' the a 'keyword' followed by ':' and any information to the end of the line

for example:

- Default Value: xxx utut erui 

or 

- Min. Val: -1D



Can anybody please help?

#### 2010-07-22 11:49:14 - ptmcg
Please repost using code tags, as shown here: 



-- Paul
#### 2010-07-22 12:55:18 - avisenna
Paul. sorry for the bad format of the orig. message. I'll try it again

<hr />
Hi,



I'm trying for a while to extract some information embedded in a 'special' commentblock in C-Header files

Header filesinclding the Comment blocks look like this:





    
    <span class="coMULTI">/** @defgroup _MSG_DMC_X1_MSF_OPTIONS_SET_ DMC_X1_MSF_Options_Set
    * @{
    */</span>
    
    <span class="coMULTI">/* Message ID for DMCmedomMSFOptionsSet . */</span>
    <span class="co2">#define HXH_DMC_X1_MSF_OPTIONS_SET 0x4098u</span>
    
    <span class="coMULTI">/** Parameter0: L2 Semi-Auto Entry, Bit 11 .
    (AA only)
    Only effective if the L2 mode has been enabled (with Bit 8). Enables/Disables the
    automatism for semi-auto L2 entry function. It means the supervision of bandwidth
    utilization as configured by PRR_TH and L2_GUARD in DMC_AA_L2_BC0_Configure.
    - Default Value: NX_DISABLE
    */</span>
    
    <span class="co2">#define HXH_4098_E7_POS 7</span>
    <span class="coMULTI">/** Parameter0: Granting of an L3 Request from UTA-R .
    (AA-CO only)
    Enables/Disables the granting of an orderly shutdown request ('L3 request')
    received from the UTA-R.
    - Default Value: NX_DISABLE
    
    @note
    If disabled, the UTA-C rejects an L3 request in any case. If enabled, the
    UTA-C may grant an L3 request (additional conditions must be fulfilled like
    e.g. Showtime)
    */</span>
    <span class="co2">#define HXH_4098_E5_MASK 0x0020u</span>
    <span class="coMULTI">/* Bit offset of @ref HXH_4098_E5_MASK . */</span>
    <span class="co2">#define HXH_4098_E5_POS 5</span>
    
    <span class="kw4">typedef</span> <span class="kw4">struct</span>
    <span class="br0">&#123;</span>
    <span class="coMULTI">/** Parameter0 . */</span>
    <span class="kw4">uint16</span> Parameter0<span class="sy0">;</span>
    <span class="br0">&#125;</span> T_HXH_DMCmedomMSFOptionsSet<span class="sy0">;</span>
    
    <span class="coMULTI">/** @} */</span>



apart from the C-expressions which are parsed correctly, ignoring the comment blocks

I need to extract the information in the lines starting with '-' the a 'keyword' followed by ':' and any information to the end of the line

for example:

- Default Value: xxx utut erui

or

- Min. Val: -1D



Can anybody please help?



regards,

Kevin
#### 2010-07-22 13:29:48 - ptmcg
Putting your C code into a variable named 'csource', I used this to extract your special lines:





    from pyparsing import *
    
    specialinfo = LineStart() + '-' + SkipTo(':')('name') + ':' + restOfLine('value')
    
    for ccomment in cStyleComment.searchString(csource):
        for info in specialinfo.searchString(ccomment[0]):
            print info.dump()



Gives:



    ['-', 'Default Value', ':', ' NX_DISABLE']
    - name: Default Value
    - value:  NX_DISABLE
    ['-', 'Default Value', ':', ' NX_DISABLE']
    - name: Default Value
    - value:  NX_DISABLE



I don't think you'll be able to get these in the same pass while parsing for non-comment content in the C code, since the comments will get ignored.  So you'll need to do 2 passes, and stitch the values together somehow.



-- Paul
#### 2010-07-22 13:31:25 - ptmcg
If you use cStyleComment.scanString instead of cStyleComment.searchString, then you'll also get the start and end locns of each comment, which might help you correlate data between passes.



-- Paul
#### 2010-07-22 13:33:28 - avisenna
cool. thanks for your prompt response.
#### 2010-07-22 15:00:38 - avisenna
Paul,

many Thanks for your help 

I#m working on some extensions and

have problems to extract more than one specialinfo-blocks out of one comment block.



in order to be able to corelate the data between two passes i have to know that the

data is related to 'parameter0' which is declared in the typedef struct block.



So i need to that the the default/min/max value is related to Parameter0. using the code below I get only Paramet0 and its desription but not the default/Min/Max value 

Any hints on that? 





    
    from pyparsing import *
    
    csource = '''
    
    /** @defgroup _MSG_DMC_MEDOM_MSF_OPTIONS_SET_ DMC_Medom_MSF_Options_Set
     *  @{
     */
    
    /** Message ID for DMCMedomFsmOptionsSet . */
    #define HMH_DMC_MEDOM_MSF_OPTIONS_SET 0x4098u
    
    /** Message properties for DMCMedomFsmOptionsSet . */
    #define HMH_DMC_MEDOM_MSF_OPTIONS_SET_EXTEND (HMH_DMC_MEDOM_MSF_OPTIONS_SET | PROP_VDSL | PROP_ADSL | PROP_V3_X | PROP_IDLE)
    
    /** Parameter0: L2 Semi-Auto Entry, Bit 11 .
       (ADSL only)
       Only effective if the L2 mode has been enabled (with Bit 8). Enables/Disables the
       automatism for semi-auto L2 entry function. It means the supervision of bandwidth
       utilization as configured by PRR_TH and L2_GUARD in DMC_AA_L2_BC0_Configure.
       - Default Value: VNX_DISABLE
       - Min. Value: 1H
       - Max. Value: FFFFFFFEH
    */
    #define HMH_4098_E11_MASK 0x0800u
    /** Bit offset of @ref HMH_4098_E11_MASK . */
    #define HMH_4098_E11_POS 11
    
    /** Parameter0: L2 Automatic Exit.
      (AA only)
      Only effective if the L2 mode has been enabled (with E8). Then an automatic L2 exit
      can be enabled, based on the settings of the L2 exit criterium.
       - Default Value: VNX_DISABLE
    */
    #define HMH_4098_E10_MASK 0x0400u
    /** Bit offset of @ref HMH_4098_E10_MASK . */
    #define HMH_4098_E10_POS 10
    
    /** Parameter0: L2 Automatic Entry.
      (AA only)
      The L2 Low Power mode has to be enabled as well (with E8). In this case the
      automatic entry of the L2 mode based on threshold observation is enabled/disabled.
       - Default Value: VNX_DISABLE
    */
    #define HMH_4098_E9_MASK 0x0200u
    /** Bit offset of @ref HMH_4098_E9_MASK . */
    #define HMH_4098_E9_POS 9
    
    /** Parameter0: L2 Low-Power Mode Enable.
      (AA only)
      Enables/Disables the L2 Low-Power Mode.
       - Default Value: VNX_DISABLE
    
    */
    #define HMH_4098_E8_MASK 0x0100u
    /** Bit offset of @ref HMH_4098_E8_MASK . */
    #define HMH_4098_E8_POS 8
    
    /** Parameter0: GHS Bonding Exchange Only .
       (VDSL only)
       Selects if the MEDOM MSF stops after GHS Bonding exchange.
       - Default Value: VNX_DISABLE
    */
    #define HMH_4098_E7_MASK 0x0080u
    /** Bit offset of @ref HMH_4098_E7_MASK . */
    #define HMH_4098_E7_POS 7
    /** Parameter0: Granting of an L3 Request from ATU-R .
       (AA-CO only)
       Enables/Disables the granting of an orderly shutdown request ('L3 request')
       received from the ATU-R.
       - Default Value: VNX_DISABLE
    
       @note
        If disabled, the ATU-C rejects an L3 request in any case. If enabled, the
       ATU-C may grant an L3 request (additional conditions must be fulfilled like
       e.g. Showtime)
    */
    #define HMH_4098_E5_MASK 0x0020u
    /** Bit offset of @ref HMH_4098_E5_MASK . */
    #define HMH_4098_E5_POS 5
    /** Parameter0: Loop Diagnostic Mode Control .
       Enable/Disable of 'Diagnostic Mode'
       - Default Value: VNX_DISABLE
    
       When enabled, the MEDOM will go to diagnostic mode after starting link
       initialization with DMC_MedomMSF_StateSet.
    */
    #define HMH_4098_E2_MASK 0x0004u
    /** Bit offset of @ref HMH_4098_E2_MASK . */
    #define HMH_4098_E2_POS 2
    /** Parameter0: Short-Initialization .
       (AA only)
       - Default Value: VNX_DISABLE */
    #define HMH_4098_E1_MASK 0x0002u
    /** Bit offset of @ref HMH_4098_E1_MASK . */
    #define HMH_4098_E1_POS 1
    /** Parameter0: Automatic Re-Start Control .
       Enable/Disable of 'automatic re-start'
    
       Automatic re-start allows the MEDOM state-machine to pass on to SILENT state
       immediately after the MEDOM entered FAIL state. This happens automatically.
       If the automatic re-start is disabled, the medom will stay in FAIL until
       explicitly set to another state by host command DMC_MedomMSF_StateSet.
       DELT:
       If enabled at a VTU_R during a loop diagnostic mode (DELT) which was not
       triggered by the VTU_R (but the VTU_O), the medom MSF transits automatically
       from DELT_COMPLETE to SILENT state. If disabled, it stays in DELT_COMPLETE
       waiting for host interaction.
       In AA and at the VDSL-CO, the medom stays in TJED_COMPLETE in any case.
    */
    #define HMH_4098_DMC_MEDOM_MSF_OPTIONS_SET_E0_MASK 0x0001u
    /** Bit offset of @ref HMH_4098_DMC_MEDOM_MSF_OPTIONS_SET_E0_MASK . */
    #define HMH_4098_DMC_MEDOM_MSF_OPTIONS_SET_E0_POS 0
    /** Configuration of options for the medom state machine. */
    typedef struct
    {
       /** Parameter0 . */
       uint16 Parameter0;
    } T_HMH_DMCMedomFsmOptionsSet;
    
    /** @} */
    
    '''
    
    IDENT = Word(alphas+'_', alphanums+'_')
    TYPES = Literal('uint32') | Literal('int')
    
    assert 'uint32' == TYPES
    assert 'int' == TYPES
    
    
    keys = Keyword ('Default Value') | Keyword ('Min. Value') | Keyword ('Min. Value')   
    
    #Constraints  = LineStart() + '-' + SkipTo(':')('name') + ':' + restOfLine('value')
    Constraints  = LineStart() + '-' + keys('key') + ':' + restOfLine('value')
    assert '- Default Value: VNX_DISABLE' == Constraints 
    
    defBlock = Literal('@defgroup') + IDENT('GroupName') +IDENT('GroupValue')
    assert '@defgroup _MSG_DMC_MEDOM_MSF_OPTIONS_SET_ DMC_Medom_MSF_Options_Set' == defBlock
    
    ParameterX = Literal('Parameter')+ Word(nums, max = 2) + SkipTo(':')('name') + ':' +  SkipTo('.')('ParameterDescr') + Suppress('.')#restOfLine('value') 
    assert 'Parameter0: Automatic Re-Start Control .' == ParameterX
    assert 'Parameter10: Automatic Re-Start Control .' == ParameterX
    
    specialinfo = OneOrMore(Constraints) | ParameterX | defBlock 
    
    tstInfoData = '''
    @defgroup _MSG_DMC_MEDOM_MSF_OPTIONS_SET_ DMC_Medom_MSF_Options_Set
    
    '''    
    
    for ccomment in cStyleComment.scanString(csource):
        print ccomment[0]
        for info in specialinfo.searchString(ccomment[0]):
            print info.dump()
    
    


#### 2010-07-22 15:52:48 - ptmcg
LineStart is messing up your Constraints, and change searchString's arg to ccomment[0][0].  Also, take off the OneOrMore on Constraints, because you want to match each one separately.
#### 2010-07-22 17:52:18 - avisenna
me again ....

wondering why I cannot get the 'Min Value' out of csource in the first Parameter0 Block 



this is the gerated output



['Parameter', '0', '', 'L2 Semi-Auto Entry, Bit 11 ']

- ParameterDescr: L2 Semi-Auto Entry, Bit 11 

- name: 

['Default Value', 'VNX_DISABLE', '\n', 'Min. Value', '1H', '\n', 'Max. Value', 'FFFFFFFEH', '\n']

- ConstraintValue: ['FFFFFFFEH']

- key: Max. Value 



that's the new code





    
    from pyparsing import *
    
    csource = '''
    
    /** @defgroup _MSG_DMC_MEDOM_MSF_OPTIONS_SET_ DMC_Medom_MSF_Options_Set
     *  @{
     */
    
    /** Message ID for DMCMedomFsmOptionsSet . */
    #define HMH_DMC_MEDOM_MSF_OPTIONS_SET 0x4098u
    
    /** Message properties for DMCMedomFsmOptionsSet . */
    #define HMH_DMC_MEDOM_MSF_OPTIONS_SET_EXTEND (HMH_DMC_MEDOM_MSF_OPTIONS_SET | PROP_VDSL | PROP_ADSL | PROP_V3_X | PROP_IDLE)
    
    /** Parameter0: L2 Semi-Auto Entry, Bit 11 .
       (ADSL only)
       Only effective if the L2 mode has been enabled (with Bit 8). Enables/Disables the
       automatism for semi-auto L2 entry function. It means the supervision of bandwidth
       utilization as configured by PRR_TH and L2_GUARD in DMC_AA_L2_BC0_Configure.
       - Default Value: VNX_DISABLE
       - Min. Value: 1H
       - Max. Value: FFFFFFFEH
    */
    #define HMH_4098_E11_MASK 0x0800u
    /** Bit offset of @ref HMH_4098_E11_MASK . */
    #define HMH_4098_E11_POS 11
    
    /** Parameter0: L2 Automatic Exit.
      (AA only)
      Only effective if the L2 mode has been enabled (with E8). Then an automatic L2 exit
      can be enabled, based on the settings of the L2 exit criterium.
       - Default Value: VNX_DISABLE
    */
    #define HMH_4098_E10_MASK 0x0400u
    /** Bit offset of @ref HMH_4098_E10_MASK . */
    #define HMH_4098_E10_POS 10
    
    /** Parameter0: L2 Automatic Entry.
      (AA only)
      The L2 Low Power mode has to be enabled as well (with E8). In this case the
      automatic entry of the L2 mode based on threshold observation is enabled/disabled.
       - Default Value: VNX_DISABLE
    */
    #define HMH_4098_E9_MASK 0x0200u
    /** Bit offset of @ref HMH_4098_E9_MASK . */
    #define HMH_4098_E9_POS 9
    
    /** Parameter0: L2 Low-Power Mode Enable.
      (AA only)
      Enables/Disables the L2 Low-Power Mode.
       - Default Value: VNX_DISABLE
    
    */
    #define HMH_4098_E8_MASK 0x0100u
    /** Bit offset of @ref HMH_4098_E8_MASK . */
    #define HMH_4098_E8_POS 8
    
    /** Parameter0: GHS Bonding Exchange Only .
       (VDSL only)
       Selects if the MEDOM MSF stops after GHS Bonding exchange.
       - Default Value: VNX_DISABLE
    */
    #define HMH_4098_E7_MASK 0x0080u
    /** Bit offset of @ref HMH_4098_E7_MASK . */
    #define HMH_4098_E7_POS 7
    /** Parameter0: Granting of an L3 Request from ATU-R .
       (AA-CO only)
       Enables/Disables the granting of an orderly shutdown request ('L3 request')
       received from the ATU-R.
       - Default Value: VNX_DISABLE
    
       @note
        If disabled, the ATU-C rejects an L3 request in any case. If enabled, the
       ATU-C may grant an L3 request (additional conditions must be fulfilled like
       e.g. Showtime)
    */
    #define HMH_4098_E5_MASK 0x0020u
    /** Bit offset of @ref HMH_4098_E5_MASK . */
    #define HMH_4098_E5_POS 5
    /** Parameter0: Loop Diagnostic Mode Control .
       Enable/Disable of 'Diagnostic Mode'
       - Default Value: VNX_DISABLE
    
       When enabled, the MEDOM will go to diagnostic mode after starting link
       initialization with DMC_MedomMSF_StateSet.
    */
    #define HMH_4098_E2_MASK 0x0004u
    /** Bit offset of @ref HMH_4098_E2_MASK . */
    #define HMH_4098_E2_POS 2
    /** Parameter0: Short-Initialization .
       (AA only)
       - Default Value: VNX_DISABLE */
    #define HMH_4098_E1_MASK 0x0002u
    /** Bit offset of @ref HMH_4098_E1_MASK . */
    #define HMH_4098_E1_POS 1
    /** Parameter0: Automatic Re-Start Control .
       Enable/Disable of 'automatic re-start'
    
       Automatic re-start allows the MEDOM state-machine to pass on to SILENT state
       immediately after the MEDOM entered FAIL state. This happens automatically.
       If the automatic re-start is disabled, the medom will stay in FAIL until
       explicitly set to another state by host command DMC_MedomMSF_StateSet.
       DELT:
       If enabled at a VTU_R during a loop diagnostic mode (DELT) which was not
       triggered by the VTU_R (but the VTU_O), the medom MSF transits automatically
       from DELT_COMPLETE to SILENT state. If disabled, it stays in DELT_COMPLETE
       waiting for host interaction.
       In AA and at the VDSL-CO, the medom stays in TJED_COMPLETE in any case.
    */
    #define HMH_4098_DMC_MEDOM_MSF_OPTIONS_SET_E0_MASK 0x0001u
    /** Bit offset of @ref HMH_4098_DMC_MEDOM_MSF_OPTIONS_SET_E0_MASK . */
    #define HMH_4098_DMC_MEDOM_MSF_OPTIONS_SET_E0_POS 0
    /** Configuration of options for the medom state machine. */
    typedef struct
    {
       /** Parameter0 . */
       uint16 Parameter0;
    } T_HMH_DMCMedomFsmOptionsSet;
    
    /** @} */
    
    '''
    
    IDENT = Word(alphas+'_', alphanums+'_')
    ALPHANUMS = Word(alphanums+'_') 
    TYPES = Literal('uint32') | Literal('int')
    
    assert 'uint32' == TYPES
    assert 'int' == TYPES
    
    
    keys = Keyword ('Default Value')  ^ Keyword ('Min. Value') ^ Keyword ('Max. Value')   
    
    #Constraints  = LineStart() + '-' + SkipTo(':')('name') + ':' + restOfLine('value')
    #Constraints  = LineStart() + '-' + keys('key') + ':' + restOfLine('value')
    #Constraints  =  Suppress('-') + keys('key') + Suppress(':') + restOfLine('value')
    Constraints  =  Suppress('-') + keys('key') + Suppress(':') + OneOrMore(ALPHANUMS)('ConstraintValue') + LineEnd()
    assert '- Default Value: VNX_DISABLE\n' == Constraints 
    assert '- Min. Value: 1H' == Constraints
    
    defBlock = Suppress(Literal('@defgroup')) + IDENT('GroupName') +IDENT('GroupValue')
    assert '@defgroup _MSG_DMC_MEDOM_MSF_OPTIONS_SET_ DMC_Medom_MSF_Options_Set' == defBlock
    
    ParameterX = Literal('Parameter')+ Word(nums, max = 2) + Optional(SkipTo(':'))('name') + Suppress(':') +  SkipTo('.')('ParameterDescr') + Suppress('.')#restOfLine('value') 
    assert 'Parameter0: Automatic Re-Start Control .' == ParameterX
    assert 'Parameter10: Automatic Re-Start Control .' == ParameterX
    
    specialinfo =  ParameterX | defBlock | OneOrMore(Constraints)
    constrBlkTest = '''
    - Default Value: VNX_DISABLE
       - Min. Value: 1H
       - Max. Value: FFFFFFFEH
          '''
    for match in Constraints.searchString(constrBlkTest):
        print match
    
    for ccomment in cStyleComment.scanString(csource):
        #print ccomment[0]
        for info in specialinfo.searchString(ccomment[0][0]):
            print info.dump()
    
    
    
    


#### 2010-07-22 21:47:55 - ptmcg
Change specialinfo to:





    specialinfo = Constraints | ParameterX | defBlock 



---
## 2010-07-23 11:47:41 - mlissner - No online documentation? 
It looks like the online documentation is now no longer working. I guess we should remove the link, but it would be great if we could get some documentation posted somewhere for people like myself that don't know how to find the documentation that ships with a package.



Thanks,



Mike

#### 2010-07-23 11:53:02 - mlissner
Looking at this some more, a link to this would be excellent:


#### 2011-12-22 10:43:27 - techtonik
Fixed. Thanks for the link - it really helps.

---
## 2010-08-10 15:12:09 - classicgamedev - operatorPrecedence is incredibly slow
I am using operatorPrecedence to implement some simple arithmetic operators as part of a simple assembler.  My pyparsing rules look something like this:





    immediate = operatorPrecedence(Word(nums),
                                   [ (oneOf('+ -'), 2, opAssoc.LEFT),
                                     (oneOf('* /'), 2, opAssoc.LEFT)])
    
    codeline = Or(['ldy', 'ldx']).setResultsName('opcode') + \
               Optional(Or([Group(Suppress('#') + immediate), oneOf('x y')])) + \
               Suppress(LineEnd())



this is obviously stripped down, but it is design to parse simple lines of assembly like so:





    ldy #2
    ldx #(1+2) * 3



What I'm finding is that it works, but it is incredibly slow.  It takes 2 minutes to parse a single line on a core2duo 3.0 GHz with 4 GB of RAM.  Is there anything I can do to speed this up?  It doesn't seem like this should take that long.  I noticed that it takes much longer if there are parenthesis involved like the second line above.



Do you have any suggestions?



Thanks.

#### 2010-08-10 15:27:59 - classicgamedev
So I just ran a simple test with the above code and it seems to be almost instantaneous.  My actual code is considerably more complex than the given example.



For instance, the opcode is defined as:



opcode = Or([CaselessKeyword(op) for op in Opcode.OPCODES])



where Opcode.OPCODES is an array of 56 different opcodes.



The base operand passed to operatorPrecedence is a complex Or that looks like:



variable_ref = Group(Name + ZeroOrMore(Suppress('.') + Name))

imm = Or([NumericValue, ArrayValue, variable_ref])

operatorPrecedence(imm, [...])



NumericValue handles parsing numbers in decimal, hexidecimal (0x0000 and $0000), binary 1000101b, etc.  The ArrayValue handles parsing things like foo[0].  The variable_ref handles variable names like 'foo' and 'bar.baz'.



So my real code is more complex but I still think it shouldn't take 2 minutes to drop into the parse action for the operatorPrecedence rule.
#### 2010-08-10 17:39:33 - classicgamedev
So I was able to get the runtime down to something quick by removing all of the clauses of the operatorPrecedence except for one.  I then played around with adding clauses back in and discovered a few things:



1. adding single argument, right-associative operators costs almost nothing.  I have eight of them in my full operator list.



2. adding in two-argument, left-associative operators increases exponentially as each one is added.



I used the time command on Linux to measure the real time it took to process a single line of input.  Each time I added a new two-argument, left-associative operator.  Here are the numbers:





<table class="wiki_table">
    <tr>
        <th>Operators

</th>
        <th>Seconds

</th>
    </tr>
    <tr>
        <td>1

</td>
        <td>0.27

</td>
    </tr>
    <tr>
        <td>2

</td>
        <td>0.32

</td>
    </tr>
    <tr>
        <td>3

</td>
        <td>0.49

</td>
    </tr>
    <tr>
        <td>4

</td>
        <td>1.18

</td>
    </tr>
    <tr>
        <td>5

</td>
        <td>3.83

</td>
    </tr>
    <tr>
        <td>6

</td>
        <td>14.67

</td>
    </tr>
    <tr>
        <td>7

</td>
        <td>57.18

</td>
    </tr>
    <tr>
        <td>8

</td>
        <td>233.77

</td>
    </tr>
</table>



So you can see, when I have all eight of my two-argument, left-associative operators, the processing time is just shy of 4 minutes per line of text.



Is there anything I can do to make this faster?
#### 2010-08-10 17:41:23 - classicgamedev
Just for reference, here's the code that sets up my operatorPrecedence:





    
    signop = oneOf('+ -')
    sizeof_ = Literal('sizeof')
    lo_ = Literal('lo')
    hi_ = Literal('hi')
    nylo_ = Literal('nylo')
    nyhi_ = Literal('nyhi')
    negop = Literal('!')
    notop = Literal('~')
    multop = oneOf('* / %')
    plusop = oneOf('+ -')
    shiftop = oneOf('\<\< \>\>')
    cmpeqop = oneOf('\< \> \<= \>=')
    eqop = oneOf('!= ==')
    andop = Literal('&')
    exorop = Literal('^')
    orop = Literal('|')
    
    variable_ref = Group(Name.exprs() + ZeroOrMore(Suppress('.') + Name.exprs()))
    imm = Or([NumericValue.exprs(),
              ArrayValue.exprs(),
              variable_ref])
    
    expr = operatorPrecedence( imm,
            [ 
              (signop,  1, opAssoc.RIGHT),
              (sizeof_, 1, opAssoc.RIGHT),
              (lo_,     1, opAssoc.RIGHT),
              (hi_,     1, opAssoc.RIGHT),
              (nylo_,   1, opAssoc.RIGHT),
              (nyhi_,   1, opAssoc.RIGHT),
              (negop,   1, opAssoc.RIGHT),
              (notop,   1, opAssoc.RIGHT),
              (multop,  2, opAssoc.LEFT),
              (plusop,  2, opAssoc.LEFT),
              (shiftop, 2, opAssoc.LEFT),
              (cmpeqop, 2, opAssoc.LEFT),
              (eqop,    2, opAssoc.LEFT),
              (andop,   2, opAssoc.LEFT),
              (exorop,  2, opAssoc.LEFT),
              (orop,    2, opAssoc.LEFT) 
            ])
    
    expr.setParseAction(klass.parse)
    return expr


#### 2010-08-10 21:07:11 - ptmcg
It looks like packrat parsing will really help your parser.  After importing pyparsing, add this line:



    ParserElement.enablePackrat()



My time to run your parser went from 28 seconds to about .1 second.
#### 2010-08-10 21:32:12 - ptmcg
Some other comments on your parser:



1) An Or of 56 different opcodes is going to be a lot less efficient than if you use MatchFirst.  MatchFirst does short-circuiting, returning the first matching term.  By contrast, Or has to examine all given options, even after finding an exact match.  Since you are creating CaselessKeywords for all your opcodes, you run no risk of incorrectly matching the wrong opcode.  You can even do some ordering of the opcodes in the list, to put more frequent opcodes ahead of less frequent ones.





2) 



    variable_ref = Group(Name + ZeroOrMore(Suppress('.') + Name))



can be replaced with





    variable_ref = Group(delimitedList(Name, delim='.'))





3) I've found that the terminal items like decimal numbers, hex, etc. are best implemented using Regexes, instead of building them up with Word, Combine, and so forth:





    decimal = Regex(r'\d+(\.\d*)?')
    hexadecimal = Regex(r'(0x|$)[0-9a-fA-F]+')
    binary = Regex(r'[01]+b')


#### 2010-08-11 10:26:47 - classicgamedev
Wow.  I just added:



    ParserElement.enablePackrat()



And now the parsing time is down to 0.291 seconds.  That's the only change I made.  Why is it so much faster?  Why isn't that enabled by default?  



There should be a note in the pyparsing operatorPrecedence docs saying something to the effect of: 'if you have lots of left-associative operators (4+), you may see significant speed improvements if you ParserElement.enablePackrat()'



Thanks for your help.  I was afraid I'd have to abandon pyparsing for my assembler.
#### 2010-08-11 11:58:12 - ptmcg
Great, I'm glad this paid off for you! You can see more discussion of packratting in pyparsing at 'packrat parsing'?

---
## 2010-08-12 18:13:10 - scripteaze - Mac Address Grammer and output
all is working fine, however, im trying to grab just the mac address.

[code]

from pyparsing import *

import os, string



<ol><li>grammer</li></ol>

firstline    = Suppress(ZeroOrMore(Word(alphas)))

secondline   = Suppress(ZeroOrMore('='))

thirdline    = ZeroOrMore(Word(alphanums + '-'))



<ol><li>pattern</li></ol>

pattern = Combine(firstline + secondline + thirdline)



ws = 'cia'

x = ''

p = os.popen('GETMAC /S %s' %(ws))

for line in p.readlines():

    x = pattern.parseString(line)

    print x

[/code]



the output is:



['']

['']

['']

['00-23-8B-20-4F-8A']

['']



im trying to just get the mac into a variable for later use, not all the remaining [], [], []





thanks!!

#### 2010-08-12 20:04:21 - ptmcg
Use a results name.  Change:



    thirdline = ZeroOrMore(Word(alphanums + '-'))



to



    thirdline = ZeroOrMore(Word(alphanums + '-')('macaddress'))



Then do:



    if x.macaddress:
            print x.macaddress


#### 2010-08-13 20:02:24 - scripteaze
thanks for the reply and solution..



i went with this, it does work, although woundnt mind cleaning it up a bit.





    from pyparsing import Suppress, ZeroOrMore, Word, alphas, alphanums
    import os, socket, sys
    
    account                 = raw_input('3 letter abr of the account ')
    room                    = raw_input('room number we are mapping ')
    amount                  = input('amount of systems in the room ')
    
    first                   = Suppress(ZeroOrMore(Word(alphas)))
    second                  = Suppress(ZeroOrMore('='))
    third                   = ZeroOrMore(Word(alphanums + '-'))
    
    pattern                 = first + second + third
    
    for x in xrange(amount):
        workstation         = account + room + '%03i' % x
    
        print '[+] Enumerating %s' % workstation
        try:
            workstationIP   = socket.gethostbyname(workstation)
    
        except socket.error:
            z = '%s%s_repairs.txt' % (account, room)
            logfile = open(z, 'a')
            logfile.write(workstation)
            logfile.write('\n')
            logfile.close()
            print '    added to %s' % z
            print ''
        else:
            cmdString       = 'getmac /S %s' % workstation
            try:
                cmdRun          = os.popen(cmdString).read()
            except socket.error:
                print ''
            else:
                macaddy     = pattern.parseString(cmdRun)
                y = '%s%s_map.txt' % (account, room)
            logfile = open(y, 'a')
            up = '%s  %s  %s' % (workstation, workstationIP, macaddy)
            logfile.write(up)
            logfile.write('\n')
            logfile.close()
            print '    added to %s' % y
            print ''


#### 2010-08-14 01:27:49 - ptmcg
I've never used getmac before, it looks like a handy utility.



Instead of using parseString and having to skip over stuff you don't want, you might try taking the approach of defining a specific expression that more narrowly describes a MAC address than Word(alphanums+'-').  I ran getmac on my own laptop, and ran the following script:





    getmacOutput = '''\
    Physical Address    Transport Name
    =================== ==========================================================
    Disabled            Disconnected
    00-E0-B8-B8-4E-38   Media disconnected
    00-14-A5-D5-B1-12   \Device\Tcpip_{EA506137-39D0-43FF-8A91-47FEE4C94944}
    Disabled            Disconnected
    00-FF-8B-CA-95-FF   Media disconnected
    00-11-67-31-18-83   Media disconnected
    00-09-0F-FE-00-01   Media disconnected
    '''
    
    from pyparsing import Combine, Word, hexnums
    
    # specific MAC address expression
    macAddressExpr = Combine(Word(hexnums,exact=2)+('-'+Word(hexnums,exact=2))*5)
    
    # use searchString to scan through output from getmac
    macAddresses = macAddressExpr.searchString(getmacOutput)
    
    for mac in macAddresses:
        print mac[0]



And I got:



    00-E0-B8-B8-4E-38
    00-14-A5-D5-B1-12
    00-FF-8B-CA-95-FF
    00-11-67-31-18-83
    00-09-0F-FE-00-01



So you might need to handle the case where there are multiple MACs installed, and perhaps filter out those that are disabled or disconnected, for instance.  In any event, with searchString (or scanString if you prefer to work with a generator), you don't have to define a parser to handle *all* the text, just the bits that you want.
#### 2010-08-14 01:33:42 - ptmcg
Here's a more complete example, that captures the MAC status description too.  Also uses results names so that you can get at the different fields more cleanly than with list indexes.



    from pyparsing import Combine, Word, hexnums, LineStart, empty, restOfLine
    
    # specific MAC address expression
    macAddressExpr = Combine(Word(hexnums,exact=2)+('-'+Word(hexnums,exact=2))*5)
    macLine = LineStart() + macAddressExpr('mac') + empty + restOfLine('status')
    
    # use searchString to scan through output from getmac
    macs = macLine.searchString(getmacOutput)
    
    for macdata in macs:
        print macdata.mac, macdata.status
    
    connectedMacs = [m.mac for m in macs if m.status != 'Media disconnected']
    print connectedMacs


#### 2010-08-14 05:49:25 - ptmcg
You really got me thinking on this one!  I looked at getmac and see that there is a '/FO' switch to specify what format you want the output, and one of the options is CSV.  Here is a script that uses Python's csv module to find the connected MACs:





    # output of getmac /FO CSV
    getmacCSV = '''\
    'Physical Address','Transport Name'
    'Disabled','Disconnected'
    '00-E0-B8-B8-4E-38','Media disconnected'
    '00-14-A5-D5-B1-12','\Device\Tcpip_{EA506137-39D0-43FF-8A91-47FEE4C94944}'
    'Disabled','Disconnected'
    '00-FF-8B-CA-95-FF','Media disconnected'
    '00-11-67-31-18-83','Media disconnected'
    '00-09-0F-FE-00-01','Media disconnected'
    '''
    
    import csv
    rdr = csv.DictReader(getmacCSV.splitlines())
    for row in rdr:
        if not row['Transport Name'].endswith('isconnected'):
            print row['Physical Address']



---
## 2010-08-16 03:08:24 - avisenna - process nested #ifdef
hi,



i'm wondering if anybody could help me out.

I am trying to 'emulate' a C preprocessor and

process nested #ifdef #else #endif blocks



I tried nestedExpr with little success because it's a bottom-up  and I'm looking for (I think it should be) a top-down method to be able to skip blocks where #ifdef evaluation results False



It seems that SkipTo is much better for this task but as I'm still learning pyparsing have some difficulties to use it with a recursion



can anybody help?



here an example



    <span class="co2">#ifndef A</span>
    <span class="co2">#define C </span>
    <span class="co2">#ifdef E</span>
    <span class="co2">#define EE</span>
    <span class="co2">#endif</span>
    <span class="co2">#else</span>
    <span class="co2">#define D</span>
    <span class="kw4">int16</span> aaa<span class="sy0">;</span>
    <span class="co2">#endif </span>

Assuming that A is not defined and E is defines the output should then be



    #define C 
     #define EE



thanks in advance,

avisenna


---
## 2010-08-17 04:00:51 - cpuddle - Nested expressions?

---
## 2010-08-19 03:10:26 - milliams - Parsing simple c++ function calls
<h1 id="toc0">Background</h1>


I'm currently writing an extension for Sphinx called . Basically it scans some XML output from Doxygen (a C++ documentation tool) and allows the Sphinx user to link to the Doxygen HTML documentation of an symbol or function they want.



<h1 id="toc1">My use of pyparsing</h1>


For each symbol in the XML file, I add it to a mapping dictionary. However, for functions, C++ allows you to have multiple versions of the same function differing only by their argument list. To this end I parse the function signatures and normalise them so that the user can easily specify which version they want.



I use pyparsing to parse the argument list, extract the argument types and normalise them into a new string.



The 'problem' I'm facing is that for a large XML file, there can easily be tens of thousands of symbols needing parsing (OGRE3D for example has about 16,000). Now, for each symbol I need to make a call to parseString() and so I'm trying to make it as fast as possible.



Currently for OGRE3D it takes about 40 seconds to parse all the items which isn't too bad but I'd like to improve it if possible.



<h1 id="toc2">Optimisations tried so far</h1>


I've tried packratting but in my case it made everything about twice as slow. This is likely due to the fact that each string I parse is only about 10-50 characters long so caching doesn't help.



For all the common cases for which I can easily account (empty argument lists mostly), I'm bypassing pyparsing entirely which gives a small performance boost (and allows me to reduce my usage of pyparsing.Optional).



<h1 id="toc3">Current code</h1>


The current code is at  and the unittests (to give you an idea of the inputs and outputs) is at .



parsing.py basically just consists of the pyparsing setup and a normalise() function.



If there's anything obvious I'm doing wrong or places where I could improve the code, please let me know. Please not that I'm not trying to make a completely conformant c++ function parser, simply something that does the job I need. 



I'm also aware that I may not be able to make it any faster since perhaps I've reached the limit of what tweaking can do.



See  and  for the discussion I had previously on the sphinx-dev mailing list.



Cheers,

Matt

#### 2010-08-19 06:14:09 - ptmcg
Matt -



Thanks for posting these examples.  I'm on my way to work now, but I'll look at them when I get a few minutes.  (Overall, I congratulate you just for taking on the task of parsing *anything* in C++, by the way!)



-- Paul
#### 2010-08-19 06:51:03 - milliams
Well, when you see the shortcuts I'm taking and the intricacies of C++ I'm having to ignore you might realise I'm a mere mortal :)
#### 2010-08-24 03:51:16 - ptmcg
Have done some testing, only minimal performance improvements, I'm afraid.  See changes at .



-- Paul

---
## 2010-08-19 15:43:08 - classicgamedev - The proper way to track current line and column?
So what is the proper way to track the current row and column being parsed so that the ParseFatalExceptions will report the correct line and column values?



Is there an example out there somewhere of how to do that?


---
## 2010-09-03 10:19:54 - classicgamedev - pyparsing masking the built-in struct module
I noticed today that when I have the pyparsing module installed, the following code example from the python struct module page () fails:





    \>\>\> from struct import *
    \>\>\> pack('hhl', 1, 2, 3)
    '\x00\x01\x00\x02\x00\x00\x00\x03'



The reason appears to be that the pyparsing module also contains a module names struct and it is being loaded before the main python libraries.



I'm not sure the proper way to fix this.  It seems like if I rearrange the search path order that pyparsing may be broken because it might import the python struct module instead of its own struct module.



Thoughts?

#### 2010-09-05 18:59:53 - ptmcg
No problems here:



    \>\>\> from pyparsing import *
    \>\>\> from struct import *
    \>\>\> pack('hhl', 1, 2, 3)
    '\x01\x00\x02\x00\x03\x00\x00\x00'



See if you have a local struct.py file.
#### 2010-09-07 08:51:37 - classicgamedev
That's exactly what was going on.  Sorry for the confusion.  I'm writing a compiler and sure enough, the rules for parsing struct definitions was in a module file called 'struct.py' :-)

---
## 2010-09-06 16:52:48 - foobar4 - match everyline except something
hi,



i'm going crazy here trying to figure this out, and i expect it is real easy, i'm just missing something.



i would like to match every line that has a = in it, or starts with whitespace followed by 'for' and do something with it, and every other line do something else.  i can't figure it out.  first i define the lines i am interested in:





    assignment = Word(printables) + '=' + restOfLine
    loop = 'for' + restOfLine
    cmd = assignment | loop



put an action on it



    cmd.setParseAction(lambda s, l, t: 'CMD: ' + ''.join(t))



now this works fine



    print cmd.transformString(testString)



however i need to do something different with all the lines that aren't cmd.  i thought i could match any line without a = in it like this



    other_line = Regex('^[^=]*$')

but that doesn't match any line!  even if it worked, i don't know to make other_line match anything but cmd.



thanks for any help



matt

#### 2010-09-06 20:54:30 - ptmcg
I think you just need this change:



    assignment = Word(printables) + '=' + restOfLine
    loop = 'for' + restOfLine
    other_line = restOfLine



Using your parse action example, I created this parser:



    assignment.setParseAction(lambda s, l, t: 'ASSIGN: ' + ''.join(t))
    loop.setParseAction(lambda s, l, t: 'LOOP: ' + ''.join(t))
    other_line.setParseAction(lambda s, l, t: 'OTHER: ' + ''.join(t))
    cmd = ( assignment | loop | other_line ) + LineEnd()



This reads the assignments, loops, and other lines in my test text.
#### 2010-09-08 14:55:40 - foobar4
thank you,  that worked just fine!

---
## 2010-09-08 13:56:28 - avisenna - parsing recursive structureswith optional elements
I'm wondering if anybody could give me a hint to solve my problem. I am trying to parse a recursive structure with optional elements

here is the code



    from pyparsing import *
    import pprint
    
    DoxyGenBlock = Keyword('/**') 
    
    DefGroupStart = Keyword('/** @defgroup').suppress()+ SkipTo(LineEnd())('defGrpDecl') + LineEnd().suppress()
    DefGroupHeader = Keyword('*  @{') + Keyword('*/') 
    DefGroupEnd = Keyword('/** @} */')
    
    defGroup = Forward()
    defGroupBody = Forward()
    
    MsgIdDoxyLine = SkipTo(Keyword('/** Message ID for'), include = True).suppress()  + SkipTo(Literal('*/').suppress(), include = True)('msgIDLine')
    MsgPropDoxyLine = SkipTo(Keyword('/** Message properties for'), include = True).suppress()  + SkipTo(Literal('*/').suppress(), include = True)('msgPropLine')
    
    
    
    DEFINE = Keyword('#define').suppress() 
    MsgId = DEFINE + Word(alphanums+'_')('defLabel') + SkipTo(LineEnd().suppress(), include = True)('ID')
    MsgProp = Keyword('#define').suppress()+  Word(alphanums+'_').suppress() + Literal('(').suppress() + delimitedList(Word(alphanums+'_'), delim='|')('msgPropValues') + Literal(')').suppress()  
    
    
    def msgIDPropAction(s,l,t):
        if t.msgIDLine:
            hlpstr = str.strip(t.msgIDLine)
        elif t.msgPropLine:
            hlpstr = str.strip(t.msgPropLine)
    
        if(hlpstr[len(hlpstr)-1] == '.'):
            hlpstr = hlpstr[:-1]
            hlpstr = str.strip(hlpstr)
        else:    
            hlpstr = SkipTo(Literal('.').suppress(), include = True).searchString(hlpstr)
        t.msgIDLine = hlpstr
    
        return  t
    
    MsgIdDoxyLine.setParseAction(msgIDPropAction)
    MsgPropDoxyLine.setParseAction(msgIDPropAction)
    
    msgIdBlock = MsgIdDoxyLine + MsgId
    
    msgPropBlock =   MsgPropDoxyLine + MsgProp
    
    
    msgBlock = msgIdBlock + Optional(msgPropBlock)
    
    defGroupBody \<\<  (msgBlock + ZeroOrMore(defGroup))
    
    
    defGroup \<\< DefGroupStart + DefGroupHeader.suppress() + defGroupBody + DefGroupEnd.suppress()
    
    
    tst1 = '''
    /** @defgroup _MSG_CMD_HS_FE_CAPABILITIES_GET_1 CMD_HS_FE_Capabilities_Get1
     *  @{
     */
    /** Message ID for CmdmdmFsmOptionsSet1 . */
    #define HMH_CMD_mdm_FSM_OPTIONS_SET1 0x1104u
    
    /** Message properties for CmdmdmFsmOptionsSet1 . */
    #define HMH_CMD_mdm_FSM_OPTIONS_SET_EXTEND1 (HMH_CMD_mdm_FSM_OPTIONS_SET1 | PROP_VDSL1 | PROP_LSDA1 | PROP_V3_X1 | PROP_IDLE1)
    
    
    /** @defgroup _MSG_CMD_HS_FE_CAPABILITIES_GET_2 CMD_HS_FE_Capabilities_Get2
     *  @{
     */
     /** Message ID for CmdmdmFsmOptionsSet2 . */
     #define HMH_CMD_mdm_FSM_OPTIONS_SET2 0x2204u
    
     /** Message properties for CmdmdmFsmOptionsSet2 . */
    #define HMH_CMD_mdm_FSM_OPTIONS_SET_EXTEND2 (HMH_CMD_mdm_FSM_OPTIONS_SET2 | PROP_LSDV2 | PROP_LSDA2 | PROP_V3_X2 | PROP_IDLE2)
    
    
     /** @defgroup _MSG_CMD_HS_FE_CAPABILITIES_GET_3 CMD_HS_FE_Capabilities_Get3
     *  @{
     */
    
     /** Message ID for CmdmdmFsmOptionsSet3 . */
     #define HMH_CMD_mdm_FSM_OPTIONS_SET3 0x3304u
    
     /** Message properties for CmdmdmFsmOptionsSet3 . */
    #define HMH_CMD_mdm_FSM_OPTIONS_SET_EXTEND3 (HMH_CMD_mdm_FSM_OPTIONS_SET3 | PROP_LSDV3 | PROP_LSDA3 | PROP_V3_X3 | PROP_IDLE3)
    
    
    
    /** @} */ 
    
    /** @} */
    
    
    
    /** @} */
    '''
    
    res = defGroup.parseString(tst1)
    pprint.pprint( res.asList() )
    
    
    tst2 = '''
    /** @defgroup _MSG_CMD_HS_FE_CAPABILITIES_GET_1 CMD_HS_FE_Capabilities_Get1
     *  @{
     */
    /** Message ID for CmdmdmFsmOptionsSet1 . */
    #define HMH_CMD_mdm_FSM_OPTIONS_SET1 0x1104u
    
    /** Message properties for CmdmdmFsmOptionsSet1 . */
    #define HMH_CMD_mdm_FSM_OPTIONS_SET_EXTEND1 (HMH_CMD_mdm_FSM_OPTIONS_SET1 | PROP_VDSL1 | PROP_LSDA1 | PROP_V3_X1 | PROP_IDLE1)
    
    
    /** @defgroup _MSG_CMD_HS_FE_CAPABILITIES_GET_2 CMD_HS_FE_Capabilities_Get2
     *  @{
     */
     /** Message ID for CmdmdmFsmOptionsSet2 . */
     #define HMH_CMD_mdm_FSM_OPTIONS_SET2 0x2204u
    
    
    
     /** @defgroup _MSG_CMD_HS_FE_CAPABILITIES_GET_3 CMD_HS_FE_Capabilities_Get3
     *  @{
     */
    
     /** Message ID for CmdmdmFsmOptionsSet3 . */
     #define HMH_CMD_mdm_FSM_OPTIONS_SET3 0x3304u
    
     /** Message properties for CmdmdmFsmOptionsSet3 . */
    #define HMH_CMD_mdm_FSM_OPTIONS_SET_EXTEND3 (HMH_CMD_mdm_FSM_OPTIONS_SET3 | PROP_LSDV3 | PROP_LSDA3 | PROP_V3_X3 | PROP_IDLE3)
    
    
    
    /** @} */ 
    
    /** @} */
    
    
    
    /** @} */
    '''
    
    res = defGroup.parseString(tst2)
    pprint.pprint( res.asList() )
    



with test1 we get the following correct result:

--

['_MSG_CMD_HS_FE_CAPABILITIES_GET_1 CMD_HS_FE_Capabilities_Get1',

 'CmdmdmFsmOptionsSet1 . ',

 'HMH_CMD_mdm_FSM_OPTIONS_SET1',

 '0x1104u',

 'CmdmdmFsmOptionsSet1 . ',

 'HMH_CMD_mdm_FSM_OPTIONS_SET1',

 'PROP_VDSL1',

 'PROP_LSDA1',

 'PROP_V3_X1',

 'PROP_IDLE1',

 '_MSG_CMD_HS_FE_CAPABILITIES_GET_2 CMD_HS_FE_Capabilities_Get2',

 'CmdmdmFsmOptionsSet2 . ',

 'HMH_CMD_mdm_FSM_OPTIONS_SET2',

 '0x2204u',

 'CmdmdmFsmOptionsSet2 . ',

 'HMH_CMD_mdm_FSM_OPTIONS_SET2',

 'PROP_LSDV2',

 'PROP_LSDA2',

 'PROP_V3_X2',

 'PROP_IDLE2',

 '_MSG_CMD_HS_FE_CAPABILITIES_GET_3 CMD_HS_FE_Capabilities_Get3',

 'CmdmdmFsmOptionsSet3 . ',

 'HMH_CMD_mdm_FSM_OPTIONS_SET3',

 '0x3304u',

 'CmdmdmFsmOptionsSet3 . ',

 'HMH_CMD_mdm_FSM_OPTIONS_SET3',

 'PROP_LSDV3',

 'PROP_LSDA3',

 'PROP_V3_X3',

 'PROP_IDLE3']

--



in test2 one item is missing which shouldn't lead to an error because the element msgPropBlock is optional but I get the following wrong result:

--

['_MSG_CMD_HS_FE_CAPABILITIES_GET_1 CMD_HS_FE_Capabilities_Get1',

 'CmdmdmFsmOptionsSet1 . ',

 'HMH_CMD_mdm_FSM_OPTIONS_SET1',

 '0x1104u',

 'CmdmdmFsmOptionsSet1 . ',

 'HMH_CMD_mdm_FSM_OPTIONS_SET1',

 'PROP_VDSL1',

 'PROP_LSDA1',

 'PROP_V3_X1',

 'PROP_IDLE1',

 '_MSG_CMD_HS_FE_CAPABILITIES_GET_2 CMD_HS_FE_Capabilities_Get2',

 'CmdmdmFsmOptionsSet2 . ',

 'HMH_CMD_mdm_FSM_OPTIONS_SET2',

 '0x2204u',

 'CmdmdmFsmOptionsSet3 . ',

 'HMH_CMD_mdm_FSM_OPTIONS_SET3',

 'PROP_LSDV3',

 'PROP_LSDA3',

 'PROP_V3_X3',

 'PROP_IDLE3']

--



Any hints how I can solve the problem?



thanks in advance,

Avisenna

#### 2010-09-08 15:42:00 - avisenna
sorry. there was an error in the sample text. now it works fine
#### 2010-09-08 16:22:39 - ptmcg
Glad you were able to resolve this on your own. Debugging these parsers isn't always the easiest thing.



-- Paul

---
## 2010-09-24 19:51:53 - Josh_English - Maybe I don't get groups
I have a mini language I want to use for a project to layout widgets in a GUI. I think pyparsing may be able to handle this, but I am not so sure, as I can't get this code to work.



A basic control string looks like this:



  name('label', Text)



I have this code:





    import pyparsing as P
    
    control_name = P.Word(string.lowercase).setResultsName('name')
    
    control_label = P.Suppress(''') +  \
        P.originalTextFor(P.OneOrMore(P.Word(P.alphas))).setResultsName('label') + \
        P.Suppress(''')
    
    control_type = P.oneOf('Text Read DatePicker Choice').setResultsName('ctrl')
    
    
    control = control_name + P.Optional( \
        P.Suppress('(') + \
        P.delimitedList(control_label | control_type ) + \
        P.Suppress(')') )
    
    control = control.setResultsName('control')
    
    
    for s in ['id', 'id('ID')', 'id('ID', Read)', 'id(Choice)', \
        'storycode('Story Code', Choice)', 'marketcode('Market')' ]:
        r = control.parseString(s)



Since some of the terms are optional, I'm testing against some standard variations.



Now I want to be able to have a single line of text with more than one control string but return multiple parsingresults objects, or a list of them.



So I tried:





    control_row = P.LineStart() + P.Group( P.OneOrMore(control) ) + P.LineEnd()
    
    lines = '''storycode('Story Code', Choice) marketcode('Market')
    '''
    for line in lines.splitlines():
        r = control_row.parseString(line)
        print '--'
        print line
        print r.asXML()
        print r.asDict()
        print r.dump()



What I get is:



--

storycode('Story Code', Choice) marketcode('Market')



\<ITEM\>

  \<control\>

    \<name\>storycode\</name\>

    \<label\>Story Code\</label\>

    \<ctrl\>Choice\</ctrl\>

    \<name\>marketcode\</name\>

    \<label\>Market\</label\>

  \</control\>

\</ITEM\>

{}





The individual controls aren't separated, but I thought that's what Group did.



Any suggestions?



Josh

#### 2010-09-24 22:34:55 - ptmcg
You are really on the right track.  The default behavior for pyparsing is to just string all the matched tokens into a single list. Group will create sublists, giving your results structure.



Your definition of control_row is:



    control_row = P.LineStart() + P.Group( P.OneOrMore(control) ) + P.LineEnd()



You want to Group each separate control within the OneOrMore:



    control_row = P.LineStart() + P.OneOrMore( P.Group(control) ) + P.LineEnd()



Now each line will give you a list of sublists, one for each control.  So to get at their data, you need to iterate over the results:





    for line in lines.splitlines():
        rr = control_row.parseString(line)
        for r in rr:
            print '--'
            print line
            print r.asXML()
            print r.asDict()
            print r.dump()    



Otherwise, it looks like you are doing very well on your own - welcome to pyparsing!



-- Paul
#### 2010-09-27 16:56:36 - Josh_English
Thank you, that actually did what I wanted.



There's a lot here to learn, and a small speed bump can make me want to scream and throw it all away.
#### 2010-09-27 17:45:32 - Josh_English
Okay, more confusion. Later on in this project I have





    comment = (P.Literal('#').suppress() + P.restOfLine).setResultsName('comment')
    
    command = P.Literal(':').suppress() + P.oneOf('box hbox').setResultsName('layout') + \
        P.Optional(P.Combine(P.quotedString), default='--').setResultsName('label') + \
        P.Optional(P.oneOf('start end'), default='start').setResultsName('se')
    
    command = command.setResultsName('command')
    
    text = '''# start layout
    :box start
    id(Read) title
    storycode('Story Code', Choice) marketcode('Market')
    :box end
    '''
    
    
    parser = P.OneOrMore(command | comment | P.Group(control_row) )('layout')
    
    print parser.parseString(text).asXML()



The resultant XML is:



    \<layout\>
      \<layout\> start layout\</layout\>
      \<command\>box\</command\>
      \<label\>--\</label\>
      \<se\>start\</se\>
    \</layout\>



The 'layout' tag name is being applied to the top level and the first child.



I had the same problem trying to parse two control rows together, I got:



    \<row\>
      \<row\> ... \</row\>
      \<control\> ... \</control\>
      \<control\> ... \</control\>
    \</row\>



What I wanted was a structure more like:



    \<layout\>
    \<comment\> ... \</comment\>
    \<command\> ... \</command\>
    \<row\> ... \</row\>
    \<row\> ... \</row\>
    \<command\> ... \</command\>
    \</layout\>



Even better would be the box command to have the rows as children:





    \<layout\>
    \<box\>
      \<row\>
        \<control\> ... \</control\>
        \<control\> ... \</control\>
      \</row\>
      \<row\>
        ...
      \</row\>
    \</box\>
    \</layout\>



Sorry for the long post and apparent bull-headedness. I'm not groking this pyparsing thing yet.
#### 2010-09-27 19:04:37 - ptmcg
There is clearly some structure in the text you are parsing. That's great, you just need to represent a corresponding structure in your parser.



Your definition of parser is:





    parser = P.OneOrMore(command | comment | P.Group(control_row) )



There is no real structure here.  There is no relationship between the comment, the command or any of the control rows.  If you want to get some kind of command-level structure, then the expression for a command should be the main element, and your parser should be simply OneOrMore(command).  Here is a more structured parser:





    cmd = P.oneOf('box hbox')
    COLON = P.Suppress(':')
    EOL = P.LineEnd()
    cmdStart = COLON + cmd + 'start' + EOL
    cmdEnd = COLON + P.matchPreviousLiteral(cmd) + 'end' + EOL
    
    command = P.Optional(comment)('comment') + P.Group(cmdStart +  
        P.OneOrMore(control_row)('controls') + cmdEnd)('command')
    
    parser = P.OneOrMore(command)





Do you see how the control_row's are defined as a field with the command?  Now when you dump the parsed data, you can iterate over the individual controls captured in the command.controls attribute:





    data = parser.parseString(text)
    print data.dump()
    for ctrl in data.command.controls:
        if isinstance(ctrl, P.ParseResults):
            print ctrl.dump()
        else:
            if ctrl.strip():
                print ''%s'' % ctrl



Prints:





    [' start layout', ['box', 'start', '\n', ['id', 'Read'], ['title'], ['storycode', ...
    - command: ['box', 'start', '\n', ['id', 'Read'], ['title'], ['storycode', ...
      - controls: [['id', 'Read'], ['title'], ['storycode', 'Story Code', 'Choice'], ['marketcode', 'Market'], '\n']
    - comment: [' start layout']
      - comment: [' start layout']
    ['id', 'Read']
    - control: ['id', 'Read']
      - ctrl: Read
      - name: id
    - ctrl: Read
    - name: id
    ['title']
    - control: ['title']
      - name: title
    - name: title
    ['storycode', 'Story Code', 'Choice']
    - control: ['storycode', 'Story Code', 'Choice']
      - ctrl: Choice
      - label: Story Code
      - name: storycode
    - ctrl: Choice
    - label: Story Code
    - name: storycode
    ['marketcode', 'Market']
    - control: ['marketcode', 'Market']
      - label: Market
      - name: marketcode
    - label: Market
    - name: marketcode



Are you really enamored with asXML()?  Or you just trying to dump out the parsed data and results names?  If that is the case, then just use dump().  asXML() is a kind of finicky method, and really requires a lot of structure over the parsed data and the corresponding results names. At this point in your parser development, I'd recommend against it. The advantage of dump() is that it shows your structure, and any results names that you can use to access the nested fields.



Hang in there! :)
#### 2010-10-03 16:48:34 - Josh_English
I understand the tree structure of XML better than the dump() results. I think I'm hitting another problem with my basic tactic of trying to build smaller parsers and include them in larger parsers as I go, instead of starting from the top and going into the specifics. 



I'm running into more trouble with this, but since I don't know my goals, I have to determine what those are first.

---
## 2010-10-05 13:01:45 - panneer - Struck in Infinite Loop while parsing
I'm new to PyParsing and I'm trying to parse the following block from a configuration file to get a dict structure out of it.



    {GeneralConfig1
    Type=1
    To_Load=1
    Is_file=1
    file=
    fill_type=0
    Dir=
    }
    
    
    {GeneralConfig2
    Type=2
    To_Load=1
    Is_file=1
    file=
    fill_type=0
    Dir=
    }
    
    {DetailedConfig
    
    {filetype1
    Name=JustAFile
    Type=1
    SubType=Text
    Path=C:\Temp\JustAFile.txt
    }
    
    {filetype2
    Name=JustAnotherFile
    Type=2
    SubType=Text
    Path=C:\Temp\JustAnotherFile.txt
    }
    }



Here is my code, which is struck in an infinte loop.





    fh_inFile = open('E:\\pyparsing_input.txt')
            self.pyparsingInput = ''.join(fh_inFile.readlines())
            String = CharsNotIn('{}')
            SectionBegin = Literal('{').suppress()
            SectionEnd = Literal('}').suppress()
            SectionName = Word(alphanums)
            ParamName = Word(alphanums,'_')
            ParamValue = restOfLine
            Parameter = ZeroOrMore(ParamName+'='+ParamValue)
            SectionHeader = SectionBegin + SectionName
            SectionGroup = Forward()
    
            SectionGroup \<\< Group(SectionHeader + OneOrMore(Parameter|SectionGroup)+SectionEnd) | String
    
            toplevel = OneOrMore(SectionGroup)
            tokens = toplevel.parseString(self.pyparsingInput)
            pp = pprint.PrettyPrinter(2)
            pp.pprint(tokens.asList())
    



Can you please help?

#### 2010-10-06 00:42:10 - ptmcg
Just a couple of misconceptions about some pyparsing classes and behavior.  But in the interests of 'teaching you to fish', let me show you how I debugged your parser.



The first thing I did was to extract your posted code, and just ran it, and I got this warning:



    paneer1.py:57: SyntaxWarning: Cannot combine element of type \<type 'NoneType'\> with ParserElement
      SectionGroup \<\< Group(SectionHeader + OneOrMore(Parameter|SectionGroup)+SectionEnd) | String





I added this warning a few releases ago to help highlight potential bugs in users' code.  Because '\<\<' has a higher precedence than '|', Python performs the '\<\<' operation first, then tries to add the '|' part afterward.  For this to work, you must wrap the right-hand side of the '\<\<' operator in ()'s:



    SectionGroup \<\< (Group(SectionHeader + OneOrMore(Parameter|SectionGroup)+SectionEnd) | String)



Then I ran your code, and sure enough, it just sits there looping forever.  So to get some insight into the parser, I started to add some names to some of your elements, and turn on debugging for them:



    SectionHeader.setName('SectionHeader')
    SectionHeader.setDebug()



This gave me this output:



    Match SectionHeader at loc 0(1,1)
    Matched SectionHeader -\> ['GeneralConfig1']





and then nothing else.  This is the default output for setDebug.  It reports whenever the expression is about to be matched (including the location), and then if successful, what the matched tokens look like, or if unsuccessful, what the parse exception was.



So I instrumented Parameter as well, and this told the story:



    Match SectionHeader at loc 0(1,1)
    Matched SectionHeader -\> ['GeneralConfig1']
    Match Parameter at loc 15(1,1)
    Matched Parameter -\> []
    Match Parameter at loc 16(2,1)
    Matched Parameter -\> []
    Match Parameter at loc 16(2,1)
    Matched Parameter -\> []
    Match Parameter at loc 16(2,1)
    Matched Parameter -\> []
    (... continue looping forever ...)



Parameter appears to be matching on an empty string, but not advancing any further through the input (as shown by the parsing location staying at 16, or line 2, column 1).  Looking a little deeper at Parameter, it is defined with these statements:



    ParamName = Word(alphanums,'_')
    ParamValue = restOfLine
    Parameter = ZeroOrMore(ParamName+'='+ParamValue)



And the culprit of the infinite loop is the ZeroOrMore construct.  This is really unnecessary, as you already define the SectionGroup later on to consist of OneOrMore Parameters.  Having seen similar parsers before, I was fairly sure that you will eventually want to group your parameters and values for easier handling later, so I changed Parameter to:



    Parameter = Group(ParamName+'='+ParamValue)



Group is very useful for providing structure to the parsed tokens; otherwise, your SectionGroup would just contain all the tokens in one list, and you'd have to walk this list to pick out the actual parameter names and values. In addition, I would suppress the '=' tokens from the parsed results - they are useful during the parsing process as helpful delimiting punctuation, but they are just in the way when reading the results.  So we make one last change to Parameter to suppress the '=' from the parsed output:



    Parameter = Group(ParamName + Suppress('=') + ParamValue)



Now we rerun the parser, and we get a little further:



    Match SectionHeader at loc 1(2,1)
    Matched SectionHeader -\> ['GeneralConfig1']
    Match Parameter at loc 16(2,1)
    Exception raised:Expected W:(abcd...,_) (at char 17), (line:3, col:1)
    Match SectionHeader at loc 17(3,1)
    Exception raised:Expected '{' (at char 17), (line:3, col:1)
    Match Parameter at loc 67(9,1)
    Exception raised:Expected W:(abcd...,_) (at char 67), (line:9, col:1)
    (... long traceback follows ...)



In trying to parse 'Type=1' as a Parameter, the parser failed to match 'Type' as a ParamName.  ParamName is defined as:



    ParamName = Word(alphanums,'_')



Word's 2-argument constructor uses the first string as the set of allowed initial characters, and the second string as the set of allowed body characters.  As you have defined it, you only allow '_' characters in the body of ParamName, so ParamName would match 'A', 'f', '7', 'B_', '8___', or even 'q_________', but not 'Type'.  Most likely you meant for the body characters to allow for any alphanum OR '_', not just '_'s.  So ParamName becomes:



    ParamName = Word(alphanums, alphanums+'_')



With these changes, you now get your full test text to parse successfully, pretty-printing as:



    [ [ 'GeneralConfig1',
        ['Type', '1'],
        ['To_Load', '1'],
        ['Is_file', '1'],
        ['file', ''],
        ['fill_type', '0'],
        ['Dir', '']],
      [ 'GeneralConfig2',
        ['Type', '2'],
        ['To_Load', '1'],
        ['Is_file', '1'],
        ['file', ''],
        ['fill_type', '0'],
        ['Dir', '']],
      [ 'DetailedConfig',
        [ 'filetype1',
          ['Name', 'JustAFile'],
          ['Type', '1'],
          ['SubType', 'Text'],
          ['Path', 'C:\\Temp\\JustAFile.txt']],
        [ 'filetype2',
          ['Name', 'JustAnotherFile'],
          ['Type', '2'],
          ['SubType', 'Text'],
          ['Path', 'C:\\Temp\\JustAnotherFile.txt']]]]







Some added tips:



An advanced pyparsing technique is to use the Dict class to provide some automatic key-value definitions from your tokens.  Since this is a config file format, key-value access seems like a pretty natural fit.  By changing SectionGroup from



    SectionGroup \<\< (Group(SectionHeader + 
                       (OneOrMore(Parameter|SectionGroup))+SectionEnd) | String)



to:



    SectionGroup \<\< Dict(Group(SectionHeader +
                       Dict(OneOrMore(Parameter|SectionGroup))+SectionEnd) | String)



pyparsing does not change any of its parsing behavior.  But it does an added post-parsing step - for each group matched within the Dict, a key-value pair will be defined using the first token in the group as the key, and all remaining tokens as the value.  Since SectionGroup is a Forward with a recursive definition, then we get back a nicely structured hierarchy.  Your pprint output is the same, but if you print out tokens.dump(), you'll see:



    [['GeneralConfig1', ['Type', '1'], ['To_Load', '1'], ['Is_file', '1'], ...
    - DetailedConfig: [['filetype1', ['Name', 'JustAFile'], ['Type', '1'], ...
      - filetype1: [['Name', 'JustAFile'], ['Type', '1'], ['SubType', 'Text'], ...
        - Name: JustAFile
        - Path: C:\Temp\JustAFile.txt
        - SubType: Text
        - Type: 1
      - filetype2: [['Name', 'JustAnotherFile'], ['Type', '2'], ['SubType', ...
        - Name: JustAnotherFile
        - Path: C:\Temp\JustAnotherFile.txt
        - SubType: Text
        - Type: 2
    - GeneralConfig1: [['Type', '1'], ['To_Load', '1'], ['Is_file', '1'], ...
      - Dir: 
      - Is_file: 1
      - To_Load: 1
      - Type: 1
      - file: 
      - fill_type: 0
    - GeneralConfig2: [['Type', '2'], ['To_Load', '1'], ['Is_file', '1'], ...
      - Dir: 
      - Is_file: 1
      - To_Load: 1
      - Type: 2
      - file: 
      - fill_type: 0



The indented names show you the keys that were extracted from the input text, and you can use ordinary Python dict-like API to retrieve individual items, or iterate over the keys() or values() or items() lists.



    print tokens['DetailedConfig']['filetype1']['Path']



When I first wrote pyparsing, I was still pretty new to Python, and was not sure whether Python programmers would prefer to use dict or object attribute style in accessing these values, so pyparsing also supports accessing the tokens as if they were attributes of a Python object:



    print tokens.DetailedConfig.filetype1.Path



I find this form to be easier to read, but it requires that you know in advance what the keys will be, and that they be valid attribute names and not collide with any Python keyword.  If you happened to have a key with a name like 'def' for instance, then you would have to shift to dict style access:



    print tokens.DetailedConfig['def'].Path



Lastly, I encourage you to follow some of the PEP8 guidelines regarding upper/lower case in names.  I find it to be very helpful to use leading capitals only for classes, and use leading lowercase for variables.  Your code would change to something like this:



    string = CharsNotIn('{}')
    sectionBegin = Literal('{').suppress()
    sectionEnd = Literal('}').suppress()
    sectionName = Word(alphanums)
    paramName = Word(alphanums, alphanums+'_')
    paramValue = restOfLine
    parameter = Group(paramName + Suppress('=') + paramValue)



This will help you in working with other Python developers, or when posting snippets on usenet or other public channels, as it is the more commonly accepted style.  But it is not strictly necessary to do this for your code to run.



Whew! After all that, you really were pretty close to having a working parser all on your own - just a few minor tweaks were all that were necessary.  I've just been meaning to write up some more elaborate parser debugging notes.  Good luck, and welcome to pyparsing!



-- Paul
#### 2010-10-07 12:40:06 - panneer
Hi Paul,



Thank you very much for your efforts to put up a step-by-step explanation to resolve my query. 



I've learned my mistakes now. You've also shown me on how to debug the pyparsing code. That'll be helpful to construct another parser in pyparsing.



Thanks for your tips about PEP8 guidelines. I'm fairly new to Python as well. That was really helpful.



pyparsing Rocks!!!

---
## 2010-10-10 05:24:18 - hadim - Parsing piece of latex file
Hello,

im trying to parse a latex file with pyparsing but i ahve a problem because i only want to parse a piece of the latex code, so i wrote my grammar but obviously pyparsing crash when it start to parse latex file because some of the code doesnt match with my grammar.... How can i say to pyparsing to ignore code wich doesnt match with my gramar??



This is my python script and a piece of my latex file : 



Thank you all for youre help

#### 2010-10-10 12:08:13 - ptmcg
Wow, latex can be a very complicated source to parse, this is very impressive!



If you are just trying to find individual snippets from within a larger source, try using scanString or searchString, instead of parseString.  These other methods were designed especially for the case you describe.



-- Paul

---
## 2010-10-16 16:18:09 - edc - re: cppStyleComment problem
Sorry -- messed up the code tag in the previous one.



Has anyone run into the problem when parsing c++ style comments at the end of a file?



For example,





    // comments
    // comments
    object1{
        attr1a value1a;
        attr1b value1b;
        list1(listEntry1 listEntry1)
    }



works, but





    object1{
    attr1a value1a;
    attr1b value1b;
    list1(listEntry1 listEntry1)
    }
    comments
    comments



does not work.



the grammar are as follows:



    fB = open(filename,'r')
            expressions = fB.read();
    
            EQ,LBRACE,RBRACE,SEMI = map(Suppress,'={};')
            LPAREN,RPAREN = map(Suppress,'()')
    
            intV = Combine(Optional('-') + Word(nums)) \
                       .setParseAction(lambda t : int(t[0]))
            realV = Combine(Optional('-') + Optional(Word(nums)) \
                                + '.' + (Word(nums))) \
                       .setParseAction(lambda t : float(t[0]))
            realV2 = Combine(Optional('-') + (Word(nums)) \
                                + '.' + Optional(Word(nums))) \
                       .setParseAction(lambda t : float(t[0]))
            numV = realV | realV2 | intV
            sciV = Combine(  numV \
                               + (CaselessLiteral('E') \
                                  + Word('+-'+nums,nums))) \
                     .setParseAction(lambda t : float(t[0]))
            realOrSciV = sciV | realV | realV2
    
            # define tokens, expressions and entries
            ObjV = Forward()
            ObjLV = Forward()
    
            keyToken = Word(alphas+'_', alphanums+'_')
            entryToken = ( keyToken | realOrSciV | intV |
                           quotedString.copy().setParseAction(removeQuotes))
    
            numL = (OneOrMore(realOrSciV)).setParseAction(lambda t :
    list([t[:]]))
            intL = (OneOrMore(intV)).setParseAction(lambda t :
    list([t[:]]))
            keyTokenL = (OneOrMore(keyToken)).setParseAction(lambda t :
    list([t[:]]))
            entryL = numL | intL | keyTokenL
    
            ObjE = Group(keyToken + ObjV) \
                    | Group (keyToken + LBRACE + RBRACE)
            ObjLE = Group(keyToken + ObjLV)
            expr = Group(keyToken + entryToken + SEMI) \
                 | Group(keyToken + LPAREN + entryL + RPAREN + SEMI)
            MixedE = (ObjE | expr | ObjLE)
    
            ObjLV \<\< ( LPAREN + Dict(OneOrMore(ObjE)) + RPAREN)
            ObjV \<\< (  LBRACE + Dict(OneOrMore(MixedE)) + RBRACE)
    
            Dict_t = Dict(OneOrMore(MixedE)) \
                      .ignore(cStyleComment).ignore(cppStyleComment)\
                      .parseString(expressions,parseAll=True)
    
            fB.close()



Thanks very much in advance!!!

#### 2010-10-16 16:19:06 - edc
arrrrgh...I meant





    object1{
    attr1a value1a;
    attr1b value1b;
    list1(listEntry1 listEntry1)
    }
    //comments
    //comments



does not work...
#### 2010-10-17 12:24:38 - ptmcg
For one thing, cppStyleComment will also match cStyleComment, so there is no need to ignore both of them.  What is it about the comments at the end of the file that is not working?  Is it possible that there is no trailing newline after the last comment?
#### 2010-10-17 14:13:14 - edc
Thanks for the reply!



Unfortunately adding a trailing new line (blank or pure spaces) did not help.  The error for the 2nd file snippet above segment would be something like this one:





    pyparsing.ParseException: Expected end of text (at char 65), (line:6, col:1)


#### 2010-10-20 00:15:18 - ptmcg
This seems to be a bug in pyparsing, that I had already fixed once, but then disabled during some testing and never re-enabled.  As a temporary fix until I get a new release out, go to line 1091 and uncomment that line.  This should fix your problem.  Thanks for bringing this up!



Yours in embarrassment,

-- Paul
#### 2010-10-20 00:25:14 - ptmcg
I had a unit test that had trailing whitespace, but none with a trailing comment.  That has been fixed now, so this should not regress again in the future.  Again, thanks for reporting this!



-- Paul
#### 2013-02-20 05:41:18 - rxadmin
Running version 1.5.7 on Python 2.7.3. cppStyleComment stops when encountering a ';' in the comment.



E.g. the folloowing will not parse:



    ///
    /// this is some text; this is another comment text
    ///


#### 2013-02-20 06:28:34 - rxadmin
I managed to solve the issue by using the following expression.





    cppBlockComment = Regex(r'(/\*){1}(.|\n)*(\*/){1}').setName ('...')



Hope this helps.
#### 2013-02-24 06:41:05 - rxadmin
Found an issue where the expression parsed everything into a single comment. Updated the code in the previous post to:





    cppBlockComment = Regex(r'(/\*)[^(\*/)]*(\*/|\n)').setName('/* ... */ block comment')



---
## 2010-10-19 17:57:45 - gobnat - How do I get a 10x speed increase?
I'm parsing a text file, about 10,000 words/70k size.  This takes about 6 seconds (according to profiling) - which is about 5.5 seconds too long for the use case.  



I read about people parsing much longer files at a greater speed, so I'm wondering what I'm doing wrong.  Do the parseactions have a lot of overhead?



I have tried packrat, but it doesn't seem to have any effect (or maybe a slight decrease in speed, but within the margin for error). I have also tried feeding the tokenizer small chunks, but that doesn't seem to make any difference either.



My intended tokenisation is:

drop whitespace

punctuation gets a separate token

some numbering (1.1 etc) gets a separate token

what's left gets a token which records its text and location.  



This is the code:



    PUNCTS_ONEOF =  u': \' ! @ # $ % ^ & * / , _ - ? [ ] { } ; ' \u201c \u201d ( \u2018 \u2019 \ufffd ) .'
    
    def tokenise(textString,extraChars='',  offset=0):
        # offset is only used when sending chunks
        # in order to synchronise token location
        if textString is None:
            return None
    
        if extraChars != '':
            for c in EXCLUDE_WORD_PUNCTS:
                extraChars = extraChars.replace(c, '')
            normWord=Word(alphanums, alphanums+alphas8bit+extraChars+'-_')
        else:
            normWord=Word(alphanums, alphanums+alphas8bit+'-_')
    
        PO = PUNCTS_ONEOF
        punctSingles = oneOf(PO)
    
        lAlphasPN=Word(string.ascii_lowercase)
        uAlphasPN=Word(string.ascii_uppercase)
    
        numLvl0=uAlphasPN+'.'
        numLvl1=Combine(Word(nums)+'.')
        numLvl2=Combine(Word(nums)+'.'+Word(nums))  
        numLvl3a=Combine(Word(nums)+'.'+Word(nums)+'.'+Word(nums))
    
        lvl3numbering = numLvl3a.setParseAction( createExTokenClauseRef )
        numbering = (numLvl2 | numLvl1).setParseAction( createExTokenClauseRef ) 
    
        punctTokens = (punctSingles).setParseAction( createExTokenPunct )
        otherTokens = (normWord ).setParseAction( createExToken ) #.setParseAction( createExToken )
        bucket = Regex('.').setParseAction(createExTokenPunct) # catch anything else???
    
        entries = MatchFirst([lvl3numbering, numbering, punctTokens,  otherTokens, bucket])
    
        t=OneOrMore(Empty()+entries)+StringEnd()
        t.parseWithTabs()
    
        try: 
            spam = parseIt(t,textString)
        except ParseException, pe:
            logging.warning('Unable to parse text:!'+'\nMessage=\n'+pe.msg)
    
        return spam
    
    # these are place holders which will add some attributes to the extended token
    # once it is created
    def createExToken(st, locn, toks,):  
        return exToken(locn, toks[0])
    
    def createExTokenClauseRef(st, locn, toks,):
        return exToken(locn, toks[0])
    
    def createExTokenClausePunct(st, locn, toks,):
        return exToken(locn, toks[0])
    
    class exToken:
        def __init__(self, locn, tokString):
            self.raw=tokString
            self.lowercase=tokString.lower()
            self.start=locn
            self.end = locn+len(tokString)





Is it something I'm doing wrong with the code, or am I using pyparsing for the wrong thing?  These are things that I can code regexs for, so PyBison is my next port of call...



tia

#### 2010-10-19 20:16:13 - gobnat
For the record, I did some tests and the parseactions seem to add about .8 of a second.
#### 2010-10-19 23:06:45 - ptmcg
You didn't include any sample input, so I could not actually test these suggestions.  I don't think they will give you 10X improvement, but maybe 2X.



Try merging all of your variants on level numbering with just:



    numbering = Regex(r'\d+(\.|(\.\d+){1,2})').setParseAction( createExTokenClauseRef ) 



and change entries to:



    entries = MatchFirst([numbering, punctTokens,  otherTokens, bucket])



Why is Empty() part of t? pyparsing should implicitly skip over any whitespace, so I think the Empty() can be removed.  This is one less expression to process.



Please write back with some sample input, and I'll see if I can come up with any more bits that will help.  But on the face of things, this is really a pretty simple parser, so I don't see a lot of places for further improvement.



-- Paul
#### 2010-10-20 00:01:47 - gobnat
re: Empty():





I have upgraded to 1.5.5 so will try without Empty().



I have done some fiddling around with flex and it gets the lexing time down to 0.03 seconds (!!) However, it doesn't deal with utf8 gracefully \<sigh\>  Extended character sets are the bane of my life.



In terms of sample data - the stuff I've tested on isn't mine to disclose, but I could find a public doc and test with that. How do I post a 70k sample here?
#### 2010-10-20 00:11:10 - gobnat
Taking out Empty() gives a ~1.4 second speedup ( (Haven't re run unit tests though to see that the location is preserved)



Taking out Empty() and using your regex gives ~3 second speedup = about 2x as you predicted.
#### 2010-10-20 00:19:02 - ptmcg
You can post a file to the pastebin -  - and then post the link to your pasted doc here.
#### 2010-10-20 18:12:27 - gobnat
I've added a .tar.gz with the working tokeniser a short scaffolding file and a sample file.  Hmmm... no I haven't, since pastebin doesn't let me do that. 



Instead here are the pieces:

Scaffold= 

Tokeniser = 

Test Data = 



The sample is only half the size of my original test.  It comes in at a little under 1s (but I'd be looking to get it under 0.25s given the scaling).



Don't spend too much time on it - if you don't think this is doable I'll go down the flex/Bison/pyBison route. There is apparently a patch to allow it to handle utf8.  



Regards





Brendan
#### 2010-10-20 18:40:30 - ptmcg
It turns out that packrat is hurting more than it's helping.  If you remove your enabling of packrat parsing, you can cut your parse time in about 1/2 again.  You can squeak out a little bit further performance on Windows using psyco, but this is only about another 6-8%.



If it is of any help, all of your expressions that you end up with in this parser all resolve internally to regular expressions, so you can access the reString attribute to get these re's as input to your flex/Bison program.



Thanks for giving pyparsing a try!

-- Paul
#### 2010-10-20 20:49:49 - gobnat
<ul class="quotelist"><li>you can access the reString attribute to get these re's as input to your flex/Bison program.</li></ul>

Thanks for the tip, and your help.  

Maybe a 4x speedup will be good enough.  I'll see. 

If I had to learn flex when I first started prototyping it would have broken my will, so pyparsing has still helped.

---
## 2010-10-20 16:51:14 - jon.dobson - import error, Python 3.1.2
Hey,



I get the following import error with Python 3.1.2 and Pyparsing 1.5.5 (win installer):





Traceback (most recent call last):

  File 'C:\Jonathan\Python\PyCode\IF\Model.py', line 2, in \<module\>

    from pyparsing import *

  File 'C:\Python31\lib\site-packages\pyparsing.py', line 2538

    except ParseException, err:

                         ^

SyntaxError: invalid syntax



I realize this is due to the updated syntax for exceptions Python 3 - I can run through and change them all, but just thought I should let you know.

#### 2010-10-20 18:43:24 - ptmcg
I created separate win installers for each of the different Python versions, since I have two different source files, so please be sure you got the right one.



If you just want to change the except statements, this is the only one that needs to be changed; all of the others are of the form 'except ParseException:', which works for both Python 2 and 3.



-- Paul
#### 2010-10-21 03:10:11 - jon.dobson
I downloaded and installed 'pyparsing-1.5.5.win32-py3.1.exe', which I assume is the right one.



\<If you just want to change the except statements, this is the only one that needs to be changed; all of the others are of the form 'except ParseException:', which works for both Python 2 and 3\>



Right on - thanks.  I changed the exception statement this morning, and up came the 'no module named <u>builtin</u>' error as mentioned previously in July by GregWatson.  That was vesion 1.5.3, so I don't know if you had meant to fix this as of 1.5.5, but it's still there.



GregWatson's solution:



[So I just commented out the line 

'import builtin'

and then changed

singleArgBuiltins.append(getattr(builtin,fname))



to singleArgBuiltins.append(getattr(builtins,fname))



(i.e. builtin =\> builtins)



Now it works.] 



This didn't completely work for me.  I could not comment out the line 'import <u>builtin</u>', but had to change it to 'import builtins'.
#### 2010-10-21 06:30:47 - ptmcg
\<sigh\>

I hope to get this right by version 1.5.46 or so.  I have some other bugfixes and changes in the works, so I think I'll have another point release out in a few weeks, and will include this fix.  Thanks for writing, and welcome to pyparsing!



-- Paul
#### 2010-10-21 10:44:55 - jon.dobson
Okay - thanks for the welcome.  And thanks for pyparsing!
#### 2011-08-26 15:41:36 - ohaaga
I'm using 1.5.6 and python 3.1.2, and I made the two changes mentioned.  In particular, I changed 

except ParseException, err:

to

except ParseException(err):

The module now loads fine, but when I try to do a recursive match using Forward() I get 'NameError: global name 'err' is not defined'.  I suspect I'm not understanding either the new syntax or the intent of the code.  Is there a better fix?



thanks,

Owen
#### 2011-08-26 16:03:31 - BrenBarn
I think the Python 3 equivalent of that code would be 'except ParseException as err'.

---
## 2010-10-26 02:09:51 - erastune - pyparsing being "greedy" - I need help
Hi,

First let me say that I really enjoy pyparsing, though I am still very new with it, trying to make some progress

I have the following small example which is working OK:



    from pyparsing import *
    
    LP, RP, DASH = map(Suppress, '()-')
    word = Word(nums + alphas)
    name = OneOrMore(word)
    name.setParseAction(lambda tokens: ' '.join(tokens))
    seq = DASH + Word(nums,min=1,max=3)
    seq.setParseAction(lambda tokens: ''.join(tokens))
    expr = LP + name('name') + Optional(seq('seq')) + RP
    
    test='''\
    (James Bond 007-01)
    (Indiana Jones-01)
    (Matrix)
    '''.splitlines()
    
    for t in test:
        res=expr.parseString(t)
        print(res.asDict())

It gives me the expected result:



    {'name': 'James Bond 007', 'seq': '01'}
    {'name': 'Indiana Jones', 'seq': '01'}
    {'name': 'Matrix'}

My problem comes if I want to allow name to contain - (dash), like in:

(Twenty-four hours-07) 

If I modify word accordingly:



    word = Word(nums + alphas + '-')

name being greedy, I miss to catch the seq part:



    {'name': 'James Bond 007-01'}
    {'name': 'Indiana Jones-01'}
    {'name': 'Matrix'}
    {'name': 'Twenty-four hours-07'} 

I could solve my problem by using a Regex:



    expr = Regex(r'\((?P\<name\>.*?)-?(?P\< seq\>\d{0,3})?\)')

Result:



    {'name': 'James Bond 007', 'seq': '01'}
    {'name': 'Indiana Jones', 'seq': '01'}
    {'name': 'Matrix', 'seq': ''}
    {'name': 'Twenty-four hours', 'seq': '07'}

But I would prefer to avoid it because I want to take benefit of pyparsing advantages (clarity and self-documentation for example) and I dont want to jump back to Regex as soon as I have a problem to define properly what I need. 



 I feel there is a smarter way to solve this problem, without Regex, but I can find it and Im getting mad



Could somebody help me so that I could progress in the use of pyparsing?

Thanks in advance.

#### 2010-10-27 03:06:52 - ptmcg
This kind of question comes up fairly often, although it hasn't been posted here for several months.  This is a good chance to revisit it.



The basic issue here is the pyparsing is pretty strict about parsing left-to-right through the input text, following along through the grammar expressions you've put into your parser. The only lookahead that is done is any kind of lookahead that you explicitly define in your expressions.  Working with this often requires a thought process very similar to 'playing computer', or to paraphrase Chevy Chase in Caddyshack, '*Be* the parser!'



Here's a short example of where pyparsing's pure left-to-right action gives some problems.  Let's try to parse a simple inequality, like 'x \< 1':





    identifier = Word(alphas)
    number = Word(nums)
    LT,GT,EQ,LE,GE,NE = map(Literal, '\< \> = \<= \>= !='.split())
    comparer = LT | GT | EQ | LE | GE | NE
    comparisonExpr = identifier + comparer + number
    
    print comparisonExpr.parseString('x \< 1')



This works great!  We get ['x', '\<', '1'].  But now let's try 'x \<= 1'





    print comparisonExpr.parseString('x \<= 1')



FAIL! If we catch the exception and using markInputline(), we get:





    try:
        print comparisonExpr.parseString('x \<= 1')
    except ParseException, pe:
        print pe.markInputline('|')



We get:



    x \<|= 1



The vertical bar shows where the parser ran into trouble.  The leading '\<' of '\<=' got parsed as LT because LT precedes LE in the MatchFirst expression comparer.  Normally, we could address this by just reordering comparer, to test for LE before LT:





    comparer = LE | GE | LT | GT | EQ | NE



Or we could use the pyparsing builtin, oneOf, which will check for and do this reordering for us:





    comparer = oneOf('\< \> = \<= \>= !=')
    print comparer
    Re:('\\\<\\=|\\\<|\\\>\\=|\\\>|\\=|\\!\\=')



(Oh, gross! We get a regular expression full of escaping backslashes! Just for readability sake, let's try:





    comparer = oneOf('\< \> = \<= \>= !=')
    print str(comparer).replace('\\','').replace('|',' | ')
    Re:('\<= | \< | \>= | \> | = | !=')



oneOf looks for matching strings that might mask longer strings later in the list, and if found, moves the longer string ahead to be checked for first.  The result is a regular expression that explicitly checks for '\<=' before '\<'.



But let's create our own lookahead using pyparsing's expression classes.  We read 'x \<= 1', and if we slow down our scanning from left-to-right, we:

- see 'x', okay that's the identifier

- see '\<', but wait, it's followed by an '=', so it's not the '\<' operation, but '\<='

- see '1', okay, done!



So we want to accept '\<' as '\<' *only* if it is not followed by '='.  Going back to our original version:





    LT,GT,EQ,LE,GE,NE = map(Literal, '\< \> = \<= \>= !='.split())
    comparer = LT | GT | EQ | LE | GE | NE



if we change LT and GT to:





    LT = '\<' + ~FollowedBy('=')
    GT = '\>' + ~FollowedBy('=')
    comparer = LT | GT | EQ | LE | GE | NE
    comparisonExpr = identifier + comparer + number
    try:
        print comparisonExpr.parseString('x \<= 1')
    except ParseException, pe:
        print pe.markInputline('|')



Now we get ['x', '\<=', '1'] - voila!



Now to your problem, we need to see when is a dash a delimiter and when is it just part of the opening name.  Here are your test cases:





    (James Bond 007-01)
    (Indiana Jones-01)
    (Matrix)
    (Twenty-four hours-07) 



So when do we want to treat '-' as part of the title?  We'll need some form of lookahead in order to know, a simple one might be 'if it isn't followed by a digit'.  Thinking ahead, this might be a little too strict, and not accepting of titles like 'Robocop-3000' or something.  Backing up to your original parser, the simplest rule might be 'if it isn't the leading '-' of the terminating seq + ')''.  This is actually pretty easy to implement in pyparsing, and probably robust enough to avoid collisions in most normally spelled titles:





    NAME_DASH = ~(seq + RP) + DASH
    name = OneOrMore(word | NAME_DASH)



In other words, 'first see if we are about to parse a seq and closing ')' - if so, this is not a '-' that will work in a name.



(We had to move the definition of seq up before name so this would work, but no harm in changing that order around.)



And now we successfully parse your last test case.



There is still one glitch - because we are using the DASH expression in NAME_DASH, the actual '-' is getting suppressed.  If we change NAME_DASH to `~(seq + RP) + '-'`, this gets a little better, but our parse action that reassembles the name puts spaces around each separately parsed word or dash-that-is-not-a-leading-dash-of-a-seq, giving us the name as 'Twenty - four hours', not 'Twenty-four hours'.



An alternative to using ' '.join in a parse action (which was really not that bad an idea, parse actions are a good tool to learn), look at using `originalTextFor`.  This wrapper was more or less tailored just for cases like this, sort of a super-Combine, but without requiring things to be adjacent, and pulling from the original input string.





    name = originalTextFor(OneOrMore(word | NAME_DASH))



originalTextFor will detect the overall leading and trailing locations of the enclosed expression within the input string, and return that slice of the input string, exactly as it is.  Finally, we get our desired result:





    {'name': 'Twenty-four hours', 'seq': '07'}



Any special reason you are calling asDict()?  Pyparsing's parse results already give dict-like access, so you can print res['name'], or since 'name' is a Python-compatible identifier, you can also print res.name.  I like using dump(), as it helps me visualize the named results, and any nesting there might be if names are defined hierarchically:





    print res.dump()



gives:



    ['James Bond 007', '01']
    - name: James Bond 007
    - seq: 01
    ['Indiana Jones', '01']
    - name: Indiana Jones
    - seq: 01
    ['Matrix']
    - name: Matrix
    ['Twenty-four hours', '07']
    - name: Twenty-four hours
    - seq: 07



Well, this really was a lot of work just to handle dashes in names! In retrospect, you might think about just choosing a different delimiter for your fields, especially if are going to add more after the sequence number, things get tricky in a hurry.  This is why people try to pick delimiters that never or at least rarely occur in their data, to minimize this whole confusion issue.  But this gave us a chance to look at this parsing stuff from several angles - hope I didn't put you to sleep!



Cheers,

-- Paul
#### 2010-10-28 01:23:34 - erastune
Paul,

Many thanks for your long and detailed answer that did not put me to sleep, all on the contrary!

I knew there was an elegant way to solve this! 



My mistake from the beginning was to try to treat the - into the word definition 

Youre right, when playing with parsing, you need to think differently (*be* the parser!). And thats why I really appreciate your answer: you are not only giving the quick solution to a problem, but you spend time to explain how you came to the solution. I am sure that it will help me a lot with my future use of pyparsing, with other problems that I will surely meet.



Thanks to your explanations, Ive also learned about:

- oneOf(): I would never think before to try to print the result of onOf(). That is quite interesting.

- originalTextFor(): that sounds useful in many cases.

- dump(): this produce a nice way of presenting results.



Regarding asDict(), I find it useful to produce results quickly on one line, when you are building a new parser and that you have a few lines off example to test.



Finally, I fully support your view on using specific delimiters to eliminate ambiguity whenever possible. But we are not living in an ideal world, and we do not always choose where and how you get data that you want to process. Thats why it is important to know some tricks, like the ones you explained before, so that we can do something, even if we do not have full control on the input.



Many thanks again for your tremendous help.



Cheers.

Phil.

---
## 2010-10-27 14:31:12 - johnny-b - pb with begin end type of expressions
Hi,

I just started using pyparsing and I find it very nice. However, I had a pb today trying to make the following text being parsed:

OKbeginhere is text1end dudu

beginand here is text 2end KO

If I use [ and ] as begin and end symbols, the parser works fine but if I want to stick to begin end rather than [/], I don't know how to make it work.

Here is my code



    whitespace_char = ' '
    any_text_char = alphas + whitespace_char 
    
    debut = Literal('begin')
    fin = Literal('end')
    free_text =  ~debut + ~fin + Word(any_text_char)
    bal_expr = Group(debut + free_text + fin)
    lbtext = OneOrMore(Group(Optional(free_text)+bal_expr)) + Optional(free_text)
    
    f = open('file2.txt')
    content = f.read()
    parsed = lbtext.scanString(content)

file2.txt containing the text I want to parse (see above). Can someone help?

It seems, I don't understand very well the ~ operator.

Thanks a lot

#### 2010-10-27 18:15:55 - ptmcg
What is happening is that your 'end' delimiter (and pretty much everything up to the end of the line) is getting sucked into the Word(any_text_char) in free_text.  You made a valiant effort by putting the ~ lookahead's on free_text, but Word has it's own repetition loop that is very tight, and does not break out to see if we have in fact reached a debut or fin.



You can do what you want by forcing Word to read no more than a single letter.  You'll have to do this, or else you'll read right past 'begin' as part of 'OKbeginhere'.  So we'll start by changing free_text to:



    free_text = ~debut + ~fin + Word(any_text_char, exact=1)



Since we only read one character at a time now in Word, we need to add back the repetition to read in the whole string:



    free_text = OneOrMore(~debut + ~fin + Word(any_text_char, exact=1))



Just one more step to fix this up - now that we are only reading one character at a time, free_text will return



    ['h', 'e', 'r', 'e', 'i', 's', 't', 'e', 'x', 't', '1']



(By the way, that trailing '1' is a separate problem, that your any_text_char does not include numeric digits. Change any_text_char to: any_text_char = alphanums + whitespace_char.)



To get things back the way we want, we can wrap the whole free_text expression in a Combine, which will take the results of matching a multi-token expression and combine it into a single string:



    free_text =  Combine(OneOrMore(~debut + ~fin + Word(any_text_char,exact=1)))



In general, I don't really favor Word's that include spaces as part of their valid character set.  But since this Word is being used to read one character at a time, I guess pyparsing's whitespace skipping isn't helping much.



Now if you have to handle the case where begin-end's can be nested, then see if nestedExpr will be of some help.



Well done on your opening pyparsing effort - welcome to pyparsing!



-- Paul
#### 2010-10-28 00:15:19 - johnny-b
That is brilliant! Thank you so much for your help! I am impressed by the quality of the library, the doc (I bought your book: Getting started with pyparsing), and also the quality of service. Keep up the good work.

A few more questions if I may: would you have any pointer to the way the ~operator works and also what is the exact meaning of OneOrMore(~debut + ~fin + Word(any_text_char, exact=1))? I am not clear yet on how the combination of the multi-letter words debut and fin work with the one letter Word(,exact=1). More generally, is there a place with a complete index of all the pyparsing constructs?

Thanks a lot.
#### 2010-10-28 04:38:10 - ptmcg
The real trick going on with OneOrMore(~debut + ~fin + Word(any_text_char, exact=1)) is that '~' does a negative lookahead, that is, at the current parsing position it looks to see that the next 5 characters are *not* 'begin' and tha the next 3 characters are *not* 'end' - but it does not advance the parsing position.  So if you had 'begix' somewhere inside your free_text, it would be processed just fine, because at the 'b', your parser doesn't just look for 'b', but for all 5 characters 'begin' - since 'begix' is not 'begin', that will pass the negative lookahead. Then check that 'beg' is not 'end' - it isn't. So, all clear, process the 'b' as a valid any_text_char and continue on to 'e', 'g', etc.



I recommend you go to the SourceForge website and download the source distribution (either the .zip or .tar.gz files), which includes quite a bit of reference documentation, including htmldoc, which is an epydoc-generated reference.  I have a tiny server running where you can see this doc directly also -  .



Cheers!
#### 2010-10-28 04:43:19 - ptmcg
Also, please read this thread, covering very similar ground - 

---
## 2010-10-29 04:43:00 - rxe - update on ambiguous grammars
Are there any news on Pyparsing's ability to deal with ambiguous grammars, more precisely, to return multiple parses?



Last remarks on that in this forum are from 2007. Has anything changed?

#### 2010-10-29 05:35:59 - ptmcg
Nothing has changed, nor would I say it is likely.
#### 2011-01-06 00:28:32 - majortal
I would like to join this non-existent conversation as well...   

I'd love to include probabilities (or weights, same thing) in the grammar.   

How would the grammar look like? a function call (just like setting a result name, yet with a number and not a string)?   

I ran into several places in my Language that are impossible^H^H^H^H^H^H^H^H^H^HVery Hard to parse in Pyparsing due to its strict left-right determinism.

---
## 2010-10-29 04:48:34 - rxe - multisets
How easy would be to extend Pyparsing to deal with productions with multisets? (sets where an element can occur several times).



I'm just looking for a practical way to deal with unordered tokens.  For example, how to extend the grammar X::=ab, Y::=cd so that it can parse {abacbd} (a multiset) as {X,X,Y} (another multiset).

#### 2010-10-29 05:34:39 - ptmcg
I don't see how 'abacbd' is XXY - 'ababcd' looks like XXY to me, and could be parsed with the pyparsing expression X*(1,None) + Y.
#### 2010-11-01 06:30:03 - rxe
I had a mistake in my question: 'how to extend the grammar X::=ab, Y::=cd so that it can parse (abacbd) (a sequence) as {X,X,Y} (another multiset).' 

Edit: (abacbd) is a sequence of tokens, not a multiset.



Referring to each element of (abacbd) by its index, (123456), then (1,2) can be parsed as an X, (3,5) can be parsed as an X and (4,6) can be parsed as an Y.  That is, the tokens can be interleaved.



A practical application of this is when the tokens a,b,c,d correspond to interleaved events in a temporal sequence (e.g. log entries of different users using a web server) and one wants to identify groups of them (e.g. each user's behaviour).



Is there any obvious way of looking at this problem using Pyparsing's facilities?
#### 2010-11-02 05:41:39 - ptmcg
Ah, I see now what you are doing. This <em>might</em> be possible with a custom pyparsing class. But I think the performance would be awful. And in the case you cite, this would be better solved parsing individual log entries, and then associating them by user id outside the parser, or perhaps in a parse action.
#### 2010-11-16 06:15:07 - rxe
Thanks for your quick answer!



I ended up using a rule engine when I needed to process interleaved tokens, and pyparsing for the rest because it is much simpler to use.

---
## 2010-11-02 08:11:30 - mikasaari - Noob with Groups once more
Hi,



  I am trying to do some test parser for small script and managed to do some parsing already. The problem is with Groups. E.g. in callstmt I am trying to Group the whole call statement to it's own separate list, but I have badly failed to do so. I found quite a few topics on Groups but at least the ones I read through didn't make the trick. printstmt behaves samekind in my poor experiment.



  Thank you very much for tips,

    -Mika



  Currently the code is exporting this:



    ['function', ''Weird Call Name #1'', ['receiver', 'local'], [['i', '=', '6'], ['i', '++'], ['b', '=', '' Test ''], 'call', [''looper'', 'i']]]
    ['function', ''loopper'', ['loop'], [['for', ['i', '=', '0'], ['i', '\<', 'loop'], ['i', '++'], ['print', [], 'print', ['i'], 'print', ['i', 'i'], 'print', [''Test''], 'if', ['i', '==', '5'], ['print', [[''Test %s'', '%', 'i']]], 'test', ['b']]]]]
    ['function', ''test'', ['r'], ['some_generic_function', ['r']]]





  And I would like to see this:



    ['function', ''Weird Call Name #1'', ['receiver', 'local'], [['i', '=', '6'], ['i', '++'], ['b', '=', '' Test ''], ['call', [''looper'', 'i']]]]
    ['function', ''loopper'', ['loop'], [['for', ['i', '=', '0'], ['i', '\<', 'loop'], ['i', '++'], ['print', [], 'print', ['i'], 'print', ['i', 'i'], 'print', [''Test''], 'if', ['i', '==', '5'], ['print', [[''Test %s'', '%', 'i']]], 'test', ['b']]]]]
    ['function', ''test'', ['r'], ['some_generic_function', ['r']]]







    #!/usr/local/bin/python3.1
    
    from pyparsing import *
    
    script = r'''
    # Function 
    function 'Weird Call Name #1'($receiver, $local)
    {
        # Assigning values
        $i = 6;
        $i++;
        $b = ' Test ';    
    
        # Calling script functions
        call('looper',$i);
    }
    
    function 'loopper'($loop)
    {
        # Looping
        for($i=0;$i\<$loop;$i++)
        {
            # Different print statements
            print();
            print($i);
            print($i,$i);
            print('Test');
    
            # If statement
            if($i == 5)
            {
                print('Test %s' % ($i));
            }
    
            # Generic function call        
            test($b);
        }
    }
    
    function 'test'($r)
    {
        some_generic_function($r);
    }
    '''
    
    class POXScriptParser:
        def __init__(self):
            pass
    
        def parse(self, script):
            # Suppress
            LPAR,RPAR,LBRACE,RBRACE,SEMI,DOL,COMMA,PROCENT = map(Suppress, '(){};$,%')
    
            # Types
            NAME = Word(alphas+'_', alphanums+'_')
            NUMBER = Word(nums)
            STRING = dblQuotedString
            CALL = Keyword('call')
            IF = Keyword('if')
            FOR = Keyword('for')
            FUNC = Keyword('function')
            PRINT = Keyword('print')
    
            # Collection types
            var = DOL + NAME
    
            # Arithmetic expression
            operand = NAME | var | NUMBER | STRING
            expr = Forward()
            expr \<\< (operatorPrecedence(operand,
                [
                ('!', 1, opAssoc.LEFT),
                (oneOf('+ -'), 1, opAssoc.RIGHT), # leading sign
                (oneOf('++ --'), 1, opAssoc.RIGHT), # Add / Substract
                (oneOf('++ --'), 1, opAssoc.LEFT), # Add / substract
                (oneOf('* / %'), 2, opAssoc.LEFT), # Multiply
                (oneOf('+ -'), 2, opAssoc.LEFT), # Add / Substract
                (oneOf('\< == \> \<= \>= !='), 2, opAssoc.LEFT), # Coparation
                ('=', 2, opAssoc.LEFT) # Assign
                ]) + Optional(LPAR + Group(Optional(delimitedList(expr))) + RPAR))
    
            # Initialize Statement
            stmt = Forward()
    
            # Body
            body = ZeroOrMore(stmt)
    
            # Function
            funcdecl = Group(FUNC + STRING + LPAR + Optional(Group(delimitedList(var))) + RPAR + LBRACE + Group(body) + RBRACE)
    
            # Keyword statements
            ifstmt = IF + LPAR + expr + RPAR + Group(stmt)
            callstmt = OneOrMore(Group(CALL + LPAR + STRING + Optional(COMMA) + Optional(delimitedList(var)) + RPAR + SEMI))
            forstmt = Group(FOR + LPAR + expr + SEMI + expr + SEMI + expr + RPAR + Group(stmt))
            printstmt = PRINT + LPAR + Optional(delimitedList(var) |  STRING + Optional(PROCENT + LPAR + delimitedList(var) + RPAR)) + RPAR + SEMI
    
            # Setup statement
            stmt \<\< (expr + SEMI | callstmt | ifstmt | forstmt | printstmt | LBRACE + ZeroOrMore(stmt) + RBRACE)
    
            # Main program
            program = ZeroOrMore(funcdecl)
            program.ignore(pythonStyleComment)
    
            # Parse the script
            ParserElement.enablePackrat()
            parsed = program.parseString(script, parseAll=True)
    
            # And return the list
            return parsed
    
    if __name__ == '__main__':
        psp = POXScriptParser()
        lst = psp.parse(script)
    
        for i in lst:
            print(i.dump())



#### 2010-11-02 22:03:33 - ptmcg
My first step was to verify that callstmt was even being parsed.  To do this, I added this statement:



    callstmt.setName('callstmt').setDebug()



Then when I ran your parser, I got this output:





    Match callstmt at loc 188(12,1)
    Exception raised:Expected 'call' (at char 188), (line:12, col:1)
    Match callstmt at loc 238(17,5)
    Exception raised:Expected 'call' (at char 238), (line:17, col:5)
    Match callstmt at loc 266(18,5)
    Exception raised:Expected 'call' (at char 266), (line:18, col:5)
    Match callstmt at loc 419(26,9)
    Exception raised:Expected 'call' (at char 419), (line:26, col:9)
    Match callstmt at loc 439(27,9)
    Exception raised:Expected 'call' (at char 439), (line:27, col:9)
    Match callstmt at loc 486(29,9)
    Exception raised:Expected 'call' (at char 486), (line:29, col:9)
    Match callstmt at loc 552(33,5)
    Exception raised:Expected 'call' (at char 552), (line:33, col:5)
    Match callstmt at loc 554(34,1)
    Exception raised:Expected 'call' (at char 554), (line:34, col:1)
    Match callstmt at loc 611(39,1)
    Exception raised:Expected 'call' (at char 611), (line:39, col:1)
    ['function', ''Weird Call Name #1'', ['receiver', 'local'], [['i', '=', '6'], ['i', '++'], ['b', '=', '' Test ''], 'call', [''looper'', 'i']]]
    ['function', ''loopper'', ['loop'], [['for', ['i', '=', '0'], ['i', '\<', 'loop'], ['i', '++'], ['print', [], 'print', ['i'], 'print', ['i', 'i'], 'print', [''Test''], 'if', ['i', '==', '5'], ['print', [[''Test %s'', '%', 'i']]], 'test', ['b']]]]]
    ['function', ''test'', ['r'], ['some_generic_function', ['r']]]



Not one match attempt of callstmt succeeded!  My suspicion was that the definition of stmt was testing callstmt too late in the list of alternatives.  In fact, the expression in stmt that is most likely to mask the others was expr + SEMI.  By moving this to the end of the list of alternatives, that is:





    stmt \<\< (callstmt | ifstmt | forstmt | printstmt | expr + SEMI | LBRACE + ZeroOrMore(stmt) + RBRACE)



I now get the following:





    Match callstmt at loc 91(6,5)
    Exception raised:Expected 'call' (at char 91), (line:6, col:5)
    Match callstmt at loc 103(7,5)
    Exception raised:Expected 'call' (at char 103), (line:7, col:5)
    Match callstmt at loc 113(8,5)
    Exception raised:Expected 'call' (at char 113), (line:8, col:5)
    Match callstmt at loc 169(11,5)
    Matched callstmt -\> [['call', ''looper'', 'i']]
    Match callstmt at loc 188(12,1)
    Exception raised:Expected 'call' (at char 188), (line:12, col:1)
    ...



Now we see that the callstmt expression is getting its chance to parse the function call.  Now the final results returned are:





    ['function', ''Weird Call Name #1'', ['receiver', 'local'], [['i', '=', '6'], ['i', '++'], ['b', '=', '' Test ''], ['call', ''looper'', 'i']]]
    ['function', ''loopper'', ['loop'], [['for', ['i', '=', '0'], ['i', '\<', 'loop'], ['i', '++'], ['print', 'print', 'i', 'print', 'i', 'i', 'print', ''Test'', 'if', ['i', '==', '5'], ['print', ''Test %s'', 'i'], 'test', ['b']]]]]
    ['function', ''test'', ['r'], ['some_generic_function', ['r']]]



which is what I think you were looking for.



So you really had Group working just fine, the problem was that you were testing the alternatives in an order where a very generic alternative came ahead of the more specific one.  Read these other recent threads, 'pb with begin end type of expressions' and 'pyparsing being 'greedy'', for some similar expression ordering questions.



-- Paul
#### 2010-11-03 01:17:09 - mikasaari
Hi,



  Thanks a lot. I was looking the problem from really wrong place. Got the idea now, and had to change a lot in the code :) Now seems to work fluently. Humble thanks !



  pyparsing really is brilliant sw !



    -Mika

---
## 2010-11-07 05:32:01 - ctctc - version 1.5.5 string bug
Salut!



I have encountered a strange bug in version 1.5.5 and could reduce it to the following minimal testcase. I don't understand the internals well enough to see where it really comes from, though.



The file 'bugtest.py' is



    <span class="co1"># -*- coding: utf-8 -*-</span>
    
    __doc__ <span class="sy0">=</span> <span class="st0">''' pyparsing test case '''</span>
    
    <span class="kw1">import</span> pyparsing_py3_v155 <span class="kw1">as</span> pp
    
    <span class="kw1">if</span> __name__<span class="sy0">==</span><span class="st0">'__main__'</span>:
        exp <span class="sy0">=</span> pp.<span class="me1">Literal</span><span class="br0">&#40;</span><span class="st0">'hello'</span><span class="br0">&#41;</span>
        <span class="kw1">with</span> <span class="kw2">open</span><span class="br0">&#40;</span><span class="st0">'./pyparsing_py3_v155.py'</span><span class="br0">&#41;</span> <span class="kw1">as</span> f:
            <span class="kw1">print</span><span class="br0">&#40;</span><span class="kw2">len</span><span class="br0">&#40;</span>f.<span class="me1">read</span><span class="br0">&#40;</span><span class="br0">&#41;</span><span class="br0">&#41;</span><span class="br0">&#41;</span>
        result <span class="sy0">=</span> exp.<span class="me1">parseFile</span><span class="br0">&#40;</span><span class="st0">'./pyparsing_py3_v155.py'</span><span class="br0">&#41;</span>
        <span class="kw1">print</span><span class="br0">&#40;</span>result<span class="br0">&#41;</span>



where 'pyparsing_py3_v155.py' is just the 'pyparsing_py3' file from the 1.5.5 release tarball on sourcefourge. The results are





    $ python3 --version
    Python 3.1.2
    
    $ python3 bugtest.py
    153301
    Traceback (most recent call last):
      File 'bugtest.py', line 12, in \<module\>
        result = exp.parseFile('./pyparsing_py3_v155.py')
      File '/data/projekte/parsing/pyparsing/pyparsing_py3_v155.py', line 1464, in parseFile
        return self.parseString(file_contents, parseAll)
      File '/data/projekte/parsing/pyparsing/pyparsing_py3_v155.py', line 1087, in parseString
        loc, tokens = self._parse( instring, 0 )
      File '/data/projekte/parsing/pyparsing/pyparsing_py3_v155.py', line 954, in _parseNoCache
        preloc = self.preParse( instring, loc )
      File '/data/projekte/parsing/pyparsing/pyparsing_py3_v155.py', line 912, in preParse
        while loc \< instrlen and instring[loc] in wt:
    TypeError: 'in \<string\>' requires string as left operand, not int



I have already encountered this in revision 192.



cTc

#### 2010-11-07 12:19:47 - ptmcg
Wow, this is either deja vu or an embarrassing regression.  I was sure I had seen *and fixed* this previously.



Could you do me a favor and, in the parseFile routine, change the 'rb' argument to open to just 'r', and see if things get better?
#### 2010-11-09 14:01:22 - ctctc
Hy, sorry for the delay ... I forgot to monitor the topic. I tried it, but it doesn't help:





    [3960](1) $ python3 bugtest.py
    153205
    Traceback (most recent call last):
      File 'bugtest.py', line 12, in \<module\>
        result = exp.parseFile('./pyparsing_py3_v155.py')
      File '/data/projekte/parsing/pyparsing/pyparsing_py3_v155.py', line 1468, in parseFile
        raise exc
      File '/data/projekte/parsing/pyparsing/pyparsing_py3_v155.py', line 1464, in parseFile
        return self.parseString(file_contents, parseAll)
      File '/data/projekte/parsing/pyparsing/pyparsing_py3_v155.py', line 1098, in parseString
        raise exc
      File '/data/projekte/parsing/pyparsing/pyparsing_py3_v155.py', line 1087, in parseString
        loc, tokens = self._parse( instring, 0 )
      File '/data/projekte/parsing/pyparsing/pyparsing_py3_v155.py', line 964, in _parseNoCache
        loc,tokens = self.parseImpl( instring, preloc, doActions )
      File '/data/projekte/parsing/pyparsing/pyparsing_py3_v155.py', line 1574, in parseImpl
        raise exc
    pyparsing_py3_v155.ParseException: Expected 'hello' (at char 0), (line:1, col:1)



I think if it would have been read in binary (the wrong way), there would have been an byte, respectively bytearray in python3, no?



If I find some time this weekend I will try to run it on python 3.2 (to be sure that it's not my python install) and try to see where that int comes from --- it's just, I want to try some grammar since 3 months or so, but this bug keeps popping up.
#### 2010-11-10 01:13:12 - ptmcg
To avoid file input issues, can you test your parser using just an inline string, like:



    testdata = '''
    here is some test data for my parser, which will
    get read from a file when I get everything working
    but for now, is just a string'''
    

Then you can focus on getting the parser right, and once that is done, deal with the file input, binary-or-not, etc. issues.
#### 2010-11-10 01:26:41 - ptmcg
Um, this *is* working.  You just got a parse error because the pyparsing module does not start with the literal word 'hello'.  What did you expect from this?
#### 2010-11-13 14:17:45 - ctctc
Thanks, I just didn't see it (just saw another exception trace popping up at night and somehow thought it was the same -- sorry). Works now like a charm (save the fact that I now finally get to debug my grammars).

---
## 2010-11-11 12:24:58 - EduardoBarros - Parsing a tag with long text inside
I have an HTML text with a few '''\<td class='right_there1'\>\</td\>''' tags, each at times full of random tags in it (I know, I hate poorly designed docs as well). As those internal tags are random, this is as far as I can go assigning a pattern.



What would be the expression to define just that pattern (that would be the whole grammar, I guess)?



I'm stuck with something like:



rthere1tag = '''\<td class='right_there1'\>''' + OneOrMore(Word( alphas )) + '\</td\>'



Thanks,



Eduardo

#### 2011-05-09 22:17:15 - ptmcg
Sorry not to respond sooner, I don't really keep up with the Discussion tabs on all of the wiki pages.



The best solution to your problem is to use makeHTMLTags, like this:





    td,tdEnd = makeHTMLTags('td')
    td.setParseAction(withAttribute(**{'class':'right_there1'}))
    rthere1tag = td + SkipTo(tdEnd) + tdEnd



This is untested, but should get you pretty close.



-- Paul
#### 2015-10-29 00:08:17 - ptmcg
Even better is using the new helper (released in version 2.0.4):



    td.setParseAction(withClass('right_there1'))



---
## 2010-11-15 03:02:57 - bsder - QuotedString oddness and question about CPP preprocessing numbers
Just starting with PyParsing, so these are probably easy questions:



1) If I leave unquoteResults as True, QuotedString unquotes the \' combination, but it also rips off the \ from the \n without doing anything about it.  That seems ... odd.



2) Is there a better way to deal with cppNumber?  This seems like I'm really fighting PyParsing to do this.  Yes, that's an odd definition for the number, but that's the way CPP does define it and is the reason I'm digging at PyParsing in the first place (I'm trying to run a parser to spot some of these kinds of things which really aren't stable between different compilers).



Thanks,

-a





    #!/usr/bin/env python
    
    import sys
    
    from pyparsing import *
    
    myPrintables = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!#$%&\'()*+,-./:;\<=\>?@[\\]^_`{|}~'
    
    cppNumberChars = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_.'
    cppExponents = 'e+ e- E+ E-'
    
    cppNumber = Combine(Optional(Literal('.')) + oneOf(' '.join(nums)) + ZeroOrMore(oneOf(' '.join(cppNumberChars) + ' ' + cppExponents)))
    
    cppIdentifier = Word(myPrintables)
    cppString = QuotedString(''', escChar='\\', unquoteResults=False)
    cppCharList = QuotedString(''', escChar='\\', unquoteResults=False)
    cppHeaderFileName = QuotedString('\<', endQuoteChar='\>')
    
    cppToken = cppNumber('num') | cppIdentifier('id') | cppString('str') | cppCharList('cl') | cppHeaderFileName('fname')
    
    gmr = ZeroOrMore(cppToken)
    
    testString = '''
    double ii = 0xE+12;
    char *cc = 'Test me \\' again\\n';
    '''
    
    def main():
    #    ssFileName = sys.argv[1]
    #    pp = gmr.parseFile(ssFileName, True)
        pp = gmr.parseString(testString)
    
        print 'Dumping tree'
        for ii in pp:
            print ii
    
    if __name__ == '__main__':
        main()



Definition of a preprocessing number from:





A preprocessing number has a rather bizarre definition. The category includes all the normal integer and floating point constants one expects of C, but also a number of other things one might not initially recognize as a number. Formally, preprocessing numbers begin with an optional period, a required decimal digit, and then continue with any sequence of letters, digits, underscores, periods, and exponents. Exponents are the two-character sequences `e+', `e-', `E+', `E-', `p+', `p-', `P+', and `P-'. (The exponents that begin with `p' or `P' are new to C99. They are used for hexadecimal floating-point constants.)



The purpose of this unusual definition is to isolate the preprocessor from the full complexity of numeric constants. It does not have to distinguish between lexically valid and invalid floating-point numbers, which is complicated. The definition also permits you to split an identifier at any position and get exactly two tokens, which can then be pasted back together with the `##' operator.



It's possible for preprocessing numbers to cause programs to be misinterpreted. For example, 0xE+12 is a preprocessing number which does not translate to any valid numeric constant, therefore a syntax error. It does not mean 0xE + 12, which is what you might have intended.

#### 2010-11-16 07:34:42 - ptmcg
Yes, this is odd, nay I would even say it's a bug.  I'll have it fixed in 1.5.6 - thanks for reporting it.

---
## 2010-11-16 07:49:03 - kbriggs4 - wordsToNum.py example doesn't work
kbriggs@gold:~/python\> python --version

Python 2.5.1



kbriggs@gold:~/python\> python wordsToNum.py 

Traceback (most recent call last):

  File 'wordsToNum.py', line 76, in \<module\>

    numWords.ignore('-')

  File '/usr/lib/python2.5/site-packages/pyparsing.py', line 2327, in ignore

    super( ParseExpression, self).ignore( other )

  File '/usr/lib/python2.5/site-packages/pyparsing.py', line 1416, in ignore

    self.ignoreExprs.append( Suppress( other.copy() ) )

AttributeError: 'str' object has no attribute 'copy'

#### 2010-11-22 08:22:23 - ptmcg
I'm sorry, but I can't reproduce this problem.  Make sure you are using the original wordToNum.py and pyparsing sources. (It looks like the automatic promotion of strings to Literals isn't happening for some reason.)



-- Paul
#### 2010-11-24 02:51:20 - kbriggs4
I don't know what the 'original' wordToNum.py and pyparsing sources are - I used the latest, current ones.   Anyway, I found the problem is fixed if



numWords.ignore('-')



is changed to



numWords.ignore(CaselessLiteral('-'))



Keith

---
## 2010-11-19 02:36:19 - bsder - Tokens *and* text that produced them
How do I get both the tokens from a match *and* the text that produced that match?



Presumably I need to do something with setParseAction and originalTextFor, but I can't seem to put together the correct incantation.

#### 2010-11-22 07:16:38 - ptmcg
Here is a simple parser, to parse a string of A's, B's, and C's:



    expr = Word('A') + Word('B') + Word('C')



It returns the separate pieces in tokens:



    instring = 'AAABBCCCCCCDDD'
    print expr.parseString(instring)



prints:



    ['AAA', 'BB', 'CCCCCC']



As you have already found, using originalTextFor gives the original substring from the input text.  This code:



    expr2 = originalTextFor(expr)
    print expr2.parseString(instring)



Prints:



    ['AAABBCCCCCC']



The default behavior for originalTextFor is to return just the parsed substring as a string.  If you have defined any results names within that expression, this gets lost, as plain old Python strings know nothing about results names.



However, you can have originalTextFor return not a string but a ParseResults object, and this will preserve any results names you have defined.



So to solve your problem, do 2 things:

- define a results name for the expression's tokens

- call originalTextFor with the optional asString parameter set to False



This code:



    expr3 = originalTextFor(expr('asTokens'), asString=False)
    result = expr3.parseString(instring)
    print result[0]
    print result.asTokens



will print this:



    AAABBCCCCCC
    ['AAA', 'BB', 'CCCCCC']



---
## 2010-11-20 12:43:34 - j.f.k. - Fixed width colun table (noob)
Hi.

I'm just starting my adventure with pyparsing.

My problem is similar to discovered in 'Reading a table by columns' subject, except:

- column width is fixed

- there are no delimiters

- cells in rows could be empty.

My primary problem is I don't know how to process empty columns.







    <span class="kw1">from</span> pyparsing <span class="kw1">import</span> *
    
    testData <span class="sy0">=</span> <span class="st0">'''<span class="es0">\</span>
    -Date1------Date2------Number----String-----------------------------Id----
     2010.10.09 2010.10.09 123.45    abc def and some other text follow 1
                2010.10.11 22.56     ghi                                2
     2010.10.09            23.45                                        3
     2010.10.09                      abc                                4
     '''</span>
    
    <span class="st0">'''
    Required result follows:
    [
    {'Date1': '2010.10.09', 'Date1': '2010.10.09', 'Number': 123.45, 'String': 'abc def and some other text follow', 'Id': 1},
    {'Date1': None, 'Date1': '2010.10.11', 'Number': 22.56, 'String': 'ghi', 'Id': 2},
    {'Date1': '2010.10.09', 'Date1': None, 'Number': 23.45, 'String': None, 'Id': 3},
    {'Date1': '2010.10.09', 'Date1': None, 'Number': None, 'String': 'abc', 'Id': 4}
    ]
    '''</span>
    
    <span class="kw1">def</span> tableColParser<span class="br0">&#40;</span><span class="br0">&#41;</span>:
    
        <span class="kw1">def</span> defineCol<span class="br0">&#40;</span>t<span class="br0">&#41;</span>:
            lineExpr <span class="sy0">=</span> Suppress<span class="br0">&#40;</span><span class="st0">' '</span><span class="br0">&#41;</span>
            <span class="kw1">for</span> col <span class="kw1">in</span> t.<span class="me1">header</span>:
                lineExpr +<span class="sy0">=</span> Optional<span class="br0">&#40;</span>Word<span class="br0">&#40;</span>printables<span class="sy0">,</span> printables + <span class="st0">' '</span><span class="sy0">,</span> exact<span class="sy0">=</span><span class="kw2">len</span><span class="br0">&#40;</span>col<span class="br0">&#41;</span><span class="br0">&#41;</span><span class="br0">&#41;</span>
            tabValueLine <span class="sy0">\<\<</span> Group<span class="br0">&#40;</span>lineExpr<span class="br0">&#41;</span>
    
        colNameSep <span class="sy0">=</span> Literal<span class="br0">&#40;</span><span class="st0">'-'</span><span class="br0">&#41;</span>
        header <span class="sy0">=</span> OneOrMore<span class="br0">&#40;</span>
            ZeroOrMore<span class="br0">&#40;</span>colNameSep.<span class="me1">suppress</span><span class="br0">&#40;</span><span class="br0">&#41;</span><span class="br0">&#41;</span> +
            Combine<span class="br0">&#40;</span>Word<span class="br0">&#40;</span>alphanums<span class="br0">&#41;</span> + OneOrMore<span class="br0">&#40;</span>colNameSep<span class="br0">&#41;</span><span class="br0">&#41;</span>
        <span class="br0">&#41;</span>
        tabLine <span class="sy0">=</span> Forward<span class="br0">&#40;</span><span class="br0">&#41;</span>
        table <span class="sy0">=</span> <span class="br0">&#40;</span>
            header<span class="br0">&#40;</span><span class="st0">'header'</span><span class="br0">&#41;</span>.<span class="me1">setParseAction</span><span class="br0">&#40;</span>defineCol<span class="br0">&#41;</span> +
            Group<span class="br0">&#40;</span>OneOrMore<span class="br0">&#40;</span>tabLine<span class="br0">&#41;</span><span class="br0">&#41;</span><span class="br0">&#40;</span><span class="st0">'hdata'</span><span class="br0">&#41;</span>
        <span class="br0">&#41;</span>
        <span class="kw1">return</span> Dict<span class="br0">&#40;</span>table<span class="br0">&#41;</span>
    
    <span class="kw1">print</span> tableColParser<span class="br0">&#40;</span><span class="br0">&#41;</span>.<span class="me1">parseString</span><span class="br0">&#40;</span>testData<span class="br0">&#41;</span>



I'll be grateful for showing me the right direction.



Regards

Jacek

#### 2010-11-22 08:00:53 - ptmcg
I've used pyparsing to parse out data that was in a table with delimiting '+', '-', and '|' characters that actually 'drew' the gridlines of a table.  But for just tabular-formatted data, I wouldn't suggest pyparsing as that good a fit, especially when you have gaps in some columns.  I took a different tack on your data, and posted it here: .  I hope it helps.



-- Paul
#### 2010-12-08 17:59:14 - northwestwolf
Paul,



I'm working on a similar project, tabular data without nice clean delimiters for columns. You say that pyparsing isn't a good fit. What would you recommend in its place?
#### 2010-12-08 18:28:37 - ptmcg
Basically, just use native Python string slicing.  You know the layout of the strings, just slice them at the known column locations.  Follow my pastebin link from my earlier post, and you'll see one way to structure a string slicer using a list of column definitions.
#### 2010-12-08 19:19:47 - northwestwolf
My problem is a bit uglier than the one posted above as my column count changes from table to table and there are multiple forms of this file, some have the tables shown others do not.



The goal here is to convert this text file to an html file (tables converted to html tables). 





    PW0^301|PW0FINAL LEAGUE STANDINGS                                RUN DATE:  04/05/10  15:00
    TAVA LANES                                                               PAGE  1
                                 LEAGUE #543: TAVA MIXERS                   
    
                                   WEEK #36 - 5/12/05
    
    CENTER PHONE: 684-9980    
    THURSDAY - 6:30 P.M.                                  90% OF DIFF - 3 POINTS
    LEAGUE PRESIDENT: MIKE LOLLIS                         PHONE:               
    LEAGUE SECRETARY: CATHY ZABLOUDIL                     PHONE: 781-4406      
    LEAGUE SANCTION NUMBER: 0                             RESULTS-WEEK 1 OF 1 (36)
    
    [[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]]
    MW0                                                                                --SEASON--
         TEAM STANDINGS                    WON   LOST  PCT.   PINS  AVG  HGS  HSS   WON   LOST
         --------------                    ---   ----  ----   ----  ---  ---  ---   ---   ----
     1. # 3-TEAM 3                          3     0   1.000  73763  682  788 2286   65    43  
     2. # 2-TEAM 2                          3     0   1.000  71752  659  813 2212   56.5  51.5
     3. # 7-SAND BAGGERS                    3     0   1.000  56315  464  646 1744   51    57  
     4. # 1-BEHIND THE 8 PIN                2     1    .666  57550  515  711 1825   38    70  
     5. # 4-TEAM 4                          1     2    .333  59635  474  580 1636   44.5  63.5
     6. # 6-VODKA, SCOTCH & BEER            0     3    .000  66904  603  725 1993   52.5  55.5
     7. # 8-TEAM 8                          0     3    .000  61966  490  575 1613   65.5  42.5
     8. # 5-SPLIT HAPPENS                   0     3    .000  61447  499  599 1742   59    49  
    PW0[[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]]
    
    
    MW0HIGH SCR. GAME--MEN          HIGH SCR. SERIES--MEN       HIGH AVERAGE--MEN
    -------------------          ---------------------       -----------------
    256 BUD SITTNIEWSKI          728 MICHAEL LOLLIS          198.8 MICHAEL LOLLIS          
    232 STEVE SMITH              635 TERRY ALDRICH           187.0 TERRY ALDRICH           
    203 DONALD STANDLEY          563 MIKE SCHMITT            182.8 BUD SITTNIEWSKI         
    PW0
    MW0HIGH HDCP. GAME--MEN         HIGH HDCP. SERIES--MEN
    --------------------         ----------------------
    285 BLAIR BOGENS             839 JEREMY COOK             
    281 RANDY PARAMO             760 MANNY IBARRA            
    276 ALAN SHIELDS             749 ROBERT BELL             
    PW0
    
    MW0HIGH SCR. GAME--WOMEN        HIGH SCR. SERIES--WOMEN     HIGH AVERAGE--WOMEN
    ---------------------        -----------------------     -------------------
    237 DEBBIE STANDLEY          649 CATHY ZABLOUDIL         186.3 CATHY ZABLOUDIL         
    202 DAWN BOGENS              589 BABS LOLLIS             166.6 BABS LOLLIS             
    171 PAT COOK                 458 GLENDA ALDRICH          158.7 DEBBIE STANDLEY         
    PW0
    MW0HIGH HDCP. GAME--WOMEN       HIGH HDCP. SERIES--WOMEN
    ----------------------       ------------------------
    300 AIDA ACOSTA              742 JENNIFER BELL           
    284 CARA SHIELDS             738 KAREN NICHOLS           
    272 CAITLIN SPIVEY           705 MONICA IBARRA           
    PW0
    
           ***1ST***       LAST WEEK'S HIGH SCORES       ***2ND***
        ---------------    -----------------------    ---------------
    PW0        235  MICHAEL LOLLIS                   MEN  HIGH  GAME  SCRATCH               222  TERRY ALDRICH       
            681  MICHAEL LOLLIS                   MEN  HIGH SERIES SCRATCH               566  TERRY ALDRICH       
            268  JEREMY COOK                      MEN  HIGH  GAME  HANDICAP              264  ALAN SHIELDS        
            750  MICHAEL LOLLIS                   MEN  HIGH SERIES HANDICAP              739  JEREMY COOK         
            210  CATHY ZABLOUDIL                 WOMEN HIGH  GAME  SCRATCH               194  DEBBIE STANDLEY     
            588  CATHY ZABLOUDIL                 WOMEN HIGH SERIES SCRATCH               539  DEBBIE STANDLEY     
            272  CAITLIN SPIVEY                  WOMEN HIGH  GAME  HANDICAP              267  KAREN NICHOLS       
            716  DEBBIE STANDLEY                 WOMEN HIGH SERIES HANDICAP              707  KAREN NICHOLS       
    PW0MW0PW0[[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]]
    ^300|
    PW0FINAL LEAGUE STANDINGS                                RUN DATE:  04/05/10  15:00
    TAVA LANES                                                               PAGE  2
                                 LEAGUE #543: TAVA MIXERS                   
                                   WEEK #36 - 5/12/05
    
    CENTER PHONE: 684-9980    
    THURSDAY - 6:30 P.M.                                  90% OF DIFF - 3 POINTS
    LEAGUE PRESIDENT: MIKE LOLLIS                         PHONE:               
    LEAGUE SECRETARY: CATHY ZABLOUDIL                     PHONE: 781-4406      
    LEAGUE SANCTION NUMBER: 0                             RESULTS-WEEK 1 OF 1 (36)
    
    [[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]][[user:northwestwolf|1291864787]]
    
    
                         ***LAST WEEK'S INDIVIDUAL ACHIEVEMENTS***
                         -----------------------------------------
    PW0       MAN  GAME  OVER 225 - MICHAEL LOLLIS       235*       MAN SERIES OVER 500 - BUD SITTNIEWSKI      528*
           MAN SERIES OVER 500 - BLAIR BOGENS         541*       MAN SERIES OVER 500 - TERRY ALDRICH        566*
           MAN SERIES OVER 500 - MIKE SCHMITT         518*       MAN SERIES OVER 600 - MICHAEL LOLLIS       681*
         WOMAN  GAME  OVER 200 - CATHY ZABLOUDIL      210*     WOMAN  GAME  OVER 200 - CATHY ZABLOUDIL      205*
         WOMAN SERIES OVER 500 - CATHY ZABLOUDIL      588*     WOMAN SERIES OVER 500 - BABS LOLLIS          537*
         WOMAN SERIES OVER 500 - DEBBIE STANDLEY      539*
    
                        * - PREVIOUSLY QUALIFIED IN THIS CATEGORY
    PW0
    PW0INDIVIDUAL AVERAGES                                   RUN DATE:  04/05/10  15:00
    TAVA LANES                                                               PAGE  1
                                 LEAGUE #543: TAVA MIXERS                   
                                   WEEK #37 - 5/19/05
    
    PW0PW0                          ENT  HGS  HSS  HGH  HSH  PINS GMS AVG                             ENT  HGS  HSS  HGH  HSH  PINS GMS AVG   
                              ---  ---  ---  ---  ---  ---- --- ---                             ---  ---  ---  ---  ---  ---- --- ---   
        TM #1-BEHIND THE 8 PIN                                            TM #5-SPLIT HAPPENS            
         AIDA ACOSTA          114  197  411  300  723 10160  93 109        CARA SHIELDS         ---  167  381  284  726 10024 102  98 
         KATHY MUGAVERO       ---  211  495  292  729  7592  54 140        ALAN SHIELDS         ---  187  484  276  742 13213  99 133 
         DANA MUGAVERO        ---  267  567  325  741  8637  54 159        PAT COOK             117  171  441  261  711 11517  96 119 
                                                                           DAN COOK             147  208  541  278  742 10132  66 153 
                                                                                                                              AVG=503 
        TM #2-TEAM 2                                                      TM #6-VODKA, SCOTCH & BEER     
         BUD SITTNIEWSKI      ---  256  655  292  763 18651 102 182        RANDY PARAMO         147  205  485  281  713  9559  70 136 
         DAWN BOGENS          124  202  441  289  705  9471  75 126        DEBBIE STANDLEY      ---  237  625  299  811 14287  90 158 
         BLAIR BOGENS         157  224  566  285  773 14804  93 159        STEVE SMITH          182  232  597  281  756 15232  90 169 
         CATHY ZABLOUDIL      186  237  649  272  757 19566 105 186        DONALD STANDLEY      ---  203  522  274  732 10913  75 145 
                                                            AVG=653                                                           AVG=608 
        TM #3-TEAM 3                                                      TM #7-SAND BAGGERS             
         GLENDA ALDRICH       140  174  458  252  692 11239  84 133        JEREMY COOK          ---  207  509  317  839 10222  95 107 
         BABS LOLLIS          177  232  589  281  725 17336 104 166        KAREN NICHOLS        ---  167  393  281  738  9458  92 102 
         TERRY ALDRICH        183  238  635  271  719 16837  90 187        JANICE STANDLEY      ---  174  455  264  725  7814  62 126 
         MICHAEL LOLLIS       207  300  728  326  803 21275 107 198       
                                                            AVG=684       
        TM #4-TEAM 4                                                      TM #8-TEAM 8                   
         ROBERT BELL          ---  177  455  272  749 12623 108 116        CAITLIN SPIVEY       ---  162  389  272  698  9176  90 101 
         ANNIE DIEHL          117  155  391  258  700 10281  93 110        MANNY IBARRA         ---  170  475  271  760 12348 102 121 
         HELEN ASH            123  165  420  256  702 10672  91 117        MONICA IBARRA        ---  150  354  267  705  8800  87 101 
         JENNIFER BELL        ---  182  463  275  742 13295 105 126        MIKE SCHMITT         ---  215  563  269  722 12439  75 165 
                                                            AVG=469                                                           AVG=488 
        *SUBSTITUTES*
         2-CLYDINE LASSLEY    131  179  504  267  768  6335  54 117       19-REGINA PIA         ---  193  500  256  689  3225  21 153 
        23-JOEY TOTH          ---  178  412  277  709  5425  48 113       27-GABE RAMIREZ       ---  126  324    0    0  1551  18  86 
        30-RICH PHILLIPS      ---  258  629  303  764  6796  39 174       32-DANI MEZA          ---  136  398    0    0  1061   9 117 
        37-WILL CANNELL       ---  106  306    0    0   306   3 102       38-SAM LAWRENCE       ---  120  337    0    0   337   3 112 
        39-JACQUELYNN FARRIS  155  158  462    0    0   462   3 154*      40-TIM FARRIS         190  182  518    0    0   518   3 172*
        44-JOHN LEWANDOWSKI   ---  135  376    0    0   376   3 125 
    
        * - INDICATES DIFFERENT AVERAGE THAN THAT USED FOR HANDICAPPING PURPOSES
    PW0
    PW0TEAM RECORDS                                          RUN DATE:  04/05/10  15:00
    TAVA LANES                                                               PAGE  1
                                 LEAGUE #543: TAVA MIXERS                   
                                   WEEK #36 - 5/12/05
    
    
                               GAME GAME GAME        GRAND
         TM #   LANE WON  LOST   1    2    3  TOTAL  TOTAL  AVG  HGS  HSS  HGH  HSH
         ----   ---- ---- ---- ---- ---- ---- -----  -----  ---  ---  ---  ---  ---
             1   27   2    1    534  543  550  1627  57550  564  711 1825  798 2106
             2   22   3    0    724  670  630  2024  71752  664  813 2212  813 2212
             3   21   3    0    718  714  761  2193  73763  682  788 2286  788 2286
             4   26   1    2    459  514  572  1545  59635  552  580 1636  774 2148
             5   25   0    3    704  613  606  1923  61447  568  599 1742  772 2132
    
             6   23   0    3    627  567  662  1856  66904  619  725 1993  778 2266
             7   28   3    0    638  630  681  1949  56315  568  646 1744  836 2314
             8   24   0    3    679  682  733  2094  61966  573  575 1613  754 2129
    
    
    
           LEAGUE TOTALS
           -------------
           SEASON  WINS :   12  
           SEASON LOSSES:   12  
    
    PW0
         MOST IMPROVED MAN       FINAL AVG    ENTERING AVG               # GAMES
         -----------------       ---------    ------------               -------
         1. JEREMY COOK             107     -       88*     =    + 19       95
         2. ALAN SHIELDS            133     -      120*     =    + 13       99
         3. MANNY IBARRA            121     -      109*     =    + 12      102
         4. DAN COOK                153     -      147      =    +  6       66
         5. DONALD STANDLEY         145     -      141*     =    +  4.50    75
    
         MOST IMPROVED WOMAN     FINAL AVG    ENTERING AVG               # GAMES
         -------------------     ---------    ------------               -------
         1. KAREN NICHOLS           102     -       86*     =    + 16.80    92
         2. CARA SHIELDS             98     -       82*     =    + 16.27   102
         3. DEBBIE STANDLEY         158     -      147*     =    + 11       90
         4. MONICA IBARRA           101     -       91*     =    + 10       87
         5. JENNIFER BELL           126     -      120*     =    +  6      105
    
         * - 21 GAME AVERAGE FOR NEW BOWLER
    
    PW0BOWLER RECORDS                                        RUN DATE:  04/05/10  15:00
    TAVA LANES                                                               PAGE  1
                                 LEAGUE #543: TAVA MIXERS                   
                                   WEEK #36 - 5/12/05
    
    
    
    MW0                       GAME  GAME  GAME         GRAND                              
      BOWLER NAME            1     2     3   TOTAL  TOTAL GMS AVG   HGS   HSS  HGH  HSH  ENT HDCP
      -----------          ----- ----- ----- -----  ----- --- ---   ---   ---  ---  ---  --- ----
        TM #1 - BEHIND THE 8 PIN         
      AIDA ACOSTA           127    83    85   295   10160  93 109   197   411  300  723  114  103
      KATHY MUGAVERO        128   166   172   466    7592  54 140   211   495  292  729  ---   75
      DANA MUGAVERO         129   144   143   416    8637  54 159   267   567  325  741  ---   58
    
        TM #2 - TEAM 2                   
      BUD SITTNIEWSKI       204   141   183   528   18651 102 182   256   655  292  763  ---   37
      DAWN BOGENS           135   129   103   367    9471  75 126   202   441  289  705  124   88
      BLAIR BOGENS          175   195   171   541   14804  93 159   224   566  285  773  157   58
      CATHY ZABLOUDIL       210   205   173   588   19566 105 186   237   649  272  757  186   34
    
        TM #3 - TEAM 3                   
      GLENDA ALDRICH        126   144   139   409   11239  84 133   174   458  252  692  140   81
      BABS LOLLIS           188   171   178   537   17336 104 166   232   589  281  725  177   52
      TERRY ALDRICH         169   175   222   566   16837  90 187   238   635  271  719  183   33
      MICHAEL LOLLIS        235   224   222   681   21275 107 198   300   728  326  803  207   23
    
        TM #4 - TEAM 4                   
      ROBERT BELL            74   124   115   313   12623 108 116   177   455  272  749  ---   97
      ANNIE DIEHL           ...   ...   ...   ...   10281  93 110   155   391  258  700  117  102
      HELEN ASH              88    86   122   296   10672  91 117   165   420  256  702  123   96
      JENNIFER BELL         103   110   141   354   13295 105 126   182   463  275  742  ---   88
    
        TM #5 - SPLIT HAPPENS            
      CARA SHIELDS          116    96    83   295   10024 102  98   167   381  284  726  ---  113
      ALAN SHIELDS          183   128   120   431   13213  99 133   187   484  276  742  ---   81
      PAT COOK              111   105    80   296   11517  96 119   171   441  261  711  117   94
      DAN COOK              159   149   188   496   10132  66 153   208   541  278  742  147   63
    
        TM #6 - VODKA, SCOTCH & BEER     
      RANDY PARAMO         (126) (126) (126)  378    9559  70 136   205   485  281  713  147   79
      DEBBIE STANDLEY       194   167   178   539   14287  90 158   237   625  299  811  ---   59
      STEVE SMITH           146   140   202   488   15232  90 169   232   597  281  756  182   49
      DONALD STANDLEY       161   134   156   451   10913  75 145   203   522  274  732  ---   71
    
        TM #7 - SAND BAGGERS             
      JEREMY COOK           162   117   142   421   10222  95 107   207   509  317  839  ---  105
      KAREN NICHOLS         104   158   118   380    9458  92 102   167   393  281  738  ---  109
      JANICE STANDLEY       111    94   160   365    7814  62 126   174   455  264  725  ---   88
    
        TM #8 - TEAM 8                   
      CAITLIN SPIVEY         86    86   162   334    9176  90 101   162   389  272  698  ---  110
      MANNY IBARRA          120   119   150   389   12348 102 121   170   475  271  760  ---   92
      MONICA IBARRA         100   122    97   319    8800  87 101   150   354  267  705  ---  110
      MIKE SCHMITT          195   177   146   518   12439  75 165   215   563  269  722  ---   53
    
      *SUBSTITUTES*
      CLYDINE LASSLEY       ...   ...   ...   ...    6335  54 117   179   504  267  768  131   96
      REGINA PIA            ...   ...   ...   ...    3225  21 153   193   500  256  689  ---   63
      JOEY TOTH             ...   ...   ...   ...    5425  48 113   178   412  277  709  ---   99
      GABE RAMIREZ          ...   ...   ...   ...    1551  18  86   126   324    0    0  ---  124
      RICH PHILLIPS         ...   ...   ...   ...    6796  39 174   258   629  303  764  ---   45
      DANI MEZA             ...   ...   ...   ...    1061   9 117   136   398    0    0  ---   96
    
    PW0BOWLER RECORDS                                        RUN DATE:  04/05/10  15:00
    TAVA LANES                                                               PAGE  2
                                 LEAGUE #543: TAVA MIXERS                   
                                   WEEK #36 - 5/12/05
    
    
    
    MW0                       GAME  GAME  GAME         GRAND                              
      BOWLER NAME            1     2     3   TOTAL  TOTAL GMS AVG   HGS   HSS  HGH  HSH  ENT HDCP
      -----------          ----- ----- ----- -----  ----- --- ---   ---   ---  ---  ---  --- ----
      WILL CANNELL          ...   ...   ...   ...     306   3 102   106   306    0    0  ---  109
      SAM LAWRENCE          ...   ...   ...   ...     337   3 112   120   337    0    0  ---  100
      JACQUELYNN FARRIS     ...   ...   ...   ...     462   3 154   158   462    0    0  155   62
      TIM FARRIS            ...   ...   ...   ...     518   3 172   182   518    0    0  190   30
      JOHN LEWANDOWSKI      ...   ...   ...   ...     376   3 125   135   376    0    0  ---   89
    PW0
    
    
    
    PW0



---
## 2010-11-21 06:00:54 - Begbie00 - Returning an empty string when Optional not found.
Hi all,



Whenever an Optional argument is not found, I'd like parseString() to return ''. For example, if my definition of a poker card were card = Group(rank + Optional(suit)), I'd like the string 'A' to produce ['A',''] instead of just ['A']. Is that possible?



Thanks,



Mike

#### 2010-11-21 06:03:08 - Begbie00
Sorry, a prettier version:





    <span class="sy0">\>\>\></span> card.<span class="me1">parseString</span><span class="br0">&#40;</span><span class="st0">'A'</span><span class="br0">&#41;</span>
    <span class="br0">&#91;</span><span class="st0">'A'</span><span class="sy0">,</span><span class="st0">''</span><span class="br0">&#93;</span>


#### 2010-11-21 06:46:03 - Begbie00
Nevermind, found it:




#### 2010-11-21 19:32:23 - ptmcg
Great, glad you found what you needed. That link is pretty old, though, try this one for more current docs: 

---
## 2010-11-22 07:52:21 - m3mento - avoiding partial matches?
I realize this is super noob, but I guess pyparsing is not working the way I thought it was.



I have this regex:

([a-zA-Z]\d{3}|[a-zA-Z]\d{5}(-\d{2})?)



I turned it into the following:

find_me = Combine( (Word(alphas,exact=1) + Word(nums,exact=5) + '-' + Word(nums,exact=2)) | (Word(alphas,exact=1) + Word(nums,exact=3)) )



Then made the following to test:

test_word = Word(alphanums+'.-*_/:+')

line = ZeroOrMore(findme('found')|test_word)



I'm trying to pull that regex out of a line of text.



In [103]: line.parseString('this a0000 is a test')

Out[103]: (['this', 'a000', '0', 'is', 'a', 'test'], {'found': [('a000', 1)]})



Why does pyparsing parse a partial word as one identifier and the rest of it as the other? I could understand if my ordering was wrong, and it was matching test_word before anything else, but in this case it doesn't even regard the splits in the words, and 'exact' only extends as far as it reads.



I guess I'm confused because I've read that pyparsing has no real read-ahead, but if that's the case i have no idea how to do this...

#### 2010-11-22 08:11:35 - ptmcg
What version of pyparsing are you using? I did not reproduce your results with your code as posted, until I changed find_me to use the pyparsing Regex class with your original RE expression:



    find_me = Regex(r'([a-zA-Z]\d{3}|[a-zA-Z]\d{5}(-\d{2})?)')



Your original RE only reads up to 3 digits after a leading alpha, and that's what you get, then pyparsing picks up from there and finds a valid test_word, '0'.  If you want to only match whole words, look into adding \b tags in your RE, or the corresponding WordStart and WordEnd classes in pyparsing.
#### 2010-11-22 08:13:09 - ptmcg
Also, if you are just searching a string for matches, look into the searchString and scanString alternatives to ParserElement.parseString - they are very easy to use, and don't require that you define an 'everything else' type expression, as you have done in test_word.
#### 2010-11-22 22:40:20 - m3mento
Not really sure why im getting these results. I'm working in a fresh virtualenv and installed with pip.







Python 2.6.6 (r266:84292, Sep 15 2010, 16:22:56)

Type 'copyright', 'credits' or 'license' for more information.



IPython 0.10.1 -- An enhanced Interactive Python.

?         -\> Introduction and overview of IPython's features.

%quickref -\> Quick reference.

help      -\> Python's own help system.

object?   -\> Details about 'object'. ?object also works, ?? prints more.



In [1]: from pyparsing import *



In [2]: find_me = Combine( (Word(alphas,exact=1) + Word(nums,exact=5) + '-' + Word(nums,exact=2)) | (Word(alphas,exact=1) + Word(nums,exact=3)) )



In [3]: test_word = Word(alphanums+'.-*_/:+')



In [4]: line = ZeroOrMore(find_me('found')|test_word)



In [5]: line.parseString('this a1234 is a test')

Out[5]: (['this', 'a123', '4', 'is', 'a', 'test'], {'found': [('a123', 1)]})



In [6]: import pyparsing



In [7]: pyparsing.<u>version</u>

Out[7]: '1.5.5'
#### 2010-11-22 22:46:44 - m3mento
changing find_me to:

In [21]: find_me = Combine( (Word(alphas,exact=1) + Word(nums,exact=5) + '-' + Word(nums,exact=2)) | (Word(alphas,exact=1) + Word(nums,exact=3)) ) + WordEnd()



In [22]: find_me.searchString('this a12345 is a test')

Out[22]: ([], {})



In [23]: find_me.searchString('this a123 is a test')

Out[23]: ([(['a123'], {})], {})



In [24]: find_me.searchString('this a12345-67 is a test')

Out[24]: ([(['a12345-67'], {})], {})



In [25]: find_me.searchString('this a12345-678 is a test')

Out[25]: ([], {})



In [26]: find_me.searchString('this a1234-678 is a test')

Out[26]: ([], {})



Appears to have worked. Still annoying, I guess I dont understand why its needed.
#### 2010-11-23 02:54:11 - ptmcg
This is still a mystery to me too.  Try using this definition of your expression, to get some debugging info:





    num5 = Word(nums,exact=5).setName('num5').setDebug()
    num2 = Word(nums,exact=2).setName('num2').setDebug()
    num3 = Word(nums,exact=3).setName('num3').setDebug()
    find_me = Combine( (Word(alphas,exact=1) + num5 + '-' + num2) | (Word(alphas,exact=1) + num3) )


#### 2010-11-23 12:53:12 - m3mento
In [1]: from pyparsing import *



In [2]: %cpaste

Pasting code; enter '--' alone on the line to stop.

:num5 = Word(nums,exact=5).setName('num5').setDebug()

:num2 = Word(nums,exact=2).setName('num2').setDebug()

:num3 = Word(nums,exact=3).setName('num3').setDebug()

:find_me = Combine( (Word(alphas,exact=1) + num5 + '-' + num2) | (Word(alphas,exact=1) + num3) )

:--



In [3]: find_me.searchString('this a1234-678 is a test')

Match num5 at loc 1(1,2)

Exception raised:Expected num5 (at char 1), (line:1, col:2)

Match num3 at loc 1(1,2)

Exception raised:Expected num3 (at char 1), (line:1, col:2)

Match num5 at loc 2(1,3)

Exception raised:Expected num5 (at char 2), (line:1, col:3)

Match num3 at loc 2(1,3)

Exception raised:Expected num3 (at char 2), (line:1, col:3)

Match num5 at loc 3(1,4)

Exception raised:Expected num5 (at char 3), (line:1, col:4)

Match num3 at loc 3(1,4)

Exception raised:Expected num3 (at char 3), (line:1, col:4)

Match num5 at loc 4(1,5)

Exception raised:Expected num5 (at char 4), (line:1, col:5)

Match num3 at loc 4(1,5)

Exception raised:Expected num3 (at char 4), (line:1, col:5)

Match num5 at loc 6(1,7)

Exception raised:Expected num5 (at char 10), (line:1, col:11)

Match num3 at loc 6(1,7)

Matched num3 -\> ['123']

Match num5 at loc 16(1,17)

Exception raised:Expected num5 (at char 16), (line:1, col:17)

Match num3 at loc 16(1,17)

Exception raised:Expected num3 (at char 16), (line:1, col:17)

Match num5 at loc 17(1,18)

Exception raised:Expected num5 (at char 17), (line:1, col:18)

Match num3 at loc 17(1,18)

Exception raised:Expected num3 (at char 17), (line:1, col:18)

Match num5 at loc 19(1,20)

Exception raised:Expected num5 (at char 19), (line:1, col:20)

Match num3 at loc 19(1,20)

Exception raised:Expected num3 (at char 19), (line:1, col:20)

Match num5 at loc 21(1,22)

Exception raised:Expected num5 (at char 21), (line:1, col:22)

Match num3 at loc 21(1,22)

Exception raised:Expected num3 (at char 21), (line:1, col:22)

Match num5 at loc 22(1,23)

Exception raised:Expected num5 (at char 22), (line:1, col:23)

Match num3 at loc 22(1,23)

Exception raised:Expected num3 (at char 22), (line:1, col:23)

Match num5 at loc 23(1,24)

Exception raised:Expected num5 (at char 23), (line:1, col:24)

Match num3 at loc 23(1,24)

Exception raised:Expected num3 (at char 23), (line:1, col:24)

Match num5 at loc 24(1,25)

Exception raised:Expected num5 (at char 24), (line:1, col:25)

Match num3 at loc 24(1,25)

Exception raised:Expected num3 (at char 24), (line:1, col:25)

Out[3]: ([(['a123'], {})], {})
#### 2010-11-23 12:54:30 - m3mento
In [4]: find_me.searchString('this a12345 is a test')

Match num5 at loc 1(1,2)

Exception raised:Expected num5 (at char 1), (line:1, col:2)

Match num3 at loc 1(1,2)

Exception raised:Expected num3 (at char 1), (line:1, col:2)

Match num5 at loc 2(1,3)

Exception raised:Expected num5 (at char 2), (line:1, col:3)

Match num3 at loc 2(1,3)

Exception raised:Expected num3 (at char 2), (line:1, col:3)

Match num5 at loc 3(1,4)

Exception raised:Expected num5 (at char 3), (line:1, col:4)

Match num3 at loc 3(1,4)

Exception raised:Expected num3 (at char 3), (line:1, col:4)

Match num5 at loc 4(1,5)

Exception raised:Expected num5 (at char 4), (line:1, col:5)

Match num3 at loc 4(1,5)

Exception raised:Expected num3 (at char 4), (line:1, col:5)

Match num5 at loc 6(1,7)

Matched num5 -\> ['12345']

Match num3 at loc 6(1,7)

Matched num3 -\> ['123']

Match num5 at loc 13(1,14)

Exception raised:Expected num5 (at char 13), (line:1, col:14)

Match num3 at loc 13(1,14)

Exception raised:Expected num3 (at char 13), (line:1, col:14)

Match num5 at loc 14(1,15)

Exception raised:Expected num5 (at char 14), (line:1, col:15)

Match num3 at loc 14(1,15)

Exception raised:Expected num3 (at char 14), (line:1, col:15)

Match num5 at loc 16(1,17)

Exception raised:Expected num5 (at char 16), (line:1, col:17)

Match num3 at loc 16(1,17)

Exception raised:Expected num3 (at char 16), (line:1, col:17)

Match num5 at loc 18(1,19)

Exception raised:Expected num5 (at char 18), (line:1, col:19)

Match num3 at loc 18(1,19)

Exception raised:Expected num3 (at char 18), (line:1, col:19)

Match num5 at loc 19(1,20)

Exception raised:Expected num5 (at char 19), (line:1, col:20)

Match num3 at loc 19(1,20)

Exception raised:Expected num3 (at char 19), (line:1, col:20)

Match num5 at loc 20(1,21)

Exception raised:Expected num5 (at char 20), (line:1, col:21)

Match num3 at loc 20(1,21)

Exception raised:Expected num3 (at char 20), (line:1, col:21)

Match num5 at loc 21(1,22)

Exception raised:Expected num5 (at char 21), (line:1, col:22)

Match num3 at loc 21(1,22)

Exception raised:Expected num3 (at char 21), (line:1, col:22)

Out[4]: ([(['a123'], {})], {})
#### 2010-12-01 15:13:51 - m3mento
anyone?

---
## 2010-11-29 11:52:29 - rick0cm - Parse Exception location
Hi,



I am having a problem with how pyparsing handles exceptions on

nested grammars.  The code below shows my issue. My problem is 

that the obvious error - BAD on DATA line 7, results in a 

ParseException pointing at line 3.  



This grammar is intended for parsing fairly large files, and debugging 

the grammar/files is very difficult with the Exception being raised so 

far from the bad grammar in the file.





    keyword = MatchFirst( map(CaselessKeyword,'''alpha beta gamma'''.split()) )
    stmts = Forward()
    singlS = keyword + '(' + Word(nums) + ')' + ';'
    hierS  = keyword + '(' + Word(nums) + ')' + '{' + stmts + '}'
    stmt = singlS ^ hierS
    stmts \<\< OneOrMore(stmt)
    top = stmts
    
    DATA='''\
     alpha(7);
     beta(99);
     alpha(3) {
       gamma(11);
       beta(999) {
         beta(0);
         BAD(1);
         gamma(2);
       }
     }
    '''
    print (top+stringEnd).parseString(DATA)



Is there any way to handle this situation better?



Thanks.. Rick

#### 2010-11-29 16:24:20 - jcress410
i'm no expert, but what about a parserelement 

designed to catch anything not in your 'keyword' set?  



top = stmts + ZeroOrMore(Word(alphas)+literal('\(')+Word(nums)+literal('\)'))



This won't match anything in 'keywords' because its defined as the entire 'bad keyword' 'bad value' pair, and if your 'bad keyword' isn't present, 'ZeroOrMore' should be ok with that. 



Does something like this seem like it might work, or am I misunderstanding your code?
#### 2010-11-29 19:07:45 - ptmcg
Catching the exact place in an input string where a parser gets erroneous input is one of the trickier parts of pyparsing.  Much of the difficulty comes from expressions like ZeroOrMore and Optional, which are tolerant of mismatches in their sub expressions, assuming that some later expression in the parser will be the correct fit.  If the real problem is that somewhere inside the ZeroOrMore or Optional there is a syntax error, this information will be lost.



Here's a simple example.  Let's say we want to match any capital letter, followed either by a number, or a nested list of letter-numbers in parentheses:



    from pyparsing import *
    
    expr = Forward()
    integer = Word(nums)
    expr \<\< (OneOrMore(oneOf(list(alphas)) + (integer | Group('(' + Optional(expr) + ')') ) ) )
    
    test = 'A12B(A7B6C(D77)E(G19))F21'
    print expr.parseString(test, parseAll=True).asList()



This gives:



    ['A', '12', 'B', ['(', 'A', '7', 'B', '6', 'C', ['(', 'D', '77', ')'], 'E', ['(', 'G', '19', ')'], ')'], 'F', '21']



But if I insert a typo in the input string, like this:



    test = 'A12B(A7B6C(D7&)E(G19))F21'
    print expr.parseString(test, parseAll=True).asList()



Then I get this exception:



    pyparsing.ParseException: Expected end of text (at char 3), (line:1, col:4)



That is, the leading 'A12' parses okay, but somewhere in the nested expression after 'B' there is a syntax error.  But this is not so helpful, especially in longer and more complex parsers.



Back around version 1.4.8 or so, I added the ability to insert an 'error stop', a notation where, if you get to a part in the grammar where all the rest of the expression *must* match else its a syntax error.  For instance, taking a bit of your parser, after one of the words 'alpha', 'beta', or 'gamma', there *must* be a left paren and the remaining part of the expression - if the keyword is followed by a '$', say, then stop right there, there is no point in parsing any further, this is an error!



The 'error stop' is a special behavior added to pyparsing's And class, so it is implemented as an alternative to the '+' operator, and I chose the '-' operator.  So I could express this as:



    oneOf('alpha beta gamma') - '(' + etc.



and the '-' tells And to catch any exception that is raised after this point, and rethrow a special exception, a ParseSyntaxException.  Unlike ordinary ParseExceptions, which are used to indicate a routine mismatch of part of a parse expression, ParseSyntaxException indicates that something failed that was fully expected to succeed, and this exception stops all parsing immediately.  With any luck, the location of this exception will fall much closer to the true error.



Now there is another wrinkle in your case, and that is that you have created a typo not after a known valid keyword, but after the list of keywords and their alternatives have been exhausted. I need something that will match first, and then the '-' error stop operator, and then an expression that will always fail.  I found that this parser fragment will do what I want:



    empty - ~Word(printables)



empty will *always* match, and the Word(printables) is a catchall that will match any other non-whitespace characters - preceded by '~' means 'don't want one of these' - so if something matches here, that is an error, which will trigger the error stop.



If I add this as a final alternative for matching the statement contents, then I'll get a much more accurate exception location:



    expr \<\< (OneOrMore(oneOf(list(alphas)) + (integer | Group('(' + Optional(expr) + ')') ) | 
                       empty - ~Word(printables).setName('\<unknown\>') ) )
    
    test = 'A12B(A7B6C(D7&)E(G19))F21'
    print expr.parseString(test, parseAll=True).asList()



And now the exception I get is a ParseSyntaxException, with the exact location of my erroneous '&' character:



    pyparsing.ParseSyntaxException: Found unwanted token, \<unknown\> (at char 13), (line:1, col:14)



Now, how to apply this to your original parser.  First of all, I simplified your parser into a more compact form:



    stmt = Forward()
    stmt \<\< (keyword + '(' + Word(nums) + ')' + 
                        (';' | '{' + ZeroOrMore(stmt) + '}' ) )
    top = OneOrMore(stmt)



Now I just need to add my error stop fragment at the end of stmt, as a last-chance alternative:



    stmt = Forward()
    stmt \<\< (keyword + '(' + Word(nums) + ')' + 
                        (';' | '{' + ZeroOrMore(stmt) + '}' ) | 
                 empty - ~Word(printables).setName('\<unknown\>') )
    top = OneOrMore(stmt)



And with this change, I now get this error:



    pyparsing.ParseSyntaxException: Found unwanted token, \<unknown\> (at char 82), (line:7, col:6)



And this is in fact the location of your 'BAD' keyword.



It does seem like this kind of technique would be good to encapsulate as a pyparsing built-in.  What to name it, I wonder...



Hope this helps, this is not a simple area in pyparsing.



-- Paul
#### 2010-11-29 19:47:51 - jcress410
Great post. Educational read. I agree this could belong in pyparsing.



IMHO it makes the parse operation more 'robust', 



so if its feasible, maybe all parse actions based on parseString() should have a 'robust' method that can be called by 



parseString(..., robust=True)

scanString(..., robust=True)



etc
#### 2010-11-30 12:32:28 - rick0cm
Paul,



Thanks for the great explanation. While I follow your

logic, I believe your proposed solution is a bit too 

simplistic.



While the 



    empty - ~Word(printables).setName('\<unknown\>')

properly detects syntax errors, it also causes

false parse errors on Good input.  It can't handle

the end of nesting - '}' in my grammar, ')' in yours.



I've tried



    CharsNotIn('}') - ~Word(printables).setName('\<unknown\>')

as an alternative to your error catcher. This improves matters

by accepting valid input, but has the side effect of causing

parse errors to be reported at the closing '}' rather than 

the mis-spelled keyword in my example above.



Conceptually, I dislike having to redundantly code the '}' also.



I'm in full agreement 'this is not a simple area in pyparsing' !!



Rick

---
## 2010-11-29 16:19:06 - jcress410 - scanstring and unused text
scanstring returns the stringposition of matches [start, stop] 



what's the easiest way to collect these positions and return the *unparsed* text from a string? 



seems like being able to return a list()

[str(unused text), [begin, end]] or

[str(unused text), begin, end] 



is important for debugging parsers. 



thoughts?

#### 2010-11-29 16:47:09 - ptmcg
Look at how transformString splices the parsed and transformed text in with the unparsed text between matches.  Basically, keep a lastEndPosition variable initialized to 0.  When processing each scanString iteration, the unparsed text is instring[lastEndPosition:startPosn].  Then reset lastEndPosition = endPosn.  Finally, after the scanString for loop, the trailing unparsed bit can be gotten using instring[lastEndPosition:]
#### 2010-11-29 17:55:57 - jcress410
I think it'd be nice if scanString had an option to return either the parsed results or the unparsed text, or both.



Could that be achieved with a decorator over the scanString method?



I mean, your solution makes a ton of sense, and I appreciate the advice.  I'll try to implement it.  I just worry I would be adding too much overhead to the scanString function.  



I'm trying to avoid adding my own bottlenecks.  The code I'm writing now is going to be feeding cpu intensive operations.  



I could be wrong, but if I call instring[lastEndPosition:startPosn] every time I get a result from scanString, I'm duplicating the effort necessary to read the string (i.e. reading it twice) instead of scanString giving me the text it (already) read but didn't parse
#### 2010-11-29 18:03:51 - ptmcg
I wouldn't worry about performance overhead of instring[lastEndPosition:startPosn] - getting a slice of a string is not the same as rereading the string, and is way cheaper than all the parsing logic of looking at each character position and evaluating the parser at the position to see if there is a match.

---
## 2010-12-02 07:49:34 - openIDTemp-624425161 - Making an array and ignoring some words
Hi!

I'm working with the examples but I have a doubt.



If I do this:



text='My name is Marco and im learning pyparsing'

parser=OneOrMore(Word(alphas))



How can I get each word into an array?

something like:

array[0]='My'

array[1]='name'

array[2]='is'

.

.



And another thing, I'm doing something like this:

parser.ignore(CaselessLiteral('-'))



But how can I ignore a list of words??



kind of I need to ingore: '-', '#', '%'



Thanks!

#### 2011-02-14 20:20:19 - ptmcg
Once you create your parser, then you process the source text using one of the parsing methods: parseString, scanString, searchString, or transformString.



For your first question, to get your array, do:





    array = parser.parseString(text)



and then you can access the individual words of array.



ignore takes a pyparsing expression, so if you need to ignore multiple elements, create a pyparsing expression that will match any of them, and then ignore that.



(Sorry not to respond sooner, I don't visit the discussion tab on this page very often.)



-- Paul

---
## 2010-12-09 04:09:52 - andyhhp - bug with indentedBlock
Hello,



I am having problems parsing the Linux Kernel Configuration files (KConfig) and appear to have found a bug with the indentedBlock helper function.



I have written a testcase which demonstrates it:



    <span class="co1">#!/usr/bin/python                                                                                                                                                                                              </span>
    <span class="kw1">from</span> pyparsing <span class="kw1">import</span> *
    <span class="kw1">import</span> <span class="kw3">unittest</span>
    
    ParserElement.<span class="me1">setDefaultWhitespaceChars</span><span class="br0">&#40;</span><span class="st0">' <span class="es0">\t</span><span class="es0">\r</span>'</span><span class="br0">&#41;</span>
    
    indentStack <span class="sy0">=</span> <span class="br0">&#91;</span><span class="nu0">1</span><span class="br0">&#93;</span>
    
    kw_config <span class="sy0">=</span> Keyword<span class="br0">&#40;</span><span class="st0">'config'</span><span class="br0">&#41;</span>
    name <span class="sy0">=</span> Word<span class="br0">&#40;</span>srange<span class="br0">&#40;</span><span class="st0">'[0-9A-Z_]'</span><span class="br0">&#41;</span><span class="br0">&#41;</span>
    bool_expr <span class="sy0">=</span> Keyword<span class="br0">&#40;</span><span class="st0">'bool'</span><span class="br0">&#41;</span> + quotedString + LineEnd<span class="br0">&#40;</span><span class="br0">&#41;</span>
    default_expr <span class="sy0">=</span> Keyword<span class="br0">&#40;</span><span class="st0">'default'</span><span class="br0">&#41;</span> + <span class="br0">&#40;</span>Literal<span class="br0">&#40;</span><span class="st0">'y'</span><span class="br0">&#41;</span> | Literal<span class="br0">&#40;</span><span class="st0">'n'</span><span class="br0">&#41;</span><span class="br0">&#41;</span> + LineEnd<span class="br0">&#40;</span><span class="br0">&#41;</span>
    
    config_attr <span class="sy0">=</span> bool_expr | default_expr
    
    config <span class="sy0">=</span> kw_config + name + LineEnd<span class="br0">&#40;</span><span class="br0">&#41;</span> + indentedBlock<span class="br0">&#40;</span>config_attr<span class="sy0">,</span> indentStack<span class="sy0">,</span> <span class="kw2">True</span><span class="br0">&#41;</span>
    
    
    <span class="kw1">class</span> TestIndentBlock<span class="br0">&#40;</span><span class="kw3">unittest</span>.<span class="me1">TestCase</span><span class="br0">&#41;</span>:
    
        <span class="kw1">def</span> setUp<span class="br0">&#40;</span><span class="kw2">self</span><span class="br0">&#41;</span>:
            <span class="kw1">print</span> <span class="st0">'indentStack: %s'</span> % indentStack
    
        <span class="kw1">def</span> test_config<span class="br0">&#40;</span><span class="kw2">self</span><span class="br0">&#41;</span>:
            <span class="kw3">string</span> <span class="sy0">=</span> <span class="st0">'''config X86_EXTENDED_PLATFORM                                                                                                                                                               
            bool 'Support for extended (non-PC) x86 platforms'                                                                                                                                                     
            default y                                                                                                                                                                                              
    '''</span>
            config.<span class="me1">parseString</span><span class="br0">&#40;</span><span class="kw3">string</span><span class="sy0">,</span> <span class="kw2">True</span><span class="br0">&#41;</span>
    
        <span class="kw1">def</span> test_config2<span class="br0">&#40;</span><span class="kw2">self</span><span class="br0">&#41;</span>:
            <span class="kw3">string</span> <span class="sy0">=</span> <span class="st0">'''config X86_EXTENDED_PLATFORM                                                                                                                                                               
            bool 'Support for extended (non-PC) x86 platforms'                                                                                                                                                     
            default y                                                                                                                                                                                              
    '''</span>
            config.<span class="me1">parseString</span><span class="br0">&#40;</span><span class="kw3">string</span><span class="sy0">,</span> <span class="kw2">True</span><span class="br0">&#41;</span>
    
        <span class="kw1">def</span> tearDown<span class="br0">&#40;</span><span class="kw2">self</span><span class="br0">&#41;</span>:
            <span class="kw1">print</span> <span class="st0">'indentStack: %s'</span> % indentStack
    
    <span class="kw1">if</span> __name__ <span class="sy0">==</span> <span class="st0">'__main__'</span>:
        <span class="kw3">unittest</span>.<span class="me1">main</span><span class="br0">&#40;</span><span class="br0">&#41;</span>



The output is as follows:



    kconfig/indent-bug.py
    indentStack: [1]
    indentStack: [1, 9]
    .indentStack: [1, 9]
    EindentStack: [1, 9]
    
    ======================================================================
    ERROR: test_config2 (__main__.TestIndentBlock)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
      File 'kconfig/indent-bug.py', line 36, in test_config2
        config.parseString(string, True)
      File '/usr/local/lib/python2.6/dist-packages/pyparsing.py', line 1100, in parseString
        raise exc
    ParseException: not a subentry (at char 37), (line:2, col:9)
    
    ----------------------------------------------------------------------
    Ran 2 tests in 0.002s
    
    FAILED (errors=1)



It appears as if indentedBlock does not clean up the indentStack on a successful match.



Is this the expected behaviour?



~Andrew

#### 2010-12-13 03:26:08 - andyhhp
After some debugging of the problem, I have a tentative fix to the indentedBlock function along the lines of:





    ...
        def reset(s, l, t):
            print 'Calling reset'
            while len(indentStack) \> 1:
                indentStack.pop()
    
        NL = OneOrMore(LineEnd().setWhitespaceChars('\t ').suppress())
        INDENT = Empty() + Empty().setParseAction(checkSubIndent)
        PEER   = Empty().setParseAction(checkPeerIndent)
        UNDENT = Empty().setParseAction(checkUnindent)
        RESET  = Empty().setParseAction(reset).suppress()
        if indent:
            smExpr = Group( Optional(NL) +
            INDENT + (OneOrMore( PEER + Group(blockStatementExpr) + Optional(NL) )) + UNDENT + RESET)
        else:
    ...



---
## 2010-12-20 15:50:58 - haidong - Pattern search with pyparsing
sample text:



'''

iojefwewjpojofw domain\login fnwoofwjeifewo

anotherDomain\login1 fwijoipwieojiofw

jfofiwioe domain5\login5

'''



objective: suck alphanums\alphanums patterns out and put them in a list like so:

['domain\login', 'anotherDomain\login1', 'domain5\login5']



Figured I've wasted for too long for such a simple task, so I thought I should ask. Here is some code I've messed with, the next thing I am searching for would be some kind of skip stuff:



from pyparsing import *



In [17]: grammar = Combine(Word(alphanums) + '\\' + Word(alphanums))



In [18]: token = grammar.parseString('jwfoleow fjlwowe\jfoew lwfweolwfo\jofweojw lifewijowe')

<hr />
ParseException                            Traceback (most recent call last)



/home/haidong/\<ipython console\> in \<module\>()



/usr/lib/python2.6/site-packages/pyparsing-1.5.5-py2.6.egg/pyparsing.pyc in parseString(self, instring, parseAll)

   1098                 # catch and re-raise exception from here, clears out pyparsing internal stack trace



   1099                 exc = sys.exc_info()[1]

-\> 1100                 raise exc

   1101         else:

   1102             return tokens



ParseException: Expected '\' (at char 8), (line:1, col:9)





Really appreciate it if you could answer these 3 questions:



1. Does somebody have a short and sweet solution for this simple thing with pyparsing?



2. Taking a step back, is pyparsing even the right tool for task like this?



3. What are the pros and cons for pyparsing vs simpleparse?



Thanks in advance.

#### 2010-12-20 19:21:58 - ptmcg
parseString assumes that your grammar fully describes the content of the input string.  Try searchString or scanString.
#### 2010-12-20 20:48:39 - haidong
Do you have a good example?



I've looked at the examples in the documentation and I didn't see one that I can base my code on, or at least I haven't found a good one yet...
#### 2010-12-20 20:58:43 - ptmcg
searchString is just a different method from parseString (like findall vs. match in regexes).  Here is your posted example, using searchString:





    \>\>\> from pyparsing import *
    \>\>\> grammar = Combine(Word(alphanums) + '\\' + Word(alphanums))
    \>\>\> token = grammar.parseString('jwfoleow fjlwowe\jfoew lwfweolwfo\jofweojw lifewijowe')
    Traceback (most recent call last):
      File '\<stdin\>', line 1, in \<module\>
      File 'c:\python26\lib\site-packages\pyparsing.py', line 1100, in parseString
        raise exc
    pyparsing.ParseException: Expected '\' (at char 8), (line:1, col:9)
    \>\>\> matches = grammar.searchString('jwfoleow fjlwowe\jfoew lwfweolwfo\jofweojw lifewijowe')
    \>\>\> for m in matches:
    ...   print m[0]
    ...
    fjlwowe\jfoew
    lwfweolwfo\jofweojw


#### 2010-12-20 21:34:56 - haidong
Thanks so much Paul. Couldn't believe I was so close, but then again that's when expertise came in that really helped me over the hump, so to speak.



Really appreciate it. Will probably blog about it.
#### 2010-12-21 01:36:33 - ptmcg
For completeness' sake, here is how scanString would look:





    \>\>\> for toks,start,end in grammar.scanString('jwfoleow fjlwowe\jfoew lwfweolwfo\jofweojw lifewijowe'):
    ...    print '%d:%d:%s' % (start,end,toks[0])
    ...
    9:22:fjlwowe\jfoew
    23:42:lwfweolwfo\jofweojw



Since scanString is a generator, it has the advantage of returning an initial match right away.  searchString is really just a simple wrapper around scanString, but since searchString returns the list of all matches, it has to process the entire string.  You can create a pathological example to demonstrate this:



    \>\>\> test = 'A' + 'B'*int(1e6) + 'A'
    \>\>\> Literal('A').searchString(test)
    ([(['A'], {}), (['A'], {})], {})
    \>\>\> for t,s,e in Literal('A').scanString(test):
    ...   print t[0]
    ...
    A
    A



Hm, not so helpful.  Here is the same code, but with calls to time.clock():



    \>\>\> import time
    \>\>\> 
    \>\>\> def deltatime():
    ...   global start
    ...   now = time.clock()
    ...   elapsed = now-start
    ...   start = now
    ...   return elapsed
    ...
    \>\>\> start=time.clock(); Literal('A').searchString(test); print deltatime()
    ([(['A'], {}), (['A'], {})], {})
    3.70045732634
    \>\>\> 
    \>\>\> def scanAll():
    ...   for t,s,e in Literal('A').scanString(test):
    ...     print t[0], deltatime()
    ...
    \>\>\> start=time.clock(); scanAll()
    A 0.00682788304835
    A 3.69812652614



You can see that the first matching value of 'A' returns right away when using scanString, and then a while later, the second 'A' is displayed.  searchString waits until the whole string is processed before showing anything.
#### 2010-12-31 15:15:32 - haidong
Thanks Paul for the detailed explanation. The same thing ran 10.24 seconds on my laptop. You've got a much better machine...

---
## 2010-12-22 09:56:44 - axil - optimization in oneOf code
Line 3316 in version 1.5.5 ( cur = other ) is excess.



Also, rearranging symbols could be slightly optimized:





--- a/pyparsing.py

+++ b/pyparsing.py

@@ -3305,17 +3305,14 @@ def oneOf( strs, caseless=False, useRegex=True ):



     i = 0

     while i \< len(symbols)-1:

-        cur = symbols[i]

-        for j,other in enumerate(symbols[i+1:]):

-            if ( isequal(other, cur) ):

-                del symbols[i+j+1]

-                break

-            elif ( masks(cur, other) ):

-                del symbols[i+j+1]

-                symbols.insert(i,other)

-                cur = other

-                break

-        else:

+        j = i+1

+        while j \< len(symbols):

+            if ( isequal(symbols[i], symbols[j]) ):

+                del symbols[j]

+                continue

+            elif ( masks(symbols[i], symbols[j]) ):

+                symbols[i], symbols[j] = symbols[j], symbols[i]

+            j += 1

         i += 1



#### 2010-12-22 10:07:30 - axil
Or slightly less readable but a bit more efficient and closed to the original code:





    --- a/pyparsing.py
    +++ b/pyparsing.py
    @@ -3306,17 +3306,17 @@ def oneOf( strs, caseless=False, useRegex=True ):
         i = 0
         while i \< len(symbols)-1:
             cur = symbols[i]
    -        for j,other in enumerate(symbols[i+1:]):
    -            if ( isequal(other, cur) ):
    -                del symbols[i+j+1]
    -                break
    +        j = i+1
    +        while j \< len(symbols):
    +            other = symbols[j]
    +            if ( isequal(cur, other) ):
    +                del symbols[j]
    +                continue
                 elif ( masks(cur, other) ):
    -                del symbols[i+j+1]
    -                symbols.insert(i,other)
    -                cur = other
    -                break
    -        else:
    -            i += 1
    +                symbols[i], symbols[j] = other, cur
    +                cur = symbols[i]
    +            j += 1
    +        i += 1
    
         if not caseless and useRegex:


#### 2010-12-22 20:46:21 - ptmcg
Thanks for submitting the patch - I actually think your second version is easier to follow, and it passes all my unit tests, so I'll drop it into the next release.



Thanks!

-- Paul
#### 2010-12-27 05:20:54 - axil
You're welcome. And thank you for this wonderful lib! My use case right now is parsing my custom binary file format. Pyparsing proved to be very handy for this.

---
## 2010-12-24 20:13:24 - sskye - setWhitespaceChars() enhancement?
So, I want to parse a whitespace sensitive language (in the style of Python or Haskell). I have read on the discussion boards that pyparsing is not the best tool for parsing these classes of languages. However, I have come up with a solution that works for 95% of cases.



I created a parser class that maintains a whitespace stack and wraps around an expression to parse on every line, much like pyparsing's indentedBlock(). However, it will also emit INDENT and DEDENT tokens into the token list at the beginning of a line.



This is all well and good, but it has one problem. The wrapped expression MUST NOT consume the newline token. If it does, the whole thing falls apart. The inner expression continues to consume all input, and no INDENT or DEDENT tokens are emitted. I need to force it to not read the newline character.



I could set the default whitespace characters to be only ' ' and '\t', or I could ensure that inner expression never consumes a newline when I create it. However, both of these solutions seem brittle. They assume something about the state of the classes involved.



Instead, I wrote a little method to set the whitespace characters for a ParserResult class, source code follows:





    <span class="kw1">def</span> setWhitespaceChars<span class="br0">&#40;</span>e<span class="sy0">,</span>chars<span class="sy0">,</span>recurse<span class="sy0">=</span><span class="kw2">False</span><span class="br0">&#41;</span>:
      <span class="kw1">if</span> recurse:
        <span class="kw1">if</span> <span class="kw2">isinstance</span><span class="br0">&#40;</span>e<span class="sy0">,</span>ParseExpression<span class="br0">&#41;</span>:
          <span class="kw1">for</span> sub <span class="kw1">in</span> e.<span class="me1">exprs</span>: setWhitespaceChars<span class="br0">&#40;</span>sub<span class="sy0">,</span>chars<span class="sy0">,</span><span class="kw2">True</span><span class="br0">&#41;</span>
        <span class="kw1">elif</span> <span class="kw2">isinstance</span><span class="br0">&#40;</span>e<span class="sy0">,</span>ParseElementEnhance<span class="br0">&#41;</span>:
          setWhitespaceChars<span class="br0">&#40;</span>e.<span class="me1">expr</span><span class="sy0">,</span>chars<span class="sy0">,</span><span class="kw2">True</span><span class="br0">&#41;</span>
      e.<span class="me1">setWhitespaceChars</span><span class="br0">&#40;</span>chars<span class="br0">&#41;</span>



This method attempts to recursively set the whitespace characters for all subexpressions of a given expression. This way, I don't need to assume anything about an expression. I just call this method on it, and it's whitespace characters are set properly.



The thing is, it fails on some expressions. In particular, it appears to fail on some outputs of nestedExpr().



Is it possible to simply redefine setWhitespaceChars() for ParseElementEnhance and ParseExpression so that this problem is handled within pyparsing (and not through some thirdparty hack like mine)? How much effort would it take to do this properly? I am willing to work on a patch if it can be done.

#### 2010-12-24 20:16:16 - sskye
Where I said ParserResult above, I meant to say, ParserElement. Sorry for any confusion.
#### 2010-12-24 22:13:42 - ptmcg
I've thought about some similar API calls in the past, specifically for those grammars that have significant newlines, something like a module level call that would set up the INDENT and DEDENT stack, and call ParserElement.setDefaultWhitespaceChars(' \t').



I think that last bit is actually what you are looking for. Unlike the instance method setWhitespaceChars,  ParseElement.setDefaultWhitespaceChars is a class-level method that sets the characters that will be treated as whitespace for all subsequently defined parsing expressions.  You should call this immediately after importing pyparsing and before defining any pyparsing expressions. Did you try this?



-- Paul
#### 2010-12-25 15:24:11 - sskye
Paul,



Thanks for your taking the time to answer questions! And Merry Christmas to you!



Yes, I knew I could set setDefaultWhitespaceChars() and it would work properly.



I just thought it would be more flexible for future development to set the whitespace characters only for the expressions that need them, while leaving the rest untouched. 



Could I emulate this functionality using setDefaultWhitespaceChars() by doing the following:



1) Saving the default whitespace chars in a variable, say 'defualtChars'

2) Calling setDefaultWhitespaceChars(' \t')

3) Creating my parsing expressions.

4) Calling setDefaultWhitespaceChars(defaultChars)



Or would the second call to setDefaultWhitespaceChars() reset the default characters for ALL parsing expressions?



Thanks!



Stephen
#### 2010-12-25 15:45:33 - sskye
By the way, I don't know how 'good' my code is, but if you're wanting an API call to parse Pythonic-style languages with significant whitespace, I can email you my code.



It subclasses ParserElement, so it's simple to use. If you like it, you can license it under the same license as pyparsing.
#### 2010-12-26 07:30:03 - ptmcg
Yes, I would be very interested in seeing your code. You can email it to my sourceforge user account ptmcg@users.sourceforge.net, or paste it to the pypyparsing pastebin at pyparsing.pastebin.com, and just post the pastebin link on this thread.



To answer your earlier question, setDefaultWhitespaceChars does not change any existing expressions' definitions of whitespace characters.  It just changes a class level variable in ParserElement, which is read at init time when new expressions are created.



Welcome to pyparsing!

-- Paul

---
## 2010-12-27 08:31:19 - ptmcg - Flurry of pyparsing
It seems the level of activity on this list has surged in the last few weeks.  Did everyone get Mark Summerfield's book for Christmas?

#### 2010-12-27 08:52:51 - ptmcg
Or Jeff McNeil's new book?  (Links to both on the  page.)

---
## 2010-12-27 17:01:00 - bruceba - parsing a proprietary config language (noob)
I'm trying to parse strings that look like:





    fooConfig { 
      name0
      name1 value1
      name2 value2
    }
    barConfig { name1 value1 }
    bapConfig { }
    bazConfig { 'str1' 'str2' 'str3' 'str4' }



So I have the cases:

1) id { }

2) id { name\n ... }

3) id { name value\n ... }

4) id { 'str1' ... }



so I have either lists of names, name/value pairs separated by linefeeds (values can be quoted strings), or quoted strings. Don't blame me, I didn't invent the language.



I can parse cases 1, 2, and 3 with:





    LBRC, RBRC = map(Suppress, '{}')
    configId = Word(alphanums)
    configNV = Group(configId + LineEnd() |
      configId + configId + LineEnd())
    configContainer = configId + LBRC + ZeroOrMore(configNV) + RBRC



but I haven't been able to figure out how to use pyparsing to also parse case 4. Any help would be appreciated.

#### 2010-12-27 18:15:20 - ptmcg
What about:



    configContainer = configId + LBRC + (ZeroOrMore(configNV) | 
                                         ZeroOrMore(quotedString)) + RBRC



-- Paul
#### 2010-12-28 09:10:19 - bruceba
To make it work, I had to look ahead to see if the configuration was either a line end or a '}':





    configId = Word(alphanums)
    configValue = configId | quotedString
    configEnd = LineEnd().suppress() | FollowedBy(RBRC)
    configNV = Group(configId + configEnd | \
               configId + configValue + configEnd)
    configStrs = Group(OneOrMore(quotedString) + configEnd)
    configNVs = OneOrMore(configNV)
    configContainer = configId + LBRC + ZeroOrMore(configNVs | configStrs) + RBRC



Is this kosher or am I missing something?



