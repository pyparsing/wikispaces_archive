## Pyparsing Wikispaces Discussion - 2011

_[Note: these entries are fairly old, and predate many new features of pyparsing,
and are predominantly coded using Python 2.
They are captured here for historical benefit, but may not contain
the most current practices or features. We will try to add editor
notes to entries to indicate when discussions have been 
overtaken by development events.]_

[2011-01-02 09:55:16 - ArthurPuty - Pickling ParseResults with Python3](all_wiki_discussion_toc_2011.md#2011-01-02-095516---arthurputy---pickling-parseresults-with-python3)  
[2011-01-03 14:08:33 - matthew.brett - Bibtex parser - for examples?](all_wiki_discussion_toc_2011.md#2011-01-03-140833---matthewbrett---bibtex-parser---for-examples)  
[2011-01-05 09:32:33 - jasto - Space as a valid token](all_wiki_discussion_toc_2011.md#2011-01-05-093233---jasto---space-as-a-valid-token)  
[2011-01-06 00:40:04 - majortal - Boosting Pyparsing using regular expressions](all_wiki_discussion_toc_2011.md#2011-01-06-004004---majortal---boosting-pyparsing-using-regular-expressions)  
[2011-01-10 22:19:06 - codeape - Why are comments at end not ignored when using parseAll=True?](all_wiki_discussion_toc_2011.md#2011-01-10-221906---codeape---why-are-comments-at-end-not-ignored-when-using-parsealltrue)  
[2011-01-11 06:38:22 - majortal - Bug in pyparsing?](all_wiki_discussion_toc_2011.md#2011-01-11-063822---majortal---bug-in-pyparsing)  
[2011-01-14 07:39:29 - DesiLinguist - Help with originalTextFor and transformString](all_wiki_discussion_toc_2011.md#2011-01-14-073929---desilinguist---help-with-originaltextfor-and-transformstring)  
[2011-01-29 11:58:50 - Griffon26 - Preserving whitespace with transformString](all_wiki_discussion_toc_2011.md#2011-01-29-115850---griffon26---preserving-whitespace-with-transformstring)  
[2011-02-04 05:33:31 - voltron8 - Need help. Parse float numbers and functions](all_wiki_discussion_toc_2011.md#2011-02-04-053331---voltron8---need-help-parse-float-numbers-and-functions)  
[2011-02-14 09:42:49 - DesiLinguist - Weird behavior with MatchFirst](all_wiki_discussion_toc_2011.md#2011-02-14-094249---desilinguist---weird-behavior-with-matchfirst)  
[2011-02-16 12:55:46 - Icoo - Simple parser or code simulator](all_wiki_discussion_toc_2011.md#2011-02-16-125546---icoo---simple-parser-or-code-simulator)  
[2011-02-18 11:46:14 - victorsmiller - Getting the text in comments](all_wiki_discussion_toc_2011.md#2011-02-18-114614---victorsmiller---getting-the-text-in-comments)  
[2011-02-20 16:41:52 - xuru1 - Parsing time increases when parsed repeatedly](all_wiki_discussion_toc_2011.md#2011-02-20-164152---xuru1---parsing-time-increases-when-parsed-repeatedly)  
[2011-02-24 09:24:19 - DiaaFayed - apostroph](all_wiki_discussion_toc_2011.md#2011-02-24-092419---diaafayed---apostroph)  
[2011-02-26 18:17:25 - dwelch91 - Handling missing operand](all_wiki_discussion_toc_2011.md#2011-02-26-181725---dwelch91---handling-missing-operand)  
[2011-03-02 21:30:38 - cmurphycode - Help with nested result names](all_wiki_discussion_toc_2011.md#2011-03-02-213038---cmurphycode---help-with-nested-result-names)  
[2011-03-06 11:13:40 - Zedric123 - make dict case insensitive](all_wiki_discussion_toc_2011.md#2011-03-06-111340---zedric123---make-dict-case-insensitive)  
[2011-03-07 13:25:42 - _Mot_ - Recursiv parsing: Unexpected abort](all_wiki_discussion_toc_2011.md#2011-03-07-132542---_mot_---recursiv-parsing-unexpected-abort)  
[2011-03-14 15:28:00 - muhackl - Strange behavior when reusing index string variables for parse actions](all_wiki_discussion_toc_2011.md#2011-03-14-152800---muhackl---strange-behavior-when-reusing-index-string-variables-for-parse-actions)  
[2011-03-17 03:49:55 - ashish.sawankar - How to match empty\/null token in a delimited list](all_wiki_discussion_toc_2011.md#2011-03-17-034955---ashishsawankar---how-to-match-empty\null-token-in-a-delimited-list)  
[2011-03-18 06:40:54 - flyingeek - Multiline parser fail](all_wiki_discussion_toc_2011.md#2011-03-18-064054---flyingeek---multiline-parser-fail)  
[2011-04-18 23:46:18 - chihiro - Plans for future releases](all_wiki_discussion_toc_2011.md#2011-04-18-234618---chihiro---plans-for-future-releases)  
[2011-04-21 15:31:44 - haggisbones - Parsing a filter expression with nestedExpr](all_wiki_discussion_toc_2011.md#2011-04-21-153144---haggisbones---parsing-a-filter-expression-with-nestedexpr)  
[2011-04-21 20:10:23 - JesterEE - removeQuotes Potential Volatile](all_wiki_discussion_toc_2011.md#2011-04-21-201023---jesteree---removequotes-potential-volatile)  
[2011-04-24 08:11:20 - DragonSpawn - Subsequent calls give different results.](all_wiki_discussion_toc_2011.md#2011-04-24-081120---dragonspawn---subsequent-calls-give-different-results)  
[2011-05-02 08:11:43 - jenia2009 - simple calculator](all_wiki_discussion_toc_2011.md#2011-05-02-081143---jenia2009---simple-calculator)  
[2011-05-06 02:06:33 - BrenBarn - Matching tags](all_wiki_discussion_toc_2011.md#2011-05-06-020633---brenbarn---matching-tags)  
[2011-05-10 09:21:40 - skeeved - help with delimitedList](all_wiki_discussion_toc_2011.md#2011-05-10-092140---skeeved---help-with-delimitedlist)  
[2011-05-28 21:42:47 - basilevs - How to extract named data from ParseResults with order preserved?](all_wiki_discussion_toc_2011.md#2011-05-28-214247---basilevs---how-to-extract-named-data-from-parseresults-with-order-preserved)  
[2011-06-03 01:09:45 - sebpiq - Need help with building a nested expression](all_wiki_discussion_toc_2011.md#2011-06-03-010945---sebpiq---need-help-with-building-a-nested-expression)  
[2011-06-04 14:45:57 - pf_moore - Parser for multiple items with no delimiter](all_wiki_discussion_toc_2011.md#2011-06-04-144557---pf_moore---parser-for-multiple-items-with-no-delimiter)  
[2011-06-10 12:12:08 - ChiefDan - Getting started, couple of questions](all_wiki_discussion_toc_2011.md#2011-06-10-121208---chiefdan---getting-started-couple-of-questions)  
[2011-06-12 05:17:38 - gareth8118 - Grammar embedded in original text - how to handle?](all_wiki_discussion_toc_2011.md#2011-06-12-051738---gareth8118---grammar-embedded-in-original-text---how-to-handle)  
[2011-06-13 05:36:36 - ChiefDan - Documentation page does not exist](all_wiki_discussion_toc_2011.md#2011-06-13-053636---chiefdan---documentation-page-does-not-exist)  
[2011-06-16 20:34:52 - grantma1 - StringEnd() hiding ParseException from parse failure at end of string or file ](all_wiki_discussion_toc_2011.md#2011-06-16-203452---grantma1---stringend-hiding-parseexception-from-parse-failure-at-end-of-string-or-file)  
[2011-06-17 12:57:20 - JesterEE - CharsNotIn Enhancement Request](all_wiki_discussion_toc_2011.md#2011-06-17-125720---jesteree---charsnotin-enhancement-request)  
[2011-06-20 01:49:10 - haikalle - How to read a file and take info](all_wiki_discussion_toc_2011.md#2011-06-20-014910---haikalle---how-to-read-a-file-and-take-info)  
[2011-06-21 09:07:47 - ptmcg - FANTASTIC PyCon 2011 Singapore Parsing presentation!!!](all_wiki_discussion_toc_2011.md#2011-06-21-090747---ptmcg---fantastic-pycon-2011-singapore-parsing-presentation)  
[2011-06-21 09:20:11 - PilgrimBeart - SetResultsName - level confusion when used on Group?](all_wiki_discussion_toc_2011.md#2011-06-21-092011---pilgrimbeart---setresultsname---level-confusion-when-used-on-group)  
[2011-06-22 08:06:39 - jasisz - simpleBool example on Python 3](all_wiki_discussion_toc_2011.md#2011-06-22-080639---jasisz---simplebool-example-on-python-3)  
[2011-06-24 16:00:50 - stalwarts - Can someone explain ?](all_wiki_discussion_toc_2011.md#2011-06-24-160050---stalwarts---can-someone-explain-)  
[2011-06-25 13:19:33 - gareth8118 - Packaging recommendations?](all_wiki_discussion_toc_2011.md#2011-06-25-131933---gareth8118---packaging-recommendations)  
[2011-06-29 17:19:07 - NathanW3 - setParseAction no tokens passed](all_wiki_discussion_toc_2011.md#2011-06-29-171907---nathanw3---setparseaction-no-tokens-passed)  
[2011-07-06 17:19:29 - joslin01 - Reading parse results dump](all_wiki_discussion_toc_2011.md#2011-07-06-171929---joslin01---reading-parse-results-dump)  
[2011-07-09 23:28:16 - basilevs - Futher problems](all_wiki_discussion_toc_2011.md#2011-07-09-232816---basilevs---futher-problems)  
[2011-07-11 23:34:33 - whoozle - Cannot implement recursive grammar](all_wiki_discussion_toc_2011.md#2011-07-11-233433---whoozle---cannot-implement-recursive-grammar)  
[2011-07-12 19:49:16 - mehrdadfeller - Verilog Parser](all_wiki_discussion_toc_2011.md#2011-07-12-194916---mehrdadfeller---verilog-parser)  
[2011-07-13 14:17:23 - sdmolloy - Simple nextedExpr parser](all_wiki_discussion_toc_2011.md#2011-07-13-141723---sdmolloy---simple-nextedexpr-parser)  
[2011-07-14 05:40:28 - joslin01 - Where is definition of "t" when initializing parse action classes?](all_wiki_discussion_toc_2011.md#2011-07-14-054028---joslin01---where-is-definition-of-t-when-initializing-parse-action-classes)  
[2011-07-14 06:25:48 - sdmolloy - Nested parser similar to jsonParser](all_wiki_discussion_toc_2011.md#2011-07-14-062548---sdmolloy---nested-parser-similar-to-jsonparser)  
[2011-07-15 12:55:24 - sdmolloy - Dict where multiple keys have the same name](all_wiki_discussion_toc_2011.md#2011-07-15-125524---sdmolloy---dict-where-multiple-keys-have-the-same-name)  
[2011-07-16 10:58:23 - joslin01 - parsing with indentation](all_wiki_discussion_toc_2011.md#2011-07-16-105823---joslin01---parsing-with-indentation)  
[2011-07-19 17:19:50 - mehrdadfeller - from tokens to strings](all_wiki_discussion_toc_2011.md#2011-07-19-171950---mehrdadfeller---from-tokens-to-strings)  
[2011-07-21 04:23:48 - nimrodra - Using operatorPrecedence for AST](all_wiki_discussion_toc_2011.md#2011-07-21-042348---nimrodra---using-operatorprecedence-for-ast)  
[2011-07-23 08:21:11 - joslin01 - 'str' object is not callable](all_wiki_discussion_toc_2011.md#2011-07-23-082111---joslin01---str-object-is-not-callable)  
[2011-07-23 13:53:30 - joslin01 - indentBlock function](all_wiki_discussion_toc_2011.md#2011-07-23-135330---joslin01---indentblock-function)  
[2011-07-27 06:09:29 - natgiot - inconsistent behavior of CloseMtch depending on number of errors](all_wiki_discussion_toc_2011.md#2011-07-27-060929---natgiot---inconsistent-behavior-of-closemtch-depending-on-number-of-errors)  
[2011-07-28 16:04:29 - espeed - Parsing a Nutch dump file](all_wiki_discussion_toc_2011.md#2011-07-28-160429---espeed---parsing-a-nutch-dump-file)  
[2011-07-29 04:12:31 - gotcha1 - Methodology to write\debug BNF](all_wiki_discussion_toc_2011.md#2011-07-29-041231---gotcha1---methodology-to-write\debug-bnf)  
[2011-07-30 06:36:53 - gotcha1 - Uncertainty with cppStyleComment](all_wiki_discussion_toc_2011.md#2011-07-30-063653---gotcha1---uncertainty-with-cppstylecomment)  
[2011-08-03 23:27:20 - BrenBarn - Generalized escape mechanism?](all_wiki_discussion_toc_2011.md#2011-08-03-232720---brenbarn---generalized-escape-mechanism)  
[2011-08-04 06:13:04 - pbouda - Python 3 question](all_wiki_discussion_toc_2011.md#2011-08-04-061304---pbouda---python-3-question)  
[2011-08-05 20:45:33 - BrenBarn - Set attributes on ParseResults](all_wiki_discussion_toc_2011.md#2011-08-05-204533---brenbarn---set-attributes-on-parseresults)  
[2011-08-06 01:30:50 - BrenBarn - parseActions and ParseResults attributes](all_wiki_discussion_toc_2011.md#2011-08-06-013050---brenbarn---parseactions-and-parseresults-attributes)  
[2011-08-06 07:12:08 - vadeskoc - _trim_arity trickiness](all_wiki_discussion_toc_2011.md#2011-08-06-071208---vadeskoc---_trim_arity-trickiness)  
[2011-08-08 12:47:54 - BrenBarn - Transformations on parse expressions](all_wiki_discussion_toc_2011.md#2011-08-08-124754---brenbarn---transformations-on-parse-expressions)  
[2011-08-09 08:42:50 - almoni - Different Instances Of The Same Rule](all_wiki_discussion_toc_2011.md#2011-08-09-084250---almoni---different-instances-of-the-same-rule)  
[2011-08-10 02:51:46 - almoni - Exact Matches as Default](all_wiki_discussion_toc_2011.md#2011-08-10-025146---almoni---exact-matches-as-default)  
[2011-08-12 02:51:12 - kumaranar - Parsing Nested function calls in expression.](all_wiki_discussion_toc_2011.md#2011-08-12-025112---kumaranar---parsing-nested-function-calls-in-expression)  
[2011-08-18 01:21:25 - BrenBarn - pyparsing hiding exceptions](all_wiki_discussion_toc_2011.md#2011-08-18-012125---brenbarn---pyparsing-hiding-exceptions)  
[2011-08-20 04:14:22 - jfosorio - setResultsName and setParseAction](all_wiki_discussion_toc_2011.md#2011-08-20-041422---jfosorio---setresultsname-and-setparseaction)  
[2011-08-25 12:14:21 - furrykef - alphas is locale-dependent in Python 2.x](all_wiki_discussion_toc_2011.md#2011-08-25-121421---furrykef---alphas-is-locale-dependent-in-python-2x)  
[2011-08-29 07:11:49 - qlppk - can't pase properly](all_wiki_discussion_toc_2011.md#2011-08-29-071149---qlppk---cant-pase-properly)  
[2011-08-29 12:08:07 - firecore - help with a virtual sql parser](all_wiki_discussion_toc_2011.md#2011-08-29-120807---firecore---help-with-a-virtual-sql-parser)  
[2011-08-29 17:31:29 - adilh - parsing words with underscore](all_wiki_discussion_toc_2011.md#2011-08-29-173129---adilh---parsing-words-with-underscore)  
[2011-09-02 03:46:41 - MattWarren2 - New to PyParsing, parsing a simple text structure](all_wiki_discussion_toc_2011.md#2011-09-02-034641---mattwarren2---new-to-pyparsing-parsing-a-simple-text-structure)  
[2011-09-02 14:27:05 - tdris01 - Hello World example? ](all_wiki_discussion_toc_2011.md#2011-09-02-142705---tdris01---hello-world-example-)  
[2011-09-07 07:41:36 - MattWarren2 - Matching whitespace and preserving the token](all_wiki_discussion_toc_2011.md#2011-09-07-074136---mattwarren2---matching-whitespace-and-preserving-the-token)  
[2011-09-11 06:24:09 - almoni - Prioritization](all_wiki_discussion_toc_2011.md#2011-09-11-062409---almoni---prioritization)  
[2011-09-11 22:55:12 - vsemionov - Possible bug in CharsNotIn](all_wiki_discussion_toc_2011.md#2011-09-11-225512---vsemionov---possible-bug-in-charsnotin)  
[2011-09-15 02:02:02 - jcronje - Newlines in quoted strings](all_wiki_discussion_toc_2011.md#2011-09-15-020202---jcronje---newlines-in-quoted-strings)  
[2011-09-25 18:58:24 - bramsc - Whitespaces](all_wiki_discussion_toc_2011.md#2011-09-25-185824---bramsc---whitespaces)  
[2011-09-28 18:25:41 - stalwarts - Help on improving performance ...](all_wiki_discussion_toc_2011.md#2011-09-28-182541---stalwarts---help-on-improving-performance-)  
[2011-09-29 23:20:26 - stalwarts - parser help](all_wiki_discussion_toc_2011.md#2011-09-29-232026---stalwarts---parser-help)  
[2011-09-30 08:25:58 - fidlej - Invalid inputs](all_wiki_discussion_toc_2011.md#2011-09-30-082558---fidlej---invalid-inputs)  
[2011-10-05 11:11:40 - graingert - Can't seem to get parseaction working on a originalTextFor token](all_wiki_discussion_toc_2011.md#2011-10-05-111140---graingert---cant-seem-to-get-parseaction-working-on-a-originaltextfor-token)  
[2011-10-05 11:26:57 - rmayr - Location of parsed tokens](all_wiki_discussion_toc_2011.md#2011-10-05-112657---rmayr---location-of-parsed-tokens)  
[2011-10-11 02:01:33 - zukifoo - nestedExpr() and newlines howto?](all_wiki_discussion_toc_2011.md#2011-10-11-020133---zukifoo---nestedexpr-and-newlines-howto)  
[2011-10-14 19:45:58 - cadourian - Handling carriage return](all_wiki_discussion_toc_2011.md#2011-10-14-194558---cadourian---handling-carriage-return)  
[2011-10-15 13:51:30 - projetmbc - Fist steps with pyparsing](all_wiki_discussion_toc_2011.md#2011-10-15-135130---projetmbc---fist-steps-with-pyparsing)  
[2011-10-18 07:56:02 - almoni - NotAny Question](all_wiki_discussion_toc_2011.md#2011-10-18-075602---almoni---notany-question)  
[2011-10-21 18:03:40 - cadourian - Extracting a hierarchical tree out of a parsed result](all_wiki_discussion_toc_2011.md#2011-10-21-180340---cadourian---extracting-a-hierarchical-tree-out-of-a-parsed-result)  
[2011-10-28 16:07:02 - stalwarts - XML Parser Help](all_wiki_discussion_toc_2011.md#2011-10-28-160702---stalwarts---xml-parser-help)  
[2011-10-31 04:13:18 - marcmolla - Sequencial parsing](all_wiki_discussion_toc_2011.md#2011-10-31-041318---marcmolla---sequencial-parsing)  
[2011-11-01 05:12:16 - freshkiwi - doing something wrong](all_wiki_discussion_toc_2011.md#2011-11-01-051216---freshkiwi---doing-something-wrong)  
[2011-11-03 13:00:51 - spastor2 - RightMost match or last match](all_wiki_discussion_toc_2011.md#2011-11-03-130051---spastor2---rightmost-match-or-last-match)  
[2011-11-04 16:44:00 - chandreas - Need help with NotAny](all_wiki_discussion_toc_2011.md#2011-11-04-164400---chandreas---need-help-with-notany)  
[2011-11-22 23:40:15 - jcronje - Where to begin to see why a parser is slow?](all_wiki_discussion_toc_2011.md#2011-11-22-234015---jcronje---where-to-begin-to-see-why-a-parser-is-slow)  
[2011-11-23 07:49:04 - masura-san - Issue with grammar with empty line support](all_wiki_discussion_toc_2011.md#2011-11-23-074904---masura-san---issue-with-grammar-with-empty-line-support)  
[2011-11-30 01:16:14 - jimcollum - New user of pyparser](all_wiki_discussion_toc_2011.md#2011-11-30-011614---jimcollum---new-user-of-pyparser)  
[2011-12-14 07:58:16 - techtonik - Split\/join without the loss of formatting](all_wiki_discussion_toc_2011.md#2011-12-14-075816---techtonik---splitjoin-without-the-loss-of-formatting)  
[2011-12-14 23:03:30 - techtonik - [patch] ParseResults in API doc](all_wiki_discussion_toc_2011.md#2011-12-14-230330---techtonik---patch-parseresults-in-api-doc)  
[2011-12-22 02:44:02 - denj - Transform embedded HTML to latex and replace newline](all_wiki_discussion_toc_2011.md#2011-12-22-024402---denj---transform-embedded-html-to-latex-and-replace-newline)  


---
## 2011-01-02 09:55:16 - ArthurPuty - Pickling ParseResults with Python3
Has anyone managed to pickle a ParseResults object using Python3? This worked back in April using Python 2.6, but doesn't seem to using Python 3.1.2 and Pyparsing 1.5.5. For example:


    rom pyparsing import Word, alphas
    import pickle
    
    two_words = Word(alphas) + Word(alphas)
    parse_result = two_words.parseString('Cat Dog')
    print(parse_result, type(parse_result))
    
    pickled_var = pickle.dumps(parse_result, 3)
    print(pickled_var)


The pickle works for protocol levels 0 and 1, but not for levels 2 and 3. Am I doing something wrong?

#### 2011-01-03 02:12:07 - ptmcg
In Python2.6, the default protocol was 0, but in Python3.x, the default is now protocol 3.  In my testing (with Py2.6.6), pickling ParseResults works for protocols 0 and 1, but fails for 2. So no, you did nothing wrong, Python just changed the default from protocol 0 to protocol 3, which is apparently also incompatible for ParseResults objects.

Protocol 2 requires the target object to implement `__reduce__` or `__reduce_ex__` methods, which ParseResults don't have.  The docs for the pickle module are not overly encouraging regarding implementing these methods. I don't have any experience using Python3 and pickle.  I suspect that sometime this year, I'll switch over my main support for pyparsing to be Python 3, and freeze/discontinue pyparsing enhancements for Python 2.x versions.

---
## 2011-01-03 14:08:33 - matthew.brett - Bibtex parser - for examples?
Hi,

I written a bibtex parser using pyparsing.  It was a very pleasant experience, thank you for writing such a nice syntax.

I put the result here:



Can I contribute the code to the examples somehow?

Thanks a lot,

Matthew

#### 2011-01-03 17:29:28 - ptmcg
Matthew -

I'm glad pyparsing was a good fit for you - I'd be happy to include your code with the pyparsing code examples, but I could not follow your Github link. Could you post your parser at the pyparsing pastebin? (pyparsing.pastebin.com)

For comparison, there is another Bibtex parser linked from the Whos Using Pyparsing page: 

Cheers,
-- Paul
#### 2011-01-04 14:57:33 - matthew.brett
Thanks for the pointer to pybtex.  I only offer my parser because it's just two files and so maybe useful for a stand-alone example.

main module here:


tests here:


Best,

Matthew

---
## 2011-01-05 09:32:33 - jasto - Space as a valid token
Hi Paul,
Thanks for the help on Stack Overflow.  The solution you recommend does work, but the way I'm approaching the problem is a bit different from yours, so I would like your input.

The original problem involved parsing ASCII dumps that can include spaces.



    l i n e . w . a .   s p a c e .
    . a n o t h e r . s t r i n g .
    . e t c . . . . . . . . . . . .


A condensed version of your suggested approach was:


    # expression for a single char or space
    dumpchar = oneOf(list(printables)+[' ']).leaveWhitespace()
    
    # expression for a whole line of dump chars - intervening spaces will
    # be discarded by delimitedList
    dumpline = delimitedList(dumpchar, delim=White(' ',exact=1)) + LineEnd().suppress()
    
    # read dumped lines from hexdump
    for t in dumpline.searchString(hexdump):
        print ''.join(t)


I couldn't get the `LineEnd()` to work.  Turns out I'm not passing the target text line by line like you show above, but instead (I'm new to pyparsing) I'm passing the entire report in one go with the equivalent of:

    result = dumpline.parseString(hexdump):

Q1: This is a relatively short report.  Is using parseString() not the recommended approach?

Q2: Is my diagnosis correct that LineEnd() isn't working because I'm using parseString() on the entire string rather than passing in a line at a time?

Q3: The Linux newlines (0x0A) are present at the end of each line and are not needed for my purposes, is there a LineEnd() equivalent that would interpret (0x0A) as a Line End?

Thanks
jasto

#### 2011-01-05 16:15:30 - ptmcg
To use parseString, you'll need and expression that describes the whole string, or at least the first part of it:


    result = OneOrMore(dumpline).parseString(hexdump)

If there are some header lines before the start of the dump, then you'll need to add parse expressions for them too.
#### 2011-01-06 13:23:50 - jasto
Since I am verifying the (layout) of the entire report, I do have parse expressions for the entire report.

Thanks for all your great help!

jasto

---
## 2011-01-06 00:40:04 - majortal - Boosting Pyparsing using regular expressions
Hi all,

I would like to try to boost PP performance using the regex engine. Most (yet not all) of the PP constructs can be easily converted to regular expressions gaining a HUGE speed boost.
I'm at a point where I either do a major change IN PP or I leave it to find some other (less convenient) parser. I'm looking to explore the first option with you guys.

Here is a draft of the Regex class: 

The idea is that usually, 2 regular expressions can be merged to a single (yet super efficient) single regular expressions removing the zillion function calls inherent in PP design. This can go on until, theoretically, your entire grammar (at least the parsing part) is a huge RE which is compiled to C by Python.

I'm playing with this code now (and I already see a speed boost) yet I need help (especially with the god-awful ParseResults class...). 

Thanks!

Tal.

#### 2011-01-06 02:04:56 - majortal
limitations (that I found while playing with it):  

Result names now need to comply with RE group name requirements (e.g. no space in string). No big deal.

Cannot have the same group name appear more than once in the regex! This is a limitation of the Python (and most) RE engines... Hmmm... I think I can overcome this - when I merge 2 regexes I need to search for duplicate names. I can then un-dup them by added a unique suffix to them. I can remove the suffix at the parseImpl().
#### 2011-01-12 14:22:17 - majortal
Here is the latest draft.
I'm getting 2X on my (very complex) grammar:


#### 2011-01-12 17:02:44 - ptmcg
Very nice! I'll give it a closer look this weekend.

-- Paul
#### 2011-01-12 22:49:15 - majortal
Thanks! I was hoping you would.
I'm having a hard time reverse engineering the code.
Here are my tests:


I also added these to play with the results:


    
        def the_dict(self):
            '''get the internal dict'''
            return self.__tokdict
    
        def the_accum_names(self):
            '''Get the internal accum names'''
            return self.__accumNames
    
        def accum_name(self, name):
            '''Accumulate the name'''
            self.__accumNames[name] = 0
    


and modified Each to support Optional regexes.
#### 2011-01-13 13:04:07 - majortal
Updated the regex class to remove some (ok, most) of the auto-names I was adding:


#### 2011-02-20 16:44:27 - xuru1
Anything happening on this front?  I'd definitely be interested in boosting my parsing time.
#### 2011-02-20 17:20:09 - ptmcg
I like this a lot, I'm pulling together the various updates and patches that have been sent in, and this will be a very welcome performance improvement.  @xuru1, you could probably just edit your local version of pyparsing and drop in the Regex class from Tal's pastebin posting, if you want to work with it sooner.

-- Paul
#### 2011-02-20 19:28:28 - ptmcg
Tal -

Your baseline Regex class is not from the latest release of pyparsing - I made a minor change in 1.5.5 for Regex to accept a compiled RE as the pattern argument to the constructor. If you get the latest code, you can see what the changes are at the top of the __init__ method, including the definition of the compiledREtype variable (used to detect if the given pattern is an already-compiled RE).

I dropped your Regex class into my unit testing environment.  Your implementation of __or__ works fine.  __and__ will be a little trickier, since the generated regex will also have to handle the whitespace skipping.  

In your posted code, you insert `[\s,]*` between the two expressions. Why the comma? This is not standard pyparsing skipping behavior. Also, we should really insert an expression representing expr2's defined whitespace chars - the default chars are ' \t\n', but this is not always the case. In fact, because '\n' is a default whitespace char, will you need to compile the combined RE with the multiline flag?  And if expr2 has an ignore expression (say for skipping over comments), then all bets are off and you'll have to just revert to ordinary And behavior.

What is the purpose of the generated results names?

Will merging expressions into a single Regex hide the separate tokens matched for each expression? Perhaps this is what the generated results names are for?

I also dabbled a little with adding `__new__` methods on some of the base classes like `Literal`, to return a `Regex(matchstring)` instead of a `Literal`. The more objects that you can get to create `Regex`'s, the more efficiency you'll get from the modified `__add__` and `__or__` methods.  I had some complications with `Literal`, but it turned out the problem was with `CaselessLiteral`, which extends `Literal`. By making some assumptions about whitespace, I am close to passing all my unit tests.

I can see __or__ working out, but `__and__` has some serious potential complications.  Still, getting more code to compile to Regex's should help overall parsing performance.

Thanks for putting this effort into this design idea.  I'm pretty sure at least some parts of it will pay off.

-- Paul
#### 2011-02-21 13:12:58 - majortal
Wow - thanks for trying this code!
I was waiting for you to join the effort before making some more changes.

`[\s,]*` - I used this separator because it is my grammar's default whitespace. This should obviously be replaced with the whitespace chars from the expressions.

(yes,  I have lots of context-specific optimizations and I also did not merge the latest branch. sorry)

I don't think `re.MULTILINE` is needed, even if \n is IN the whitespace chars. See 

Yes, for now lets revert to original functionality if an ignore expression exists (I don't use those - they are EXTREMELY slow).
((May think about merging the ignore expression if both the ignore-expression and the expression itself are regexes.))

I tried very hard to maintain the original functionality so the the 
ParseResults object resulting would be the same with or without the optimization. 
This means that even though I'm merging 2 or more expressions into a single 
regex I still need to maintain both the list and the dictionary parts of the 
ParseResults. My solution for maintaining the list was to add a new 'namelist' 
attribute to the resulting regex. I name every atomic-regex-part (generating a 
unique name when needed) and in the `Regex.parseImpl()` I look for this attribute 
and if found I generate the list part of the ParseResults by reading the 
relevant groups in the regex match.

Did you try my `Optional` class?    

In my grammar it is extremely important (speed-boost-wise).

Thanks!

Tal.

---
## 2011-01-10 22:19:06 - codeape - Why are comments at end not ignored when using parseAll=True?


    from pyparsing import *
    
    Statement = Word(alphas) + Literal('=') + Word(nums) + Literal(';')
    Body = ZeroOrMore(Statement)
    Body.ignore(dblSlashComment)
    
    # This works as expected
    Body.parseString('a=1 //foo\n; b=22;', parseAll=True)
    
    # Why does this fail?
    Body.parseString('a=1; b=22; //foo', parseAll=True)




---
## 2011-01-11 06:38:22 - majortal - Bug in pyparsing?
Sorry for assuming there is a bug.
I downloaded a fresh copy of pyparsing to make sure I have the latest and greatest.
Please review code:



    import pyparsing_155
    parser = ((pyparsing_155.Regex('two') + pyparsing_155.Regex('(pairs of)?')) | pyparsing_155.Regex('pairs of') ).setResultsName('multiple', listAllMatches = True)
    print parser.saveAsList 
    result = parser.parseString('two pairs of') 
    print result, type(result)
    print result['multiple'], type(result['multiple'])
    parser = ((pyparsing_155.Regex('two') + pyparsing_155.Regex('(pairs of)?'))).setResultsName('multiple', listAllMatches = True)
    print parser.saveAsList 
    result = parser.parseString('two pairs of') 
    print result, type(result)
    print result['multiple'], type(result['multiple'])


The results are:

    False
    ['two', 'pairs of'] <class 'pyparsing_155.ParseResults'>
    ['two'] <class 'pyparsing_155.ParseResults'>
    True
    ['two', 'pairs of'] <class 'pyparsing_155.ParseResults'>
     <class 'pyparsing_155.ParseResults'>


I think the bug is that the first parser does not have the saveAsList attribute set, so that when the ParseResults is constructed only the first element in the list is branded with the 'multiple' name.

I <strong>THINK</strong> that when I ask the entire parser to set a results name, then that name should cover the entire result and not just the first expression.

Please help.

Tal Weiss.


---
## 2011-01-14 07:39:29 - DesiLinguist - Help with originalTextFor and transformString
Hello,

I am trying to write an application that describes certain kinds of strings in input text that should be identified and enclosed in a tag.

Since I only care about the full text that matches, I am using the 'originalTextFor' helper method. However, I am not sure how to write the parse action for this kind of element. 

I tried:


    def myParseAction(s, l, t):
        return '<tag>' + t[0] + '</tag>'


but I get the error: 


    TypeError: cannot concatenate 'str' and 'int' objects


In short, how do I access the original text in the parse action for an `originalTextFor().parse` element?


Thanks Much! pyparsing is just amazing and has saved my bacon many times now. Thanks so much for creating it!

#### 2011-01-14 10:06:02 - ptmcg
Can you post the expression where you use `originalTextFor`?  You should be getting a string, not an int.
#### 2011-01-14 14:27:44 - DesiLinguist
Sure. 

For example, in order to find the pattern of the form 'the author asserts that ...' (or its variants), I use this expression:



    agentWord =  oneOf('speaker author writer')
    assertVerb = oneOf('asserts claims states writes assumes contends alleges')
    expr1 = originalTextFor ( Literal('the') + agentWord + assertVerb + \
            Literal('that') ).setParseAction(markUpAction)


#### 2011-01-14 14:28:41 - DesiLinguist
So, just to clarify, `'markUpAction'` is the same as `'myParseAction'` that I showed in the first post.
#### 2011-01-14 21:19:02 - ptmcg
I still don't see where the ints are coming from.  You can get some insight into what is happening by putting pyparsing's `@traceParseAction` decorator on the line before `markUpAction`.  It will echo the arguments passed to the parse action, and the rsults returned from it.
#### 2011-01-18 07:09:53 - DesiLinguist
Thanks for the reply and for the helpful decorator tip.

So, I tried this on a dummy sentence that had the part I wanted to tag and the rest was junk and here's what I get: 


    >>entering markUpAction(line: 'the author asserts that sdsdflkskldf slkfj sdfjsdk flsjfk sdkfj sdfs]', 0, [0, 'the', 'author', 'asserts', 'that', 23])
    <<leaving markUpAction (ret: <shell>the author asserts that</shell>)


#### 2011-01-18 07:12:19 - DesiLinguist
Sorry for the incomplete post. So, you can see that I am getting integers in the token list. Any ideas why?
#### 2011-01-18 08:24:18 - ptmcg
Ach! Yes! `originalTextFor` uses a parse action of its own to actually splice out the desired text, and you are overriding it by calling `setParseAction` with your own!  Instead of `setParseAction`, use `addParseAction`, so that you don't replace the parse action that implements `originalTextFor`'s logic.
#### 2011-01-18 09:36:23 - DesiLinguist
Yay! That works. I did try to read the documentation (and I even looked in the O'reilly thing I bought) but I couldn't find anything. Perhaps I didn't look hard enough since I noticed that you are pretty quite to respond.

BTW, pyparsing is so awesome I can't even tell you :)

---
## 2011-01-29 11:58:50 - Griffon26 - Preserving whitespace with transformString
Is there some way I can set the default to preserving whitespace when transforming?

I have a rather large grammar and only a few elements that I need to transform and I don't see an easy way of doing this.

Regards,
Maurice.

#### 2011-01-30 02:38:49 - ptmcg
Can you post a small example of what the problem is? `transformString` pretty much leaves things alone unless there is a match.  For example, here is a string transformer that upcases words that start with a vowel:


    text = '''
    Lorem ipsum dolor sit amet, consectetur adipisicing elit, 
    sed do eiusmod tempor incididunt ut labore et dolore 
    magna aliqua.    
    '''
    
    from pyparsing import *
    
    targetWord = Word('aeiou',alphas)
    targetWord.setParseAction(lambda t:t[0].upper())
    untargetWord = Word(alphas)
    print (targetWord | untargetWord).transformString(text)


giving


    Lorem IPSUM dolor sit AMET, consectetur ADIPISICING ELIT, 
    sed do EIUSMOD tempor INCIDIDUNT UT labore ET dolore 
    magna ALIQUA. 


I had to add `untargetWord` as well as `targetWord` - otherwise `targetWord` would match within other words, as soon as it found a vowel.  But the whitespace is left completely intact.  So an example of what whitespace is gettting lost would be helpful.

-- Paul
#### 2011-01-30 10:32:41 - Griffon26
It's as simple as this:


    text = 'alsk daskldj alskjdlaksj'
    words = OneOrMore(Word(alphas))
    print words.transformString(text)


It prints:


    alskdaskldjalskjdlaksj


I'm using 1.5.5, python 2.6.5
#### 2011-01-30 10:55:02 - Griffon26
I've tried using `originalTextFor` as well. But on one hand I can't use `originalTextFor` on 
a higher level in the parse tree than where I want to perform modifications 
(because I lose the modifications that way). While on the other I can only 
use `originalTextFor` on the top-level (because otherwise I lose the spacing and newlines contained only in the higher level elements).
#### 2011-01-31 08:58:13 - Griffon26
Maybe I'm actually looking for a special case of 

---
## 2011-02-04 05:33:31 - voltron8 - Need help. Parse float numbers and functions
I work on algebraic expression parser that support variables and functions. And stuck with one problem. Float numbers  defined as


    point = Literal('.')
    e = CaselessLiteral('E')
    plusorminus = Literal('+') | Literal('-')
    number = Word(nums) 
    integer = Combine( Optional(plusorminus) + number )
    floatnumber = Combine( integer +
                           Optional( point + Optional(number) ) +
                           Optional( e + integer )
                         )

Functions names defined as


    fn = Word( alphas )

The problem is when function name starts with 'e' or 'E' I get parse error.
Any help would be appreciated

#### 2011-02-04 10:27:26 - Griffon26
You'll need to provide more of the grammar than you did. How are fn and floatnumber tied together? It works just fine for me with grammar = (fn | floatnumber) for instance.
#### 2011-02-05 00:29:53 - voltron8
Here is full grammar

#### 2011-02-08 05:04:27 - voltron8
I think this problem is solved. I replace\

    e = CaselessLiteral('E')


with

    e = CaselessKeyword('E')

and seems it works as expected

#### 2011-02-09 13:34:50 - ptmcg
I know that I have a number of examples that use this method for defining floating point numbers, but over the years, I have found that for low-level terminals, especially this one in particular, parsing is greatly improved using a Regex, as in:


    floatNumber = Regex(r'\d+(\.\d*)?([eE]\d+)?')


This will speed up your parsing, *and* get rid of your 'E' problem.

-- Paul
#### 2011-02-10 00:12:14 - voltron8
Thanks, Paul!

---
## 2011-02-14 09:42:49 - DesiLinguist - Weird behavior with MatchFirst
Hello,

I am writing a pyparsing grammar to match some specific type of text that I am interested in. 

Here is the my pyparsing grammar made up of two expression



    from pyparsing import *
    
    # define the action
    def markUpAction(s, l, t):
        return '<text>' + t[0] + '</text>'
    
    # the author's/speaker's first/second arguments/argument etc.
    agentWord = Literal('speaker 's') | Literal('author 's') | \
                oneOf('speaker's author's')
    numeral = oneOf('first second ')
    assertNoun = Literal('point of view') | Literal('points of view') | \
                 oneOf('issues issue \
                        views view \
                        arguments argument')
    expr1 = originalTextFor(Optional(Literal('the')) + agentWord + Optional(numeral) + \
            WordStart(alphanums) + assertNoun + WordEnd(alphanums) + \
            Optional(Literal('that'))).addParseAction(markUpAction)
    
    # the argument is not entirely (un)convincing etc.
    argumentWord = Literal('premise of the argument') | \
                    oneOf('arguments argument \
                           statements statement')
    verbWord = Literal('is not') | oneOf('seems sounds appears is')
    veryWord = oneOf('very completely wholly entirely somewhat')
    argumentPhrase = Literal('well reasoned') | Literal('well-reasoned') | \
                     Literal('unconvincing') | Literal('convincing') | \
                     Literal('unreasonable') | Literal('reasonable') | \
                     Literal('unpersuasive') | Literal('persuasive') | \
                     Literal('incredible') | Literal('credible')
    expr2 = originalTextFor(argumentWord + WordStart(alphanums) + Optional(verbWord) + Optional(veryWord) + \
             WordStart(alphanums) + argumentPhrase + WordEnd(alphanums) + \
             Optional(Literal('that'))).addParseAction(markUpAction)
    
    shellexpr = expr2 | expr1
    


If I test this code on the following two sentences using transformString:


    the author's argument is entirely unconvincing
    the author's agent says that the argument is not entirely convincing that


I get the following output:


    <text>the author's argument </text>is entirely unconvincing
    the author's agent says that the <text>argument is not entirely convincing that</text>


I am confused as to the output of the first sentence. Shouldn't expr2 have higher precedence and therefore the 'argument is entirely unconvincing' be marked up rather than 'the author's argument'? Am I missing something that makes pyparsing prefer to use expr1 before expr2. expr2 does seem to be functional as indicated by the output of the second sentence. 

Any help is greatly appreciated!

#### 2011-02-14 19:56:19 - ptmcg
What is confusing you is that you are human, and are used to reading an entire sentence at once. Pyparsing is unfortunately not human, and so just reads left to right, trying both alternative expressions in each position in the input string. In your first sentence, pyparsing is able to match an expr1 with 'the author's argument' - unfortunately, this consumes the word 'argument', so it won't be reused as the lead-in to an expr2.  Since you want to process the entire string with each alternative expr2 or expr1 (as opposed to pyparsing's behavior of testing the full MatchFirst expression at each successive location in the string), you'll have to call transformString with the two separate expressions yourself, and break out if you get a match, something like:


    for line in sourcelines:
        orig_line = line    
        for e in (expr2, expr1):
            line = e.transformString(line)
            if line != orig_line:
                break


-- Paul
#### 2011-02-14 20:59:53 - DesiLinguist
Aha, so that's the issue. I guess I didn't know that pyparsing tries each expression at each position in the input string. I guess another thing I could do is to include an optional thing in expr2 that matches 'the author's' and then things should work.

Many thanks for the clarification. I really appreciate the explanation and all the effort you put into making pyparsing such a fantastic resource!

---
## 2011-02-16 12:55:46 - Icoo - Simple parser or code simulator
So I am new at pyparsing and I am trying to get the hang of it quickly by reading the examples on the page here. First let me explain my problem:

As part of a bigger project I need to be able to evaluate small snippets of code written in a simple programming/scripting language I made up. The language onyl supports integer variables, the common operations between them (+,-,*,/,(,)) and a simple for loop and if statement. An example would be this:

    a = 5;
    b = a * 4;
    [FOR 5]
        b = b * 3;
        a = b + a;
        [IF a > 10]
            a = 5;
        [ENDIF]
    [ENDFOR]

So really simple...I figured out the variables and basic operations part on my own via the sample files (SimpleCalc for instance) but I am having  a hard time figuring out how to do the for loop and teh if statement. I only need to evaluate the piece of code (simulate it if u will) and get the final values of the a and b variable. Could someone here who is more expirienced with pyparsing point me in the right direction? Thank you in advance!

#### 2011-02-16 16:02:19 - ptmcg

    a = 5;
    b = a * 4;
    [FOR 5]
        b = b * 3;
        a = b + a;
        [IF a > 10]
            a = 5;
        [ENDIF]
    [ENDFOR]

The first thing to do is to write down for yourself a BNF describing your language-to-be.  This helps you get some insights into potential ambiguities in your planned language, starts to layout which pieces related to (or nest within) other pieces, and gives you something of a grocery list of items to implement.

For this language, I see a simple BNF of a statement, which can be one of 3 types: assignment, if-statement, or for-statement.  The if-statement and for-statement forms in turn can be made up of statements in their body.  So a simple BNF would look like:



    statement ::= assignment | if-statement | for-statement
    assignment ::= varname '=' arith-expression ';'
    if-statment ::= '[IF' boolean-expression ']' statement... '[ENDIF]'
    for-statement ::= '[FOR' arith-expression ']' statement... '[ENDFOR]'


Now to translate the to Python using pyparsing.

You've got arithmetic expression working, you'll need to define a boolean expression:

    COMP_OPERATOR = oneOf('< = > <= >= !=')
    boolean_expression = arith_expression + COMP_OPERATOR + arith_expression

varname could be `oneOf(list('abcdefghijklmnopqrstuvwxyz'))` or `Word(alphas)`, or `Word(alphas, alphanums)`, depending on how complex you want your variable names to get.

Lastly, since you have statements which can be made up of statements, then you will need to use a pyparsing Forward term to define this:



    SEMI,EQ,LBRACK,RBRACK = map(Suppress,';=[]')
    statement = Forward()
    assignment = varname('var') + EQ + arith_expression('rhs') + SEMI
    if_statment = LBRACK + 'IF' + boolean_expression('condition') + RBRACK + \
                    ZeroOrMore(statement) + LBRACK + 'ENDIF' + RBRACK
    for_statement = LBRACK + 'FOR' + arith_expression('for_condition') + RBRACK + \
                    ZeroOrMore(statement) + LBRACK + 'ENDFOR' + RBRACK
    
    statement << (assignment | for_statement | if_statement)


Note how I added parenthesized names after the significant terms in your parser definition - this will make it easier to extract those bits from the results of the parsing process.

So that's a little more lead - run with that and see if you can make further progress tying these pieces together.

Welcome to pyparsing!

---
## 2011-02-18 11:46:14 - victorsmiller - Getting the text in comments
I'm writing a translator from one language to another using pyparsing.  The source language has c++ style comments.  I'd like to access the contents of these comments so that I can insert them as comments into the target text.  Is there an easy way of doing this?

#### 2011-02-20 19:36:18 - ptmcg
By 'language' do you mean programming language?  You will have some interesting problems here.  The usual way one deals with comments when parsing some program code is to ignore them, so that comments that might appear within some valid language construct don't confuse the construct parser.  However in your case, you don't want to ignore them, you want to preserve them so they can be included in the resulting text.

I would tackle this by supporting comments only at a high level, such as at the same grammar level as a programming statement. This rules out end-of-line comments that might be embedded in a larger structure definition, or comments embedded within a function definition.  Then you would *not* ignore comments in your grammar, but just expose them as a statement-level grammar construct, and convert from old-language-comment-style to new-language-comment-style.
#### 2011-02-24 10:43:28 - Griffon26
This limitation has been bothering me as well. I wanted to replace or transform certain parts of the parsed text while leaving the skipped text (comments, spaces) in place.

I ended up modifying pyparsing to maintain a tuple of skipped text and parsed text in a ParseResults instead of just the parsed text.

Paul, do you know of a more elegant way to solve this issue? I'd really like pyparsing to be able to do this out of the box.
#### 2011-02-25 05:21:51 - ptmcg
Try adding a parse action to the comment expression, that will save it and its location off to a list of comments.
#### 2011-02-26 05:46:13 - Griffon26
Paul, thinking about your comment I came up with a new approach. I'm using parse actions to save custom ReplacementMarker objects to the ParseResults. This makes sure that if the parser backtracks those objects are automatically removed.

Victor, maybe it will help you too. I created a simple example showing how to replace bits of text and preserve everything else.

Take a look at the example here:


---
## 2011-02-20 16:41:52 - xuru1 - Parsing time increases when parsed repeatedly
I'm not exactly sure that this is an issue in the way I have my grammar setup, or if it's an issue with pyparsing.  When I run my parse a file with my grammar, repeatedly, each iteration takes longer.  I made up a test case that looks like this:


    
    testfile = os.path.join(basedir, 'data', 'pass', 'b06.v')
    vparser = None
    
    def parseit(testfile):
        global vparser
        vparser.parseFile(testfile)
    
    def check_parse(testfile):
        ''' Yielded test function to parse a verilog file '''
        memfunc(timefunc, parseit, testfile)
    
    def test_repeated_speed():
        global vparser
        for x in range(40):
            vparser = None
    
            vparser = OneOrMore(directives + (module_def | primitive_def) + directives)
            vparser.ignore(comments)
    
            name = os.path.basename(testfile).split('.')[0]
            name = name.replace('_', ' ')
            name = str(x) + ' ' + name
            check_parse.description = str(name)
            yield (check_parse, testfile)


BTW, memfunc checks how much memory the method uses, and timefunc times how long is spent in that method.

When I run this through nose I get the following:



    0 b06 ...  T:0:00:00.494  M:1784 KB ok
    1 b06 ...  T:0:00:00.728  M:528 KB ok
    2 b06 ...  T:0:00:00.980  M:528 KB ok
    3 b06 ...  T:0:00:01.224  M:0 KB ok
    4 b06 ...  T:0:00:01.434  M:0 KB ok
    5 b06 ...  T:0:00:01.689  M:264 KB ok
    6 b06 ...  T:0:00:01.906  M:660 KB ok
    7 b06 ...  T:0:00:02.132  M:528 KB ok
    8 b06 ...  T:0:00:02.377  M:528 KB ok
    9 b06 ...  T:0:00:02.608  M:748 KB ok
    10 b06 ...  T:0:00:02.841  M:528 KB ok
    11 b06 ...  T:0:00:03.076  M:528 KB ok
    12 b06 ...  T:0:00:03.301  M:528 KB ok
    13 b06 ...  T:0:00:03.591  M:804 KB ok
    14 b06 ...  T:0:00:03.795  M:528 KB ok
    15 b06 ...  T:0:00:04.033  M:528 KB ok
    16 b06 ...  T:0:00:04.258  M:528 KB ok
    17 b06 ...  T:0:00:04.500  M:804 KB ok
    18 b06 ...  T:0:00:04.767  M:528 KB ok
    19 b06 ...  T:0:00:04.979  M:528 KB ok
    20 b06 ...  T:0:00:05.190  M:528 KB ok
    21 b06 ...  T:0:00:05.444  M:996 KB ok
    22 b06 ...  T:0:00:05.683  M:396 KB ok
    23 b06 ...  T:0:00:05.953  M:528 KB ok
    24 b06 ...  T:0:00:06.151  M:528 KB ok
    25 b06 ...  T:0:00:06.385  M:840 KB ok
    26 b06 ...  T:0:00:06.613  M:528 KB ok
    27 b06 ...  T:0:00:06.858  M:528 KB ok
    28 b06 ...  T:0:00:07.104  M:528 KB ok
    29 b06 ...  T:0:00:07.410  M:1096 KB ok
    30 b06 ...  T:0:00:07.674  M:264 KB ok
    31 b06 ...  T:0:00:07.815  M:528 KB ok
    32 b06 ...  T:0:00:08.131  M:528 KB ok
    33 b06 ...  T:0:00:08.331  M:528 KB ok
    34 b06 ...  T:0:00:08.649  M:824 KB ok
    35 b06 ...  T:0:00:08.877  M:528 KB ok
    36 b06 ...  T:0:00:09.174  M:528 KB ok
    37 b06 ...  T:0:00:09.479  M:528 KB ok
    38 b06 ...  T:0:00:09.667  M:852 KB ok
    39 b06 ...  T:0:00:09.935  M:528 KB ok


I know I'm not showing my grammer, but I thought I would see what you thought about this first.

Thanks for any help!

#### 2011-02-20 17:12:27 - ptmcg
Are you using `enablePackrat()` in your grammar? If so, you should reset the packrat cache between parses. Otherwise, I don't know why there would be an increasing time to parse due to pyparsing itself, but a self-modifying grammar, or one with a parse action that maintains an external data structure, could have growth problems.

_[ED - latest versions of pyparsing have an optimized packrat cache that 
would not need to be reset between parses.]_

#### 2011-02-20 17:48:45 - xuru1
I'm not using `enablePackrat()`, but I am using parse actions.  I'll cut those out, and see what happens.  Thanks.
#### 2011-05-01 23:04:18 - ptmcg
There is a reported memory leak running pyparsing under Python 3.x.  The next release will include fixes to address this problem.

---
## 2011-02-24 09:24:19 - DiaaFayed - apostroph
when the parsed string contains apostroph,
the apostoroph appears as &apos;  in the ouput  xml file

thanks


---
## 2011-02-26 18:17:25 - dwelch91 - Handling missing operand
I am trying to parse search expressions of the form:



    'a' & 'b' & -('c' | -'d')


That would mean not 'c' or not 'd'; and 'a' and 'b'. 

Here's the code I'm using so far (somewhat simplified):


    LPAR,RPAR = map(Suppress,'()')
    search = Forward()
    AND_OP = Keyword('&') | Keyword('AND', caseless=True)
    OR_OP = Keyword('|') | Keyword('OR', caseless=True)
    NOT_OP = Keyword('-') | Keyword('NOT', caseless=True)
    INTEGER = Regex(r'[+-]?\d+')
    PATTERN = dblQuotedString
    
    expr = Forward()
    
    expr_term = ( LPAR + expr + RPAR | PATTERN )
    
    expr << operatorPrecedence(expr_term,
        [
         (NOT_OP, UNARY, opAssoc.RIGHT),
         (AND_OP, BINARY, opAssoc.LEFT),
         (OR_OP, BINARY, opAssoc.LEFT),
        ]).setParseAction( printer )
    
    search << ZeroOrMore( expr )

 
This works fine if there's a `&` or `|` between all terms of the expressions. 
However, in reality, some of the `&`'s are implied and are missing from the string. So a real search expression might look like:



    'a' 'b' -('c' | -'d')


I've tried to use the searchparser.py example as a guide but can't understand how it handles the missing 'and' keywords.

Any pointers on how to handle this?

Thanks,

Don

#### 2011-02-26 20:04:41 - ptmcg
In your expression, the operands are just PATTERN's - `operatorPrecedence` 
takes care of the nesting in parentheses, so you don't need to define the 
`expr` and `expr_term` definitions.

Search parsing is a pretty lively topic lately. Here is a link to a Lucene parser I worked on last week: .  You might also look into Whoosh, and Booleano, some search and expression parsers built with pyparsing.

---
## 2011-03-02 21:30:38 - cmurphycode - Help with nested result names
Hi all,

Been struggling with a parser I'm trying to write for recipes. This is what I have:

        real = Regex(r'\d+(\.\d*)?').setParseAction(lambda t: float(t[0]))

        frac = real('numerator') + '/' + real('denominator')
        frac.setParseAction(lambda t: t.numerator / t.denominator)
        quantity = frac | real + Optional(frac)
        quantity = quantity.setResultsName('quantity')


        unit = Combine(oneOf(self.units) + Optional('s') + Optional('.')).setResultsName('unit')
        amount_base = quantity + Optional(unit)
        amount_base = amount_base.setResultsName('amountbase')
        amount_paren = '(' + amount_base + ')'
        amount_paren = amount_paren.setResultsName('amountmod')
        amount = amount_base + Optional(amount_paren)
        amount = amount.setResultsName('amount')

        modword = oneOf(self.modwords).setResultsName('modword')
        processing = oneOf(self.processing).setResultsName('processing')


        food = OneOrMore(Word(alphanums)).setResultsName('food')
        ingredientGrammar = amount + Optional(modword) + Optional(processing) + food + Optional(processing)

I've been trying various combinations, with the intent of parsing this string:
 1 can (28 oz.) good quality crushed tomatoes

such that I can access the amount base (1 can), amount paren ((28 oz.)), etc.

My results object looks like madness:

    (
        [1.0, 'can', '(', 28.0, 'oz.', ')', 'good', 'quality', 'crushed', 'tomatoes'],
        {
            'food': [
                        (
                            (
                                ['good', 'quality', 'crushed', 'tomatoes'],
                                {}
                            ),6
                        )
                    ],
            'amountbase': [
                            (
                                (
                                    [1.0, 'can'],
                                    {'unit': [('can', 1)], 'quantity': [(1.0, 0)]}
                                ), 0
                            ), 
                            (
                                (
                                    [28.0, 'oz.'],
                                    {'unit': [('oz.', 1)], 'quantity': [(28.0, 0)]}
                                ), 3
                            )
                          ],
            'amount': [
                        (
                            (
                                [1.0, 'can', '(', 28.0, 'oz.', ')'],
                                {'amountbase': [
                                                (([1.0, 'can'], {'unit': [('can', 1)], 'quantity': [(1.0, 0)]}), 0),
                                                (([28.0, 'oz.'], {'unit': [('oz.', 1)], 'quantity': [(28.0, 0)]}), 3)
                                                ],
                                 'quantity': [(1.0, 0), (28.0, 3)],
                                 'unit': [('can', 1), ('oz.', 4)],
                                 'amountmod': [
                                                (
                                                    (
                                                        ['(', 28.0, 'oz.', ')'],
                                                        {'amountbase': [
                                                                        (
                                                                            (
                                                                                [28.0, 'oz.'],
                                                                                {'unit': [('oz.', 1)], 'quantity': [(28.0, 0)]}
                                                                            ), 1
                                                                        )
                                                                       ],
                                                        'unit': [('oz.', 2)],
                                                        'quantity': [(28.0, 1)]
                                                        }
                                                    ), 2
                                                )
                                              ]
                                }
                            ), 0
                        )
                       ],
            'amountmod': [
                            (
                                (
                                    ['(', 28.0, 'oz.', ')'], 
                                    {'amountbase': [
                                                    (
                                                        (
                                                            [28.0, 'oz.'], 
                                                            {'unit': [('oz.', 1)], 'quantity': [(28.0, 0)]}
                                                        ), 1
                                                    )
                                                   ],
                                    'unit': [('oz.', 2)],
                                    'quantity': [(28.0, 1)]
                                    }
                                ), 2
                            )
                        ],
            'unit': [('can', 1), ('oz.', 4)],
            'quantity': [(1.0, 0), (28.0, 3)]
        }
    )

(I spaced it by hand to try to make sense of the structure)

Not only is that a lot more complicated than I thought, I can't seem to access the '1 can' portion. Looking at the object, you'd think that results.amountbase would have [1.0, can]... as the first item. But:

    ([28.0, 'oz.'], {'unit': [('oz.', 1)], 'quantity': [(28.0, 0)]})

Which is the second item. I should note that the 28oz stuff should not exist in amountbase at all!

What am I doing wrong, here? Clearly I don't understand some part of the structure of the grammar. 

Thanks!

#### 2011-03-02 21:33:46 - cmurphycode
Oh, I horribly mucked up the code tags. Sorry!

Here's the relevant pastebins:
1) my grammar 
2) results 

Again, sorry for the noise!
#### 2011-03-02 22:14:37 - ptmcg
(For future reference, the code tags are [[code]], and they have to be on a line by themselves. I know, its non-standard, but that's what they use here.)

You've really taken this pretty far, and your grammar has so much variability in it, you *have* to use results names, so good work on all that.

When printing out the repr for ParseResults, there are two major components: the list of strings that are the tokens, and then the dict of defined results names. Your grammar has quite a bit of depth and structure, and many results names, so this repr looks kind of intimidating.

The method you are missing has the unflattering name 'dump', and it will show a nice indented form of your parsed data:



    test = '1 can (28 oz.) good quality crushed tomatoes'
    result = ingredientGrammar.parseString(test)
    print result.dump()


This prints out the (I think) pretty legible structure:


    [1.0, 'can', '(', 28.0, 'oz.', ')', 'good quality', 
     'crushed', 'tomatoes']
    - amount: [1.0, 'can', '(', 28.0, 'oz.', ')']
      - amountbase: [28.0, 'oz.']
        - quantity: 28.0
        - unit: oz.
      - amountmod: ['(', 28.0, 'oz.', ')']
        - amountbase: [28.0, 'oz.']
          - quantity: 28.0
          - unit: oz.
        - quantity: 28.0
        - unit: oz.
      - quantity: 28.0
      - unit: oz.
    - amountbase: [28.0, 'oz.']
      - quantity: 28.0
      - unit: oz.
    - amountmod: ['(', 28.0, 'oz.', ')']
      - amountbase: [28.0, 'oz.']
        - quantity: 28.0
        - unit: oz.
      - quantity: 28.0
      - unit: oz.
    - food: ['tomatoes']
    - modword: good quality
    - processing: crushed
    - quantity: 28.0
    - unit: oz.


Even though I'm not familiar with your parser, I can look at this output and come up with these lines:


    print result.quantity
    print result.unit
    print result.food
    print '%s, %s, %s' % (' '.join(result.food), result.processing, result.modword)


and get 



    28.0
    oz.
    ['tomatoes']
    tomatoes, crushed, good quality


You could also reference subfields using repeated dot notation, like 
`result.amount.quantity`.

I'm impressed at your progress on this - parsing recipes is a pretty ambitious natural-language-y endeavor, and probably will push pyparsing to its limits.  Recipes right out of a magazine or a book will follow *some* regular format, but there are just hundreds of ways of expressing ingredients, amounts, preparations, etc. You may have to define for yourself a reasonable but restricted structured format for your recipe parsing.

In any event, you've really made a lot of progress, good luck!
#### 2011-03-03 06:50:16 - cmurphycode
Thanks for your help! I realized I forgot to include definitions for some of the list variables, but you pretty much guessed it.

I did run across dump a while ago, but at the time, it wasn't working- I have out commented out with a??? :) But that would've been pretty useful for my post, d'oh!

Unfortunately, as you can see, amoutmod andamount base show the same thing, where the base should actually be 1 can. You can see this in the token list (?) for amount, but I couldn't get to it.

I think there must be something wrong with my definition of the base and mod terms, since they really shouldn't be the same....

PS: you are totally right about the problem of parsing recipes in general. I am starting with a pretty standardized data set, and whenever I need more generality, ill probably have to use something else. Not that pyparsing hasn't been great, but as you say, it's not really a natural language tool.

#### 2011-03-03 18:51:07 - cmurphycode
PS: here's the lists I spoke of. 


    units = ['teaspoon','tablespoon','cup','can','pound','lb','oz']
        #do we need this?
        modwords = ['of']
        #things that have to be done to the food item
        processing = ['chopped', 'drained','pureed','melted','softened','crushed']


#### 2011-03-03 21:37:32 - ptmcg
Replace this code:


    quantity = frac | real + Optional(frac)
    quantity = quantity.setResultsName('quantity')
    unit = Combine(oneOf(self.units) + Optional('s') + Optional('.')).setResultsName('unit')
    amount_base = quantity + Optional(unit)
    amount_base = amount_base.setResultsName('amountbase')
    amount_paren = '(' + amount_base + ')'
    amount_paren = amount_paren.setResultsName('amountmod')
    amount = amount_base + Optional(amount_paren)
    amount = amount.setResultsName('amount')


with this:



    quantity = frac | real + Optional(frac)
    unit = Combine(oneOf(units) + Optional('s') + Optional('.'))
    
    amount_base = Group(quantity('quantity') + Optional(unit('unit')))
    amount = Group(amount_base('base') + Optional('('+amount_base('mod')+')'))('amount')



A number of things are going on here.

First, I'm leaving off the results names for the base building blocks quantity and unit, and for the combined expression amount_base.  I like to leave the basic items unnamed, so it is easy to add them into larger expressions, and name them then.  For instance:



    integer = Word(nums)
    date = integer('year') + '/' + integer('month') + '/' + integer('day')


Of course, this would accept many invalid dates, but my point is that all the fields are integers, it is their positions in the larger expression that warrant giving them results names.  And this is simpler than:



    integer = Word(nums)
    year_integer = integer('year')
    month_integer = integer('month')
    day_integer = integer('day')
    date = year_integer + '/' + month_integer + '/' + day_integer


and just as easy to follow, I think.

Second, I'm using the short notation for setting results names, 
`expr('name')` instead of `expr.setResultsName('name')`. Much as I like the 
addition of results names as a core feature in pyparsing, I am unhappy with how 
awkward and ungainly the method name `'setResultsName'` is. I find it very 
distracting visually when trying to read through a grammar definition.  
About 3 years ago I added the new short notation, and this cleans things up 
quite a bit.

Lastly, I am adding Group constructs to the expressions.  `Group` maintains 
the structure of the resulting tokens, and keeps the results names from 'leaking' out to the higher-level expressions. 

With these changes, `results.dump()` gives you the nice normalized structure:

    [[[1.0, 'can'], '(', [28.0, 'oz.'], ')'], 'good quality', 'crushed', 'tomatoes']
    - amount: [[1.0, 'can'], '(', [28.0, 'oz.'], ')']
      - base: [1.0, 'can']
        - quantity: 1.0
        - unit: can
      - mod: [28.0, 'oz.']
        - quantity: 28.0
        - unit: oz.
    - food: ['tomatoes']
    - modword: good quality
    - processing: crushed


No duplicates.  Access fields as `result.amount.base.quantity`, `result.amount.mod`, 
`result.food`, etc.

#### 2011-03-05 06:57:56 - cmurphycode
That works like a charm! Thank you!

I like that shorthand for resultName- seems a little like magic at first, but I think I see how it works, now. 

Seems like the missing piece was the Group. That, and not naming the base pieces until they are referenced as part of a larger item, is really slick!

Looking forward to polishing this up and running my dataset against it. 

Thanks again for all your work.

---
## 2011-03-06 11:13:40 - Zedric123 - make dict case insensitive
Hello,

I have an input file which contains a part where the data looks like this

    Key1    Val1
    Key2    Val2
    ...
    Key2    ValN

I really like the approach with using `dictOf` and then access the results 
with `results['Key1']` or even `results.Key1` . 
My question is, if it is possible to make the dict case insensitive, so I can 
access the values also by using `results.KEY1`.

I tried making all keys lower-case, but as the key names can be quite long it makes them nearly unreadable. 
I guess it should be possible to get this by deriving the Dict-class without being too much work?
Maybe I'm a bit naive, but I guess it should be sufficient to add a '.lower() to the line where keys are compared?
Anyone has any suggestion which function I would have to overwrite? (maybe Dict.postParse??)

Best Regards,

#### 2011-03-06 14:03:13 - ptmcg
This is a very interesting question, and thanks for giving pyparsing a try for your application.

Note that, even though we speak of 'dict's in the API, what you are really getting 
back are ParseResults. When I first wrote pyparsing, I wasn't sure if established 
Python coders would prefer to use dict-style (`results['Key1']`) or object-attribute-
style (`results.Key1`) to get at the named results fields, so I put in support for 
both.

It would be interesting to see the expression you are using for matching your key 
fields.  If they are `CaselessLiteral`s or `CaselessKeyword`s, then the data you get back will be of the case used in defining the expression, regardless of what the input string case was (this was to simplify post-parsing processing, just as you are struggling with now).  For instance:



    cmd = CaselessKeyword('GO') | CaselessKeyword('STOP')
    
    print OneOrMore(cmd).parseString('GO Go gO go')


will return:



    ['GO', 'GO', 'GO', 'GO']


So if you have this condition, then you can just always use 'KEY1' as your results name, even if the input string uses 'key1', 'Key1', etc. (Or pick a nice looking mixed case form, like 'thisIsALongKeyButAtLeastItIs

So my first thought would be to see if you can get your key expression to do the work for you, to give you results that are consistent in upper/lower case, even if the inputs are varied.

A close second suggestion would be to use a parse action on your key field, to auto-convert all keys to all lowercase.  Then you could always use 'key1'. Unfortunately, your post also mentions that some keys could be like 'thisisareallylongkeythatwhenconvertedtotalllowercaseishardtoread'.

So the caseless lookup approach that you specifically asked about could possibly be accomplished by monkeypatching the `ParseResults.__getitem__` method.  Something like:



    def caseless_getitem(pr, key, orig_getitem=ParseResults.__getitem__):
        if key not in pr:
            caselessmap = dict((k.lower(),k) for k in pr.keys()) 
            if key.lower() in caselessmap:
                key = caselessmap[key.lower()]
        return orig_getitem(pr,key)

    ParseResults.__getitem__ = caseless_getitem


This is kind of expensive, since it rebuilds the caselessmap every time you look up a key. But if you go this route, and this does the right thing, then we can look closer at tuning this code.

Please write back and let us know how you are progressing!

-- Paul
#### 2011-03-06 14:07:13 - ptmcg
This version might be a little more optional, using exceptions instead of looking up the key every time:



    def caseless_getitem(pr, key, orig_getitem=ParseResults.__getitem__):
        try:
            return orig_getitem(pr,key)
        except KeyError:
            caselessmap = dict((k.lower(),k) for k in pr.keys()) 
            if key.lower() in caselessmap:
                return orig_getitem(pr, caselessmap[key.lower()])
            else:
                raise

    ParseResults.__getitem__ = caseless_getitem


#### 2011-03-06 15:20:15 - Zedric123
Hello Paul,

thanks for the fast response.
I do use the CaselessLiteral for some mandatory keys, so it is good to know, that the literal expression is used.

But there are many options I don't want to specify an extra Literal for.

So I used


    dict = dictOf(label, OneOrMore(number | integer | label ) ).setResultsName('options') + END


maybe one of the worse keywords is 'AtomicCoordinatesAndSpecies', where everyone uses his own way of capital letters ;-)
Making it all lower case is not satisfying as the resulting file should remain human readable.

I parse this input file, but I don't have to work with every value at the moment, so I want to keep the spelling of the keys with unchanged values.
In fact, I don't need to change the key spelling at all, I 'just' need to get access to the value spelling it 'my way' ;-)

I think your function goes this way. I will test it tommorrow. Today I'm going to bed now...

Best Regards,
#### 2011-03-07 01:21:48 - Zedric123
Hello,

back at work, I tested your function and it works quite well using the dict syntax:


    result.['option'] == result.['oPTIOn']
    True

Both work fine.
A nice to have would be to get also 


    result.option == result.oPTIOn 

working, but this is probably not that easy.
So, many thanks for your help.

Best Regards,
#### 2011-03-08 02:03:33 - Zedric123
Hello,

I came across another Problem: 

I used this line to create a dict of key value pairs


    dict = dictOf( label, Group(OneOrMore( number | integer | label )  ) ).setResultsName('options') + END
    
    grammar = OneOrMore(dict | block)
    

and it works pretty well, when the input file looks like this 


    key1   value1
    key2   value2
    %block testblock
    block data
    %endblock testblock


Apparently the order between key value pairs and other structures can be arbitrary, so if the list of key value pairs gets interrupted the dict is overwritten (only the last key value pairs appear in the dict).
Is there a way to continue a started dict after an interrupt?

Best Regards,
#### 2011-03-09 05:07:11 - Zedric123
Hello,

after I stumbled over some other issues I decided to make things a lot easier for me by writing a class, which takes the ParseResults. 
This way I have a better control to store and access the data instead of trying to alter the pyparsing classes.
Here is the result I have so far. Maybe it's useful to someone:

The relevant part of the class looks like this:
it basically keeps a dict with the lowercase names of the options.


    class container(object):
    
        def __init__(self):
            self.lowerAttr  = {}
    
        def __getattr__(self, name):
            if name.lower() in self.lowerAttr:
                return self.__dict__[self.lowerAttr[name.lower()]]
    
        def addAttr(self, newAttr):
            if len(newAttr[1]) == 1:
                self.__dict__[newAttr[0]] = newAttr[1][0]
            else:
                self.__dict__[newAttr[0]] = newAttr.asList()[1]
            self.lowerAttr[newAttr[0].lower()] = newAttr[0]


a part of the parsing code:


    struc = container()
    option = label + Group( OneOrMore( label | number ) ) + END
    option.addParseAction(lambda t: struc.addAttr(t) )
    
    pResults = option.parseString(inputstring)


this way I can access the parsed attributes using


    struc.option1
    # or
    struc.oPTIon1

as the `__getattr__` function is called if I spell the option 'wrong'.

Best Regards,
#### 2011-03-09 18:05:35 - ptmcg
Great, glad you got this worked out!

---
## 2011-03-07 13:25:42 - _Mot_ - Recursiv parsing: Unexpected abort
Hi,

I can't seem to find the problem about parsing the last line (which contains nested lambdas).



    from pyparsing import *
    
    upperChars = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'
    lowerChars = upperChars.lower()
    digits = '0123456789'
    
    lparen = Word( '(' )
    rparen = Word( ')' )
    identifier = Word(upperChars + lowerChars)
    integer = Word(digits)
    operator = Word('->')
    _lambda = Forward()
    element = Or( [identifier, integer, _lambda] )
    expr = Forward()
    expr << Group(OneOrMore(element) + Optional(Or([operator + OneOrMore(expr),OneOrMore(expr)])))
    _lambda << Group(lparen + OneOrMore(expr) + rparen)
    
    print expr.parseString('add 5 5')
    print expr.parseString('do (foo bar)')
    print expr.parseString('do (bar foo) -> writeLn -> someIO 8 barfooz')
    print expr.parseString('do (bar (foo 5))')
    print expr.parseString('world -> replicateIO 3 (readInt) -> do (map (intToString square))')


A lambda is `( + expr* + )` and an expression itself can contain lambdas. So why does it stop after the 'do'?

#### 2011-03-07 19:56:03 - ptmcg
You have really gotten this very close. Surprisingly, the issue is that you 
defined `lparen` and `rparen` using `Word`. `Word` will match a 'word' or character 
group made up of the characters in the string passed to Word. So `Word(')')` will 
match `')'`, `'))'`, or even `')))))))'`. What you want instead is something like `Literal`, 
which will match only the exact init string, in this case ')', so the trailing 
'))' will be parsed as *2* rparen's, not just 1.  But since you are using 
`Group` to structure the results, the enclosing parens, while important during 
parsing, just get in the way afterward.  So I would recommend define `lparen` 
as `Suppress('(')` and `rparen` as `Suppress(')')`. This will give you this structure 
for the last string as:


    [['world', '->', ['replicateIO', '3', [['readInt']], '->', ['do', [['map', [['intToString', 'square']]]]]]]]


Nice job!
#### 2011-03-07 20:01:15 - ptmcg
Oh, also, `operator = Word('->')` should be `operator = Literal('->')`.

#### 2011-03-08 09:48:09 - _Mot_
Thanks four your help. I guess I was to focused on the definition of expr and _lambda to notice that lparen,rparen are wrong :(
#### 2011-03-08 10:53:14 - _Mot_
Ok. I changed my code to



    from pyparsing import *
    
    upperChars = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'
    lowerChars = upperChars.lower()
    digits = '0123456789'
    
    lparen = Suppress( '(' )
    rparen = Suppress( ')' )
    identifier = Word(upperChars + lowerChars).setResultsName('identifier')
    integer = Word(digits).setResultsName('integer')
    operator = Literal('->').setResultsName('operator')
    _lambda = Forward().setResultsName('lambda')
    element = Or( [identifier, integer, _lambda] )
    expr = Forward()
    expr << Group(OneOrMore(element) + Optional(Or([operator + OneOrMore(expr),OneOrMore(expr)]))).setResultsName('expression')
    _lambda << Group(lparen + OneOrMore(expr) + rparen)
    
    result = expr.parseString('world -> replicateIO 3 (readInt) -> do (map (intToString square))')
    
    print result.asXML()


The XML-Structure looks correct. 


    <expression>
      <expression>
        <identifier>world</identifier>
        <operator>-&gt;</operator>
        <expression>
          <identifier>replicateIO</identifier>
          <integer>3</integer>
          <lambda>
            <expression>
              <identifier>readInt</identifier>
            </expression>
          </lambda>
          <operator>-&gt;</operator>
          <expression>
            <identifier>do</identifier>
            <lambda>
              <expression>
                <identifier>map</identifier>
                <lambda>
                  <expression>
                    <identifier>intToString</identifier>
                    <identifier>square</identifier>
                  </expression>
                </lambda>
              </expression>
            </lambda>
          </expression>
        </expression>
      </expression>
    </expression>


But the dump structure looks kinda wierd



    [['world', '->', ['replicateIO', '3', [['readInt']], '->', ['do', [['map', [['intToString', 'square']]]]]]]]
    - expression: [['world', '->', ['replicateIO', '3', [['readInt']], '->', ['do', [['map', [['intToString', 'square']]]]]]]]
      - expression: ['world', '->', ['replicateIO', '3', [['readInt']], '->', ['do', [['map', [['intToString', 'square']]]]]]]
        - expression: [['replicateIO', '3', [['readInt']], '->', ['do', [['map', [['intToString', 'square']]]]]]]
          - expression: ['replicateIO', '3', [['readInt']], '->', ['do', [['map', [['intToString', 'square']]]]]]
            - expression: [['do', [['map', [['intToString', 'square']]]]]]
              - expression: ['do', [['map', [['intToString', 'square']]]]]
                - identifier: do
                - lambda: [[['map', [['intToString', 'square']]]]]
            - identifier: replicateIO
            - integer: 3
            - lambda: [[['readInt']]]
            - operator: ->
        - identifier: world
        - operator: ->


(From where are the last few lines??)

The result.items structure looks even more wierd to me.

Is there a way to iterate over the parse results like the xml structure? I can not iterate through the structure as a list (with strings) because then the resultsName-Information is lost.
#### 2011-03-08 11:03:35 - _Mot_
The result.items() structure looks like: 



    [('expression', (['world', '->', (['replicateIO', '3', ([(['readInt'], {'identifier': [('readInt', 0)]})], {'expression': [((['readInt'], {'identifier': [('readInt', 0)]}), -1)]}), '->', (['do', ([(['map', ([(['intToString', 'square'], {'identifier': [('intToString', 0), ('square', 1)]})], {'expression': [((['intToString', 'square'], {'identifier': [('intToString', 0), ('square', 1)]}), -1)]})], 


which is totally different from the XML structure. I don't get what items() should represent?

I think a structure like

[('expression',[('identifier','world'),('operator','->'),('expression',[('identifier','replicateIO'),...])]) would make much more sense.

---
## 2011-03-14 15:28:00 - muhackl - Strange behavior when reusing index string variables for parse actions
Hi all,
the following behavior boggles my mind: The following two test methods vary only in the use of the index string s1,s0 respectively s. In my opinion the results should be identical.

    import pyparsing as ppar
    
    class TstPyPar:
        r0, r1 = '', '' # parsed contents
    
        def __str__(self):
            return 'r0='{0}', r1='{1}''.format(self.r0, self.r1) 
    
        def test0(self, pstr):
            ''' This produces expected results '''
            self.r0, self.r1 = '', ''
    
            s1 = 'r1'
            w1 = ppar.Word(ppar.alphas).setParseAction(
                lambda t: setattr( self, s1, t[0]) )
            s0 = 'r0'
            w0 = ppar.Word(ppar.alphas).setParseAction(
                lambda t: setattr( self, s0, t[0]) )
    
            pExpr = w0 +  w1
            pExpr.parseString( pstr)
            return self.__str__()    
    
        def test1(self, pstr):
            ''' This produces strange result '''
            self.r0, self.r1 = '', ''
    
            s = 'r1'
            w1 = ppar.Word(ppar.alphas).setParseAction(
                lambda t: setattr( self, s, t[0]) )
            s = 'r0'
            w0 = ppar.Word(ppar.alphas).setParseAction(
                lambda t: setattr( self, s, t[0]) )
    
            pExpr = w0 + w1
            pExpr.parseString( pstr)
            return self.__str__()
    
    if __name__ == '__main__':
        TP = TstPyPar()
        print( TP.test0('hi there') ) 
        # Output: r0='hi', r1='there'
        print( TP.test1('hi there') ) 
        # Output: r0='there', r1=''
    # tested on python 2.6.6


Is this behavior intended?
Thanks, dietrich

#### 2011-03-16 19:36:58 - ptmcg
This isn't really a pyparsing issue so much as a Python one.  In the parse actions in test1, both setattr calls refer to s to get the attribute name.  Those parse actions get called much later, during string parsing time, and by that time, s has been set to the value 'there'. By the time either parse action is run, the parsed token will be assigned to self.there, first in the parse action on w0 then the parse action on w1.

If you want to preserve these values by some name, please look into the results naming capability of pyparsing. This is a more natural assignment of parsed tokens to named fields of the returned ParseResults - and the names are accessible within parse actions too!

Hope this helped clear some things up.

-- Paul
#### 2011-03-17 16:05:24 - muhackl
Thanks for the prompt reply, Paul.
I was suspecting something of that sort - your comments cleared things for now.
Cheers, dietrich

---
## 2011-03-17 03:49:55 - ashish.sawankar - How to match empty\/null token in a delimited list
Hi,

I am trying to write rules to parse this kind of function call:



    function_name(comma separated list of parameters)


The comma separated list here can be of form - `max(a,,b)` i.e. we need not mention anything if we do not want to pass a parameter. In this case we are not passing second parameter.

I tried writing following pyparsing code to match this:



    
    lpar = Literal('(').suppress()
    rpar = Literal(')').suppress()
    identifier = Word(alphas, alphanums + '_')
    emptyString = Empty()
    
    function_call = identifier  + lpar +
                    Optional(delimitedList(Group(identifier | emptyString))) + rpar
    

Issue here is that I need to pass actual empty string '' like this: `max(a,'',b)`

It doesn't match `max(a,,b)`. 

Is there any way to do this?

Thanks
Ashish

---
## 2011-03-18 06:40:54 - flyingeek - Multiline parser fail
Hi, I am quite new to pyparsing and python but having read many messages here, I'm still stuck for few hours .

Basically I want to extract data from quite unstructured text, here is the code:



    # BNF
    # eol = '\n' | '\r\n'
    # whitespace = ' '
    # blank = whitespace | '\t'
    # zip ::= [digit] 4*digit
    # city ::= {word[-_']}
    # line :: {digit|alpha|whitespace}eol
    # address_line ::= line
    # phone_sep ::= '.' | whitespace
    # phone ::= [+]{digit[phone_sep]}
    # phone_label ::= {alpha}
    # phone_line ::= phone_label[{blank}]:[{blank}]phone eol
    # address ::= {address_line} zip city eol {phone_line}
    
    # Since my BNF maybe wrong, have a look at the test string:
    
    test = '''
    Zone commerciale Auchan
    ZI des Paluds
    13400 Aubagne
    Tel : 04.98.98.98.99
    Fax : 04.99.99.99.99
    '''  
    
    # In real world I need to use unicode strings but for this forum I use ASCII for simplification.
    
    # From my tries, I had some problem with End of Lines, so first
    
    ParserElement.setDefaultWhitespaceChars(' \t')
    NL = Suppress(LineEnd())
    SL = Suppress(LineStart())
    
    # Since the address_line are really free form, I tried to define them as
    
    line = SL + restOfLine + NL
    address_line = line # just an alias for now
    zip = Word(nums,max=5,min=4)('zip')
    city = restOfLine
    phone_line = SL + Group(oneOf('Tel Fax') + Suppress(Literal(':')) +  restOfLine) + NL
    city_line = Group(SL + zip + city + NL)
    # So now
    #Let's try a search
    address = (phone_line | Group(SL + zip + city + NL) | address_line)
    pys = address.searchString(test)
    print(pys.dump())
    #>>> [[''], ['Zone commerciale Auchan'], ['ZI des Paluds'], [['13400', ' Aubagne']], [['Tel', ' 04.42.18.63.90']], [['Fax', ' 04.42.18.63.99']], ['']]
    #everything seems fine
    
    #lets try to parse, first let's say it is just a series of line
    address = OneOrMore(line)
    pyp = address.parseString(test)
    print(pyp.dump())
    #>>>['', 'Zone commerciale Auchan', 'ZI des Paluds', '13400 Aubagne', 'Tel : 04.42.18.63.90', 'Fax : 04.42.18.63.99', '']
    #once again all is ok
    
    # now lets try to be more specific
    address = OneOrMore(address_line) + city_line
    pyp = address.parseString(test)
    print(pyp.dump())
    # an exception is raised:
    #>>>pyparsing.ParseException: Expected start of line (at char 95), (line:7, col:1)
    # Why ?
    
    #Let's try with the whole bsn
    address = OneOrMore(address_line) + city_line + OneOrMore(phone_line)
    pyp = address.parseString(test)
    print(pyp.dump())
    # same exception is raised:
    #>>>pyparsing.ParseException: Expected start of line (at char 95), (line:7, col:1)
    # Why ? 


Any help would be appreciated.

Thanks.
Eric

#### 2011-03-27 04:58:02 - flyingeek
Well I give up and I'm using a regex:



    pattern = re.compile(ur'''
        # first lines
        #--------------------------------------------------
        \A             # beginning of string
        \n?            # strip first newline if any
        (?P<address>
           (?:.|\n)+   # take all chars including newline
        )
        \n             # we take only lines
        # mandatory zip city line
        #--------------------------------------------------
        ^                      # start at the beginning of line
        (?P<zipcode> \d{4,5} ) # take zipcode
        \s+                    # allow spaces beetween zipcode and city
        (?P<city> .+ )         # city
        $                      # until end of line
        # optional phone line
        #--------------------------------------------------
        (?:                        # non capturing group
           \n                         # yes begin by a newline
           ^(?:T[e]l\.?|Standard)    # start with Tel or Tl or Tl. or Standard
           \s+                        # allow spaces
           :                          # mandatory : separator
           \s+                        # allow spaces
           (?P<phone> .+ )            # capture number
           $                          # until eol
        )?                         # end of optional group
        # optional fax line
        #--------------------------------------------------
        (?:                        # non capturing group
           \n                         # yes begin by a newline
           ^Fax                       # start with Fax
           \s+                        # allow spaces
           :                          # mandatory : separator
           \s+                        # allow spaces
           (?P<fax> .+ )              # capture number
           $\n                        # until eol
        )?                         # end of optional group
        #--------------------------------------------------
                        # anything after
                ''', re.VERBOSE|re.MULTILINE|re.IGNORECASE|re.UNICODE)
    



---
## 2011-04-18 23:46:18 - chihiro - Plans for future releases
Hi Paul,

What is the roadmap for pyparsing 2.x ? What kinds of improvement can users expect to see in the not-so-far-distant future ? Thanks.

#### 2011-04-20 03:50:54 - ptmcg
Interesting question.  It gives me a chance to think through and write down some things that have been drifting around in various emails and scratch .PY files on my computer.

The trends for pyparsing's future are:
- switch over of main support from Python 2.x to Python 3.x, probably in Pyparsing 1.6; I expect this will happen before the end of 2011
- Pyparsing 2.0 would be time to discontinue support for Python 2.x entirely (that is, no more effort to keep back-compatibility with pre-3.0 Python, no more 1.x releases); probably a year after 1.6
- Current things on deck for the next release (1.5.6):
  . contribution by Raymond Hettinger to improve the parse action normalization robustness and compatibility
  . removal of exception cacheing, to address Python 3.0 memory leaks
  . enhanced version of `countedArray`, to accept an optional expression for matching the leading count integer
  . inclusion of more full featured SQL SELECT statement parser and Verilog parser in examples
  . minor enhancement to the shortcut version of setResultsName, to accept a special trailing '*' to indicate setting `listAllMatches=True`

Other things I've thought about, but if/when I get to them is undetermined:
- implementation with Cython (have dabbled with this off and on, recently taking another run at it)
- smarter compilation of expression into regular expressions (had some nice work in this direction submitted by Tal Weiss)
- conversion of string parsing to stream parsing, possibly using coroutines; maybe implement as a tandem project, alongside pyparsing (copyparsing? pycoparsing? coparsing?)
- revamp/simplification of ParseResults, perhaps some marriage of namedtuples and OrderedDict
- API conversion to PEP8
- multilingual API ('Grupo' and 'UnoOMas' map to 'Group' and 'OneOrMore', for instance)

I don't really have any *major* new features on the horizon. The scope of the parsing problem is pretty well-defined, and I think pyparsing covers it well enough. I've already somewhat bloated the API with helper methods that may or may not be generally useful (countedArray, upcaseChars, matchPrevious); if there is a big swell of interest in more helpers, I'll probably refactor them to a supporting package.  I *really* like that pyparsing is importable as a single Python source file, and I don't plan on changing that.

It has been a while since I put out any updated articles with recent enhancements to pyparsing, especially since the demise of Python Magazine.  I'm thinking of doing a little quarterly magazine just on Pyparsing, with a number of articles, updates to previously published work, maybe some reader questions/answers.  I've had nice results using the magcloud service from HP on another project.
#### 2011-04-20 04:44:14 - chihiro
Thanks Paul. 

By the way the select_parser.py example will only run well with packrat enabled. Otherwise it just crawls when parsing the 3 sample expressions.

As a newbie, how does one know when to turn on packrat ? Is that due to the grammar ?
#### 2011-05-06 14:39:56 - Traxus04
Hi Paul,
Not sure if this feature is in pyparsing already (I couldn't find it), or if it's in the pipeline, but I wrote a little function to make certain code a little more readable IMO:



    def ignoreCaseAndWS(literal):
         '''
         Returns pyparsing expresion where each word in literal is separate
    
         Changes 'string like this' to CL('string') + CL('like') + CL('this')
         '''
         words = literal.split()
         result = CL(words[0])
         for word in words[1:]:
             result += CL(word)
    
         return result


(CL = CaselessLiteral)

I found having long phrases as parse elements started to look untidy when every word was being added separately - but if you do a long literal you lose the benefit of ignoring whitespace between tokens and variations in case that don't matter
#### 2011-05-06 15:07:56 - ptmcg
Well I am certainly a fan of tidiness in defining parsers. And I too have seen cases where I needed to break up a simple list of strings so that I could be sure that comments and whitespace would be skipped properly between the different strings.  I'm a little worried about API bloat, but I can see that this crops up often enough to warrant adding a helper method.

I took your submission and fleshed it out a bit to support literal defintion options similar to oneOf.  Also, while I discourage people from using And, MatchFirst, etc. explicitly in their parsers, I often use them within helper methods in a quasi-functional style:



    def literalList(literals, caseless=False, keyword=False):
        '''
        Helper to quickly define a sequence of Literals, joined in an And
        expression.  Simpler than explicit addition of strings, but maintains
        the benefits of comment and whitespace skipping between literals..
    
        Parameters:
         - literals - a string of space-delimited literals, or a sequence or
           generator of string literals
         - caseless (default=False) - treat all literals as caseless
         - keyword (default=False) - define literals as Keywords instead of Literals
        '''
        if isinstance(literals, basestring):
            literals = literals.split()
    
        cls = {
            (True,  True)  : CaselessKeyword,
            (True,  False) : CaselessLiteral,
            (False, True)  : Keyword,
            (False, False) : Literal,
            }[caseless, keyword]
    
        return And(map(cls, literals))


`oneOf` has had to accommodate various forms of input sequences (and generators, too!), so I'm adding in that support from the start. You can certainly paste this into your own code now, and remove it after I get the next release out.

Thanks for the idea!
-- Paul
#### 2011-05-06 15:13:46 - ptmcg
That last post was untested, the line before the return should be:



    }[(caseless,keyword)]


#### 2011-08-23 15:12:13 - dimothy
Let me just say I absolutely *love* the API - so simple and straightforward, even a child can use it !

On the downside though -- the performance is a real deal-breaker.

In my case, I've got kind of a CSV file, TAB separated, with the number of lines running in the thousands, upper limit in the order of a couple of thousands.
Columns - 40 - with data types as: real numbers (thousands grouped, as in 1,234,567), integers, lots of dates (27.07.2011), general purpose text and more.

A typical file with ~2200 lines takes 16 sec. to parse, on a 2.8 GHz CPU. Tried to parse one with 100,000 lines but my patience ran thin, so I had to Ctrl+C it :(

So I found Cython -- and let me tell you it looks very promising.
Got pyparsing compiled with Cython out of the box !
Well, almost - just a few trivial tweaks needed.
By the way - I'm on Windows, mingw32, which is another story :)

Outcome -- a surprising 50% increase in performance - right from the start.

Now I'll start working on the pyparsing.pyx (Cython equivalent) file right away and begin to apply optimization tricks and techniques outlined on their doc page.

So obviously I'd opt for Cython as the next improvement of pyparsing. Actually any kind of improvement wich would speed up the library is top priority, IMO.

-= Dimitri =-

---
## 2011-04-21 15:31:44 - haggisbones - Parsing a filter expression with nestedExpr
Hi,

I'm new to pyparsing so please excuse any dumb questions :). I am trying to parse expressions of the form:
`'ip.src==192.168.1.1'`
`'(srcport lte 3020)` and `((protocol == udp)` and `not(ip.src==192.168.1.10))'`

The code I've pulled together so far is:


    
      # IP Address Filters
      ipexpr = Combine(Word(nums,max=3) + Literal('.') + Word(nums,max=3) + Literal('.') +
                       Word(nums,max=3) + Literal('.') + Word(nums,max=3))
    
      # Filter name
      fname = Combine(Word(alphanums) + Optional('.') + Optional(Word(alphanums)))  
    
      # Filter value
      fvalue = (ipexpr|Word(alphanums))                                             
    
      # Comparator operators (ignore spaces)
      compare = Combine(Optional(White()) + oneOf('== lt lte gt gte') + Optional(White()))
      #(Optional(Suppress(' ')) + CaselessLiteral('==') + Optional(Suppress(' ')))|CaselessLiteral('lte')|CaselessLiteral('lt')|CaselessLiteral('gte')|CaselessLiteral('gt')
    
      # Filter expression (e.g. 'ip.src == 192.168.1.1', 'protocol == udp', 'srcport lte 3020'  
      fexpr = fname + compare + fvalue
    
      # Nested parentheses
      terms = Forward()
      parens = nestedExpr('(',')',content = terms).leaveWhitespace()
    
      # Filter operators
      operator = CaselessLiteral('and')|CaselessLiteral('or')|CaselessLiteral('not')
    
      term = Combine(fexpr) | parens
      terms << term + ZeroOrMore(Optional(operator) + terms)
    
      terms.parseString(filter).asList()


It seems to cope with the basic case but can't deal with the nested brackets or the 'not' case. Any guidance as to where I am going wrong? I am trying to break the expressions down into their simplest form (e.g. `ip.src == 192.168.1.1`) + the operators used to combine them. This will allow me to verify network data against the filter set.

Best regards,
Martin

#### 2011-04-22 11:40:52 - haggisbones
What a difference a day makes :) ...

Made some progress but still have a couple of issues. New code is:

    def parseFilter(filter):  
    
      # a = 'not(ip.src==192.168.1.1)'
      #      => ['not', ['ip.src==192.168.1.1']]
    
      # b = '(srcport == 3020) or (not(ip.src==192.168.1.1) and (dstport lt 6000))'
      #      => [['srcport == 3020'], 'or', ['not', ['ip.src==192.168.1.1'], 'and', ['dstport lt 6000']]]
    
      # c = '(dstport lte 6000) or (not((ip.src==192.168.1.1) and (ip.dst=10.10.10.2)) and (srcport lte 3020))' 
      #      => [['dstport lte 6000']]
    
      # d = '(srcport == 3020) or (not(ip.src==192.168.1.1) and not(ip.dst==10.10.10.2))'
      #      => [['srcport == 3020'], 'or', ['not', ['ip.src==192.168.1.1'], 'and', 'not', ['ip.dst==10.10.10.2']]]
    
      # e = '(srcport==3020 and dstport lt 6000) or (ipu.src==192.168.1.1 and ipu.dst==10.10.10.2)'
      #      => [['srcport==3020', 'and', 'dstport lt 6000'], 'or', ['ipu.src==192.168.1.1', 'and', 'ipu.dst==10.10.10.2']]
    
      # IP Address Filters
      ipexpr = Combine(Word(nums,max=3) + Literal('.') + Word(nums,max=3) + Literal('.') +
                       Word(nums,max=3) + Literal('.') + Word(nums,max=3))
      #ipv6expr = 
    
      # Filter name
      fname = Combine(Word(alphanums) + Optional('.') + Optional(Word(alphanums)))  
    
      # Filter value
      fvalue = (ipexpr|Word(alphanums))                                             
    
      # Comparator operators (ignore spaces)
      compare = Combine(Optional(White()) + oneOf('== lt lte gt gte') + Optional(White()))
    
      # Filter inversion
      notexpr = Optional(CaselessLiteral('not'))
    
      # Filter operators
      operator = CaselessLiteral('and')|CaselessLiteral('or')
    
      # Basic filter expression (e.g. 'ip.src == 192.168.1.1', 'protocol == udp', 'srcport lte 3020' )
      fexpr = fname + compare + fvalue
    
      # Full filter expression '(ip.src == 192.168.1.1)'
      fullfexpr = (Optional(Suppress('(')) + Combine(fexpr).leaveWhitespace() + Optional(Suppress('(')))
    
      # Nested parentheses 
      terms = Forward()
      parens = notexpr + nestedExpr('(',')',content = terms).leaveWhitespace()
    
      subterm =  parens | fullfexpr
      terms << (subterm + ZeroOrMore(Optional(operator) + terms))
    
      return terms.parseString(filter).asList()


So, when the filter c is used it doesn't work (doesn't like 'not(filter 1 or filter2)' type expressions). Also, when brackets are omitted it returns a string instead of a single element list (probably can work around that but would be nice if it was consistent). Any ideas on how to fix this?

Best regards,
Martin
#### 2011-04-23 19:57:20 - ptmcg
Martin -

The quick response is that fullfexpr is ending with '(', but I think you really want ')'.

You have certainly gotten a lot of progress on your parser, but some of your efforts are 
working against each other.  `nestedExpr` is there to simplify parsing open and 
closing delimiters. For this parser, you can define a full BNF, and implement 
parenthetical grouping and nesting using a recursive parser.  Also, it is 
unnecessary to sprinkle `Optional(White())` expressions in your grammar, pyparsing skips 
over whitespace by default.

Here is a revision of one of the pyparsing examples, using your test cases.

    
    from pyparsing import *
    
    TRUE = CaselessLiteral('True')
    FALSE = CaselessLiteral('False')
    ipv4_address = Regex(r'\d{1,3}(\.\d{1,3}){3}')
    ipv6_address = Regex(r'[0-9a-fA-F:]+')
    ip_address = ipv4_address | ipv6_address
    integer = Word(nums).setParseAction(lambda t:int(t[0]))
    
    ident = Word(alphas, alphanums)
    fullident = Combine(delimitedList(ident, '.'))
    
    compareOp = oneOf('== lt lte gt gte')
    term = fullident | ip_address | integer
    boolComparison = Group(term + compareOp + term) | TRUE | FALSE
    
    parser = operatorPrecedence(boolComparison,
        [
        ('not', 1, opAssoc.RIGHT),
        ('and', 2, opAssoc.LEFT),
        ('or', 2, opAssoc.LEFT),
        ])
    
    tests = '''\
        not(ip.src==192.168.1.1)
        (srcport == 3020) or (not(ip.src==192.168.1.1) and (dstport lt 6000))
        (dstport lte 6000) or (not((ip.src==192.168.1.1) and (ip.dst=10.10.10.2)) and (srcport lte 3020))
        (srcport == 3020) or (not(ip.src==192.168.1.1) and not(ip.dst==10.10.10.2))
        (srcport==3020 and dstport lt 6000) or (ipu.src==192.168.1.1 and ipu.dst==10.10.10.2)'''.splitlines()
    
    for t in tests:
        t = t.strip()
        print t
        print parser.parseString(t).asList()
        print
    


Prints:



    not(ip.src==192.168.1.1)
    [['not', ['ipsrc', '==', '192.168.1.1']]]
    
    (srcport == 3020) or (not(ip.src==192.168.1.1) and (dstport lt 6000))
    [[['srcport', '==', '3020'], 'or', [['not', ['ipsrc', '==', '192.168.1.1']], 'and', ['dstport', 'lt', '6000']]]]
    
    (dstport lte 6000) or (not((ip.src==192.168.1.1) and (ip.dst=10.10.10.2)) and (srcport lte 3020))
    [['dstport', 'lte', '6000']]
    
    (srcport == 3020) or (not(ip.src==192.168.1.1) and not(ip.dst==10.10.10.2))
    [[['srcport', '==', '3020'], 'or', [['not', ['ipsrc', '==', '192.168.1.1']], 'and', ['not', ['ipdst', '==', '10.10.10.2']]]]]
    
    (srcport==3020 and dstport lt 6000) or (ipu.src==192.168.1.1 and ipu.dst==10.10.10.2)
    [[[['srcport', '==', '3020'], 'and', ['dstport', 'lt', '6000']], 'or', [['ipusrc', '==', '192.168.1.1'], 'and', ['ipudst', '==', '10.10.10.2']]]]
    


#### 2011-04-23 19:59:27 - ptmcg
Look at the example SimpleBool.py to see how to enhance the parser with evaluator classes that will help you actually evaluate these parse results with provided values for srcport, port, etc.
#### 2011-04-24 16:52:31 - haggisbones
Hi Paul,

Thank you very much for your response on this. I've been testing it out and trying to make a few changes to get things in the format I need for the next part of my program. Unfortunately everything I do breaks it! I guess I am not understanding something in the way it all works :(. Sorry, I am still fairly new to this Python programming lark :).

The current issues I have are:

1) I would like to preserve the extracted filter statements as single strings e.g. 'ip.src == 192.168.1.1'. This makes the next stage processing simpler for me. My cunning plan is to trawl through the list produced by the parseFilter algorithm recursively and verify each individual filter expressions result against a data packet. I would then replace the entry in the list with the boolean result of the check and then use the SimpleBool structure to evaluate the total expression. The way I was planning to traverse the list was to check each list elements type. If it was 'list' then I recurse. A string would result in a check to see if it was a filter or an operator and then act accordingly. 

2) In the case `'(dstport lte 6000) or (not((ip.src==192.168.1.1) and (ip.dst=10.10.10.2)) and (srcport lte 3020)'` it doesn't seem to be able to cope with the grouped logic statement and fails to give the correct nested list

3) I'd rather preserve the `'ip.src'` construct if possible to be consistent with code elsewhere. I thought I could do that by replacing the delimitedList bit with my 'fname' statement it wasn't too fond of that idea

Thanks again for taking the time to look at this and also for writing pyparsing ... I think I might lose the plot if was doing all this with RegEx!

Best regards,
Martin

#### 2011-04-24 17:14:43 - ptmcg
2) fails to parse because of the typo in the ip.dst comparison, you use '=', should be '=='.

3) I incorrectly used the Combine class to wrap the delimitedList.  Instead, use `fullident = delimitedList(ident, '.', combine=True)`

For 1), try `boolComparison = originalTextFor(term + compareOp + term) | TRUE | FALSE`

`originalTextFor` will return the original source text from the input string.

#### 2011-04-28 14:07:33 - haggisbones
Perfect! All working a treat now. Thanks for all the support Paul and apologies for my typos ... that'll teach me to try and code when I'm knackered :)

Best regards,
Martin

---
## 2011-04-21 20:10:23 - JesterEE - removeQuotes Potential Volatile
Hey Paul,

I say if every time I use your module, but THANK YOU SO MUCH!!!!!  pyparsing is (by far) the best python module I keep in my arsenal.

Anyway, onto the post topic.  I recently have been working on a very complex grammar that involves a lot of stacked grammar statements, and parse actions to get the results in the fashion I need them.  I have an instance where I want to take a quoted string, and after passing it through the parseAction, get the unquoted name and place it in a named parse variable.  I originally tried to use this:



    unqtdVar = dblQuotedString( Word(alphanums)('var') )


but pyparsing didn't like the setResultsName inside the QuotedString function.  Fair enough.  I then made my own hacky parse action to do the task:



    def Unquote(tokens): # setParseAction - Unquote
        return tokens[0][1:-1]
    
    unqtdVar = dblQuotedString( Word(alphanums) ).setParseAction(Unquote())('var')


This worked, but I wasn't too happy about just chopping off the front and the back.  But I kept it till I had a chance to look at it again.

Later, I found that you included your own helper function removeQuotes parse action.  To my surprise, it was the same one I cooked up *shudder*.  I think I came up with a better way, and wanted to share it with you to see what you think.



    def Unquote(tokens): # setParseAction - Unquote
        tokens[0]=tokens[0].strip(''') #Strip single quotes
        tokens[0]=tokens[0].strip(''') #Strip double quotes
        return tokens[0]
    unqtdVar = dblQuotedString( Word(alphanums) ).setParseAction(Unquote())('var')


Though it's not as quick of an operation as string slicing, you are ensured that the parser won't lop off something it shouldn't.

-JesterEE

#### 2011-04-23 19:14:48 - ptmcg
Thanks for the high praise! I'm glad pyparsing is getting a good workout.

`dblQuotedString` is not a constructor, it is itself a complete expression.  
All you should need to get an unquoted string extracted would 
be `dblQuotedString.setParseAction(remoteQuotes)('var')`.  Why are you 
including the `Word(alphanums)` argument to `dblQuotedString`?

-- Paul
#### 2011-05-04 10:03:35 - JesterEE
Thanks Paul ... *hangs head in shame*

-JesterEE
#### 2011-05-04 11:37:17 - ptmcg
No sweat, glad it was an easy one! :)

-- Paul

---
## 2011-04-24 08:11:20 - DragonSpawn - Subsequent calls give different results.
I am stumped.
I am using pyparsing to merge a bunch of files.

I have created a function called `ParseFile()` which contains all of the pyparsing calls and to this i submit the name of the file i want to parse.
Different files may want to be compared to the same file and this is where I noticed a problem with that the first time this file is parsed it gives an incorrect result, but subsequent parses of the same file gives the correct result.

For example:
Say I want to parse example.txt which contains the text:

    ;comment
    
    { = Strings
        WHATEVER = {
        {
        MOO
        }
        }
    ;
    }

The first time i run pyparsing's parseFile on this file it returns a list which looks like this:

    [[';', 'comment\r'], ['=', ' Strings\r', ['WHATEVER', '=', , ';\r']], '']

This may look correct but there are quite a few \n missing in this result. The reason is because I would like to keep my line spacings for after the files have been merged.

However immediately after this result has been returned a different file wants to be merged with this same basefile, so it gets parsed again with the same identical call as before and this time it returns:

    [[';', 'comment\r'], '\n', ['=', ' Strings\r', ['WHATEVER', '=', ['\n', ['\n', 'MOO', '\n']], ';\r']], '']

Here you can see that I get quite a bunch of '\n'.

The way I have worked around this for now is that I added a boolean to my ParseFile function which just flags if I want to make an empty call to pyparsing. So first I call ParseFile with the dummy flag and then I call it again immediately after without the dummy flag.

Example:

    if(dummy):
                content = fileParser.parseString('').asList()
            else:
                content = fileParser.parseFile(f, parseAll=True).asList()

Then it magically works the next time round I call my ParseFile() function. However what is even stranger is that it DOESN'T work if i just do:

    fileParser.parseString('')
    content = fileParser.parseFile(f, parseAll=True).asList()

Then it will still be incorrect the first time i call my ParseFile() function. So there seems to be something going on behind the scenes if I have to go out of scope between the calls for it to work.

#### 2011-05-01 01:11:18 - ptmcg
Yep, that's pretty weird, definitely should not work that way.  Can you post your parser code so I can reproduce this behavior?

---
## 2011-05-02 08:11:43 - jenia2009 - simple calculator

Please, if someone can help me, it would be of great help and highly appreciated.

Thanks

f

#### 2011-05-02 08:28:42 - jenia2009
The interface for writing a question is not conducive to good formatting and clear writing.
That is why i wrote my actual question on paste bin.
If you wish for me to re-write it here, please let me know.

Thanks

---
## 2011-05-06 02:06:33 - BrenBarn - Matching tags
Hi, I'm getting started with pyparsing and it looks quite neat.  I made some simple parsers and am now trying to do more complicated recursive stuff.

The problem I'm having is in trying to match HTML-style tags where an opening tag has to be matched by a closing tag with the same name.  I looked at nestedExpr, but this only seems to handle cases where you know the form of both delimiters in advance; as far as I can see, it doesn't handle the case where the end-delimiter depends on the beginning delimiter.  I also saw the makeHTMLtags thing, but that doesn't work for me because I want a generic way to match any open/close tag pair.

Basically, I just want to know how to write a parsing element that will say 'match an opening tag of the form {foo} and all its contents up to the corresponding {foo}, allowing for nested tags inside, including possible nested {foo}s'.  What I tried is this:


    thingy = Word(alphas)
    tag = Forward()
    tag << '{' + thingy + '}' + Group(OneOrMore(Word(alphas) | tag)) + '{/' + matchPreviousExpr(thingy) + '}'
    
    print tag.parseString('{hey}there is {whoa}really{/whoa} a thing{/hey}')


But this gives me the cryptic exception `'pyparsing.ParseException:  (at char 0), (line:1, col:1)'`.  So how can I accomplish my task?

Thanks.

#### 2011-05-06 03:06:58 - ptmcg
The source distribution for 1.5.5 includes an example tagCapture.py that does what I think you are asking. (Need to upload that to the Examples wiki page.)

Welcome to Pyparsing!
-- Paul
#### 2011-05-06 03:11:09 - ptmcg
Hmmm, I'm just running that example now, and it doesn't seem to work... :(
#### 2011-05-06 03:47:23 - ptmcg
Wow, I thought I had this working, but it turned up some new bugs in pyparsing!

Try this script, it should work with the latest released version of pyparsing:



    # 
    # tagCapture.py
    #
    # Simple demo showing how to match HTML tags
    #
    
    from pyparsing import *
    
    src = 'this is test <b> bold <i>text</i> </b> normal text '
    
    def matchingCloseTag(other):
        ret = Forward()
        ret << anyCloseTag.copy()
    
        def setupMatchingClose(tokens):
            opentag = tokens[0]
    
            def mustMatch(tokens):
                if tokens[0][0].strip('<>/') != opentag:
                    raise ParseException('',0,'')
    
            ret.setParseAction(mustMatch)
    
        other.addParseAction(setupMatchingClose)
    
        return ret
    
    for m in originalTextFor(anyOpenTag + SkipTo(matchingCloseTag(anyOpenTag), 
                                                  include=True,
                                                  failOn=anyOpenTag) ).searchString(src):
        print m.dump()
    
    for m in originalTextFor(anyOpenTag + SkipTo(matchingCloseTag(anyOpenTag), 
                                                  include=True) ).searchString(src):
        print m.dump()


prints


    ['<i>text</i>']
    ['<b> bold <i>text</i> </b>']


-- Paul
#### 2011-05-06 11:25:05 - BrenBarn
Hmmm, thanks for the help, that code doesn't work for me.  It produces no output.  There are a couple things I see in it that puzzle me, so I'd like to understand them, to understand why it's not working or how it ought to work.

How is this


    ret = Forward()
        ret << anyCloseTag.copy()

different from just


    ret = anyCloseTag.copy()


Why is the parseAction taking only a single 'tokens' argument instead of the 's, l, c' format described in the docs?

Lastly I don't understand how the various parseActions there are working.  It looks like the SkipTo calls a function that sets a parseAction on anyOpenTag, and that parseAction goes back and sets a second parseAction on the SkipTo element.  How does this code keep track of which tag is being matched?  Are the parseActions called in left-to-right order so that the open tag's parseAction sets the mustMatch on the remainder of the expression, and then that mustMatch parseAction subsequently runs and checks if the open and close tags match?

Thanks for the help.
#### 2011-05-06 12:15:08 - BrenBarn
Okay, after poking around a bit I managed to get it to deliver the desired output by going into the mustMatch function and changing 'tokens[0][0]' to 'tokens[0]'.  I'm not sure why it had tokens[0][0] there, because the tokens list seems to just be a flat list.

However, it still doesn't completely do what I want, because if I give it this input I get the following output:



    src = 'this is test <b> bold <i>text</i> and <b>even bolder and <i>more italic</i></b> text and </b> normal text'
    
    ['<i>text</i>']
    ['<i>more italic</i>']
    ['<b> bold <i>text</i> and <b>even bolder and <i>more italic</i></b>']


The problem is that it's not extracting the nested `<b>...</b>` tag, only the 
outer `<b>` and the `<i>`s.  So it seems that it can handle tags nested in other tags, but not tags nested within other instances of themselves.  How can it be made to handle this case?

Thanks.

---
## 2011-05-10 09:21:40 - skeeved - help with delimitedList
Having a hard time getting to grips with pyparsing, especially as related to syslog parsing.

Using the following line as a sample, how would you parse the delimited list of key=value pairs?  I've got the date/time, hostname, syslog tag parts without problem but I'm unsure how to use the delimitedList operator.

I'm using the following:



    equals = Literal('=').setName('equals')
    key_value = Word(printables) + equals + Word(printables)
    delimitedList(OneOrMore(key_value)) + Optional(Word(printables))
    
    2011-05-08T04:07:18-04:00 host01 postfix/qmgr[12757]: 5A7D92478EE: from=<7obtx4-1p275-xe8n7-isclz3-185ssr0-h-m2-20110508-fff087ed817292ee@gsitoysrus.bounce.ed10.net>, size=13908, nrcpt=1 (queue active)

I get the following error:

    Expected equals (at char 104), (line:1, col:105)

#### 2011-05-10 13:58:39 - ptmcg
Your leading expression in key_value can't be 'Word(printables)' unless you are 
sure there will be whitespace between the key and the '=' sign.  Unlike 
regexp's, pyparsing does no lookahead, and so it will read the complete string 
from `'from=<7...'` to `'.net>,'`.  To see for yourself, add '.setDebug()' on that 
leading expression to see what gets matched.

You will have equal trouble with the value, since it is going to read everything 
up to AND INCLUDING the delimiting comma, and then decide that, since there is 
no comma to read, that the list is completed.

Since your key is (presumably) any printable character other than '=', and your value 
is any printable character other than ',', you can define key_value as 
`Word(printables.replace('=','')) + '=' + Word(printables.replace(',',''))`.

Pyparsing makes very few assumptions about what you want (other than the skipping of whitespace), and works purely left-to-right.  If you want it to do any looking ahead, you'll have to include that yourself in your parser.  See if there is a log parser on the examples page that might give you some other insights/hints toward your project.

Welcome to Pyparsing!

-- Paul
#### 2011-05-11 08:43:20 - skeeved
Thanks for the advice.  I did figure out that using printables was causing it to eat all the characters.

It's easy to forget that the 'alphas', 'nums', 'printables' etc are simply strings.

---
## 2011-05-28 21:42:47 - basilevs - How to extract named data from ParseResults with order preserved?
I'm writing subset-of-C to Python converter.
I'm now stuck with a problem of parsing a list of expressions of different types.

For example: a sting

    'ctx, vect_2_1[8], vect_3_2, 22.75841209'

contains a named variable references, numerical literals and array reference.

pyparsing processes this string without problems, but i can't build a typed ordered representation of this list using ParseResults.

Illustrative example follows:



    from pyparsing import Literal, SkipTo, lineEnd, restOfLine,  Word, alphanums, White, Combine, Group, Forward, nums, alphas, ParserElement, ParseException
    from pyparsing import OneOrMore, ZeroOrMore, Optional, Or
    from pyparsing import ParseResults
    
    signed = Optional(Literal('-')|'+')+Word(nums)
    number = Combine(signed + Optional('.'+Word(nums)) + Optional('e'+signed)).setResultsName('number')
    name = Word(alphanums+'_').setResultsName('name')
    
    index=Group(Literal('[')+Word(nums) +']').setResultsName('index')
    indexedValue = Combine(name.setResultsName(None)+index.setResultsName(None)).setResultsName('indexedValue')
    
    expression = indexedValue|number|name
    expressionList = Group(Optional(expression+ZeroOrMore(Literal(',')+expression))).setResultsName('list')
    
    tokens = expression.parseString('22.75841209', True)
    tokens = expressionList.parseString('ctx, vect_2_1[8], vect_3_2, 22.75841209', True)
    
    print tokens.asXML('result')
    #I can get all items with order preserved as strings:
    print [token for token in tokens['list']     if token != ',']
    
    #As sorted named data (items() sorts tokens by name):
    print [str(type)+':'+token for type,token in tokens['list'].items() if token != ',']
    
    #How do I get named data with order preserved?



#### 2011-05-28 21:44:27 - basilevs
Output:


    <result>
      <list>
        <name>ctx</name>
        <ITEM>,</ITEM>
        <indexedValue>vect_2_1[8]</indexedValue>
        <ITEM>,</ITEM>
        <name>vect_3_2</name>
        <ITEM>,</ITEM>
        <number>22.75841209</number>
      </list>
    </result>
    ['ctx', 'vect_2_1[8]', 'vect_3_2', '22.75841209']
    ['indexedValue:vect_2_1[8]', 'name:vect_3_2', 'number:22.75841209']


#### 2011-05-28 21:47:58 - basilevs
I've explored the source and see no way to extract positional and name information simultaneously - it is stored in private variable `__tokDict` and can't be obtained from public API.

#### 2011-05-30 20:57:32 - ptmcg
I looked at how asXML does it, and the following code works very similarly:


    valuemap = dict((id(v),k) for k,v in tokens.list.items())
    for t in tokens.list:
        if id(t) in valuemap:
            print valuemap[id(t)],
        else:
            print '<none>',
        print t

prints:


    <none> ctx
    number 22.75841209
    indexedValue vect_2_1[8]
    name vect_3_2


#### 2011-07-02 02:50:01 - basilevs
Thanks this works fine.

---
## 2011-06-03 01:09:45 - sebpiq - Need help with building a nested expression
Hi !

I have failed in building an expression for parsing the following :

    - nested two parenthesis, ex :


    ' bla bla (Thats the content (some other)) blabla '


    - BUT only if the two parenthesis contain a nested 'wiki-like' link (ex : '')

For example : 



    ''' blabla (This shouldn't ( match ) !! ), 
    the following outer parenthesis shouldn't match : ( blabla ( but [[ this | [[should]] match (!) ]] ! ) bla bla )
    '''


So, to summarize, only the part `'( but  match (!) ]] ! )'` should match the expression ! So nested parenthesis, but only if the first pair directly contains a nested '[[' ']]'. I have tried many many different things (mainly with nestedExpr, and by setting the 'content' parameter), but I cannot have it working, and it's making me crazy !!!

Thanks for your help !

#### 2011-06-03 01:13:22 - sebpiq
and of course (stupid) I got tricked by the nasty wiki-links ...

<ul class="quotelist"><li>- BUT only if the two parenthesis contain a nested 'wiki-like' link (ex : '')</li></ul>
must be read : 

BUT only if the two parenthesis contain a nested 'wiki-like' link (ex : '')

etc ...
#### 2011-06-03 01:14:40 - sebpiq
Crap ... got me again ... :-(
#### 2011-06-03 07:07:05 - ptmcg
Sorry about wikispaces' crazy markup, it is a constant hassle for everybody.

So you *must* have at least one wiki-link, and zero or more just plain text words or punctuation, and not sure of the order.

For these situations where order may be reversed, use pyparsing's Each class (shortcut using the '&' operator).  Each is smart about keeping straight which expressions are required and which are optional.  This parser works with your sample text:



    test = '''( blabla ( but [[ this | [[should]] match (!) ]] ! ) bla bla )'''
    
    from pyparsing import *
    
    punc = '.?!-'
    plain = Word(alphas+punc)
    wikimarkup = originalTextFor(nestedExpr('[[',']]'))
    
    wikicontent = (OneOrMore(wikimarkup) & ZeroOrMore(plain))
    
    wikiexpr = originalTextFor(nestedExpr(content=wikicontent))
    # use this version for nested results
    # wikiexpr = (nestedExpr(content=wikicontent))
    
    print wikiexpr.searchString(test)


prints:



    [['( but [[ this | [[should]] match (!) ]] ! )']]


If you leave out the wrapping originalTextFor in wikiexpr, then you'll get the structured output from nestedExpr using the nesting delimiters to define a nested list of lists:



    [[['but', '[[ this | [[should]] match (!) ]]', '!']]]


#### 2011-06-05 13:59:40 - sebpiq
Hi !

Great ! Thanks for your answer !

Actually instead of :



    Word(alphas+punc)


I was thinking of using :



    CharsNotIn('[()')


But I don't know how well it goes with the '&'... I'll test tomorrow.

Appart from that just a little feedback : pyparsing is really great ! I already managed a couple of very complicated parsings in only a couple of very readable lines ... and it's now part of my Python stack ! However, I think the learning curve is a bit steep ! Of course, parsing is no simple deal, however I think the users would really benefit in good online docs (the link on the main page seems to be broken). The current ones are good, but they somehow lack of nice very simple examples, and they are not always easy to browse when you don't know what you are looking for. Have you considered rewriting them with sphinx for example, ... putting them on readthedocs, and adding a couple of example as doctests ? I guess that would be a lot of work though :-S

_[ED - docstrings have been greatly enhanced, converted to sphinx, and are now
hosted on readthedocs at https://pyparsing-docs.readthedocs.io/en/latest/index.html]_
---
## 2011-06-04 14:45:57 - pf_moore - Parser for multiple items with no delimiter
I have a string which contains some initial text followed by one or more 'abilities'. An ability is one of the keywords 'Active', 'Passive' or 'Aura', optionally prefixed by 'Unique', then a colon, then free text. The end of an ability is not delimited - it's recognised by either the start of the next ability or the end of the string.

What is the best way to write a parser for such a string? It's easy enough to write one for a single ability, but I'm having trouble coming up with a means of repeating them.

I guess what I want is some way of saying that the trailing text in an ability should use non-greedy matching (in regex terms). I tried Regex('.*?') but that didn't work :-(

Thanks
Paul.

#### 2011-06-05 06:48:45 - ptmcg
So if you wrote a quasi-BNF for this kind of thing, it would be:


    ability-header ::= ['Unique'] ('Active'|'Passive'|'Aura') ':' 
    ability-defn ::= ability-header ability-text

And now the trick is, how do you define ability-text? 

To paraphrase Chevy Chase from Caddyshack, 'Be the parser.' If you looked at some abilities, how would you know they had ended? Is this line-oriented? Can you just use restOfLine for ability-text? This would take you to the end of the current line, and would assume that the next ability starts on the next line.  

But if your ability-text goes on for several lines, then you must have some other way to detect that it has ended. Well, the start of the next ability is the next ability-header, or the end of the string. You have two ways to think of this (I'll write in pseudo-pseudo-BNF first, since we are 'being the parser'):


    ability-text ::= <everything up to the next ability-header OR the end of string>
    OR
    ability-text ::= <anything that is not an ability-header>*

In pyparsing, the first is easy:


    ability_text = SkipTo(ability_header | StringEnd())

If you are *only* parsing abilities, then this is the simplest and the way to go. But if this is just the first step in writing some kind of data that will have abilities, weapons, shields, spells, etc., then you would need to expand the terminating SkipTo expression to something like any-header, and then define any-header as:



    any_header = ability_header | weapon_header | spell_header | ... etc.


The second approach is similar, but instead of SkipTo, you define what is allowable ability-text, something like:


    ability_text = OneOrMore(~ability_header + Word(printables))

This is like saying 'take anything non-whitespace, but first make sure it's not the start of the next ability.' This is how you do lookahead with pyparsing.  Your Regex approach doesn't work, because the regex's lookahead can't see outside the '.*' definition to the next expression to see what it should match on.
#### 2011-06-05 09:48:31 - pf_moore
Thanks - the SkipTo approach does exactly what I need. Yes, there are extra data items, but they come before the abilities, so I can easily parse these and then whatever is left is a series of ability-defns.

Thanks for the comprehensive reply - it really helps me to understand how to think about this stuff.

---
## 2011-06-10 12:12:08 - ChiefDan - Getting started, couple of questions
Looks like pyparsing might be perfect for a project I am getting started on. My source data is going to be a CART model, here is a quick snippet:


    1) root 132 236.000 LOW ( 0.2500 0.6288 0.12120 )   
       2) SALINITY<31.55 34  60.450 HIGH ( 0.6471 0.1471 0.20590 )   
         4) DWL<6.61 28  37.410 HIGH ( 0.7857 0.1071 0.10710 ) * 
         5) DWL>6.61 6   7.638 MEDIUM ( 0.0000 0.3333 0.66670 )   
          10) MOON<0.98 4   0.000 MEDIUM ( 0.0000 0.0000 1.00000 ) * 
          11) MOON>0.98 2   0.000 LOW ( 0.0000 1.0000 0.00000 ) * 
       3) SALINITY>31.55 98 126.700 LOW ( 0.1122 0.7959 0.0918


From what I gather, the indentation tells me the nesting. Will it be possible to use whitespace for the nesting? I thought this would be similar to parsing JSON data, however JSON has {} to group the sections which makes things a bit more straight forward.

Thanks for any pointers.

Dan

#### 2011-06-10 12:27:47 - ChiefDan
Ahhh I didn't see the indented Grammar example!

---
## 2011-06-12 05:17:38 - gareth8118 - Grammar embedded in original text - how to handle?
Hi there,

I'm trying to write a parser for the gEDA file format. Some aspects of the grammar are effectively embedded in the file itself, e.g. the path primitive



has the number of path element lines ('numlines') embedded in the first line, then the lines themselves on the next (1:numlines) lines of the file.

How would one capture this kind of grammar definition in a pyparsing grammar? Presumably I have to create a grammar defn on the fly, but I'm not sure the best way to go about that.

Cheers
Gareth

#### 2011-06-12 09:00:17 - ptmcg
Pyparsing has some features built into it for creating an adaptive 'on-the-fly' grammar. Fortunately, the one you describe is very easy, and is in fact already implemented in the `countedArray` helper method, as in this example:



    >>> from pyparsing import *
    >>> wd = Word(alphas)
    >>> words = countedArray(wd)
    >>> words.parseString('5 ABD DEF HDK SLKJ OIUEWR').asList()
    [['ABD', 'DEF', 'HDK', 'SLKJ', 'OIUEWR']]
    >>>


In your case, you'll need to define the grammar for the various forms of path lines (let's call it `pathexpr`), and then in place of `numpathlines path path path...` use `countedArray(pathexpr)`.

#### 2011-06-24 15:03:37 - gareth8118
Thanks, Paul. I think I'll use the `countedArray` implementation as 'inspiration' rather than using it direct because I want to check for the line end and the end of the first line /after/ the count value, then have various counted path expressions after that.

---
## 2011-06-13 05:36:36 - ChiefDan - Documentation page does not exist
The documentation link in the left menu area goes to a link that times out.
Is there another API reference?

#### 2011-06-13 05:38:10 - ChiefDan
Sorry, the Documentation link does work, however the link on that page to:  does not work.
#### 2011-06-13 10:16:41 - ptmcg
Thanks, I need to find a new home for this.
#### 2011-07-06 20:28:21 - ptmcg
PyPI now hosts online docs for registered packages. I've updated the wiki doc link to point to the latest version of the docs.  Thanks for letting me know!

_{ED - docs are now online at https://pyparsing-docs.readthedocs.io/en/latest/index.html}_

---
## 2011-06-16 20:34:52 - grantma1 - StringEnd() hiding ParseException from parse failure at end of string or file 
Hi!

Any advise on this?  The grammar structures I have can be 100 or so lines long as I am grouping lines in a DNS zone file...

I would like to get ParseFatalException on the last structure and get out of there...

#### 2011-06-20 18:29:20 - ptmcg
One suggestion might be to try using '-' in place of '+' operator in selected places. '-' will raise a ParseFatalException if the following expressions are not matched.  This is a good way to keep repeating expressions like ZeroOrMore or OneOrMore from consuming a partial list of matches without signaling a bad entry.  For instance, if you had an expression like:



    twoDigitReal = Regex(r'\d+\.\d{2}')
    money = Group('$' + twoDigitReal)
    data = '$1.00 $2.20 $10.23 $.23'
    print OneOrMore(money).parseString(data)


prints



    [['$', '1.00'], ['$', '2.20'], ['$', '10.23']]


Changing the call to parseString to add parseAll=True gives:



    Traceback (most recent call last):
      File '<stdin>', line 1, in <module>
      File 'c:\python26\lib\site-packages\pyparsing-1.5.5-py2.6.egg\pyparsing.py', l
    ine 1100, in parseString
        raise exc
    pyparsing.ParseException: Expected end of text (at char 19), (line:1, col:20)


Not much help.

Since we know that after the '$' there *has* to be a real number, we can change '+' to '-', meaning 'don't backtrack if any errors found after this expression.



    money = Group('$' - twoDigitReal)


Now we'll get a ParseFatalException from the last value.



    >>> print OneOrMore(money).parseString(data)
    Traceback (most recent call last):
      File '<stdin>', line 1, in <module>
      File 'c:\python26\lib\site-packages\pyparsing-1.5.5-py2.6.egg\pyparsing.py', l
    ine 1100, in parseString
        raise exc
    pyparsing.ParseSyntaxException: Expected Re:('\\d+\\.\\d{2}') (at char 20), (lin
    e:1, col:21)



---
## 2011-06-17 12:57:20 - JesterEE - CharsNotIn Enhancement Request
Recently I was working with the 'CharsNotIn' class and I wanted to define a set that takes a superset of characters into consideration and subtracts the contents of 'CharsNotIn'.  One can accomplish this with code like this that matches both Windows and Unix paths.



    LegalPathChars = ''.join(set(printables) - set('<>:'/\\|?*'))
    WinPath = Combine( ( (Combine(Word(alphas, max=1) + ':') + oneOf('\\ /')) | Literal('\\').setParseAction(lambda: '\\\\') ) + OneOrMore( Word(LegalPathChars) + Optional( oneOf('\\ /') ) ) )
    UnixPath = Combine( oneOf('~ /') + OneOrMore(Word(LegalPathChars) + Optional('/')) )
    path = WinPath | UnixPath


This uses the set type added to python in V2.6 to create a set of 'printables' and a set of illegal path characters and then takes the difference and places them in a pyparsing friendly character string.

It would be nice to have this feature in CharsNotIn as an optional 'superset' argument.

Using my PEP (pyparsing Enhancement Proposal), the former code would look like:



    LegalPathChars = CharsNotIn('<>:'/\\|?*' superset=printables)


Paul,
What do you think?  I'm sure you're going have something more spectacular rolled up your sleeve, but I thought I'd ask ;).  Also, if someone has better path matching code than the above, I'm all ears!

-JesterEE

#### 2011-06-20 19:27:21 - ptmcg
Thanks for the vote of confidence! :)

Before making a change like this to an API, I try to step back and make sure I understand what you are asking for. It sounds like you want an expression that:
- contains any printable
- except for the characters '<>:'/\\|?*'

Is that it?

-- Paul
#### 2011-06-21 05:53:02 - JesterEE
Yup, thats exactly what I want to do.  As a previous Perl programmer (before I made friends with the snake) I learned the expression 'TIMTOWTDI' (There Is More Than One Way To Do It).  This holds true in this case, but I thought the 'more pythonic' syntax implementation would be with the PEP.

-JesterEE
#### 2011-06-26 08:23:38 - ptmcg
In the spirit of preferring assertive conditions over negative ones, I'm going to implement this in the Word class, by adding an excludeChars argument - your superset would be the strings used to define the Word, and the characters to exclude would be given in the optional excludeChars argument.  Here's how my unit test looks:


    from pyparsing import Word, printables
    allButPunc = Word(printables, excludeChars='.,:;-_!?')
    
    test = 'Hello, Mr. Ed, it's Wilbur!'
    result = allButPunc.searchString(test).asList()
    assert result == [['Hello'], ['Mr'], ['Ed'], ['it's'], ['Wilbur']]

I'm just packaging up 1.5.6 now, I hope to have it up on SVN and PyPI by this afternoon.

-- Paul
#### 2011-06-28 07:23:54 - JesterEE
Paul,

I like that implementation much better!  Thanks for considering (and subsequently implementing) my suggestion.

-JesterEE

---
## 2011-06-20 01:49:10 - haikalle - How to read a file and take info
I have tried to find a solution here but I'm going nowhere. I'm trying to read text.txt file and then take Vertices: info from there. How can I do it. I have tried with parseFile with no luck. If someone could help me to start this project I would be very grateful. Thanks.

#### 2011-06-20 08:12:23 - haikalle


    from pyparsing import *
    # sample string with enums and other stuff
    sample = '''
        stuff before
        Vertices: 11.4, 2, 5,
        5,66;
        in the middle
        Vertices:
            alpha,
            beta,
            gamma  ,
            zeta
            ;
        at the end
        mitas
        tassa enaan
        tehtaisiin
        '''
    
    # syntax we don't want to see in the final parse tree
    _lcurl = Suppress(':')
    _rcurl = Suppress(';')
    _equal = Suppress('=')
    _comma = Suppress(',')
    _enum = Suppress('Vertices:')
    
    digits = '0123456789.'
    number = Word( digits )
    identifier = Word(alphas,alphanums+'_')
    integer = Word(nums)
    enumValue = Group(identifier('name') + Optional(_equal + integer('value')))
    enumList = Group(number + ZeroOrMore(_comma + number))
    enum = _enum + enumList('list') + _rcurl
    
    # find instances of enums ignoring other syntax
    read_file = enum.parseFile('data.txt')
    for item,start,stop in enum.scanString(read_file):
        print(item.list)


This is what I have so far. I get error using parseFile. Could someone be so kind and show basic exmaple using parseFile with scanString. I would like to read a file where is all kind of data. Read 'vertices:' and read all floats data after. I'm almost there but not quite :)
#### 2011-06-20 16:49:21 - ptmcg
parseFile is really just a simple shortcut around parseString, pretty much the equivalent of `expr.parseString(open(filename).read())`. 
So parseFile takes a file name as a string, reads it, and passes the file's contents 
as a string to parseString.  

On the other hand, scanString takes a string and looks through it for matches of the given expression.  Change your read_file assignment statement to `read_file = open('data.txt').read()` and you should start getting more sensible results.

Also, enumList could be more simply defined using 

    enumList = Group(delimitedList(number)).  
    
delimitedList takes care of the ZeroOrMore, and suppression of delimiters.

-- Paul
#### 2011-07-03 09:30:04 - haikalle
Thanks a lot! Those tips really helped me a lot. Now I'm again on track and excited where it leads me :)

---
## 2011-06-21 09:07:47 - ptmcg - FANTASTIC PyCon 2011 Singapore Parsing presentation!!!
Through the magic of Google, I found that Gavin Bong gave a great presentation at the Singapore PyCon - see it on scribd at 
https://www.scribd.com/document/58207327/Introduction-to-PEG-Parsing-Expression-Grammar-in-python

---
## 2011-06-21 09:20:11 - PilgrimBeart - SetResultsName - level confusion when used on Group?
I'm a newbie and I've hit a problem and need some help please.

I am parsing a recursive grammar. I want to use SetResultsName() to tag parse elements, so that as my execution engine later traverses the parse tree, it can use the name of each parse element to decide how to handle it. But SetResultsName() seems to be naming not just  the Group itself, but also its contents, which confuses my engine.

As you can see in the code below, I define a simple grammar, and I name the Group. Yet in the resulting parse tree the Group object AND the list object within it both end-up being named 'choice', which confuses my execution engine. All ideas much appreciated!



    choice_expr = Group(Literal('{') + Word(alphas) + Literal('}')).setResultsName('choice')
    x = choice_expr.parseString('{a}')
    print 'x=',x.dump()
    print 'x.getName()=',x.getName()
    print 'x[0].getName()=',x[0].getName()


This returns the following:


    x= [['{', 'a', '}']]
    - choice: ['{', 'a', '}']
    x.getName()= choice
    x[0].getName()= choice


Surely x[0].getName() should equal None?

#### 2011-07-09 23:51:43 - basilevs
That has gave me a hard time too.
Parsing result inherits name from parsing element (or first named token). This name doesn't make any sense anyway - you want to parse first item of result in most cases. Note tath this is true for result returned by parseString function - deeper levels of ParseResults tree are more sane.
#### 2011-08-03 05:13:11 - PilgrimBeart
Now that I'm less of a newbie, I still think this is definitely a BUG - so anyone care to comment on whether I'm right, and whether it can be fixed?

Thanks,

---
## 2011-06-22 08:06:39 - jasisz - simpleBool example on Python 3
Everything is working like a charm under Python 2, but with Python 3 I am getting strange exception in all BoolOperands constructors:


    File 'bool.py', line 55, in __init__
        self.arg = t[0][1]
    AttributeError: 'int' object has no attribute 'arg'

While porting example to Python 3 I've just added print brackets, removed BoolOperand inheriting object and renamed `__nonzero__` to `__bool__`
Any ideas?

#### 2011-06-22 08:32:29 - ptmcg
We may be bumping up against version incompatibility in how pyparsing calls parse actions - Raymond Hettinger helped me come up with a much more robust approach to this, I think it is checked into SVN as the latest pyparsing_Py3.py.  (I've *got* to get this next 1.5.6 release released!)

-- Paul
#### 2011-06-28 02:19:45 - jasisz
It is working! Thanks a lot, you are doing an excellent work.

---
## 2011-06-24 16:00:50 - stalwarts - Can someone explain ?


    Code:
    [[user:stalwarts|1308956450]][[user:stalwarts]]
    
        # filter to parse clients NODEID
        nodeID = Literal('ZPeerInventory:init self nodeid') + '=' + PARSER.EOL

        # Alterate method 1 to find NODEID if we didn't get using the previous filter
        nodeIDAlt_1 = Literal('setting signer to ') + PARSER.EOL

        # Alterate method 2 to find NODEID if we didn't get using the previous filter
        nodeIDString = (Literal('node=')|Literal('machineID='))
        skipToNodeID = (SkipTo('node=') |SkipTo('machineID='))
        nodeIDLogLines = (Literal('contacting net mgr:') | Literal('Opening') | Literal('Connecting to') | Literal('ZHTTPSession::cancel cancelling') | Literal('ZMandatorySubManager::send() URL:' ))
        nodeIDAlt_2 = delimitedList(nodeIDLogLines + skipToNodeID + nodeIDString + Word(printables) )

        # Alterate method 3 to find NODEID if we didn't get using the previous filter
        machineIDString = Literal('machineID=')
        skipTomachineID = SkipTo('machineID=')
        nodeIDAlt_3 = delimitedList(nodeIDLogLines + skipTomachineID + machineIDString + SkipTo('&client_tag'))

        # filter to parse clients ipaddress
        ipField = Word(nums,max=3)
        ipaddress = Combine( ipField + '.' + ipField + '.' + ipField + '.' + ipField )
        nodeRef = delimitedList(Word(alphas) + '=' + ipaddress + Word(alphas)+ '=' + Word(nums) + Word(alphas) + '=' + Word(alphanums+'.'))
        nodeRefLine = Literal('Addr and Grid Policy: noderef=[') + nodeRef

        def clientNodeID(self,logString):
          # Lets do the first attempt to find the nodeID
          nodeList =  self.nodeID.searchString(logString)
          if len(nodeList) > 0:
            return nodeList

          nodeList =  self.nodeIDAlt_1.searchString(logString)
          if len(nodeList) > 0:
            return nodeList

          nodeList =  self.nodeIDAlt_2.searchString(logString)
          if len(nodeList) > 0:
            return nodeList

          nodeList =  self.nodeIDAlt_3.searchString(logString)
          if len(nodeList) > 0:
            return nodeList

          return 'Unable to find NODEID'

    



Sample Text:
-   <small>Jun 24, 2011</small>
-   <small>Jun 24, 2011</small>
-   <small>Jun 24, 2011</small>
[36c]
ZTimer          [Jun 23 20:23:02] HTTP     ZHTTPSession::cancel cancelling 

I want to extract machineID from the above logLine. 

Here is what i'm observing ...



    
    def clientNodeID(self,logString):
              # Lets do the first attempt to find the nodeID
              nodeList =  self.nodeID.searchString(logString)
              if len(nodeList) > 0:
                return nodeList
    
              nodeList =  self.nodeIDAlt_1.searchString(logString)
              if len(nodeList) > 0:
                return nodeList
    
              nodeList =  self.nodeIDAlt_2.searchString(logString)
              if len(nodeList) > 0:
                return nodeList
    
              nodeList =  self.nodeIDAlt_3.searchString(logString)
              if len(nodeList) > 0:
                return nodeList
    
              return 'Unable to find NODEID'


the above code gives the following output...

    [['ZHTTPSession::cancel cancelling', '', 'machineID=', 'vlnWnBeCbhUMeOplyXvC0Rs4KX%2fZ2o%2b7bJ6%2f72HsgHV3%2bSSrEPxR0BI1Fhvpulps%2bkqQzNdDLM9kuVZ%2bxwm31c7FMRjZpi6l8jMiuVWObtP6cK%2fXGwbb8%2fmt3sBy62eZ9VOp1FDr5SirOLYDDE8TKiCrjgpB81JNWiRI14dkhOep&client_tag=770ada7f%2dcde9%2d2ce8%2d4922%2db3390c0c929f']]

the problem is 

    'machineID=', 'vlnWnBeCbhUMeOplyXvC0Rs4KX%2fZ2o%2b7bJ6%2f72HsgHV3%2bSSrEPxR0BI1Fhvpulps%2bkqQzNdDLM9kuVZ%2bxwm31c7FMRjZpi6l8jMiuVWObtP6cK%2fXGwbb8%2fmt3sBy62eZ9VOp1FDr5SirOLYDDE8TKiCrjgpB81JNWiRI14dkhOep&client_tag=770ada7f%2dcde9%2d2ce8%2d4922%2db3390c0c929f']

the MachineID is not separated from 

    '&client_tag=770ada7f%2dcde9%2d2ce8%2d4922%2db3390c0c929f'

but here is what i don't understand , if i comment some of the Conditions in the above code , it seems to parse the line the way i wanted.



    
    def clientNodeID(self,logString):
              # Lets do the first attempt to find the nodeID
    ##          nodeList =  self.nodeID.searchString(logString)
    ##          if len(nodeList) > 0:
    ##            return nodeList
    ##
    ##          nodeList =  self.nodeIDAlt_1.searchString(logString)
    ##          if len(nodeList) > 0:
    ##            return nodeList
    ##
    ##          nodeList =  self.nodeIDAlt_2.searchString(logString)
    ##          if len(nodeList) > 0:
    ##            return nodeList
    
              nodeList =  self.nodeIDAlt_3.searchString(logString)
              if len(nodeList) > 0:
                return nodeList
    
              return 'Unable to find NODEID'



Result:

    [['ZHTTPSession::cancel cancelling', '', 'machineID=', 'vlnWnBeCbhUMeOplyXvC0Rs4KX%2fZ2o%2b7bJ6%2f72HsgHV3%2bSSrEPxR0BI1Fhvpulps%2bkqQzNdDLM9kuVZ%2bxwm31c7FMRjZpi6l8jMiuVWObtP6cK%2fXGwbb8%2fmt3sBy62eZ9VOp1FDr5SirOLYDDE8TKiCrjgpB81JNWiRI14dkhOep']]

MachineId is clearly separated. 

    'machineID=', 'vlnWnBeCbhUMeOplyXvC0Rs4KX%2fZ2o%2b7bJ6%2f72HsgHV3%2bSSrEPxR0BI1Fhvpulps%2bkqQzNdDLM9kuVZ%2bxwm31c7FMRjZpi6l8jMiuVWObtP6cK%2fXGwbb8%2fmt3sBy62eZ9VOp1FDr5SirOLYDDE8TKiCrjgpB81JNWiRI14dkhOep'


What's happening ? I'm i missing anything.I'm fairly new to Python and PyParser.

#### 2011-07-06 20:57:20 - ptmcg
Sorry to be so late in responding.  You really have the pieces of a long URL here, and instead of hacking up a pyparsing parser that steps over the list delimiters and quotation marks, work with this data as the Python list that it really is.  You can combine everything after 'ZHTTPSession: etc.' into one long URL string, and then use urlparse.urlparse and urlparse.parse_qs to extract the fields in the url.  Here's how it would look:



    data = [
    [
    'ZHTTPSession::cancel cancelling', 'http://vc2-wa1.corp.kontiki.com/zodiac/servlet/zodiac?action=pub.ZClientLogin&username=b3910480%2d6c16%2d4694%2d9593%2dcbb4ccfe1836&password=3KQP3kTtDQKE6guaouqw1QalICg%3d&agent=kdx&clientVersion=7%2e33%2e1105%2e191&', 'machineID=', 'vlnWnBeCbhUMeOplyXvC0Rs4KX%2fZ2o%2b7bJ6%2f72HsgHV3%2bSSrEPxR0BI1Fhvpulps%2bkqQzNdDLM9kuVZ%2bxwm31c7FMRjZpi6l8jMiuVWObtP6cK%2fXGwbb8%2fmt3sBy62eZ9VOp1FDr5SirOLYDDE8TKiCrjgpB81JNWiRI14dkhOep&client_tag=770ada7f%2dcde9%2d2ce8%2d4922%2db3390c0c929f'
    ]
    ] 
    
    url = ''.join(data[0][1:])
    import urlparse
    
    q = urlparse.urlparse(url).query
    print q
    qq = urlparse.parse_qs(q)
    print qq

prints


    action=pub.ZClientLogin&username=b3910480%2d6c16%2d4694%2d9593%2dcbb4ccfe1836&password=3KQP3kTtDQKE6guaouqw1QalICg%3d&agent=kdx&clientVersion=7%2e33%2e1105%2e191&machineID=vlnWnBeCbhUMeOplyXvC0Rs4KX%2fZ2o%2b7bJ6%2f72HsgHV3%2bSSrEPxR0BI1Fhvpulps%2bkqQzNdDLM9kuVZ%2bxwm31c7FMRjZpi6l8jMiuVWObtP6cK%2fXGwbb8%2fmt3sBy62eZ9VOp1FDr5SirOLYDDE8TKiCrjgpB81JNWiRI14dkhOep&client_tag=770ada7f%2dcde9%2d2ce8%2d4922%2db3390c0c929f
    {'username': ['b3910480-6c16-4694-9593-cbb4ccfe1836'], 'clientVersion': ['7.33.1105.191'], 'machineID': ['vlnWnBeCbhUMeOplyXvC0Rs4KX/Z2o+7bJ6/72HsgHV3+SSrEPxR0BI1Fhvpulps+kqQzNdDLM9kuVZ+xwm31c7FMRjZpi6l8jMiuVWObtP6cK/XGwbb8/mt3sBy62eZ9VOp1FDr5SirOLYDDE8TKiCrjgpB81JNWiRI14dkhOep'], 'agent': ['kdx'], 'action': ['pub.ZClientLogin'], 'client_tag': ['770ada7f-cde9-2ce8-4922-b3390c0c929f'], 'password': ['3KQP3kTtDQKE6guaouqw1QalICg=']}


Hope that gets you further along.
-- Paul

---
## 2011-06-25 13:19:33 - gareth8118 - Packaging recommendations?
I'm planning to release my parser for general use as a package rather than just as part of a program. Any recommendations or best practices on how to expose the parser to an importing application?

Cheers
Gareth

#### 2011-07-06 20:34:22 - ptmcg
I think most people keep the API pretty simple, especially if the parser just returns a simple list of strings, or structure with some named results.  Just a module-level 'parse' method, that takes a string and returns the desired data structure.  
You might want this method to catch ParseExceptions, and then raise an exception that is more specific to your API.  

Also, follow the general Python packaging styles, such as defining `__all__` in your module so that a '*' import only gets the significant names exported (not every scratch variable or imported name at module scope).

If your parser is more involved than this - like a search query parser that has some more meaningful fields in the parsed results - write back with some more detail on just what it is you are packaging.

(And a) thanks for using pyparsing! and b) congrats on launching your own general purpose code module!) 

-- Paul
#### 2011-07-14 13:07:50 - gareth8118
Hi Paul,

No, thank /you/ for the release and continued support of this awesome resource. I wouldn't have taken on the project without its existence.

I'm packaging up what I have right now and the single parse() method is probably best. I'm trying to do the right thing with setuptools etc for my module so it will play nice if I write some more Python to manipulate the results of the parse in another module.

I'll keep you up to date with my (slow) progress...

Cheers
Gareth

---
## 2011-06-29 17:19:07 - NathanW3 - setParseAction no tokens passed
I very new to pyparsing and Python so this is a warning that I might be doing something really wrong.

What I am trying to do is build a SQL parser and build up tree with nodes that I can then walk.

I'm trying to copy this kind of thing from a yacc/bison grammer file:



    | scalar_exp '^' scalar_exp   { $$ = new QgsSearchTreeNode(QgsSearchTreeNode::opPOW,  $1, $3); joinTmpNodes($$,$1,$3); }


This is the code that I have in Python:



    LPAR = Suppress('(')
    RPAR = Suppress(')')
    COMMA = Suppress(',')
    
    AND = CaselessKeyword('AND')
    ASC = CaselessKeyword('ASC')
    DESC = CaselessKeyword('DESC')
    ON = CaselessKeyword('ON')
    USING = CaselessKeyword('USING')
    INNER = CaselessKeyword('INNER')
    JOIN = CaselessKeyword('JOIN')
    AS = CaselessKeyword('AS')
    NOT = CaselessKeyword('NOT')
    SELECT = CaselessKeyword('SELECT')
    FROM = CaselessKeyword('FROM')
    WHERE = CaselessKeyword('WHERE')
    GROUP = CaselessKeyword('GROUP')
    BY = CaselessKeyword('BY')
    ORDER = CaselessKeyword('ORDER')
    LIMIT = CaselessKeyword('LIMIT')
    BETWEEN = CaselessKeyword('BETWEEN')
    
    UNARY = 1
    BINARY = 2
    TERNARY = 3
    
    keyword = MatchFirst(( ASC, DESC, ON, USING, INNER,
     JOIN, AS, NOT, SELECT, FROM, WHERE, GROUP, BY,
     ORDER, BY, LIMIT,BETWEEN))
    
    identifier = ~keyword + Word(alphas, alphanums+'_')
    collation_name = identifier.copy()
    column_name = Suppress('[') + ~keyword + Word(alphas, alphanums+'_') + Suppress(']')
    column_alias = identifier.copy()
    table_name = identifier.copy()
    table_alias = identifier.copy()
    index_name = identifier.copy()
    function_name = identifier.copy()
    parameter_name = identifier.copy()
    
    expr = Forward().setName('expression')
    select_stmt = Forward().setName('select statement')
    
    integer = Regex(r'[+-]?\d+')
    numeric_literal = Regex(r'\d+(\.\d*)?([eE][+-]?\d+)?')
    string_literal = QuotedString(''')
    literal_value = ( numeric_literal | string_literal)
    
    expr_term = (
        function_name + LPAR + Optional(delimitedList(expr)) + RPAR |
        literal_value |
        identifier |
        column_name
        )
    
    expr << operatorPrecedence(expr_term,
        [
        (oneOf('- + ~') | NOT, UNARY, opAssoc.LEFT, setObject),
        ('||', BINARY, opAssoc.LEFT),
        (oneOf('* / %'), BINARY, opAssoc.LEFT,setObject),
        (oneOf('+ -'), BINARY, opAssoc.LEFT),
        (oneOf('<< >> & |'), BINARY, opAssoc.LEFT),
        (oneOf('< <= > >='), BINARY, opAssoc.LEFT),
        (oneOf('= == != <>') , BINARY, opAssoc.LEFT),
        ('||', BINARY, opAssoc.LEFT),
        ((BETWEEN,AND), TERNARY, opAssoc.LEFT),
        ])
    
    ordering_term = expr + Optional(ASC | DESC)
    
    join_constraint = ON + expr('join_expression')
    
    join_op = COMMA | (INNER + JOIN)
    
    join_source = Forward()
    single_source = ( table_name('table') +
                        Optional(Optional(AS) + table_alias('table_alias')))
    
    join_source << single_source + Group(ZeroOrMore(join_op + single_source + Optional(join_constraint)))('join')
    
    result_column = '*' | table_name + '.' + '*' | (expr + Optional(Optional(AS) + column_alias))
    select_core = (SELECT + Group(delimitedList(result_column))('columns') +
                    Optional(FROM + join_source).setParseAction(setObject) +
                    Optional(WHERE + expr('where_expr')) +
                    Optional(GROUP + BY + Group(delimitedList(ordering_term)('group_by_terms')))
                    )
    
    select_stmt << (select_core + ZeroOrMore(select_core) +
                    Optional(ORDER + BY + Group(delimitedList(ordering_term))('order_by_terms'))
                    )


note: it's a strip down version of select_parser.py by Paul McGuire

I think I have to use setParseAction but when ever I do I always get None for the tokens in the method that I call.  I get the full string and location but no tokens.

Where would the best place to call setParseAction to copy the yacc/bison logic?


---
## 2011-07-06 17:19:29 - joslin01 - Reading parse results dump
Hi, I've made my best attempt at implementing the python grammar. However, I'm rather concerned about the length of results I'm getting (since I presume it's recursive). Moreover, what does each indentation represent in the dump() 

I was hoping to create an AST to pass over to my partner, but am rather confused as each expression that it passes gets fully formed and defined. Perhaps I'm just misunderstanding parser techniques. 

Here's a pastebin of the results of 'x * x' beginning the parse from the testlist node:
You can view code here: 

Any push in the right direction would be really appreciated!

#### 2011-07-06 19:26:22 - ptmcg
Wow, this is a major project! Congratulations on getting so far.

On one hand I'd say you might have gone too far with the results names. But they do come in handy when navigating thru the parsed data.

I think what is confusing me (and you!) is just how the hierarchical structure is built. If you printed out these results using asList, I don't think you'd get nearly as complicated a structure.  As it is, I agree this crazy ultra-nested ParseResults isn't really very helpful. And mostly, that's because a raw AST of all these names is not really the best way to navigate through the data parsed by a language like Python.

I think the answer is really more that you should start looking at using parse actions to convert your parsed data into a nested structure of expression classes that is really usable.  See how the classes are used in the SimpleBool.py example.  Instead of having a generic nesting of named things, you'll have a parsed Statement or Module object which you can navigate through as a series of Statements, processing WhileStatements separately from IfStatements or ForStatements, etc. Of course, you would instantiate all of these as parse actions attached to the appropriate expression in your grammar.  I think these will be more meaningful than the nested structure of named tokens.

Also, please take a look at the python_grammar_parser example, which parses the grammar.txt file from Python and creates a pyparsing parser from it.  It *almost* works, except it doesn't handle INDENT/DEDENT very well. But it does handle parsing single Python statements.

-- Paul
#### 2011-07-06 19:38:52 - ptmcg
Oh, to your question about what each indentation level represents in the dump() output, usually this has to do with some form of grouping and/or name nesting, and it's most useful when scraping data from an input file to make it easy to see what attributes are accessible, and how to navigate the nesting to get to them.  In your case, you have so many results names that I don't readily see how you would navigate the results to extract a statement, containing an expression, containing a term containing a factor, an operator and a factor.
#### 2011-07-06 19:50:34 - joslin01
Haha right, nor do I. Thanks Paul! I'm reading the examples now, and reviewing parseActions in your book. I think this makes the most sense.

My only remaining concern is that I end up with memory error when I set toDebug() on a grammar and pyparsing helper tells me that maximum recursion depth was exceeded, so I can't help but feel it's.. too recursive? 

    Traceback (most recent call last):
      File 'tests.py', line 109, in <module>
        file_input()
      File 'tests.py', line 99, in file_input
        tokens = grammar.file_input.parseFile('code_test.gup')
      File 'c:\python27\lib\site-packages\pyparsing.py', line 1410, in parseFile
        return self.parseString(file_contents, parseAll)
      File 'c:\python27\lib\site-packages\pyparsing.py', line 1021, in parseString
        loc, tokens = self._parse( instring, 0 )
      File 'c:\python27\lib\site-packages\pyparsing.py', line 864, in _parseNoCache
        self.debugActions[0]( instring, loc, self )
      File 'c:\python27\lib\site-packages\pyparsing.py', line 656, in _defaultStartD
    ebugAction
        print ('Match ' + _ustr(expr) + ' at loc ' + _ustr(loc) + '(%d,%d)' % ( line
    no(loc,instring), col(loc,instring) ))
      File 'c:\python27\lib\site-packages\pyparsing.py', line 122, in _ustr
        return str(obj)
      File 'c:\python27\lib\site-packages\pyparsing.py', line 2755, in `__str__`
        self.strRepr = '[' + _ustr(self.expr) + ']...'
      File 'c:\python27\lib\site-packages\pyparsing.py', line 122, in _ustr
        return str(obj)
      File 'c:\python27\lib\site-packages\pyparsing.py', line 2390, in `__str__`
        self.strRepr = '{' + ' '.join( [ _ustr(e) for e in self.exprs ] ) + '}'
      File 'c:\python27\lib\site-packages\pyparsing.py', line 122, in _ustr
        return str(obj)
      File 'c:\python27\lib\site-packages\pyparsing.py', line 2963, in `__str__`
        retString = _ustr(self.expr)
      File 'c:\python27\lib\site-packages\pyparsing.py', line 122, in _ustr
        return str(obj)
      File 'c:\python27\lib\site-packages\pyparsing.py', line 2447, in `__str__`
        self.strRepr = '{' + ' ^ '.join( [ _ustr(e) for e in self.exprs ] ) + '}'
      File 'c:\python27\lib\site-packages\pyparsing.py', line 122, in _ustr
        return str(obj)
      File 'c:\python27\lib\site-packages\pyparsing.py', line 2447, in `__str__`
        self.strRepr = '{' + ' ^ '.join( [ _ustr(e) for e in self.exprs ] ) + '}'
      File 'c:\python27\lib\site-packages\pyparsing.py', line 122, in _ustr
        return str(obj)
      File 'c:\python27\lib\site-packages\pyparsing.py', line 2390, in `__str__`
        self.strRepr = '{' + ' '.join( [ _ustr(e) for e in self.exprs ] ) + '}'
      File 'c:\python27\lib\site-packages\pyparsing.py', line 122, in _ustr
        return str(obj)
      File 'c:\python27\lib\site-packages\pyparsing.py', line 2447, in `__str__`
        self.strRepr = '{' + ' ^ '.join( [ _ustr(e) for e in self.exprs ] ) + '}'
      File 'c:\python27\lib\site-packages\pyparsing.py', line 122, in _ustr
        return str(obj)
      File 'c:\python27\lib\site-packages\pyparsing.py', line 2390, in `__str__`
        self.strRepr = '{' + ' '.join( [ _ustr(e) for e in self.exprs ] ) + '}'
      File 'c:\python27\lib\site-packages\pyparsing.py', line 122, in _ustr
        return str(obj)
      File 'c:\python27\lib\site-packages\pyparsing.py', line 2390, in `__str__`
        self.strRepr = '{' + ' '.join( [ _ustr(e) for e in self.exprs ] ) + '}'
      File 'c:\python27\lib\site-packages\pyparsing.py', line 122, in _ustr
        return str(obj)
      File 'c:\python27\lib\site-packages\pyparsing.py', line 2390, in `__str__`
        self.strRepr = '{' + ' '.join( [ _ustr(e) for e in self.exprs ] ) + '}'
      File 'c:\python27\lib\site-packages\pyparsing.py', line 122, in _ustr
        return str(obj)
      File 'c:\python27\lib\site-packages\pyparsing.py', line 2447, in `__str__`
        self.strRepr = '{' + ' ^ '.join( [ _ustr(e) for e in self.exprs ] ) + '}'
      File 'c:\python27\lib\site-packages\pyparsing.py', line 122, in _ustr
        return str(obj)
      File 'c:\python27\lib\site-packages\pyparsing.py', line 2390, in `__str__`
        self.strRepr = '{' + ' '.join( [ _ustr(e) for e in self.exprs ] ) + '}'
    MemoryError

#### 2011-07-06 20:23:26 - ptmcg
Yuk - it's a MemoryError just trying to create a string representing an expression.  

Wherever you are calling setDebug, call setName also, as in:  

    integer = Word(nums).setName('integer').setDebug()  

The distinction of setName vs. setResultsName is that setName describes the expression itself - in this case a word group of numeric digits is an 'integer'. This same expression might be used in various places with different results names, like 'age' or 'house_number' or 'zip_code'.  But they are all 'integer' expressions.  If setName is not called, then pyparsing will try to synthesize a descriptive string - that is what all of that strRepr stuff is.  Just call setName whenever you call setDebug.  Then leave the setName's in, since those names are also used in exception messages. It's easier to understand 'expected to find integer' vs. 'expected to find W:(0123...)'.

#### 2011-07-06 21:18:54 - joslin01
Gotcha! With proper names set, it works much better. Thanks!

---
## 2011-07-09 23:28:16 - basilevs - Futher problems
Unfortunately, method provided by ptmcg work only if all parsing elements for named members of the parsed lists were initialized with setResultsName('xxx', False), lists that are filled with such elements would be parsed incorrectly as only one named element per name will be associated with its name.

If all of them would be initialized with setResultsName('xxx', True), the code would have to be changed:


    def tokensToNVP(tokens):
        valuemap = dict((id(v),k) for k,values in tokens.items() for v in values)
    #    print(tokens,tokens.items(),valuemap)
        def name(token):
            if id(token) in valuemap:
                return valuemap[id(token)]
            return None    
        return [(name(token), token) for token in tokens]


Anyway there is no easy way to handle cumulative and single parsing elements mixed together.

#### 2011-07-09 23:30:02 - basilevs
Sorry fo new subject creation. his post was intended to be a comment for another thread about parsing a list of named elements:

#### 2016-01-30 12:32:57 - mscuthbert
It's nearly 5 years later, but I found this to be somewhat useful:

```
def orderedItems(parseResult):
    '''
    missing from pyparsing: iterate .items() but in order.  
    '''
    valuemap = dict((id(v),k) for k,v in parseResult.items())
    weakConnectionMap = [(v, k) for k,v in parseResult.items()]

    pList = parseResult.asList()
    for iValue in pList:
        iType = valuemap.get(id(iValue), None)
        if iType is None: # not match on id, match on value
            for v, k in weakConnectionMap: 
                if v == iValue or (hasattr(v, 'asList') and v.asList() == iValue):
                    iType = k
                    break
        yield(iType, iValue)
```

---
## 2011-07-11 23:34:33 - whoozle - Cannot implement recursive grammar
This code is loosely based on the one of examples, but I can't get it work. 
Please help

    from pyparsing import *
    
    line_end = White('\n')
    
    binary_op = Literal('+') | '-' | '*' | '/'
    
    label = Word(alphas, alphanums)
    
    hex_number = Word(nums, hexnums) + 'h' #always start with numeric char
    dec_number = Word(nums)
    bin_number = Word('01') + 'b'
    number = hex_number | dec_number | bin_number
    
    expr = Forward()
    atom = number | label | (Literal('(') + expr + Literal(')'))
    #atom.setDebug(True)
    
    expr << (atom + ZeroOrMore(binary_op + expr))
    expr.setDebug(True)
    
    equ = label + CaselessKeyword('equ') + expr
    
    statement = equ
    
    source = ZeroOrMore(line_end | (statement + line_end))
    source = source.ignore(';' + restOfLine)
    source.setDefaultWhitespaceChars(' \t')
    
    source.parseString('''
    
    ;comment
    x equ 0
    
    
    ''', parseAll = True)

#### 2011-07-12 18:23:24 - ptmcg
Try changing:


    line_end = White('\n')

to:


    line_end = LineEnd()


#### 2011-07-13 23:35:38 - whoozle
Thank you, but I think I found the problem:

setDefaultWhitespaceChars is static and must be called before any object created. Maybe its value cached somewhere or such.

I've added `ParserElement.setDefaultWhitespaceChars(' \t')` 
as a first line of grammar and everything started working as expected

---
## 2011-07-12 19:49:16 - mehrdadfeller - Verilog Parser
The new release of the pyparsing (1.5.6) is said to come with the verilog parser in the examples, but I can't find anything on verilog. Can someone help me out here?

#### 2011-07-12 21:12:23 - ptmcg
My mistake, I left this out of the release directory when I built the distribution. The parser is online at .
#### 2011-07-13 13:33:19 - mehrdadfeller
Thanks Paul, really appreciate your help.

---
## 2011-07-13 14:17:23 - sdmolloy - Simple nextedExpr parser
I am trying the following, and I just can't understand why it doesn't work.


    >>> import pyparsing as pyp
    >>> cont = pyp.Word(pyp.alphanums) + pyp.Literal(':') + pyp.OneOrMore( pyp.Word(pyp.printables) )
    >>> grammar = pyp.Word(pyp.alphanums) + pyp.Literal(':') + pyp.nestedExpr('{', '}', content=cont)
    >>> data = '''
    ...         Version : { 
    ...             Application : Omega3P V8.2.0  
    ...         }'''
    >>> print data
    
            Version : { 
                Application : Omega3P V8.2.0  
            }
    >>> grammar.parseString(data)
    Traceback (most recent call last):
      File '<stdin>', line 1, in <module>
      File '/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pyparsing-1.5.6-py2.7.egg/pyparsing.py', line 1032, in parseString
        raise exc
    pyparsing.ParseException: Expected '}' (at char 74), (line:4, col:10)


I don't understand why it is expecting another '}' at data[74] (which is beyond the end of the data string) when the initial '{' was already closed.  Adding additional (spurious) '}''s to the end of the string results in the same error, but with the index incremented.

I know I must be doing something very dumb, but I just can't see it.  Any advice or hints would be greatly appreciated.

Thanks.

#### 2011-07-13 23:19:39 - ptmcg
Your definition of cont is overly broad, in that it can parse the closing '}' or opening '{' 
as part of the `OneOrMore(pyp.Word(pyp.printables))`.  You need to remove '{' and '}' from the set of characters that you will accept 
in a content word.  

If you are using the latest version of pyparsing, the Word class takes a new keyword argument, `excludeChars`, which makes it easy to use a large, inclusive string of characters, and then specify a few characters you would like excluded from it.  

In this instance, you would just change your Word definition to `pyp.Word(pyp.printables, excludeChars='{}')`.  Otherwise, you would need to do some string operations to create your subset of content word characters, like:


    contentWordChars = ''.join(c for c in pyp.printables if c not in '{}')

and then use contentWordChars as the string of characters in your content word.

The thing to keep in mind is that pyparsing is very near-sighted - while parsing a particular bit of your grammar, it does not look ahead to the next part to see where it should stop. As long as the current expression matches, it will keep processing the input string.

Hope this helps,
-- Paul
#### 2011-07-13 23:21:38 - ptmcg
I must be blind! Yes you are using the latest pyparsing (version 1.5.6), so just use the excludeChars='{}' argument when creating your content Word.
#### 2011-07-14 00:47:26 - sdmolloy
Great!  Thanks Paul.  I knew I was overlooking something very simple, but I just couldn't see it.
Cheers,
Steve

---
## 2011-07-14 05:40:28 - joslin01 - Where is definition of "t" when initializing parse action classes?
For example,



    class Test(object):
        def __init__(self,t):
            self.arg = t


What is `t` in this regard? What's it type? How is it typically used? I was able to find ample documentation for parse action functions, but not really for classes

#### 2011-07-14 06:01:48 - joslin01
I'm trying to convert the entire expression into a string, and eval it, but since it seems to be a token list.
#### 2011-07-14 08:03:12 - ptmcg
If you were to write your own code to build a Test instance, it would look like


    newtest = Test(some_initialization_argument)


This initializer only takes one argument. So if used as a parse action, it will use the single-argument parse action form, in which the argument passed in is the list of parsed tokens, packaged as a ParseResults object.

ParseResults support list and dict access, so if you want to convert the whole thing to a string, then ''.join(t) will likely do the trick.

-- Paul
#### 2011-07-14 10:18:06 - joslin01
Gotcha. Thanks Paul

---
## 2011-07-14 06:25:48 - sdmolloy - Nested parser similar to jsonParser
I've been working on this for a few hours now, and I think it's time to admit that I'm well and truly stuck :(

I'm trying to parse a file containing the following:


    Version : { 
                Application : Omega3P V8.2.0  
            }   
            Environment : { 
                start : Wed Jun 22 07:58:22 2011 
                commsize : 32  
                user : smolloy 
            }   
            Input : { 
                PostProcess : { 
                    Toggle : on  
                    SymmetryFactor : 1 
                    ModeFile : modes 
                }   
                MPI : { 
                    rank : 0 
                    size : 0 
                }   
                FiniteElement : { 
                    Order : 2 
                    CurvedSurfaces : on  
                }   
                EigenSolver : { 
                    FrequencyShift : 0.5e9 
                    NumEigenvalues : 50  
                    Arnoldi : { 
                        Preconditioner : MP  
                        LinearSolver : { 
                            Preconditioner : MP  
                            Solver : CG  
                        }   
                        EigenvalueLocation : AboveShift 
                        MaxIterations : 500 
                        Tolerance : 1e-12 
                    }   
                    SolverMode : automatic 
                    Preconditioner : MP  
                }   
            }


To do this, I have written (with the help of the jsonParser I found online) the following:



    #!/opt/local/bin/python                                                                                                        
    
    import pyparsing as pyp                                                                                                        
    
    kvcObject = pyp.Forward()                                                                                                      
    
    kvcStr = pyp.OneOrMore( ~pyp.lineEnd + pyp.Word( pyp.printables, excludeChars='{}' ) )                                         
    kvcNum = pyp.Combine( pyp.Optional('-') + ( '0' | pyp.Word('123456789',pyp.nums) ) +                                           
                pyp.Optional( '.' + pyp.Word(pyp.nums) ) +                                                                         
                pyp.Optional( pyp.Word('eE',exact=1) + pyp.Word(pyp.nums+'+-',pyp.nums) ) )                                        
    
    kvcVal = ( kvcStr | kvcNum | pyp.Group(kvcObject) )                                                                            
    
    memberDef = pyp.Group( kvcVal )                                                                                                
    kvcMembers = pyp.delimitedList( memberDef, delim=pyp.lineEnd )                                                                 
    
    kvcVarName = pyp.Word( pyp.alphanums )                                                                                         
    kvcVarVal  = pyp.Suppress(pyp.Optional('{')) + kvcMembers + pyp.Suppress(pyp.Optional('}'))                                    
    
    kvcObject << pyp.Dict( kvcVarName + pyp.Suppress(':') + kvcVarVal)                                                             
    
    def convertnumbers(s, l, toks):                                                                                                
        n = toks[0]                                                                                                                
        try:                                                                                                                       
            return int(n)                                                                                                          
        except ValueError, ve:                                                                                                     
            return float(n)                                                                                                        
    
    kvcNum.setParseAction( convertnumbers )                                                                                        
    kvcStr.setParseAction ( lambda s, l, toks: ' '.join(toks) )                                                                    
    
    if __name__=='__main__':                                                                                                       
        f = open('test.txt')                                                                                                       
        testdata = f.read()                                                                                                        
    
        print testdata                                                                                                             
    
        results = kvcObject.parseString(testdata)


This results in a pretty long, convoluted error:


    Press ENTER or type command to continue
            Version : { 
                Application : Omega3P V8.2.0  
            }
    
    Traceback (most recent call last):
      File 'kvcParser.py', line 38, in <module>
        results = kvcObject.parseString(testdata)
      File '/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pyparsing-1.5.6-py2.7.egg/pyparsing.py', line 1021, in parseString
        loc, tokens = self._parse( instring, 0 )
      File '/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pyparsing-1.5.6-py2.7.egg/pyparsing.py', line 894, in _parseNoCache
        loc,tokens = self.parseImpl( instring, preloc, doActions )
      File '/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pyparsing-1.5.6-py2.7.egg/pyparsing.py', line 2623, in parseImpl
        return self.expr._parse( instring, loc, doActions, callPreParse=False )
      File '/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pyparsing-1.5.6-py2.7.egg/pyparsing.py', line 894, in _parseNoCache
        loc,tokens = self.parseImpl( instring, preloc, doActions )
      File '/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pyparsing-1.5.6-py2.7.egg/pyparsing.py', line 2623, in parseImpl
        return self.expr._parse( instring, loc, doActions, callPreParse=False )
      File '/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pyparsing-1.5.6-py2.7.egg/pyparsing.py', line 894, in _parseNoCache
        loc,tokens = self.parseImpl( instring, preloc, doActions )
      File '/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pyparsing-1.5.6-py2.7.egg/pyparsing.py', line 2368, in parseImpl
        loc, exprtokens = e._parse( instring, loc, doActions )
      File '/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pyparsing-1.5.6-py2.7.egg/pyparsing.py', line 894, in _parseNoCache
        loc,tokens = self.parseImpl( instring, preloc, doActions )
      File '/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pyparsing-1.5.6-py2.7.egg/pyparsing.py', line 2623, in parseImpl
        return self.expr._parse( instring, loc, doActions, callPreParse=False )
      File '/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pyparsing-1.5.6-py2.7.egg/pyparsing.py', line 894, in _parseNoCache
        loc,tokens = self.parseImpl( instring, preloc, doActions )
      File '/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pyparsing-1.5.6-py2.7.egg/pyparsing.py', line 2478, in parseImpl
        ret = e._parse( instring, loc, doActions )
      File '/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pyparsing-1.5.6-py2.7.egg/pyparsing.py', line 894, in _parseNoCache
        loc,tokens = self.parseImpl( instring, preloc, doActions )
      File '/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pyparsing-1.5.6-py2.7.egg/pyparsing.py', line 2623, in parseImpl
        return self.expr._parse( instring, loc, doActions, callPreParse=False )
      File '/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pyparsing-1.5.6-py2.7.egg/pyparsing.py', line 894, in _parseNoCache
        loc,tokens = self.parseImpl( instring, preloc, doActions )
      File '/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pyparsing-1.5.6-py2.7.egg/pyparsing.py', line 2623, in parseImpl
        return self.expr._parse( instring, loc, doActions, callPreParse=False )
      File '/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pyparsing-1.5.6-py2.7.egg/pyparsing.py', line 900, in _parseNoCache
        tokens = self.postParse( instring, loc, tokens )
      File '/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pyparsing-1.5.6-py2.7.egg/pyparsing.py', line 3061, in postParse
        dictvalue = tok.copy() #ParseResults(i)
    AttributeError: 'str' object has no attribute 'copy'


Since I've been looking at this for so long now, I simply can't see what I'm doing wrong :(  I would really appreciate any tips or advice people can give.

Thanks,
Steve

#### 2011-07-14 08:27:58 - ptmcg
If I were to take on this project, I would start with a rough BNF description.  Your example gives a nice breadth of sample values to work with, so our BNF is likely to be pretty complete. By writing out these expressions, it helps me formulate some thoughts on how to detect string values vs. numbers vs. submembers, etc.

Here's how I would describe your format:


    varname ::= word of alphanumeric chars, starting with an alpha
    number ::= integer or real number, with optional E exponent
    memberDefn ::= varname ':' varvalue
    varvalue ::= number | '{' memberDefn* '}' | everything to the end of the line


Converting this to pyparsing gives some code not very different from what you have, with some minor changes:

- I've found that building up numeric values by assembling pyparsing expressions is wordy and slow; this is one case where the Regex class really helps out.

- Instead of trying to read in string values as several words and then summing together, I just use restOfLine. restOfLine normally starts immediately at the current parse action, so I precede it with an empty to skip over whitespace - saves me a lstrip() operation on the results.

- The braces in the varvalue housing multiple submembers are not optional, you had to make them so because you had them at the wrong level in the alternation in the value definition.  They *are* suppressable though, since they aren't of much use after the parsing work is done.



    LBRACE,RBRACE,COLON = map(pyp.Suppress,'{}:')
    kvcVarName = pyp.Word(pyp.alphas, pyp.alphanums)
    kvcNum = pyp.Regex(r'-?(0|[1-9]\d*)(\.\d+)?([eE][+-]?\d+)?')
    kvcStrValue = pyp.empty + pyp.restOfLine
    
    kvcMemberDef = pyp.Forward()
    kvcObject = pyp.Dict(LBRACE + kvcMemberDef*(1,None) + RBRACE)
    kvcValue = kvcNum | kvcObject | kvcStrValue
    kvcMemberDef << pyp.Group(kvcVarName + COLON + kvcValue)


(I'm trying the multiplication notation in kvcObject instead of OneOrMore, mostly because if I used OneOrMore, I'd have to say pyp.OneOrMore.)

Then add on your parse actions like you have, and see if this gets you further along.

-- Paul
#### 2011-07-14 13:54:25 - sdmolloy
Wow!  First, let me say that folk like me always really appreciate it when the developer of a library (you are the dev, yes?) we use takes the time to answer personally on a forum like this.  But to spend so much time on my problem is really above-and-beyond!  Thank you so much!  I really think you have earned the $10 I will spend on the O'Reilly book... :)

I have a few questions if that's ok.

The first four lines of your suggested code are very clear, and I think I understand them.

The last four, I'm not 100% sure of quite yet.
You build kvcObject in such a way that it contains the stuff defined in kvcMemberDef, surrounded by suppressed braces, right?  The last two lines are just filling the forward-declared kvcMemberDef with the necessary information.

But I don't quite understand why you did that, since some of the objects (e.g. 'Version' in my example above) aren't contained in braces.  This was why I made the braces optional as well.

Have I misunderstood something?
#### 2011-07-14 14:22:48 - ptmcg
That's because 'Version' is not a kvcObject, it is the name part of a kvcMemberDef.

I did misunderstand a bit what you were parsing, I guess I had JSON parser on the brain.  What your example shows is not a kvcObject, but a succession of kvcMemberDef's.  So I built a little script with my snippet, and parsed your sample text with this:



    dd = pyp.Dict(kvcMemberDef*(1,None)).parseString(data)
    print dd.dump()
    print dd.Input.FiniteElement.CurvedSurfaces


The last line is to illustrate the object-attribute style of token access that this Dict nesting gives you - instant deserializer!

This prints:


    [['Version', ['Application', 'Omega3P V8.2.0  ']], ['Environment', ['start', 'Wed Jun 22 07:58:22 2011 '], ['commsize', '32'], ['user', 'smolloy ']], ...]]]]
    - Environment: [['start', 'Wed Jun 22 07:58:22 2011 '], ['commsize', '32'], ['user', 'smolloy ']]
      - commsize: 32
      - start: Wed Jun 22 07:58:22 2011 
      - user: smolloy 
    - Input: [['PostProcess', ['Toggle', 'on  '], ['SymmetryFactor', '1'], ['ModeFile', 'modes ']], ...]
      - EigenSolver: [['FrequencyShift', '0.5e9'], ['NumEigenvalues', '50'], ['Arnoldi', ['Preconditioner', 'MP  '], ['LinearSolver', ['Preconditioner', 'MP  '], ['Solver', 'CG  ']], ['EigenvalueLocation', 'AboveShift '], ['MaxIterations', '500'], ['Tolerance', '1e-12']], ['SolverMode', 'automatic '], ['Preconditioner', 'MP  ']]
        - Arnoldi: [['Preconditioner', 'MP  '], ['LinearSolver', ['Preconditioner', 'MP  '], ['Solver', 'CG  ']], ['EigenvalueLocation', 'AboveShift '], ['MaxIterations', '500'], ['Tolerance', '1e-12']]
          - EigenvalueLocation: AboveShift 
          - LinearSolver: [['Preconditioner', 'MP  '], ['Solver', 'CG  ']]
            - Preconditioner: MP  
            - Solver: CG  
          - MaxIterations: 500
          - Preconditioner: MP  
          - Tolerance: 1e-12
        - FrequencyShift: 0.5e9
        - NumEigenvalues: 50
        - Preconditioner: MP  
        - SolverMode: automatic 
      - FiniteElement: [['Order', '2'], ['CurvedSurfaces', 'on  ']]
        - CurvedSurfaces: on  
        - Order: 2
      - MPI: [['rank', '0'], ['size', '0']]
        - rank: 0
        - size: 0
      - PostProcess: [['Toggle', 'on  '], ['SymmetryFactor', '1'], ['ModeFile', 'modes ']]
        - ModeFile: modes 
        - SymmetryFactor: 1
        - Toggle: on  
    - Version: [['Application', 'Omega3P V8.2.0  ']]
      - Application: Omega3P V8.2.0  
    on  


(Yes, I am the developer, thanks for your kind words - flattery will get you everywhere! Mostly I got into the habit of responding to the emails that were hating on pyparsing and calling my baby ugly because of a misconception on the poster's part, so I just gently redirected them and got them on the correct course. The posters have gotten nicer lately, but there are still some common 'classic blunders' - I need to write up a new article on this, except Python Magazine isn't around any more. O'Reilly doesn't really do enough volume in my ebook for them to be interested in a 2nd Ed, maybe I'll self publish something...)
#### 2011-07-14 14:26:22 - ptmcg
Hey, I'm really starting to like this rendition of OneOrMore:


    dd = pyp.Dict((1,)*kvcMemberDef).parseString(data)


ZeroOrMore would be just (0,).  Should I adopt this as another suggested pyparsing idiom, or is this too obscure?
#### 2011-07-14 14:48:31 - ptmcg
Um, looking at your question again, I'm not sure I really answered it.  *Every* member definition is name : something.  If the something is a sequence of one or more nested member definitions, then the braces *must* be there. Otherwise it is either a number (no braces) or the rest of the string to the end of line (also no braces).  If you want to get fancy, you could also define a date_time expression matcher (to parse the Environment/start value), and add a parse action to convert to a datetime object.
#### 2011-07-15 01:14:01 - sdmolloy
Many thanks again for your replies.

I don't have time to look at this until this evening, but I will be sure to let you know how I get on.

Your answers are quite clear (although I reserve the right to change my mind on that when I try to make it run myself!), and I think I understand your strategy.

I like the (1,) idiom as well, especially since it allows you to extend the idea of OneOrMore to nOrMore (i.e. (n,) where n>=0) quite simply.

It is a little obscure, but advanced users are sure to appreciate it.

More later.....

Steve
#### 2011-07-15 13:43:56 - sdmolloy
Hi again,
I've tried out your suggestions, and everything appears to be working well.  I had a few difficulties, one which I started a new thread about.

Another one was due to something that I hadn't included in my little example.


    Input : { 
                ModelInfo : { 
                    File : highbetacav.ncdf 
                    BoundaryCondition : { 
                        Electric : 6 
                        Magnetic : 1 , 2 , 3 
                    }   
                    Type : netcdf 
                }   
            }

Notice the comma-separated list in Input.ModelInfo.BoundaryCondition.Magnetic.  This messed up the output of the parser.

To fix this, I added two extra lines.  The following to add a new definition of a variable type.


    kvcNumVec = pyp.OneOrMore(kvcNum + COMMA) + kvcNum

(I added the definition of COMMA to pyp.Suppress mapping near the top of the code.)

And this one to include that variable type as a possibility for the grammar to find.


    kvcValue = kvcNumVec | kvcNum | kvcObject | kvcStrValue


Note that I originally had the first two types reversed in kvcValue as follows,


    kvcValue = kvcNum | kvcNumVec | kvcObject | kvcStrValue

But this didn't work.  My guess is that the '|' test stops when it finds a match (and so stopped at kvcNum when it came across the first number in the comma-separated list), and doesn't look for the 'best' match it can.

Thanks for all your help.

Steve
#### 2011-07-16 07:55:53 - ptmcg
pyparsing has a built-in called `delimitedList` that is a nice shortcut for `expr + ZeroOrMore(Suppress(',') + expr)`.  And `delimitedList` accepts a 'list' containing a single entry.  So you could replace `kvcNumVec | kvcNum` with just `delimitedList(kvcNum)`.

#### 2011-07-16 08:00:24 - ptmcg
And yes, `'a | b'` is the same as `'MatchFirst([a,b])'`.  If you wanted the longest match, you can use `'a ^ b'`, which is the same as `'Or([a,b])'`.  Or's can get expensive though as they often end up evaluating many expressions that are not really possible.  For example, there is no reason to write `'Literal('xyzzy') ^ Literal('plugh')'` - if 'xyzzy' matches, there is no way it might possibly match 'plugh'.  It *might* make sense if you wanted to parse `'Literal('xyz') ^ Literal('xyzzy')`, but as you have already found, this is a predictable ambiguity, and so can be easily resolved by reordering to `'Literal('xyzzy') | Literal('xyz')'`.

---
## 2011-07-15 12:55:24 - sdmolloy - Dict where multiple keys have the same name
Hi,
Sorry for posting so many questions recently, but I am having a lot of fun learning how to parse complex structures.

I have a data file that, when parsed as a dictionary, generates many keys with the same name.  Now I wouldn't like to have to rename each of these, since it makes sense that they do have the same name, but would I *would* like to happen is for each of these to be put into an array of objects, where the array has the duplicate name.

In other words, I would like to access it as following:


    energy = data.AMRLevel.mode[3].TotalEnergy

Where 'mode' is duplicated ~50 times in the original data file.

I don't know if this is something that can be done by the user (i.e. me), or if it is a feature request to the developer.  Either way, I'd love to hear comments on how this could be accomplished.

Thanks,
Steve

#### 2011-07-16 08:03:52 - ptmcg
Try putting a Group construct around the expression that you are getting the results name of 'AMRLevel' - this should keep the modes separated that are in different levels.  But if they are in the same level, then modify your call to setResultsName to add the listAllMatches=True argument. Then all of your modes will be preserved and accessible just as you have written. If you are using the 'expr('mode')' shortcut, then try 'expr('mode*')', new in version 1.5.6, which does the same thing.
#### 2011-07-17 12:50:23 - sdmolloy
I didn't know that setResultsName existed.  I guess I really should invest those ten bucks on your book.....

So what I need to do is modify the expression that finds the 'mode's, and alter it to include the setResultsName (or the expr('mode*'))?  I tried this as you can see in the following code, but with no success.   (I have shown which lines I added in the end-of-line comments.)



    #!/opt/local/bin/python
    
    import pyparsing as pyp 
    
    LBRACE,RBRACE,COLON,COMMA = map(pyp.Suppress,'{}:,')
    kvcVarName = pyp.Word(pyp.alphas, pyp.alphanums)
    kvcNum = pyp.Regex(r'-?(0|[1-9]\d*)(\.\d+)?([eE][+-]?\d+)?')
    kvcNumVec = pyp.OneOrMore(kvcNum + COMMA) + kvcNum
    kvcStrValue = pyp.empty + pyp.restOfLine
    
    kvcMemberDef = pyp.Forward()
    kvcMemberDefmode = pyp.Forward()
    kvcObject = pyp.Dict(LBRACE + (kvcMemberDefmode*(1,None) | kvcMemberDef*(1,None)) + RBRACE)
    kvcValue = kvcNumVec | kvcNum | kvcObject | kvcStrValue
    kvcMemberDef << pyp.Group(kvcVarName + COLON + kvcValue)
    kvcMemberDefmode << pyp.Group(pyp.Literal('Mode') + COLON + kvcValue)          # This is a new line
    kvcMemberDefmode = kvcMemberDefmode('Mode*')                                   # So is this
    
    f = open('test.txt')
    data = f.read()
    
    dd = pyp.Dict(kvcMemberDef*(1,None)).parseString(data)
    print dd.dump()


Sadly, this doesn't seem to work.  I know the correct information is in there, since I can access it by giving the right index to dd.AMRLevel[n], but it isn't being parsed out into the dd.AMRLevel.Mode array as I hoped.

Can you see where I am going wrong?

Thanks for your time.

Steve
#### 2011-07-18 19:13:10 - ptmcg
Just a total wild guess, but what happens if you do this:


    kvcMemberDefmode << pyp.Group(pyp.Literal('Mode').setParseAction(pyp.replaceWith('Mode*')) + COLON + kvcValue)          # This is a new line
    # remove this next line
    #kvcMemberDefmode = kvcMemberDefmode('Mode*')


#### 2011-07-18 23:42:51 - sdmolloy
Thanks for your reply.

This change gives exactly the same results.

I tried altering `replaceWith('Mode*')` to `replaceWith('Temp*')`, but no 'Temp' structure was ever created.  It appears that the objects defined by kvcMemberDefmode are never being found....
#### 2011-07-19 00:36:18 - ptmcg
Steve -

Well, this may call for a dose of brute force.  The following small example shows a parse action that gets attached to a Dict expression, and manually assembles the multi-valued key into a new ParseResults object. This is something of an advanced topic for pyparsing, but I think this example should give you enough to go on - no need for trailing '*'s or anything.



    from pyparsing import *
    
    data = '''\
    A 1
    B 2
    B 19
    B 7
    C 12
    D 6'''
    
    def makeMultiKey(t):
        ret = ParseResults(t.asList())
        for item in t:
            ret += ParseResults(item.val, name=item.key, asList=False, modal=(item.key!='B'))
            del ret[-1]
        return ret
    
    key = Word(alphas)
    val = Word(nums).setParseAction(lambda t:int(t[0]))
    pattern = Dict(OneOrMore(Group(key('key')+val('val'))))
    print pattern.parseString(data).dump()
    
    pattern = Dict(OneOrMore(Group(key('key')+val('val')))).setParseAction(makeMultiKey)
    print pattern.parseString(data).dump()

prints


    [['A', 1], ['B', 2], ['B', 19], ['B', 7], ['C', 12], ['D', 6]]
    - A: 1
    - B: 7
    - C: 12
    - D: 6
    
    [['A', 1], ['B', 2], ['B', 19], ['B', 7], ['C', 12], ['D', 6]]
    - A: 1
    - B: [2, 19, 7]
    - C: 12
    - D: 6


#### 2011-07-21 05:52:58 - ptmcg
Aha! I found my mistake before, such brute force isn't necessary.  You can write the parse action simply as:


    def makeMultiKey(t):
        t += ParseResults([], name='B', asList=False, modal=False)

This adds an empty ParseResults object, but changes then results name 'B' to be non-modal ('modal' meaning 'just report the last value for this name', non-modal thus meaning 'report all values for this results name').

Hope this wasn't too late,
-- Paul
#### 2011-07-24 08:18:20 - sdmolloy
Hi Paul,
Sorry about not replying sooner.  Been busy with some stuff.

I like your example, and I think it might work for me (with a bit of hacking).  I'll reply later to let you know.

Cheers,

Steve
#### 2011-08-03 08:01:53 - sdmolloy
Hi Paul,
I managed to get around to trying this, with varying results.

Here's the code I used:


    #!/opt/local/bin/python
    
    import pyparsing as pyp 
    
    LBRACE,RBRACE,COLON,COMMA = map(pyp.Suppress,'{}:,')
    kvcVarName = pyp.Word(pyp.alphas, pyp.alphanums)
    kvcNum = pyp.Regex(r'-?(0|[1-9]\d*)(\.\d+)?([eE][+-]?\d+)?')
    kvcNumVec = pyp.OneOrMore(kvcNum + COMMA) + kvcNum
    kvcStrValue = pyp.empty + pyp.restOfLine
    
    kvcMemberDef = pyp.Forward()
    kvcObject = pyp.Dict(LBRACE + kvcMemberDef*(1,None) + RBRACE)
    kvcValue = kvcNumVec | kvcNum | kvcObject | kvcStrValue
    kvcMemberDef << pyp.Group(kvcVarName + COLON + kvcValue)
    
    def makeMultiKey(t):
        t += pyp.ParseResults([], name='Mode', asList=False, modal=False)
    
    f = open('test.txt')
    data = f.read()
    
    pattern = pyp.Dict(kvcMemberDef*(1,None)).setParseAction(makeMultiKey)
    dd = pattern.parseString(data)
    print dd.dump()
    print ' ' 
    print dd.Mode[0]


And here's the file it worked on:


    Mode : { 
                    TotalEnergy : 4.4270939088102e-12 
                    QualityFactor : inf 
                    File : modes.l0.m0000.6.9317525e+08.mod 
                    PowerLoss : 0 
                    Frequency : 693175248.88107 
                }   
                Mode : { 
                    TotalEnergy : 4.4270939088102e-12 
                    QualityFactor : inf 
                    File : modes.l0.m0001.6.9629133e+08.mod 
                    PowerLoss : 0 
                    Frequency : 696291326.67171 
                }   
                TimeMatrixAssembly : { 
                    max : 1.244439125061 
                    min : 1.042809009552 
                    average : 1.120772600174 
                    stddev : 0.045629800955608 
                }  


This worked prefectly well.  I was able to print out each of the two modes individually by referencing the appropriate index of the array.  The only problem is this is a much simpler version of the file than I really want to process.  The following is the *real* file.


    Version : {
                Application : Omega3P V8.2.0
            }
            Environment : {
                start : Wed Jun 22 07:58:22 2011
                commsize : 32
                user : smolloy
            }
            Timestamp : {
                begin : Wed Jun 22 07:58:22 2011
            }
            Timestamp : {
                end : Wed Jun 22 08:19:47 2011
            }
            AMRLevel : {
                Mode : { 
                    TotalEnergy : 4.4270939088102e-12 
                    QualityFactor : inf 
                    File : modes.l0.m0000.6.9317525e+08.mod 
                    PowerLoss : 0 
                    Frequency : 693175248.88107 
                }   
                Mode : { 
                    TotalEnergy : 4.4270939088102e-12 
                    QualityFactor : inf 
                    File : modes.l0.m0001.6.9629133e+08.mod 
                    PowerLoss : 0 
                    Frequency : 696291326.67171 
                }   
                TimeMatrixAssembly : { 
                    max : 1.244439125061 
                    min : 1.042809009552 
                    average : 1.120772600174 
                    stddev : 0.045629800955608 
                }   
            }


Note how the 'Mode' elements are hidden a level down from the top inside 'AMRLevel'.  Also, note how 'Timestamp' is also duplicated, but with different information in each element.

Can you give any tips on how I can preserve the behaviour I want when the code recurses down a few levels, and how to make it more generic so that any repeated element is correctly handled?

Maybe I need to go back to your brute force example to see if I can get some hints from it....

Thanks,  Steve
#### 2011-08-04 00:56:23 - sdmolloy
Success!! (Kinda....)

I realised that the setParseAction from the previous version of the code was only being added to the Dict object, which is right at the top of the tree, so I moved it to the object defining the main members of my data.  This way it is defined at all levels of the tree.

Like so:


    !/opt/local/bin/python
    
    import pyparsing as pyp
    
    def makeMultiKey(t):
        t += pyp.ParseResults([], name='Mode', asList=False, modal=False)
    
    LBRACE,RBRACE,COLON,COMMA = map(pyp.Suppress,'{}:,')
    kvcVarName = pyp.Word(pyp.alphas, pyp.alphanums)
    kvcNum = pyp.Regex(r'-?(0|[1-9]\d*)(\.\d+)?([eE][+-]?\d+)?')
    kvcNumVec = pyp.OneOrMore(kvcNum + COMMA) + kvcNum
    kvcStrValue = pyp.empty + pyp.restOfLine
    
    kvcMemberDef = pyp.Forward()
    kvcObject = pyp.Dict(LBRACE + kvcMemberDef*(1,None) + RBRACE)
    kvcValue = kvcNumVec | kvcNum | kvcObject | kvcStrValue
    kvcMemberDef << pyp.Group(kvcVarName + COLON + kvcValue)
    kvcMemberDef.setParseAction(makeMultiKey)     # <---------  This is the line I changed
    
    f = open('test.txt')
    data = f.read()
    
    pattern = pyp.Dict(kvcMemberDef*(1,None))     # <----------  And this one
    dd = pattern.parseString(data)
    print dd.dump()
    for i in dd.AMRLevel.Mode:
        print i.File


Now I just need to figure out how to make makeMultiKey more generic.  i.e., not just work for members whose name is 'Mode' so that it can catch all repeated names.

Thanks

Steve

---
## 2011-07-16 10:58:23 - joslin01 - parsing with indentation
I'm getting a bit lost. I had implemented my own form of indentation seen here: 

Then, when I go to match this block of code:


    if x <= 5:
        print 'what'd you do with 5?'
        x = 5 - 4
    else:
        print 'say hi'


Basically, I'm trying to make sense of the output below. I set debug on both indent & undent to watch what it does. It looks correct when it matches line 2 for example, but then both indent & undent match line 4 (else). Furthermore, what I really don't understand is that it says it matches loc96(5,24). When I open up the code file, there is no (5,24). The last column is (5,19). Can you help me understand why this is?



    
    $ python tests.py if
    Match indent at loc 12(2,1)
    Matched indent -> []
    Match UNDENT at loc 66(4,1)
    Match indent at loc 66(4,1)
    Matched indent -> []
    Matched UNDENT -> []
    Match indent at loc 73(5,1)
    Matched indent -> []
    Match UNDENT at loc 96(5,24)
    Match indent at loc 96(5,24)
    Matched indent -> []
    Matched UNDENT -> []
    Match indent at loc 12(2,1)
    5
    Matched indent -> []
    Match UNDENT at loc 66(4,1)
    Match indent at loc 66(4,1)
    Matched indent -> []
    1
    Exception raised:Alteration of initial indent length in undent (at char 0), (lin
    e:1, col:1)
    []



#### 2011-07-16 16:20:40 - joslin01
Wow, I didn't even recognize the 
indentedBlock function! I suppose I'll try to use that.
#### 2011-07-16 17:02:32 - joslin01
Well, I'm seeing the else unfortunately get matched as part of the block statement despite being undented to col. 1.



    $ python tests.py if
    Match if statement at loc 0(1,1)
    Match if statement at loc 16(2,5)
    Exception raised:Expected 'if' (at char 16), (line:2, col:5)
    Match if statement at loc 55(3,9)
    Exception raised:Expected 'if' (at char 55), (line:3, col:9)
    Match if statement at loc 66(4,1)
    Exception raised:Expected 'if' (at char 66), (line:4, col:1)
    Match if statement at loc 70(4,5)
    Exception raised:Expected 'if' (at char 70), (line:4, col:5)
    Match if statement at loc 70(4,5)
    Exception raised:Expected 'if' (at char 70), (line:4, col:5)
    Matched if statement -> [['if', ['x', '<=', '5']], [['print', [''what\'d you do
    with 5?''], ['x'], '=', ['5', '-', '4'], ['else']]]]
    Match if statement at loc 0(1,1)
    Match if statement at loc 16(2,5)
    Exception raised:Expected 'if' (at char 16), (line:2, col:5)
    Match if statement at loc 55(3,9)
    Exception raised:Expected 'if' (at char 55), (line:3, col:9)
    Match if statement at loc 66(4,1)
    Exception raised:Expected 'if' (at char 66), (line:4, col:1)
    Match if statement at loc 70(4,5)
    Exception raised:Expected 'if' (at char 70), (line:4, col:5)
    Match if statement at loc 70(4,5)
    Exception raised:Expected 'if' (at char 70), (line:4, col:5)
    Exception raised:not an unindent (at char 70), (line:4, col:5)
    []


tricky stuff
#### 2011-07-16 18:55:57 - joslin01
This solution ended up for working for me in the end. I just removed Undent and kept tracking of # of indents for each line. It won't catch a false indent yet, but it's a start.



    indentStack = [1]
    indentLevel = 0
    
    def checkIndent(s,l,t):
        #Grab current column
        currentCol = col(l,s)
        print currentCol
    
        #The indent stack.. 
        global indentStack, indentLevel
        while(currentCol < indentStack[-1]):
            indentLevel -= 1
            indentStack.pop()
        #..revives itself after death
        if(not indentStack):
            indentStack = []
            indentLevel = 0
    
        #If the column is greater than the previous line's column
        if(currentCol > indentStack[-1]):
            indentStack.append(currentCol)
            indentLevel += 1
        else:
            raise ParseException(s,l,'Not an indent')
    
    
    INDENT = lineEnd.suppress() + empty + empty.copy().setParseAction(checkIndent).setDebug().setName('indent')



---
## 2011-07-19 17:19:50 - mehrdadfeller - from tokens to strings
Hi,

I am new to both python and pyparsing. I have two questions that are related to each other:

1) is it possible that after tokenizing and parsing a string, covert the ParseResults object back to a string?

Say we input ['hello','world'], and get 'hello, world!' (of course this is trivial, but say in a more general case with a more complex grammar )

2) is it possible to directly modify the input string/text as you are parsing it?

For example, say I am parsing a function definition like this (in some language):

'''
function myfun (i1,i2,o)
o = i1 + i2
'''

now I want to tell my parser, whenever it runs into a function declaration with name 'myfun', then extend such that:

function myfun (i1,i2,o,o2)
o = i1 + i2
o2 = i1 - i2

what's the best way to deal with such problems?

#### 2011-07-19 22:54:49 - ptmcg
1) Pyparsing returns its parsed data as a ParseResults object. ParseResults can be accessed like a Python list (index by integer value, len, slicing). If results names are defined, then you can use them to directly access the respective tokens, similar to named groups in regular expressions. As a list-like object, you can use it just like you would a Python list, as in:



    greeting = Word(alphas) + ',' + Word(alphas) + oneOf('. !')
    tokens = greeting.parseString('Hello, World!')
    
    print ' '.join(tokens)


2) Pyparsing allows you to define parse-time callbacks, or parse actions. Parse actions can be implemented with any of the method signatures:

- fn()
- fn(tokens)
- fn(location, tokens)
- fn(sourceString, location, tokens)

A parse action can modify the parsed tokens at parse time, to embellish the parsed tokens, convert them to another type (such as converting a numeric string to int or float), do additional validation, or update an external data structure. In the above parser, we could add the line:


    def reverseTokens(tokens):
        for i,t in enumerate(tokens):
            tokens[i] = tokens[i][::-1]
    
    greeting.setParseAction(reverseTokens)


By changing the tokens parameter in place (or by returning a new list or string), the parse action can change the results of the parsing - at parse time.

ParseResults? results names? parse actions? Lots of new concepts. Please get the source release of pyparsing which includes how-to documentation and many example scripts. Or if you can part with $10, pick up 'Getting Started with Pyparsing' from O'Reilly. Or visit this wiki's Documentation page for some links to other tutorials and articles.

-- Paul

_[ED: "Getting Started with Pyparsing" is no longer available from O'Reilly.]_

#### 2011-07-20 11:02:42 - mehrdadfeller
Thanks a lot Paul for your time and answer. 

I am in fact trying to extend the token lists;

Can you explain what is wrong with the following:

    def addExtraExclamation(tokens):
            tokens.append('!!')
    
    greeting = Word(alphas) + ',' + Group(Word(alphas)) + oneOf('. !')
    
    greeting.setParseAction(reverseTokens)
    tokens = greeting.parseString('Hello, World!')
    



    [cad@mylinuxbox probe_insertion]$ python example.py
    Traceback (most recent call last):
      File 'example.py', line 18, in ?
        tokens = greeting.parseString('Hello, World!')
      File '/usr/lib/python2.4/site-packages/pyparsing-1.5.6-py2.4.egg/pyparsing.py', line 1021, in parseString
        loc, tokens = self._parse( instring, 0 )
      File '/usr/lib/python2.4/site-packages/pyparsing-1.5.6-py2.4.egg/pyparsing.py', line 921, in _parseNoCache
        tokens = fn( instring, tokensStart, retTokens )
      File '/usr/lib/python2.4/site-packages/pyparsing-1.5.6-py2.4.egg/pyparsing.py', line 675, in wrapper
        return func(*args[limit[0]:])
    TypeError: reverseTokens() takes exactly 1 argument (0 given)
#### 2011-07-20 11:12:28 - mehrdadfeller
sorry I made mistake in the above code; but still the same issue exists:



    def addExtraExclamation(tokens):
        tokens.append('!!')
    
    greeting = Word(alphas) + ',' + Group(Word(alphas)) + oneOf('. !')
    
    greeting.setParseAction(addExtraExclamation)
    tokens = greeting.parseString('Hello, World!')


and after running:



    Traceback (most recent call last):
      File 'example.py', line 13, in ?
        tokens = greeting.parseString('Hello, World!')
      File '/usr/lib/python2.4/site-packages/pyparsing-1.5.6-py2.4.egg/pyparsing.py', line 1021, in parseString
        loc, tokens = self._parse( instring, 0 )
      File '/usr/lib/python2.4/site-packages/pyparsing-1.5.6-py2.4.egg/pyparsing.py', line 921, in _parseNoCache
        tokens = fn( instring, tokensStart, retTokens )
      File '/usr/lib/python2.4/site-packages/pyparsing-1.5.6-py2.4.egg/pyparsing.py', line 675, in wrapper
        return func(*args[limit[0]:])
    TypeError: addExtraExclamation() takes exactly 1 argument (0 given)


#### 2011-07-20 11:39:48 - mehrdadfeller
OK! I found the answer; here's the trick on how to do it:



    def addExtraExclamation(tokens):
            tokens = tokens.asList() 
            tokens.append('!!')
            print tokens
            return tokens


#### 2011-07-20 20:02:18 - ptmcg
That is odd, there is no append() method on ParseResults.  You *can* add another ParseResults to it, so you could simplify your parse action to:


    tokens += ParseResults('!!')

Seems like an oversight to me, not having append on ParseResults.

-- Paul

---
## 2011-07-21 04:23:48 - nimrodra - Using operatorPrecedence for AST
Hey, I am writing a parser for math expressions, and am using operatorPrecedence.


    integer = Word(nums)
        operand = integer 
    
        plusop = oneOf('+ -')
    
        expr = operatorPrecedence( operand,
            [(plusop, 2, opAssoc.LEFT)]
            )
        # TODO: forward
        grammar = expr
    
        test = '1+2+3'
        print grammar.parseString(test)

(This is not my real grammar, but it shows what I need. Changing my real grammar to get what I want won't be easy like changing this one).
this prints:

    [['1', '+', '2', '+', '3']]

I would like it to be like a tree, and print:

    [[['1', '+', '2'], '+', '3']]

or something like this.

Is there any built-in way to do this? I can't find a way to combine operatorPrecedence with Group, which I would normally use for this task.

Thanks!

#### 2011-07-24 05:15:57 - ptmcg
Sorry for the delay. I could have sworn I just answered this question recently, but now I can't track down where this discussion thread occurred so I could link to it.  I do have some of the sample code left over though.

Having '1+2+3' parse to ['1', '+', '2', '+', '3'] can be converted to the more traditional AST form using a parse action in the call to operatorPrecedence:



    from pyparsing import *
    
    # parse action -maker
    def makeASTlike(numterms):
        if numterms is None:
            # None operator can only by binary op
            initlen = 2
            incr = 1
        else:
            initlen = {0:1,1:2,2:3,3:5}[numterms]
            incr = {0:1,1:1,2:2,3:4}[numterms]
    
        # define parse action for this number of terms,
        # to convert flat list of tokens into nested list
        def pa(s,l,t):
            t = t[0]
            if len(t) > initlen:
                ret = ParseResults(t[:initlen])
                i = initlen
                while i < len(t):
                    ret = ParseResults([ret] + t[i:i+incr])
                    i += incr
                return ParseResults([ret])
        return pa
    
    
    # setup a simple grammar for 4-function arithmetic
    varname = oneOf(list(alphas))
    integer = Word(nums)
    operand = integer | varname
    
    # ordinary opPrec definition
    arith1 = operatorPrecedence(operand,
        [
        (oneOf('+ -'), 1, opAssoc.RIGHT),
        (oneOf('* /'), 2, opAssoc.LEFT),
        (oneOf('+ -'), 2, opAssoc.LEFT),
        ])
    
    # opPrec definition with parseAction makeASTlike
    arith2 = operatorPrecedence(operand,
        [
        (oneOf('+ -'), 1, opAssoc.RIGHT, makeASTlike(1)),
        (oneOf('* /'), 2, opAssoc.LEFT, makeASTlike(2)),
        (oneOf('+ -'), 2, opAssoc.LEFT, makeASTlike(2)),
        ])
    
    # parse a few test strings, using both parsers
    tests = '''\
    A+B+C+D+E
    A+B+C--D+E
    A+B+C*D+E
    12X+34Y+C*7+E'''.splitlines()
    for arith in (arith1, arith2):
        for t in tests:
            print t
            print arith.parseString(t)[0]
        print
    


This makeASTlike method handles 1, 2 and 3-operand operators - there is an attempt at a provision for None as an operator (as in 'y=mx+b', where 'mx' has no operator but implies multiplication), but this gets things fouled up with unary operators.

HTH,
-- Paul
#### 2013-04-09 10:27:18 - mbmccurdy
Hallo!

I am very interested -- is there a way to do this that does not hijack the parseActions of operatorPrecedence.

In my example, I have an operatorPrecedence of the following form:

    operatorPrecedence( gadget,
         [
               (MYOP, 1, opAssoc.RIGHT, myAction),
               (MYOTHEROP, 2, opAssoc.LEFT, myOtherAction),
         ]

and I would like to somehow obtain the same nested structure. In point of fact, I have about thirty different parseActions, so it will be impossible to rewrite all of them.

#### 2013-04-09 18:42:24 - ptmcg
See if a decorator-like approach can work for you, something like:



    def unaryASTfn(fn):
        # assumes right associativity
        def pa(s,l,t):
            i,incr = 2,1
            ret = fn(ParseResults(t[-i:]))
            i += 1
            while i < len(t):
                ret = fn(ParseResults(t[-i:-i+incr] + [ret]))
                i += incr
            return ParseResults([ret])
        return pa
    
    def binaryASTfn(fn):
        # assumes left associativity
        def pa(s,l,t):
            i,incr = 3,2
            ret = fn(ParseResults(t[:i]))
            i += 1
            while i < len(t):
                ret = fn(ParseResults([ret] + t[i:i+incr]))
                i += incr
            return ParseResults([ret])
        return pa
    
    expr = operatorPrecedence( gadget,
            [
            (MYOP, 1, opAssoc.RIGHT, unaryASTfn(myAction)),
            (MYOTHEROP, 2, opAssoc.LEFT, binaryASTfn(myOtherAction)),
            ])


Now write myAction, myOtherAction, etc. as if you had a more traditional AST argument set passed to you.

-- Paul

---
## 2011-07-23 08:21:11 - joslin01 - 'str' object is not callable
Hi, does anyone have any idea as to where to look for this kind of error message: 

    TypeError: 'str' object is not callable*

The debug statements make it look like everything is going to get parsed perfectly, but then it hits this error and stops. Some of my grammars compile into classes that look like this:



    class WhileStatement(CompoundStatement):
        def __init__(self,t):
            self.arg = t
            pprint.pprint(t.asList())
        def __str__(self):
            return pprint.pformat(self.arg.asList())


They're not complete..

This is the entire traceback:


    Traceback (most recent call last):
      File 'tests.py', line 151, in <module>
        test_files(arg + '.gup')
      File 'tests.py', line 124, in test_files
        tokens = grammar.file_input.parseFile(file)
      File 'c:\python27\lib\site-packages\pyparsing.py', line 1410, in parseFile
        return self.parseString(file_contents, parseAll)
      File 'c:\python27\lib\site-packages\pyparsing.py', line 1021, in parseString
        loc, tokens = self._parse( instring, 0 )
      File 'c:\python27\lib\site-packages\pyparsing.py', line 894, in _parseNoCache
        loc,tokens = self.parseImpl( instring, preloc, doActions )
      File 'c:\python27\lib\site-packages\pyparsing.py', line 2735, in parseImpl
        loc, tokens = self.expr._parse( instring, loc, doActions, callPreParse=False
     )
      File 'c:\python27\lib\site-packages\pyparsing.py', line 894, in _parseNoCache
        loc,tokens = self.parseImpl( instring, preloc, doActions )
      File 'c:\python27\lib\site-packages\pyparsing.py', line 2478, in parseImpl
        ret = e._parse( instring, loc, doActions )
      File 'c:\python27\lib\site-packages\pyparsing.py', line 894, in _parseNoCache
        loc,tokens = self.parseImpl( instring, preloc, doActions )
      File 'c:\python27\lib\site-packages\pyparsing.py', line 2623, in parseImpl
        return self.expr._parse( instring, loc, doActions, callPreParse=False )
      File 'c:\python27\lib\site-packages\pyparsing.py', line 872, in _parseNoCache
        loc,tokens = self.parseImpl( instring, preloc, doActions )
      File 'c:\python27\lib\site-packages\pyparsing.py', line 2435, in parseImpl
        return maxMatchExp._parse( instring, loc, doActions )
      File 'c:\python27\lib\site-packages\pyparsing.py', line 872, in _parseNoCache
        loc,tokens = self.parseImpl( instring, preloc, doActions )
      File 'c:\python27\lib\site-packages\pyparsing.py', line 2478, in parseImpl
        ret = e._parse( instring, loc, doActions )
      File 'c:\python27\lib\site-packages\pyparsing.py', line 872, in _parseNoCache
        loc,tokens = self.parseImpl( instring, preloc, doActions )
      File 'c:\python27\lib\site-packages\pyparsing.py', line 2368, in parseImpl
        loc, exprtokens = e._parse( instring, loc, doActions )
      File 'c:\python27\lib\site-packages\pyparsing.py', line 872, in _parseNoCache
        loc,tokens = self.parseImpl( instring, preloc, doActions )
      File 'c:\python27\lib\site-packages\pyparsing.py', line 2478, in parseImpl
        ret = e._parse( instring, loc, doActions )
      File 'c:\python27\lib\site-packages\pyparsing.py', line 872, in _parseNoCache
        loc,tokens = self.parseImpl( instring, preloc, doActions )
      File 'c:\python27\lib\site-packages\pyparsing.py', line 2368, in parseImpl
        loc, exprtokens = e._parse( instring, loc, doActions )
      File 'c:\python27\lib\site-packages\pyparsing.py', line 872, in _parseNoCache
        loc,tokens = self.parseImpl( instring, preloc, doActions )
      File 'c:\python27\lib\site-packages\pyparsing.py', line 2769, in parseImpl
        loc, tokens = self.expr._parse( instring, loc, doActions, callPreParse=False
     )
      File 'c:\python27\lib\site-packages\pyparsing.py', line 894, in _parseNoCache
        loc,tokens = self.parseImpl( instring, preloc, doActions )
      File 'c:\python27\lib\site-packages\pyparsing.py', line 2623, in parseImpl
        return self.expr._parse( instring, loc, doActions, callPreParse=False )
      File 'c:\python27\lib\site-packages\pyparsing.py', line 907, in _parseNoCache
        tokens = fn( instring, tokensStart, retTokens )
      File 'c:\python27\lib\site-packages\pyparsing.py', line 675, in wrapper
        return func(*args[limit[0]:])



#### 2011-07-23 11:05:23 - joslin01
I set a parse action as ('...') by accident!! So used to using setName('..') and ('...').
#### 2011-07-24 05:02:09 - ptmcg
:) Glad you found your bug!  This does seem to be a downside to the new parse action wrapping, in that exceptions raised in parse actions end up in some pretty cryptic stack traces.  I'll try to look at that some more to see if I can be more selective about exception catching in the trim_arity code.

-- Paul

---
## 2011-07-23 13:53:30 - joslin01 - indentBlock function
Sorry about million questions, but I've been working on this all day and can't figure it out.

I'm using indentedBlock as so:


    indentStack = [1]
    suite = indentedBlock(suite_stmt, indentStack)


When I look at the debug match results, everything *seems* to parse as I intend it to. Things are nested properly and suites are properly nested inside if/elif/else statements.

However, I keep getting an end of line error. I've been looking at the pyparsing method, and just put some debugs on it to try to follow what's happening.

At the bottom of code like this:


    if x <= 5:
        print 'what'd you do with 5?'
        x = 5 - 4
    else:
        x = 6 * 6

It reports that it wants an EOL at line 3, column 9. Assuming spaces are converted into tabs, and then removed? Is how I assumed it got column 9. At any rate, here's what I see at the end:



    Matched BLOCK_STMT -> [<actions.SmallStatement object at 0x022D47F0>]
    Match LINE END!! at loc 55(3,9)
    Exception raised:Expected end of line (at char 55), (line:3, col:9)
    Match PEER at loc 55(3,9)
    Exception raised:illegal nesting (at char 55), (line:3, col:9)
    Exception raised:illegal nesting (at char 55), (line:3, col:9)
    Exception raised:illegal nesting (at char 55), (line:3, col:9)
    Exception raised:illegal nesting (at char 55), (line:3, col:9)


Does it actually *match* the line end? I set a debug & name('LINE END!!!') on the NL inside the indentBlock function.

I left the default whitespace chars as is, but I do see that the NL does set it to space + tab thereby leaving in a newline. So why this confusion?

Playing around, I did `ParserElement.setDefaultWhitespaceChars(' \t')` to be sure newlines are left in everywhere. This works OK with my grammar, but I wanted to see if it would change anything with the indentBlock. It does, and instead raises an exception that (3,9) is not an *unindent* rather than not a newline. But still, I'm confused why the token stream hasn't advanced to 4,1, see that it's unindented and break out.

Sorry for the essay! But I am genuinely curious + confused, so any poke in the right direction would really help.

#### 2011-07-23 13:55:03 - joslin01
For completeness sake:



    Matched BLOCK_STMT -> [<actions.SmallStatement object at 0x02450B70>]
    Match LINE END!! at loc 55(3,9)
    Exception raised:Expected end of line (at char 55), (line:3, col:9)
    Match PEER at loc 55(3,9)
    Exception raised:not a peer entry (at char 55), (line:3, col:9)
    Match MATCHED DUNDENT at loc 55(3,9)
    Exception raised:not an unindent (at char 55), (line:3, col:9)
    Exception raised:not an unindent (at char 55), (line:3, col:9)
    Exception raised:not an unindent (at char 55), (line:3, col:9)
    Exception raised:not an unindent (at char 55), (line:3, col:9)


#### 2011-07-24 07:33:55 - ptmcg
No need to apologize, it is a compliment that you are so deeply immersed in pyparsing!

That said, parsing an indented syntax is one of the more, um, problematic areas in pyparsing, and would not be my recommended first application.  But this is where you are, let's see if I can add anything to what you have already.

First off, I would definitely make sure you were not assuming tab-space equivalence here.  Pyparsing defaults to converting tabs to spaces before starting the parsing process - can disable this by calling parseString with parseWithTabs=True. And I think this would become even more of an issue when trying to parse an indented grammar.

Why was no EOL parsed at the end of line 2 I wonder?  Is your print statement doing something weird with wraparound that is messing up the indent logic?  Try removing the print statement and see if your example parses any better.  Or at least setDebug on the expression that parses the print statement, to see what you are getting.
#### 2011-07-24 09:13:14 - joslin01
Thanks for the library at any rate! It's great. Now let's go through my blunders..



    First off, I would definitely make sure you were not assuming tab-space equivalence here.
    [[code]]I was. My thought process was like this:
    * Tabs turn into spaces such that 1 tab = 4 spaces
    * Spaces keep there location bound (column 4)
    * Indented block would pick up these columns with indented block
    
    I don't know if this still is true, but parsing **with** tabs -- a simple one line change -- has everything parsing perfectly now. I did a test (as you'll see below), and verified that even though 
    tabs are parsed as spaces, I still got a location 18 (2,17) for b with 2 tabs indented in. 
    

Why was no EOL parsed at the end of line 2 I wonder?
I watched my print statement all the way through and it parsed fine. I was so confused because I followed it to its logical conclusion where it stopped (and parsed) at 3,9. It would get to EOL, and lineEnd would not match 3,9. To be sure, I injected some emptys before lineEnd wondering if maybe the token stream had to get moved forward? The emptys all stayed on 3,9 and did not help. Now, I believe the same lineEnd parses at 4,1.

Confused by most of this last night, I ran some tests. I'll write down my findings (which really helped themselves -- but did not fix) for reference. Plus, as author, you can verify they're true! :D
<ul><li>LineEnd() matches with default characters on or off. It makes no difference whether you set ParserElement.setDefaultWhitespaceChars('') Or ('\n\t ').</li><li>When debugging parser elements, it goes through the following phases for hypothetical element `A`:<ul><li>Begins parsing element : 'Match A at loc 1 (1,1)'</li><li>If no match: Throw Parse Exception (e..g 'Exception raised: expected A at 1 (1,1)') Else...</li><li>If match, reports '<strong>Matched</strong> A at loc 1 (1,1)'<ul><li>Throughout this entire time, I had basically misunderstood how debugging works confusing 'Match' for actual matches</li></ul></li></ul></li><li>Empty does advance the token stream</li></ul>
For example,


    c = a + LineEnd() + b
    data = 
    '''a
          b
    '''

<ul><li>Putting an empty in between a + LineEnd() <strong>will cause a parse exception on LineEnd()</strong> because you would have moved token stream to second line.</li><li>The first empty after LineEnd() will show a location of Line 2 Column 1. Appending <em>another</em> empty will match line Line 2 Column 7 where the next token is located (b). Any subsequent emptys after that will all match (2,7) and will always match but never move forward.</li><li>Turning off default whitespace characters, it still would report b at location (2,7)</li><li>Newline after b in this case gives a location of (2,1) <strong>but is actually location 15</strong> whereas b matched at <strong>(2,7) location 8</strong>. Another NewLine() after b will match (3,1) location 16, and another NewLine() after that will be a parse exception.</li></ul>

That's all! Thanks for your help Paul, I wouldn't have believed it was such as simple fix but I'm relieved!
#### 2011-07-24 09:31:26 - joslin01
Actually, I'll be damned. After setting parseWithTabs, I restored the pyparsing.py file to its original state. This caused my parsing tests to fail. What works for me is placing an empty before the LineEnd() as so:



    NL = OneOrMore(empty + LineEnd().setWhitespaceChars('\t ').suppress())


#### 2011-07-24 18:10:22 - ptmcg
What happens if you do:


    empty.setWhitespaceChars('\t ')


The problem with calling `ParserElement.setDefaultWhitespaceChars` is that you don't get a chance to call it until *after* the helpers like 'empty', 'lineEnd', and so on are already defined. If your code calls `ParserElement.setDefaultWhitespaceChars`, you should probably take time to redefine the helpers that you use, so that they pick up the new defaults:


    empty = Empty()

and so on.  (I'm going to fix this in the next release, to have the helpers get redefined as part of `setDefaultWhitespaceChars`.)

#### 2011-07-24 18:12:34 - ptmcg
And yes, empty *does* advance the token stream - sometimes I use it as a 'whitespace eater', to advance the parser to the next non-whitespace character in the input text.  There are some constructs though, like NotAny and FollowedBy that don't advance the token stream, they just evaluate the corresponding negative or positive lookahead.
#### 2011-07-24 18:22:29 - joslin01
Hmm.. well if I do .setWhitespaceChars('\t ') in the NL declaration, it doesn't match correctly. 

I think I understand the setDeafultWhitespacesChars a little better now. It was at the top of grammar code file, and sometimes it would make a difference sometimes it wouldn't. 

At any rate, I don't the grammar relies on changing the default whitespace mechanisms. I tried to make that possible after reading a bit of your posts on getting it to work with the grammar rather than pushing up against it. It really hasn't been all that difficult to do honestly. I suppose the only exception is that I have to parse with tabs, so far.

---
## 2011-07-27 06:09:29 - natgiot - inconsistent behavior of CloseMtch depending on number of errors
Hi, 
I am using CloseMatch to identify, within a fasta file, a given string. I want to get both exact matches and mismatches, so in my code (see below) I call the function that uses CloseMatch allowing for 2 mismatches. My function reports at the end number of exact matches and mismatches in forward and reverse strand. Hiowever, when I allow for 2 mismatches, I do not find an exact match that I checked is present in the sequence. I do find it only when I ask for 0 mismatches (in which case no mismatches are reported). This behavior is strange, unless I miss something fundamental with CloseMatch. I tried 3 different strings, and it only happened in one of them. I checked the sequence, the strings, everything I could do so far. Here is the code ,the problem is with exact matches of motif d in the second file (tetra-smalla_sub1770-16916.fa). I could provide the fasta as well but I don't know how to attach files here, so I just copy an extract of the sequence that contains the d motif.
Any help would be greatly appreciated, cause i am kind of stuck in my understanding..
Thanks!
Natassa



    from Bio import SeqIO
    import os
    import pyparsing
    
    def complement(seq):  
        complement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A'}  
        complseq = [complement[base] for base in seq]  
        return complseq
    
    def reverse_complement(seq):  
        seq = list(seq)  
        seq.reverse()  
        return ''.join(complement(seq))  
    
    
    
    
    def FindMotif_pyparse(seqrec, motif, num_errors, report=False):
        '''Gets one sequence and one motif, searches it in both strands and returns4 elements: Counts_For_exact,Counts_For_Mis,Counts_Rev_exact, Counts_REv_Mis ALWAYS IN THIS ORDER .    allows number of errors'''
    
        Counts_For_exact=0
        exactForLocs=[]
        Counts_For_Mis=0
        MisForLocs=[]
        Counts_Rev_exact=0
        exactRevLocs=[]
        Counts_Rev_Mis=0
        MisRevLocs=[]
    
    
        revCompmotif=reverse_complement(motif)
    #get the forward  matches 
        foundseqF = pyparsing.CloseMatch(motif, num_errors)
        #print 'searching for: ', motif
    
     # for every match in forward
    
        for t,startLoc,endLoc in foundseqF.scanString(seqrec):
    
            matched, mismatches = t[0]
    
            print 'matched: ', matched
            print 'motif: ', motif
            #print 'mismatches: ', mismatches
            #print 'searching for: ' +str(motif) 
            #print 'debugF  ' + str(matched)+ ' ' + str(startLoc) +' - '+ str(endLoc)
    
     #if it is exact
            if matched==motif:
                #print 'exact'+'\n'
                Counts_For_exact= Counts_For_exact+1
                #print 'Counts_For_exact: ', Counts_For_exact
                if report==True:
                    exactForLocs.append((startLoc,endLoc))
    
    
            else:
                #matched, mismatches = t[0]
                #print '      ', ''.join(' ' if i not in mismatches else '*' 
                   #                        for i,c in enumerate(foundseqF.sequence))
                Counts_For_Mis= Counts_For_Mis+1
                if report==True:
                    MisForLocs.append((startLoc,endLoc))
    
    
    ##get the reverse matches
        foundseqR = pyparsing.CloseMatch(revCompmotif,num_errors)
    
        for t,startLoc,endLoc in foundseqR.scanString(seqrec):
            matched, mismatches = t[0]
    
            #print 'searching for: ' +str(revCompmotif) 
            #print 'debugR  ' + str(matched)+ ' '+ str(startLoc) +' - '+ str(endLoc)
            if matched==revCompmotif:
                #print 'exact'+'\n'
                Counts_Rev_exact= Counts_Rev_exact+1
                if report==True:
                    exactRevLocs.append((startLoc,endLoc))
            else:
                #matched, mismatches = t[0]
                #print '      ', ''.join(' ' if i not in mismatches else '*' 
                  #                      for i,c in enumerate(foundseqR.sequence))
                Counts_Rev_Mis= Counts_Rev_Mis+1
                if report==True:
                    MisRevLocs.append((startLoc,endLoc))
        #print Counts_For_exact,Counts_For_Mis,Counts_Rev_exact, Counts_Rev_Mis  
        if report==False:          
            return Counts_For_exact,Counts_For_Mis,Counts_Rev_exact, Counts_Rev_Mis
        if report==True:
            print 'ForEx: ', Counts_For_exact,exactForLocs
            print 'ForMis: ', Counts_For_Mis,MisForLocs
            print 'RevEx: ', Counts_Rev_exact, exactRevLocs
            print 'RevMis: ', Counts_Rev_Mis,MisRevLocs
    
    
    
    if __name__ == '__main__':
        crossovers_sep10=dict(a='TCTTCCTTCC', b='TCACAGCAAGC', d='TCTTCTTCTTC')
        for motif in crossovers_sep10.keys():
            print 'motif: '+str(motif)+'   '+ str(crossovers_sep10[motif])
            for i in ['/Users/nat/Data/Mat_Scaffolded/GC_content/tetraA_sub62746-80901.fa','/Users/nat/Data/Mat_Scaffolded/GC_content/tetra-smalla_sub1770-16916.fa']:
                print 'in: '+ i   
                for req in SeqIO.parse(open(i), 'fasta'): 
                    seqrec=req.seq
                    FindMotif_pyparse(seqrec, crossovers_sep10[motif], 2, report=True)

And sequence extract:

    ATGGCGACCATACGGAGTCTCGATCATACCAAGTGCGTTTGTCCAAGTCATACTTCCCGAATGGCTAGGCGCAT
    GTACACTGACCGATACTGCCGCTGTACAGATCCGAAGCCGAACTGGCCATCAACATCCGAAAGGCCACCAGCGC
    CGAAGAGTCTGCCCCGAAACGCAAGCATGTTCGAAGTTGTATCGTTTACACGTGGGACCACAAGTCTTCCCAGT
    CCTTCTGGGCAGGCATGAAGGTTCAACCTATCATGGCCGACGAGGTCCAGACTTTCAAGGCCCTCATCACCATT
    CACAAGGTCTTACAGGAGGGCCACCCGGCGACACTGAGAGAGGCCATGGCAAACCGCGGTTGGATTGACAGTCT
    GAACCGCGGCATGGGCGGCGAAGGTATGCGTGGATATGGGCCCTTGATCAAGGAATACGTTTATTATCTCCTTG
    CGAAGCTTTCGTTTCATCAACAGCACCCCGAGTTCAACGGCACATTCGAGTACGAGGAATACATCAGTCTCAAG
    GCCATTAACGATCCGAATGAGGGTTACGAGACCATCACCGACCTCATGACGCTGCAGGATAAAATTGACCAGTT
    CCAGAAGCTCATCTTCTCTCACTTCCGGAACGTCGGCAACAACGAGTGCAGGATATCTGCCCTCGTCCCCCTAG
    TCACCGAAAGTTATGGCATCTACAAGTTCATCACCAGCATGTTGCGCGCCATGCACTCATCTACTGGCGACGCC
    GAGGCGCTGGAACCGTTGCGCGGAAGATATGATGCCCAGCATTACCGGCTGGTCAAGTTTTACTACGAGTGCTC
    AAACCTCCGCTATCTCACCAGTTTGATCACGATTCCCAAGCTTCCCCAGGATCCGCCCAACCTCCTGGCCGAGG
    ATGAGAACGCACCTGCTTTGCCTGCTCGGCCAAAGCACGAAATCGAAAAGCGCCCGACACCTCCGCCCCCAGCA
    CCGAAATCCGAAGAGCCCGATGACATGGCCGAGTTCTGGAAGAATGAGCTTGACAGGCAGAACCGGGAATACGA
    GGAGCAGCAGCGGGTTTTGGAAGCACAGCAGCAGCAGGCGTTGCTAGCGCAGCAGCAAGCACAACTACAAGCGC
    AACGTGATTTCGAAGAACAGCAACGGCGGTTGATGGAGCAACAGCAAAGGGAGCAAGAGGCCTTGATGGCCCAG
    CAGACACAATGGCAGACTCAAGGGCGGCTTGCGGAGCTTGAACAGGAGAATCTGAATGCCCGGGCCCAGTACGA
    GCGGGATCAACTAATGCTTCAGCAGTACGACCAGAGGGTCAAGGCTCTAGAAAGCGAGTTGCAACAAATCCAGG
    GTAACTACGGGCAGCAAATAAACAGCAGAGACGACCAAATCCGTGCTCTACAAGAACAACTCAACACTTGGAGG
    ACAAAGTACGAAGCGCTTGCGAAGCTCTATTCCCAGCTTCGCCACGAGCACCTTGACCTCCTCCAAAAGTTCAA
    GACGGTGCAGCTCAAGGCGGCTTCCGCTCAGGAAGCGATTGAGAAGCGGGAGAAGCTTGAGCGTGAGATCAAGA
    CCAAGAACTTGGAGCTGGCAGACATGATTCGTGAGCGCGATCGTGCTCTTCATGAAAAGGACAGGCTGCAAGGC
    TCGAATAAGGACGAGGTTGAGAAGCTCAAGAGGGAGTTGCGGATGGCCTTGGACCGCGCCGATAATCTGGAGCG
    CAGCAAGGGCAACGAGCTTTCTACAATGCTTTCCAAGTACAACCGCGAGATGGCCGATCTGGAGGAAGCTTTGC
    GTATCAAGTCACGAGCACTGGAAGAGGCCCAAGCCAAGCTACGGGATGGCAGCTCGGATCTTGAAGCGCTACTT
    CGCGAGAAGGAGGAGGAGCTCGAGGTATACAAAGCTGGCATGGACCAGACACTGATCGAGCTCAATGAGCTCAG
    GAACAACCAGGGTGTCTCCGATTCAGCCCTTGACGGCCAGCTGGACGCTTTGATCCTCTCCCAGCTGGAGAAGG
    TCAACGAGATTATCGACTCTGTGCTTCAAGCTAGCGTTCAGCGTGTTGATGATGCCATGTATGAGCTCGATTCT
    GACATGCAAGCCGGAAACCAGAACGCCTCGCCTTCCTATGTCCTCTCGCAGATTGAGAAGGCGTCTGCGAGTGC
    CACGGAGTTCGCTACCGCTTTCAACGTCTTCATCGCCGATGGTCCCAGCAGCAGCCCGGCGGAGCTGATCAAGA
    ACGCCAACGTTTTCGCTGGCGCCATCGCCGATGTGTGCAGCAACACCAAGGGTCTTGTTCGCCTCGCTACTGAC
    GAGAAGAAGGGAGATTCTTTGATCAACGGTGCTCGTTCATCTGCCCATTCGACAGTCAAGTTCTTCCGTGGTCT
    GTTGAGCTTCCGCCTTGAGGGCATGGATCCCCTTCAGAAGACCGACGTCGTCATCAACAGCAATAACGACGTCC
    AGATCAACCTGCAAAAGCTGAACAAGCTTGTCGAGACCTTCGCCCCTGGCTTTGGCCAGCTCGCTAACAAGGGT
    GACCTTGGCGACATCGTCGATCAAGAACTCAGCAGGGCGGCCGATGCCATTGCTGCTGCCGTTGCCCGTTTGCA
    GAAGCTCAAGAACAAGCCTCGCGATGGATATACTACCTACGAACTGAAGGTCCACGACTCAATCCTCGATGCCG
    CCATGGCCATTACTACTGCTATCGCGCAGCTCATCAAGGCTGCCACCGCTACACAGCAGGAGATTGTGCAAGCT
    GGTCGTGGCTCGTCTTCGCGCACCGCCTTCTACAAGAAGAACAACAGATGGACCGAGGGTCTCATTTCCGCCGC
    CAAGGCTGTTGCCAACTCTACCAGCACTCTCATAGAGACTGCTGACGGTGTTCTTTCCAACCGCAACAGCCCTG
    AGCAGCTCATCGTTGCATCTAATAATGTTGCGGCCTCCACCGCCCAGCTCGTGGCTGCCAGCCGTGTAAAGGCC
    GGCTTCATGAGCAAGAACCAGGAGAGCCTGGAGGAGGCCAGCAAGGCTGTCGGTGCCGCCTGCCGTGCACTCGT
    CAGACAAGTCCAGAGCATGATCAAGGACCGCAGTGCCGAGGATGAGCAAGTGGACTACAGCAAGCTTGGGTCGC
    ACGAGTTCAAAGTGCGTGAGATGGAGCAACAGGTACGTATTTCCTCCGAGATTTCCTTCTTCTTTTATTTATGG
    TACCCTTGGTCTTGCCTATCGTGGTTCATTGCTGACCAAGTCATTCTCTCCCATCACAGGTCGAAATCCTTCAA
    CTTGAAAACGCATTGAATGCTGCTCGCCACAGACTTGGCGAGATGCGTAAGATTTCGTACCAACAGGACGAGGA
    CTAATTGTCCGGCAACGCATGGGGTTGTTGAGGTAGTATACTCGGTGGCTTCATTCATCTCGCTGGCTTTACGC
    TTCAATGCGCAGCGTGGGCACATTAATCCGACCAGATCCCCTCCCTCATTCTGTTTTCGTCCCGCAGCATCTTC
    CAAAATCTTCATCAGATGTTTTACATCTTAACTACCCCCTCTGTACCTAGTTCGAATTCCTTTTGGAGTCATCA
    GGTGTTAGTGTTAAACTCAGTTTCATTCACCTTGTTCTTTTTCATCCACTGAAGTCGTCAATTGGGCGTCGTGA
    CTTCGGCCTCGGATATGTTCCGTTCGCTCTTCGGGCGGGTGTTTGGTAACACATAGGCAAACCGCCACTCCCGC
    TGTCATACTGCTGGCTTCCGTTGTGTTTCTTTTATGACCATGTACGTTTTTTTTTGTAGTTTCGATTTTCGCGT
    TCGCTGCTCGCTGCTCGCTCGATCGGTCGGTCTGCCAGAGAGTTTACCGATACGTCAAGAGTGAGTAGGCGACC
    AAATTTCGTTCTTTGTGCTGCTGAAATCACCCCTTCTTCCGCCTCCTCTTCCCCTTCCCCTCTCGCGTCAGTTC
    TTCAGTCCTCGGCTGTCCAATTTCCATACGTCCTCTTATGCGGCGCATTTCACTCCCTCTCGTCGCTTTTTCGT
    CGCAGCCCGTTCGTTGTATCGTCATCGGTGGTTGCTTTCGTTCGTGAAGCAAGATTCAGAACGAGGAAATTTCT
    GATGGAGGGAGTTCAAGTTCCGTTTCCTTCATTTCGCCTGGTCCAATATTCTGTTTTCTCTCTTTCTTTTCGTT
    CCCCTCGTCAGCTTGAAAGTCTGGACCCTGTCTCCCTCTTGACTTGCATTTCCTAGTTCTTCCCATTCTGGCAG
    GCCAAGCTGAAGTCGAACCATCACGCTCTCCTGCTTCCTTTCTCCAGTCGGTTGGCGTTCACGTTATGCTATTC
    AAGTGCTCCTGCGGAGTCGTCGTTGTCGTACCAACCAATAACTTGGATGATTGGTAAATGGATGTACCAATGCC
    AGGTATTGGATCCGCTGGTATTTGCTGTGGGCAGATGGTTGTGATGTCCATGAGTAATTTACTCCCATTCTCGC
    TCATGGTCCCTCCCCCCTTTACCGCTTGATCTTGGTTCTTTCTTCTTCTTCCATGAATCAATTCGACATCCAAC
    AACCGATATACCAAACCACACCGTCCTTCGCCTTGTTGTCCTGGAACGTCACTGGTTGTGTTGCTGTTGGAGAT
    ATCGGGTGGTTCTTAATGGAAATCGGGTTTGCCGCTGCGTTGGGGCTTGGTGTCGAGGGTGGCAGTTTGCCGGG
    TGGTAGACGAGGGCGCTTTACGCGAGAGTATGTGGTCTTGAAGGAGGGGTAGGGAATGTACGGAGTCTCCGTGT
    TGAACTGTATATTGATCTATCCAGAATCTCCCTGCGCGAATTTCAATGGTAAAGCGGTGAATTAAAGAATGTGG
    AACTTGGAGGCTCCGTCAATGCATTGAGTGGAATTGTACCTCCTCTATAAATACTTACCAGATGGACATCGATA
    CGCCTGGAACGTATTGTAGTGTTCATTCTGGATCAACTCCTCACGGCCCGCTTGCACAATATGAAGGCAGAGGG
    CCCGGAAGTGTTTGAAACAATCATGCTACGAGGACCACAATACTAAGAGCCCTTCTCGCGTATCACCAAGAAAT
    TGAATGTACAATCTTTTTATCTGATTCATTGGCTCGGTTGTCAAGACATGGGTACATAGATTCCATCCTTCTGG
    GGACCACAGAAGATTGCATGACAAAGCGAGTGAGCAAATGGAAAGTTTCAAGACTATCGCGAGCAACATACCTT
    TCTTCCCCCCCTTTTTTCCTCTCATCTTTCTTTTTTTTTCTTTTCTTTTTTTCAAAGTTTCTTCAAGCCATGTG
    ATACAATTTCCTGCAAAGCAATTACAGGGCGACAAGGGAAGGAAGAAAAAAGGAACAAAAGGGATATAGGCTGC
    CGACCACAGTAAACATGCGCTGCTCCGAGTTGGACGGAGGACTGGAATGATATACCAAGCTAACGTGCGCAGCC
    CTCTGCTCTCTCTTTCTTAACCTTACTGAGATGATTGCTGCCGAAGTTTGTTTCACGCAAGCTACATAACGGAG
    ACAGGAATTGAGCAACCTAGCTCCCGTTTAGGTCACCAACGGAAGGTAACATGAAGGGATTCCTCAACTACGTA
    AAA

#### 2011-07-27 06:37:48 - ptmcg
Your exact match string is masked by a close match right before it.  Change your call to scanString to:


    for t,startLoc,endLoc in foundseqF.scanString(seqrec, overlap=True):

And you should start seeing the exact match case too.

Thanks for wringing out CloseMatch, I'll be very interested to hear any further comments or experiences you have with this class!

-- Paul
#### 2011-07-27 10:04:21 - natgiot
Thanks a lot for your quick response.  i will report any further comments, it is a great class to use.
Regards, 
Natassa
#### 2011-08-09 02:23:33 - natgiot
Well, I think I have tried testing what you proposed before posting the above comment, but, now i get an error message:

TypeError: scanString() got an unexpected keyword argument 'overlap'

when i look at the scanString() method it indeed does not seem to recognize this argument. 
I thought this might be a version issue, so I downloaded using easy_install the 1.5.5 version, which just created the Adding pyparsing 1.5.5 to easy-install.pth file in my Library site packages. I am not very familiar with this kind of install, I am not sure if it has indeed installed (the pyparsing.py is the same) and I am not still sure if the error I get is related to this. Some help?
Thanks, 
Natassa
#### 2011-08-09 05:14:16 - ptmcg
overlap was added in pyparsing 1.5.6. If you already easy_install'ed 1.5.5, then you should be able to do 'easy_install -U pyparsing'.  '-U' means to install an updated version if one is found.  Once you have done the install, you can confirm you are using it by running python and entering:


    import pyparsing
    print pyparsing.__version__

You should see '1.5.6'.  If you don't then you may have copied pyparsing.py to your local directory, perhaps to make some customizations - rename this local copy to pyparsingX.py, and then retry.  Once you are successfully running 1.5.6, the 'overlap' parameter to scanString should work.

-- Paul
#### 2011-08-09 05:33:47 - natgiot
thanks,
I ended up manually downloading 1.5.6 but then I could not get the script to run (the one above)as CloseMatch is no longer a class of pyparsing, and I could not find it anywhere else (like in pyparsing_py2/3). I copied the new scanString method to the old pyparsing of versions earlier than 1.5.6 and it worked (unless i am doing something really wrong?)
Cheers, 
Natassa
#### 2011-08-09 05:54:31 - ptmcg
Ouch! I didn't realize that I had removed that class in 1.5.6.  You can manually extract the class from your 1.5.5 version, or get it from this example: .  I'll fix this in 1.5.7, so sorry!
-- Paul

---
## 2011-07-28 16:04:29 - espeed - Parsing a Nutch dump file
The CrawlDatum section in the Nutch dumpfile isn't parsing as I would expect. 

The first few headers are parsed properly, then it combines the remaining ones are combined into one long string with the newline characters.



    
    
    PROBLEM SECTION:
    

    CrawlDatum::
    Version: 7
    Status: 33 (fetch_success)
    Fetch time: Wed Jul 27 21:41:28 CDT 2011
    Modified time: Wed Dec 31 18:00:00 CST 1969
    Retries since fetch: 0
    Retry interval: 2592000 seconds (30 days)
    Score: 1.0
    Signature: null
    Metadata: _ngt_: 1311817284979_pst_: success(1), lastModified=0
    
    
        
        OUTPUT:
        
    
    ['Recno::', ' 39', 'URL::', ' ', 'CrawlDatum::', '', 'Version', ':', ' 7', 'Status', ':', ' 33 (fetch_success)', '\nFetch time: Wed Jul 27 22:30:53 CDT 2011\nModified time: Wed Dec 31 18:00:00 CST 1969\nRetries since fetch: 0\nRetry interval: 2592000 seconds (30 days)\nScore: 1.0\nSignature: null\nMetadata: _ngt_: 1311817284979_pst_: success(1), lastModified=0']


#### 2011-07-28 16:09:40 - espeed


    
    
    import re
    from pyparsing import *
    
    def define_header(key):
        header = key + '::' + restOfLine
        return header
    
    recno_header = define_header('Recno')
    url_header = define_header('URL')
    crawldatum_header = define_header('CrawlDatum')
    parsetext_header = define_header('ParseText')
    content_header = define_header('Content')
    
    header = (recno_header |
              url_header |
              crawldatum_header |
              parsetext_header |
              content_header)
    
    other_subheader = Word(alphas) + ':' + restOfLine
    content_subheader = 'Content' + ':' + SkipTo(header)
    
    url_section = url_header
    crawldatum_section = crawldatum_header + \
        ZeroOrMore(other_subheader) + \
        SkipTo(header)
    parsetext_section = parsetext_header + SkipTo(header)
    content_section = content_header + \
        ZeroOrMore(other_subheader) + \
        content_subheader + \
        SkipTo(header)
    
    sections = (url_section |
                crawldatum_section |
                content_section)
    
    record = recno_header + OneOrMore(sections)
    
    def parse():
        data = file('sample.txt').read()    
        for param in record.searchString(data):
            print param.dump()
    
    parse()
    
    


#### 2011-07-28 20:26:37 - ptmcg
Not sure about all the issues with your parser, but some of your other_subheader key names are not just a single Word(alphas), like 'Fetch time', 'Modified time', 'Retries since fetch' and 'Retry interval'.  Try changing other_subheader to:


    other_subheader = OneOrMore(Word(alphas)) + ':' + restOfLine


-- Paul

---
## 2011-07-29 04:12:31 - gotcha1 - Methodology to write\debug BNF
Hi,

I'm converting bnf into pyparsing constructs.
Is there any methodolody to do this?
How to debug incomplete description?

Thanks a lot.

#### 2011-07-29 07:42:39 - ptmcg
I think there might be a comment or two in 'Getting Started with Pyparsing', but in general:
- start bottom up - most BNF's work from the outside in, defining the largest construct first, composed of its next level down pieces, and then in turn those pieces, and so on down to the terminals; because pyparsing builds up the parser using Python expressions, you have to start with the terminals, then assemble them into next level etc.
- use pyparsing idioms, not BNF - BNF typically defines repetitive elements using a recursive definition, like: list_of_items :: item list_of_items | item.  In pyparsing do list_of_items = OneOrMore(item).  Optional elements are usually given as optional_number :: integer | empty; in pyparsing, using Optional: optional_number = Optional(integer).
- as you build up your parser, you can validate subsets using '==':  assert('6.02e23' == real_number); also you can use setDebug() to get parse-time logging of the parse cycle for a specific element: when/where the parse is going to be tried, and the success failure of the parse

That's pretty quick and high-level, but should move you along.

-- Paul

---
## 2011-07-30 06:36:53 - gotcha1 - Uncertainty with cppStyleComment
Hi,

t = ''' this one is a'//fake'
        same as '/*me*/'
    '''
print cppStyleComment.searchString(t)
<ul class="quotelist"><ul class="quotelist"><li>[['//fake''], ['/*me*/']]</li></ul></ul>
Could be fixed by
pattern = cppStyleComment
pattern.ignore(QuotedString(quoteChar='''))

Maybe such behavior has a special purpose

#### 2011-08-02 09:28:34 - ptmcg
These expressions get defined independently, as there may be a variety of conditions and contexts where they could occur.  cppStyleComment is defined to be just the comment structure itself, but likely to be used as the argument to some other expression's ignore() function:



    rvalue = Word(nums) | quotedString
    assignment = Word(alphas)('lvalue') + '=' + rvalue('rvalue')
    assignment.ignore(cppStyleComment)
    
    test = 'a = '/* a comment in quotes */' // an end of line comment'
    
    print assignment.parseString(test).dump()

prints:


    ['a', '=', ''/* a comment in quotes */'']
    - lvalue: a
    - rvalue: '/* a comment in quotes */'

(Note that the end-of-line comment was suppressed.)

You could just as easily do special handling with htmlStyleComment or javaStyleComment - the expressions themselves can't do their own escaping within quoted strings, you have to do that in the larger parser that uses them both.

If you wanted to extract just the comments, but to avoid quoted strings, you could scan for (quotedString.suppress() | cppStyleComment).  This way, your expression looks for and matches quoted strings first, including any comments they might contain (and discards them due to the suppress()), leaving just unquoted comments to match the cppStyleComment expression.

-- Paul

---
## 2011-08-03 23:27:20 - BrenBarn - Generalized escape mechanism?
Is there any general way to say 'parse this chunk as normal, but allow such-and-such characters to be escaped in such-and-such way'?

I'm trying to write a grammar that includes things like quoted strings, but I want the contents of the string to be parsed according to the grammar as well.  Quote characters might occur in the quoted text, so they'd need to be escaped.  But they don't need to be escaped when outside the quoted string.

Is the only solution to create duplicate versions of all my parsing rules, one that includes the escape information, and one that doesn't?  That's obviously going to be a real pain.  It would be nice if things like character escapes could be modularly specified as part of a parsing rule.  Something like



    myRule = pyp.Group(<whatever>) + someRule + pyp.Suppress(''') + someOtherRule.escape(''', '\\') + pyp.Suppress(''')


... which would apply someOtherRule, but allow ' characters to be escaped with \\ within that rule application.

#### 2011-08-05 12:16:11 - ptmcg
This really puzzles me - I don't understand the purpose of the quotes, if the quoted body is also supposed to be parsed.  If the quotes are like enclosing braces, just define a recursive definition of a statement as something like:


    statement = Forward()
    statement << (if_stmnt | assignment | while_stmnt | for_stmnt | ''' + statement + ''')

-- Paul
#### 2011-08-05 16:51:49 - BrenBarn
I'm trying to create a little document format in which tokens may consist of raw text or commands that will later be processed.  The quotes are necessary to delimit bits of text that may be passed as arguments to these commands, but the arguments might themselves contain other commands with their own arguments.

The quoted content can contain arbitrary text, and may be a sequence of arbitrary text tokens and command tokens, so there's no clear way for me to make a 'statement' token like in your example.  Instead I want to say 'consume everything up to the matching quote, but ignore escaped quotes'.  This is sort of like nestedExpr, I guess, except that I want to specify an escape character instead of an ignoreExpr.

The key thing is that I want to able to specify the escape character irrespective of the type of token that's being parsed.  I might have my token stream sometimes delimited by quotes, sometimes by parentheses, etc., so I'd prefer to be able to 'add in' the information about how to escape delimiters when I define the enclosing context, not when I defined the expressions that might occur inside delimiters.

---
## 2011-08-04 06:13:04 - pbouda - Python 3 question
I still don't get it: do I have to pass a bytearray or a string to parseString() under Python3?

It seems like parseFile() passes the file's content as bytearray, but when I do the same thing (open a file binary, pass the content to parseString()), I get an error because the whitespaces of the parser are defined as string, not as bytearray:



```
File &quot;/usr/local/lib/python3.1/dist-packages/pyparsing-1.5.6-py3.1.egg/pyparsing.py&quot;, line 789, in preParse
    while loc &lt; instrlen and instring[loc] in wt:
TypeError: 'in &lt;string&gt;' requires string as left operand, not int
```


How do I do it?

#### 2011-08-04 13:31:00 - ptmcg
parseString really wants a string, not a bytearray.  The code in parseFile is a typo, it should open the file with 'r', not 'rb' - I'll fix that in the next release.
#### 2011-08-05 05:06:14 - pbouda
Thanks for the answer, this helps.

It's still an interesting case, as I am working on a port of pydot to Python 3. The charset of a file is written down within the file, and is one thing I want to parse out.

Under Python 2, everything was a 'str' and I could pass the file's content (opened via 'rb') to the parser. Under Python 3, I now have to 'decode(charset)' first, but I don't know the charset before I parse. Does somebody have a recommendation for that case? I can imagine that this is often the case, that some charset is written in a file and is one of the things that should be parsed out.

---
## 2011-08-05 20:45:33 - BrenBarn - Set attributes on ParseResults
Is there a way to set arbitrary attributes with specified values on ParseResults objects as part of the parse-rule definition?  It can be done with setParseAction, but this is only for the case of creating an attribute whose key is the result name and whose value is a ParseResult object.

I have cases where there are 'shortcut' commands that should be handled like a more general command type.  I'd like to be able to route them along in the parsing.  A simple example would be:

    groupOf = (Literal('GroupOf') + '(' + Word(nums).setResultsName('groupSize') + ')').setParseAction(handleGroup)
    pair = 'Pair'.setVar('groupSize', 2).setParseAction(handleGroup)

The idea is that I want to make Pair an 'alias' for GroupOf(2), and use the same parseAction for both.  To do that, I need to set the required groupSize variable from within the pair rule, but I want to set it to a value (namely, 2) that's not represented by any token within that parse rule.

#### 2011-08-05 21:47:34 - ptmcg
You can assign multiple parse actions in setParseAction, so how about defining setVar as:


    def setVar(varname, varvalue):
        def parseAction(tokens):
            tokens[varname] = varvalue
        return parseAction
    pair = Literal('Pair').setParseAction(setVar('groupSize',2), handleGroup)


You can also use addParseAction to add another parse action to the chain of actions on an expression.

Also, `expr.setResultsName('abc')` can be more clearly written simply as `expr('abc')`.  `Word(nums)('groupSize')` is equivalent to `Word(nums).setResultsName('groupSize')`.
#### 2011-08-06 01:16:46 - BrenBarn
That works great, thanks.

---
## 2011-08-06 01:30:50 - BrenBarn - parseActions and ParseResults attributes
I have a parse rule set up and a parseAction is applied to the rule.  Some components of the rule also have result names specified.  Inside my parse action, I can print out the corresponding attributes of the ParseResults object (e.g., toks.myResultName) and see that they are properly set.

My parse action then modifies the token stream and returns it.  But the resulting ParseResults (the final return value of the parse operation) do not have the correct result names set.

Looking in the pyparsing source, I see that the parseAction application code creates a new ParseResults object from the returned token list.  This appears to erase the set resultsNames.  Am I missing something?  This seems like a huge problem, since it means parseActions that modify the token stream lose all the resultName information.  This code snippet shows the problem.



    import pyparsing as pyp
    
    def myAction(string, loc, toks):
        return toks[-1:1]
    
    rule = (pyp.Word(pyp.alphas)('foo') + pyp.Suppress('{') + pyp.Word(pyp.alphas)('bar') + pyp.Suppress('}') + pyp.Word(pyp.alphas)('baz'))
    
    x = rule.parseString('one{two}three')
    print '*'.join([repr(a) for a in [x.foo, x.bar, x.baz]])
    
    rule = (pyp.Word(pyp.alphas)('foo') + pyp.Suppress('{') + pyp.Word(pyp.alphas)('bar') + pyp.Suppress('}') + pyp.Word(pyp.alphas)('baz')).setParseAction(myAction)
    
    x = rule.parseString('one{two}three')
    print '*'.join([repr(a) for a in [x.foo, x.bar, x.baz]])


The output is


    'one'*'two'*'three'
    ''*''*''


The two rules are the same except that one applies a parseAction, but that modification causes all the resultNames to disappear from the parse result.

#### 2011-08-06 05:57:34 - ptmcg
If you want to preserve the results names, just modify the provided toks parameter, don't return something new.


    del toks[-1]
    del toks[0]


#### 2011-08-06 05:59:01 - ptmcg
Or toks[:]=toks[-1:1] should work too. (Didn't notice that you were reversing the list of tokens.)
#### 2011-08-06 17:01:23 - BrenBarn
Ah, okay, thanks.  Is this documented?  The 'how to use pyparsing' page I was looking at gave me the impression that only the return value of the parseAction was relevant; I didn't realize that mutating the token list was part of the API.
#### 2011-08-06 20:27:29 - ptmcg
No it isn't documented, I need to update that part of the parse action description.

---
## 2011-08-06 07:12:08 - vadeskoc - _trim_arity trickiness
Hello,

I'd like to voice some concern over the new `_trim_arity` decorator included in recent versions of pyparsing.

Basically, if you have a parseAction that ends up throwing a TypeError for whatever reason, the `_trim_arity` decorator will intercept it, assume it has to do with the number of arguments to the parse action (even if it doesn't), and then try a different length argument list, which ultimately results in a *different* TypeError that makes no sense.  

Pyparsing is already a pretty complicated beast, and it just doesn't feel right that it should be intercepting my real exceptions and replacing them with bogus ones.  And it's very disorienting to see an exception having to do with the arguments to a function that depends on what is in the *body* of that function.

What was wrong with the original mechanism?  This one seems maybe too tricky for its own good.

Thanks!
Dave

#### 2011-08-14 17:16:39 - ptmcg
Dave -

This issue of wrapping the parse action to adapt to the number of parameters to be passed is a tricky one, with many variants for the parse action decorator detector (which I will refer to from now on as the PADD) to have to detect and adapt to:
- function
- member function (extra leading 'self' argument)
- class function (extra leading 'cls' argument)
- class name (find `__init__` function and wrap it)
- function with *args arguments
- function with arguments with default values

The old code was trying to pick apart all of these variations using version-dependent code introspection functions. There were version sensitivities even among 2.x versions, and then further work ahead for 3.x.

`_trim_arity` was submitted by Raymond Hettinger in response to some issues he was having with a gap in the logic of the original code.  (He was trying to invoke a class function `__new__` with *args arguments, a combination of two other cases.) Rather than add one more special case to fill this gap in the logic, Raymond felt (and I already felt this too) that the PADD was inherently fragile in its implementation, trying to cover a variety of special cases, and worse, combinations of special cases. And that it might be simpler to let Python itself do the hard work - just try making the calls until you get the one that works, and then use that one from then on.  The result was a much smaller method, without lots of special case logic, which tends to indicate a net improvement in the long-term robustness of the code.

Unfortunately, this method relies on using Python's exception mechanism for trapping TypeError's raised by the early trial-and-error code. If the code itself raises exceptions (especially TypeError), this can confuse the PADD arity-detection logic.

I too have run into some of these exception masking issues in my own work, and I'll look at what I can do to make trim_arity a bit smarter about what it does with exceptions. I won't be shy about bringing Raymond back into the discussion, either.

Thanks,
-- Paul

_[ED: This issue was incrementally resolved in pyparsing versions 2.1.0, 2.1.2, 2.1.3, and 2.1.8, and has been
stable since that version.]_

#### 2011-08-26 07:50:17 - vadeskoc
Wow, this is starting to drive me nuts.  It's like pyparsing is my new neurotic programming partner, constantly injecting useless/misleading information and obfuscating things while I try to debug.

As a compromise, would it be possible to at least contain the damage and shortcut the TypeError trick if the primary arity is fixed and knowable via inspect?  Personally, I'm almost always using lambdas for parse actions anyways.  This is obviously not a full solution, but a contract of 'pyparsing-won't-act-tricky-as-long-as-you-don't' would be an improvement over the current situation.

Or maybe there's a hack-on-top-of-a-hack that could be done by inspecting the traceback depth to see if it is the exception that _trim_arity thinks it is?  Would that just be digging a deeper hole?

Thanks!
Dave
#### 2011-08-26 15:05:27 - BrenBarn
I've been looking at this as well.  Is the purpose of the _trim_arity call just to allow parseActions to accept different argument signatures (i.e., just the token list versus the string/loc/token triple)?  I don't understand why it's even a good idea to do this at all.  Wouldn't the normal approach simply be to say 'Parse actions must accept three arguments: string, locations, and token list.  That is the API.'  Why bother creating these extra things to support different argument signatures?  Just define a consistent API and keep to it.
#### 2011-08-31 18:18:55 - vadeskoc
BrenBarn: I agree with you wholeheartedly on this one.  The api around the parse actions is quite confusing in general.

But whether or not Paul agrees (and it's certainly his right not too!), I can see why he might not want to change things at this point, for compatibility's sake.  

I guess that's why I'm wondering if we can at least have the option to avoid going to crazy-town in the first place, e.g. if we present _trim_arity with a plain, known/knowable arity function.  That way we could always choose to write a lambda that gets from arity A to arity B, and always be guaranteed to get our exceptions back out unmolested.

(in the meantime, I've rolled back to 1.5.5, and the difference is night and day!)

---
## 2011-08-08 12:47:54 - BrenBarn - Transformations on parse expressions
Sorry to be so persistent, but I've been fiddling with pyparsing this weekend and have another question.

I've been trying to tweak my grammar so that it produces more helpful exceptions for invalid input.  I looked at this old post:  .  I'm having more or less the same issues as described there --- in particular, the unresolved issue mentioned in the last post on that thread.  Using a special 'blocker' token doesn't work when the expression containing it might be nested, since then the blocker will raise an exception even when it would be valid to end the containing block.

I've found a way that seems to get around this.  I look at which expressions seem to be causing the error to appear in the wrong place.  Usually there is one quite permissive expression (involving CharsNotIn or some such) that is the culprit.  Then I make a 'strict' version of this expression, in which all +s are changed to -s; this forces the expression to match 'one step at a time' and fail as soon as a step fails.  Then, I include this strict version as an extra matchFirst choice at the end of my alternatives, similar to the way described in that post.  (I.e., I wind up with something like 'OneOrMore(foo | bar | baz | strictFoo)'.)

My question is, is there a way to automate the creation of a strict version of a parse expression?  Currently I have to copy and paste the expression and explicitly change + to -.  But it would be cool if I could just do myExpressions.strictVersion().  I tried to write a function that does this programmatically by inserting _ErrorStop inside Ands, but even when using .copy() to get a copy of the original parseExpression, it seemed to be mutating that object (probably because the copy is only shallow).  I don't want to recurse through all parse expressions, but I need to do some recursion because multi-part Ands are recursive rather than flat.

Is there a way to do 'transformations' on parse expressions?  That is, is there a safe way to make a copy of an expression and modify its tree of ParserElements, while leaving the original expression intact?

Thanks.

#### 2011-08-08 13:58:32 - ptmcg
I think that poster (or another with a similar complaint) addressed this using:


    allAlternativesDescription = NoMatch().setName('not foo nor bar nor baz')
    OneOrMore(allAlternativesDescription | foo | bar | baz)

This should not raise any additional exceptions than the one that would already be raised by the absences of foo, bar, or baz's, but gives a more complete description of what was expected.

If you have an And built up with '+', you can do a function something like this (untested):


    def makeStrict(andexpr):
        ret = andexpr.copy()
        strictExprs = sum(zip(andexpr.exprs,[_ErrorStop()]*len(andexpr.exprs), ())[:-1]
        ret.exprs[:] = list(strictExprs[:-1])
        return ret


But before you go the strictAnd route, please take one more shot at the leading NoMatch on the list of alternatives in a MatchFirst or Or expression.

-- Paul

---
## 2011-08-09 08:42:50 - almoni - Different Instances Of The Same Rule
Hello.

I want to use the same rule more than once, and assign a different function to each instance. For example:

    rule = Word(alphas)
    
    instance_1 = rule.setParseAction(function_1)
    instance_2 = rule.setParseAction(function_2)

However, setParseAction associates the function with the rule and not with the instance, so only function_2 is called. Some solutions:

    instance_1 = (rule + Empty()).setParseAction(function_1)
    instance_2 = (rule + Empty()).setParseAction(function_2)
    
    instance_1 = Group(rule).setParseAction(function_1)
    instance_2 = Group(rule).setParseAction(function_2)

and in the function, set tok = tok[0]. But they are not elegant, and I would like to know if there is a simple way to do this.

Thank you.

#### 2011-08-09 13:22:22 - ptmcg

What you are calling 'rule' is also an instance, an instance of Word, and instance1 and instance2 are just names pointing the same Python value.  (This is a Python thing, not a pyparsing thing.) The easiest way to achieve what you want is to copy rule when you assign it to an instance:


    instance_1 = rule.copy().setParseAction(function_1)
    instance_2 = rule.copy().setParseAction(function_2)


If you like the idea 'rule' being a sort of factory, then define rule as:


    rule = Word(alphas).copy
    instance_1 = rule().setParseAction(function_1)
    instance_2 = rule().setParseAction(function_2)


-- Paul
#### 2011-08-10 00:15:05 - almoni
Thanks.

---
## 2011-08-10 02:51:46 - almoni - Exact Matches as Default
Hello.

I want matches to be exact. For example:

    rule = Word(alphas)
    result_1 = rule.parseString('word')  # good
    result_2 = rule.parseString('word!') # bad

However, result_2 returns 'word' instead of failing. A solution:

    rule = Word(alphas) + StringEnd()

But I have lots of rules, and adding StringEnd() to each of them clutters the code, so I would like to know if there is a better way to force exact matches as a default.

Thank you.

#### 2011-08-10 11:18:38 - BrenBarn
You can pass parseAll=True to the parseString method to make it attempt to match the entire input string against the rule.

    rule.parseString('word!', parseAll=True)
#### 2011-08-17 22:51:07 - almoni
Thanks.

---
## 2011-08-12 02:51:12 - kumaranar - Parsing Nested function calls in expression.
Hi,

  Thanks for the great module.

I am trying to parse a informatica expression with pyparsing and pickup only the identifiers, i want to ignore the literals and the function names. I was able to get the code working for plain expressions but when an argument of a function call is by itself another function call the program hangs forever. I am not sure what i am missing any help would be greatly appreciated . 

For simplicity i have just given 3 test in my code. 1st test gives me the output correctly and the 2nd test is the one which runs forever. test#3 was the one which i am actually interested in. I have made the test1 and test2 so that it is easily understandable by all.



    from pyparsing import *
    
    identstack = []
    def pushFirst(tokens):
        identstack.insert(0,tokens[0])
    
    LPAR,RPAR,COMMA,PERIOD = map(Suppress,'(),.')
    # define grammar
    identifier = Word(alphas, alphanums+'._')
    function_name =  Word(alphas, alphanums+'_')
    parameter_name = '$$'+Word(alphas, alphanums+'_')
    integer = Regex(r'[+-]?\d+')
    numeric_literal = Regex(r'\d+(\.\d*)?([eE][+-]?\d+)?')
    string_literal = QuotedString(''')
    literal_value = ( numeric_literal | string_literal )
    expr = Forward()
    function_call=( function_name.suppress() + LPAR + Optional(delimitedList(expr)) + RPAR )
    #function_call=function_call+StringEnd()
    atom = literal_value |function_call|identifier.setParseAction(pushFirst)
    UNARY,BINARY,TERNARY=1,2,3
    expr << ZeroOrMore( operatorPrecedence(atom,
        [
        (oneOf('- + '),  UNARY, opAssoc.LEFT),
        ('||', BINARY, opAssoc.LEFT),
        (oneOf('* / %'), BINARY, opAssoc.LEFT),
        (oneOf('+ -'), BINARY, opAssoc.LEFT),
        (oneOf('< <= > >='), BINARY, opAssoc.LEFT),
        (oneOf('= == != <>') , BINARY, opAssoc.LEFT),
        (('BETWEEN','AND'), TERNARY, opAssoc.LEFT),
        ]))
    bnf=expr
    pattern =  bnf + StringEnd()
    
    def test(s):
        del identstack[:]
        bnf.parseString(s,parseAll=True)
        print identstack
    
    test('SIN(C+2)+A+B')
    test('SIN(COS(C)+2)+A+B')
    test('IIF(INTERNAL_EXTERNAL_FLAG='INT', 'Y', IIF(INTERNAL_EXTERNAL_FLAG='EXT', 'N', NULL))')



#### 2011-08-12 07:55:06 - ptmcg
1. Remove ZeroOrMore in your definition of expr. I don't think you want an empty string to match expr.
2. Leading unary +/- should be RIGHT associative, you currently have it LEFT associative.
3. I tried your program using less complex hierarchy of operators and things worked okay, but then got bad as I added more levels.  This is usually a sign that you need to enable packrat parsing. Do this by adding 'ParserElement.enablePackrat()' right after importing pyparsing.
4. No need to add StringEnd() to the end of pattern, since you are calling parseString with parseAll=True. In fact, pattern isn't even used any more, probably just remove it.

Interesting problem, good luck with it!
-- Paul
#### 2011-08-13 05:19:33 - kumaranar
Thanks Paul.

Your suggestions solved the issue.

Thanks,
Kumaran.
#### 2011-08-14 13:46:56 - kumaranar
Hi,

  Thanks for the help in the previous post. I wrote the script and ran a test against some real programs. Most of the expressions were parsed successfully. But I have some issues with the expressions of the types one in the example below. The expression is a valid one according to the grammar but the script errors out saying maximum recursion depth exceeded.



    from pyparsing import *
    ParserElement.enablePackrat()
    
    class ParseInfa:
    
        identstack = []
        literalstack=[]
        constantstack=[]
        paramstack=[]
        unqidentstack = []
        unqliteralstack=[]
        unqconstantstack=[]
        unqparamstack=[]
    
        def pushIdent(self,tokens):
            self.identstack.insert(0,tokens[0])
    
        def pushLiteral(self,tokens):
            self.literalstack.insert(0,tokens[0])
    
        def pushConstant(self,tokens):
            self.constantstack.insert(0,tokens[0])
    
        def pushParams(self,tokens):
            self.paramstack.insert(0,tokens[0])
    
        def parseExpr(self,expression):
    
    
            del self.identstack[:]
            del self.literalstack[:]
            del self.constantstack[:]
            del self.paramstack[:]
    
            LPAR,RPAR,COMMA,PERIOD = map(Suppress,'(),.')
            # define grammar
            (AND, DD_DELETE, DD_INSERT, DD_REJECT, DD_UPDATE,FALSE, NOT, NULL, OR, PROC_RESULT, SESSSTARTTIME, 
             SPOUTPUT, SYSDATE, TRUE, WORKFLOWSTARTTIME) =  map(CaselessKeyword, '''AND, DD_DELETE, DD_INSERT, 
             DD_REJECT, DD_UPDATE, FALSE, NOT, NULL, OR, PROC_RESULT, SESSSTARTTIME, SPOUTPUT, SYSDATE, TRUE, 
             WORKFLOWSTARTTIME'''.replace(',','').split())
            #keywords = MatchFirst( DD_DELETE, DD_INSERT, DD_REJECT, DD_UPDATE,FALSE,  NULL,  PROC_RESULT, SESSSTARTTIME, SPOUTPUT, SYSDATE, TRUE, WORKFLOWSTARTTIME)
            infa_constants = NULL|SPOUTPUT|FALSE|TRUE|SESSSTARTTIME|SYSDATE|WORKFLOWSTARTTIME
    
            identifier = Word(alphas, alphanums+'._')
            function_name =  Word(alphas, alphanums+'_')
            parameter_name = Combine('$$'+identifier)
            lookup_name = Combine(':'+identifier)
            integer = Regex(r'[+-]?\d+')
            numeric_literal = Regex(r'\d+(\.\d*)?([eE][+-]?\d+)?')
            string_literal = QuotedString(''')
            literal_value = ( numeric_literal | string_literal )
            expr = Forward()
            function_call=( function_name + LPAR + Optional(delimitedList(expr)) + RPAR )
            informatica_lkps=(lookup_name+LPAR+Optional(delimitedList(expr)+RPAR))
    
            atom = (literal_value.setParseAction(self.pushLiteral) |function_call|
                    infa_constants.setParseAction(self.pushLiteral)
                    |parameter_name.setParseAction(self.pushParams)|informatica_lkps
                    |identifier.setParseAction(self.pushIdent)
                    )
    
            UNARY,BINARY,TERNARY=1,2,3
            expr <<  operatorPrecedence(atom,
                [
                (oneOf('- + '),  UNARY, opAssoc.RIGHT),
                ('||', BINARY, opAssoc.LEFT),
                (oneOf('* / %'), BINARY, opAssoc.LEFT),
                (oneOf('+ -'), BINARY, opAssoc.LEFT),
                (oneOf('< <= > >='), BINARY, opAssoc.LEFT),
                (oneOf('= == != <>') , BINARY, opAssoc.LEFT),
                (AND|OR,BINARY,opAssoc.LEFT),
                (('BETWEEN','AND'), TERNARY, opAssoc.LEFT),
                ])
            bnf= Forward()
            bnf<<expr
            bnf.parseString(expression,parseAll=True)
            self.unqidentstack=list(set(self.identstack))
            self.unqliteralstack=list(set(self.literalstack))
            self.unqconstantstack=list(set(self.constantstack))
            self.unqparamstack=list(set(self.paramstack))
            #print self.unqidentstack 
    
        def __init__(self):
            pass
    
        def getParams(self):
            return self.unqparamstack
    
        def getIdents(self):
            return self.unqidentstack
    
        def getConst(self):
            return(self.unqconstantstack)
    
        def getLiteral(self):
            return(self.unqliteralstack)
    
    
    classhandle=ParseInfa()
    #classhandle.parseExpr('IIF(UPDATE_FLG = 'I' or UPDATE_FLG='B', SESSSTARTTIME, LKP_W_INSERT_DT)')
    classhandle.parseExpr('IIF(ISNULL(SOURCE_CODE),'X',IIF(ISNULL(LKP_SOURCE_CODE),'N',IIF(SOURCE_NAME_1 != LKP_SOURCE_NAME_1, 'U',IIF(SOURCE_NAME_2 != LKP_SOURCE_NAME_2, 'U',IIF(LKP_MASTER_DATASOURCE_NUM_ID != LKP_MASTER_DATASOURCE_NUM_ID, 'U',IIF(MASTER_CODE  != LKP_MASTER_CODE, 'U',IIF(MASTER_VALUE  != LKP_MASTER_VALUE, 'U',IIF(TENANT_ID  != LKP_TENANT_ID, 'U','X'))))))))')
    print classhandle.getIdents()
    print classhandle.getParams()
    print classhandle.getLiteral()


any pointers or directions should be of great help.
#### 2011-08-14 16:54:31 - ptmcg
The answer is, 'there's a whole lotta recursion going on!'

Again, I commented out a few of the levels of operators in the call to operatorPrecedence (the only one in this expression is '!=') and things parsed just fine.

Python sets the maximum recursion limit to 1000 just to keep things from running away.  But sometimes you have to go really deep.  In your example, there is an 8-level deep nested IIF expression, so already there is heavily recursive data.

Just before calling parseExpr, add this code:


    import sys
    sys.setrecursionlimit(2000)


I just picked the number 2000, and it is concerning that this number will be purely subject to the complexity of the input string (through trial-and-error, I found that you have to set this value to 1256 in order to parse this particular expression).  If there truly were a runaway recursion happening, then it wouldn't matter what I set this number to, your program would blow up. So the issue is not the grammar, your data is just nested really deep.

Python's use of 1000 is just an arbitary value.  If this expression is typical of the ones you'll need to parse, maybe pick 2000 or 5000 or 10000.  It doesn't really matter much what the value is, as long as there is *some* limit.  If you hit this again, try retracing my steps (simplify/minimize the opPrec operator definition, increase Python recursion limit). If THAT doesn't work, write back, we'll try something else.

-- Paul

---
## 2011-08-18 01:21:25 - BrenBarn - pyparsing hiding exceptions
The recent post on `_trim_arity` mentioned this issue of pyparsing gobbling up common exception classes, assuming it knows what they're from, and then masking them.  I've encountered this a few times, but I just ran into a really irritating instance of it that took quite a while to track down.  I had created a custom class that was created as part of a parseAction, and the `__repr__` of that class contained a bug that cuased it to raise an IndexError.

The result of this was that my pyparsing code failed with ParseException 'reasons' (i.e., 'error on line X, column Y') only when debugging was enabled.  This really just doesn't seem right.

There is various code in pyparsing that blithely captures IndexErrors and replaces them with ParseExceptions.  It seems to me like most of this code is too high up the stack.  If an IndexError is raised in the nitty-gritty of the parsing operations (e.g., reading beyond the end of the input string), shouldn't those cases be caught in a very local try/except?

At the very least, it seems like parseActions need to be called in a context that does not catch common exceptions like IndexError.  parseActions can contain arbitrary user code, and assuming that any IndexError raised when a parseAction is called is due to a simple parsing blockage that needs a backtrack is unwarranted.

Is there any way to guard against this?  Basically it seems like pyparsing is rather cavalier about exception handling, and since my parsing-related code is mediated by pyparsing, I'm running into annoying problems where pyparsing hides errors in my own code.

#### 2011-08-22 12:42:38 - ptmcg
Well, I'll take another look at this for 1.5.7, and I'll ask Raymond Hettinger for his advice on this too. I too have run into some exception confusion with the new parse action decorating behavior and it would be nice to have a little more clarity on just what happens where, and why.

Thanks for your patience, and for keeping up the attention on this point.

-- Paul
#### 2011-08-22 21:04:43 - BrenBarn
Thanks for taking a look at it.  The stuff I'm talking about is actually just in the core of pyparsing.  I'm not exactly sure how the IndexErrors arise.

From some comments in the code, I gather that certain parse elements advance the position in the stream, and the idea is that this might advance it too far.  Is the problem with the various reads of instring[...]?  I was looking at these and it looks like most of them are already protected with an explicit bounds check, or are slices, which won't IndexError anyway.  If the few remaining cases are the sources of IndexErrors, they can just be wrapped with a local try/except and the exception can be replaced there with a ParseException.
#### 2011-08-22 22:35:54 - ptmcg
Yes, I was trying to avoid making too many calls to len(instring) if possible, in my efforts to improve parsing performance. So I would just index through the input string and let IndexError signal up the stack that we had run off the end of the input string. Then I found that many of the expressions already were defensive this way, and would never raise IndexError; these classes have mayIndexError set to False. It is probably time to refactor this behavior too, since I can be smarter about slicing that can safely go beyond the string bounds, or call len(instring) if necessary, or as you suggest, catch IndexError locally and reraise as ParseException.

---
## 2011-08-20 04:14:22 - jfosorio - setResultsName and setParseAction
Hello everbody, 
I have a problem while trying the examples that are proposed in the O'Reilly 'Getting Started with Pyparsing'
I could not find the particular example in pages 18-21 that is related with school scores. 
The issue appears when a Token of the grammar is defined as:

    schoolName.setResultsName('school')

if later i want to use setParseAction method as: 

    schoolName.setParseAction(...) 

the method is not executed when the text is being parsed. 
I prepare an example (Code below) that include three different grammar definitions that in my concept should give the same list at the end. 
Grammar=1 do it correctly but Grammar=2,3 don't (the join command never executes). 
Could you help me to understand why is it like that?

regards 

Juan osorio


    #!/usr/bin/python 
    
    import pyparsing 
    import random
    import time
    from pyparsing import *
    import os
    
    
    ''' 
    Greetings example
    ''' 
    '''
    BNF (Backus--Naur Form) Grammar
    gameresult ::= date schoolresult schoolresult
    schoolresult ::= schoolname score
    date ::=  digit+ '/' digit+ '/' digit+
    schoolname ::= (alpha+)+
    score ::= digit+
    digit ::= '0' ..'9'
    alpha ::= 'A'..'Z' 'a'..'z'
    '''
    
    Grammar = 2
    
    # Grammar
    if Grammar == 1 :
        num = Word(nums)
        date = Combine(num + '/' + num  + '/' + num)
        schoolName = OneOrMore(Word(alphas))
        score = Word(nums)
        schoolResult  = schoolName + score
        gameResult = date + schoolResult + schoolResult
    
    # old grammar with namesif Grammar == 2 :
        num = Word(nums)
        date = Combine(num + '/' + num  + '/' + num)
        schoolName = OneOrMore(Word(alphas))
        score = Word(nums)
        schoolResult  = schoolName.setResultsName('school') + score.setResultsName('score')
        gameResult = date.setResultsName('date') + schoolResult.setResultsName('team1')+ \
                     schoolResult.setResultsName('team2')
    
    # new grammar with namesif Grammar == 3 :
        num = Word(nums)
        date = Combine(num + '/' + num  + '/' + num)
        schoolName = OneOrMore(Word(alphas))
        score = Word(nums)
        schoolResult  = schoolName('school') + score('score')
        gameResult = date('date') + schoolResult('team1')+ schoolResult('team2')
    
    
    # In parsing actionsschoolName.setParseAction( lambda tokens: ' '.join(tokens) )
    
    # Validate timedef validateDateString(tokens):
        try:
            time.strptime(tokens[0], '%m/%d/%Y')
        except ValueError,ve:
            raise ParseException('Invalid date string (%s)' % tokens[0])
    
    date.setParseAction(validateDateString)
    score.setParseAction(lambda tokens : int(tokens[0]))
    
    tests = '''\
        12/29/2004 Virginia      44    Temple             14
        09/04/2004 LSU          22     Oregon State         21
        09/09/2004 Troy State      24     Missouri            14
        01/02/2003 Florida State 103     University of Miami 2  '''.splitlines()
    '''
    Combine only work for adjecent Tokens
    if there are embedeed spaces it does not work
    '''
    
    for test in tests:
        stats = gameResult.parseString(test)
        # print str(type(stats.team2.score))
        print stats.asList()
    # print gameResult
    '''    
        if stats.team1.score != stats.team2.score: 
        if stats.team1.score > stats.team2.score:
            result = 'won by ' + stats.team1.school
        else:
            result = 'won by ' + stats.team2.school
        else:
            result = 'tied'
        print(stats.date + stats.team1.school + str(stats.team1.score) + \
             stats.team2.school + str(stats.team2.score) + result) 
    '''
    


#### 2011-08-20 04:15:57 - jfosorio


    #!/usr/bin/python 
    
    import pyparsing 
    import random
    import time
    from pyparsing import *
    import os
    
    
    ''' 
    Greetings example
    ''' 
    '''
    BNF (Backus--Naur Form) Grammar
    gameresult ::= date schoolresult schoolresult
    schoolresult ::= schoolname score
    date ::= digit+ '/' digit+ '/' digit+
    schoolname ::= (alpha+)+
    score ::= digit+
    digit ::= '0' ..'9'
    alpha ::= 'A'..'Z' 'a'..'z'
    '''
    
    Grammar = 2
    
    Grammar
    
    if Grammar == 1 :
    num = Word(nums)
    date = Combine(num + '/' + num + '/' + num)
    schoolName = OneOrMore(Word(alphas))
    score = Word(nums)
    schoolResult = schoolName + score
    gameResult = date + schoolResult + schoolResult
    
    old grammar with names
    if Grammar == 2 :
    num = Word(nums)
    date = Combine(num + '/' + num + '/' + num)
    schoolName = OneOrMore(Word(alphas))
    score = Word(nums)
    schoolResult = schoolName.setResultsName('school') + score.setResultsName('score')
    gameResult = date.setResultsName('date') + schoolResult.setResultsName('team1')+ \
    schoolResult.setResultsName('team2')
    
    new grammar with names
    if Grammar == 3 :
    num = Word(nums)
    date = Combine(num + '/' + num + '/' + num)
    schoolName = OneOrMore(Word(alphas))
    score = Word(nums)
    schoolResult = schoolName('school') + score('score')
    gameResult = date('date') + schoolResult('team1')+ schoolResult('team2')
    
    
    In parsing actions
    schoolName.setParseAction( lambda tokens: ' '.join(tokens) )
    
    Validate time
    def validateDateString(tokens):
    try:
    time.strptime(tokens[0], '%m/%d/%Y')
    except ValueError,ve:
    raise ParseException('Invalid date string (%s)' % tokens[0])
    
    date.setParseAction(validateDateString)
    score.setParseAction(lambda tokens : int(tokens[0]))
    
    tests = '''\
    12/29/2004 Virginia 44 Temple 14
    09/04/2004 LSU 22 Oregon State 21
    09/09/2004 Troy State 24 Missouri 14
    01/02/2003 Florida State 103 University of Miami 2 '''.splitlines()
    '''
    Combine only work for adjecent Tokens
    if there are embedeed spaces it does not work
    '''
    
    for test in tests:
    stats = gameResult.parseString(test)
    # print str(type(stats.team2.score))
    print stats.asList()
    print gameResult
    
    ''' 
    if stats.team1.score != stats.team2.score: 
    if stats.team1.score > stats.team2.score:
    result = 'won by ' + stats.team1.school
    else:
    result = 'won by ' + stats.team2.school
    else:
    result = 'tied'
    print(stats.date + stats.team1.school + str(stats.team1.score) + \
    stats.team2.school + str(stats.team2.score) + result) 
    '''
    


#### 2011-08-21 19:11:26 - ptmcg
Juan -

There are several things going on here, but the most relevant is that you are assigning a parse action to the schoolName expression *after* schoolName has been used in the schoolResult expression using a results name.  As described in this thread (), assigning a results name also creates a copy of the expression (so that the same expression can be used multiple times with different results names).  For your parse actions to work, you have to move the call to setParseAction higher up in the script, before you use the expression with a results name.  Just move your definition of schoolResult and gameResult down in the script, after all expression definitions of schoolName and date are complete.

-- Paul

---
## 2011-08-25 12:14:21 - furrykef - alphas is locale-dependent in Python 2.x
The documentation claims that 'alphas' is 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghikjlmnopqrstuvwxyz'. But in reality this is not the case! At least as of version 1.5.6, on Python 2.x, it constructs it out of alphas.uppercase and alphas.lowercase, which is locale-dependent -- and on my system is full of accented characters! It doesn't make sense to have a programming language whose legal identifiers vary from system to system.

What I really don't understand is that the code uses string.ascii_uppercase and string.ascii_lowercase in Python 3.x. These also exist in Python 2.x, so there is no reason to do it differently depending on Python version. While you're at it, you can use string.ascii_letters instead of concatenating ascii_uppercase and ascii_lowercase, though the result is the same.

#### 2011-08-25 12:28:33 - furrykef
I also just noticed that this errant line actually appears twice -- once before 'def _ustr' and once after. So alphas is assigned to three times: once if _PY3K is true and twice if not. All three should be replaced with a single 'alphas = string.ascii_letters' outside of the conditional.
#### 2011-08-26 06:33:45 - ptmcg
Seems to me that when I conditionalized the reference to ascii_lowercase that I was keeping compatibility with an old Python version, maybe 2.3.  Is string.ascii_lowercase in Python 2.4? I'm still trying to stay compatible with at least that version.

Well this takes me by surprise too, I thought string.ascii_uppercase and ascii_lowercase were just the stated values, not locale-ized. In fact, that's part of the reason I also added alphas8bit.

I haven't had any other complaints about alphas, perhaps as much because pyparsing is used to create many kinds of parsers, not just programming languages.  Just as often, pyparsing is used for parsing personal data files, web pages, or other files that could contain localized data strings.

And thanks for pointing out the replicated definitions, I'll get them cleaned up too.
#### 2011-08-26 19:00:26 - furrykef
The documentation shows that ascii_letters, ascii_lowercase, and ascii_uppercase were added in 2.2: 
#### 2011-08-29 09:53:45 - ptmcg
Well, heck! There was *some* change made in the string module definitions in Py3.0, what was it?
#### 2011-12-24 06:28:15 - furrykef
I just checked the SVN repository and I noticed this still has not been fixed.

I have to be honest with you: when I saw this bug in pyparsing, and how easily it could have been avoided, I was turned off and went back to PLY without investigating pyparsing much further. You know what they say: first impressions count, and I encountered this on my first day with pyparsing. Finding a serious and easily preventable bug on one's first day with a library is not a very good sign; it makes one wonder what else could be wrong. So I'm quite disappointed that my second impression -- that it still has not been fixed, even four months later, despite requiring the change of only three lines -- does not improve things. I don't mean to be mean; I'm just explaining why I'm not inclined to use pyparsing.

Nevertheless, this time I have submitted a patch --  -- but nonetheless I'm quite concerned about the way pyparsing is handled. I don't mean to insult you, of course. It's just that I think things could be handled much better, and I'm surprised that a bug that causes issues as serious as legal identifiers being locale-dependent (a likely result of the bug for many users) not being taken seriously. How many lexers are out there that have this bug as a result of this?

By the way, I did one thing differently in my patch than what I suggested above. I used ascii_lowercase + ascii_uppercase instead of ascii_letters, since the former puts the lowercase letters first (as the code apparently always has) and the latter puts the uppercase letters first. I decided it's better not to change the order, just in case some crazy code out there depends on it...

Oh, by the way, I think I've found the answer to your question when you said, 'There was *some* change made in the string module definitions in Py3.0, what was it?'. The change you were thinking of was probably that the variables 'uppercase', 'lowercase', and 'letters' were removed in Python 3 -- I assume because their names caused bugs exactly like this one.
#### 2011-12-24 09:58:58 - ptmcg
Your suggested patch has been applied and checked into SVN, and will go out in 1.5.7. Thanks for your patience and persistence.

-- Paul

---
## 2011-08-29 07:11:49 - qlppk - can't pase properly
So I have this code:

For parsing files like this:


The thing is when the parser reaches token 'FILE_0', it stops and doesnt parse everything after it in the file.
Why?

The file structure is like this:
KEY=VALUE

for each line.

#### 2011-08-29 09:51:59 - ptmcg
Try this:


    print variables.parseString('FILE_0=imo_cosF_06A.tmb')


I suggest that you might also change text to be just restOfLine, so that you don't run into other unexpected text content characters.

-- Paul
#### 2011-08-29 11:34:08 - qlppk
Hm, I either don't unerstand how that's a solution to my problem or you don't understand my problem.

I need to parse text files which contain KEY=VALUE pair on each line, where KEY can be anything from a predefined list and value is either a number or text string.
It works but when the parser reaches the key 'FILE_0', it doesn't parse the rest of the lines (like with key 'FILE_1'). You can see that by copy-pasting the text in my second link and saving it as a file before running the code on it.
If you understan all this, then I'm sorry but I don't understand the solution.
#### 2011-08-29 11:49:40 - BrenBarn
The line he pointed out shows the problem: your 'text' grammar allows only alphanumerics, underscore, or space.  But that line contains a period, which isn't allowed in 'text'.  Since the text doesn't match, it can only match the variable name.  If your text can contain periods, you need to make the grammar allow that (or use restOfLine like ptmcg said, to allow any text).
#### 2011-08-29 12:23:05 - qlppk
Oh, ok then. I guess I could use 'printables' then.
How to exclude a character (for example if I wanted to exclude newline and = from printables).
#### 2011-08-29 12:45:43 - ptmcg
Newline is not in printables (none of the whitespace is), and why don't you want '='? Once you have passed the first '=', isn't anything after that desired?
#### 2011-08-29 12:57:25 - qlppk
I don't know, I assumed that if printables included space and newline then = in VALUEs could accidetaly split the key-value pair into 2, but guess not.

---
## 2011-08-29 12:08:07 - firecore - help with a virtual sql parser
Hi All (or Paul) xD

I'm working on a small task for school, I need to play a couple of sentences similar to sql.
I have a very, very primitive version of a simple parse in php, but I want to implement a functional version in python / pyparsing.

My grammar is as follows (similar a sql, but the table is virtual and containing couple options)



    SELECT TIME, title , source_content 'my_cat_tom' AS my_pet , 20 AS years  FROM url 'http://pyparsing.wikispaces.com/' retry 3 WHERE title = 'pyparsing_Parser' OR TIME > 1000


I have the following primitive parser in php (it's a joke xD)



    <?php
    function parse_query( $q )
    {
        echo 'Debug Query\n';
        $q = str_replace( array(',','  ') , array(' , ',' ') ,trim( $q  ) );
        echo '\t'.$q.'\n\n';
        $tokens = split(' ', $q );
        $vars = array();
        $methods = array();
        $conditions = array();
    
        // little parser xD
        foreach( $tokens as $k => $v )
        {        
            if ( $v == 'select' )
            {
                $i = $k+1;
                // identifier
                while(  $tokens[$i] != 'from'  )
                {
                    // internal var
                    if ( preg_match('/\w/', $tokens[$i] ) )
                    {
                        // containing alias ?
                        if ( $tokens[$i+1] == 'as'  )
                        {
                            $vars[] = array(  $tokens[$i] , $tokens[$i+2] );
                            echo 'Debug Internal Var\t'.$tokens[$i].' as '.$tokens[$i+2].' \n';
                            $i++;$i++;
                        }
                        else
                        {
                            $vars[] = array(  $tokens[$i]  );
                            echo 'Debug Internal Var\t'.$tokens[$i].'\n';
                        }
                        // containing alias ?
                    }
                    $i++;                
                }
    
                if ( $tokens[$i] == 'from' )
                {
                    // method                
                    if ( $tokens[$i+1] == 'url'   )
                    {
                        $i++;$i++;                    
                        // options, example retry
                        if ( $tokens[$i+1] == 'retry' )
                        {
                            echo 'Debug URL \t'.$tokens[$i].' config '.$tokens[$i+1].' '.$tokens[$i+2].' \n';
                            $methods[] = array('url',$tokens[$i], array('retry' => $tokens[$i+2] ) );
                            $i++;$i++;
                        }
                        else
                        {
                            echo 'Debug URL \t'.$tokens[$i].'\n';
                            $methods[] = array('url',$tokens[$i]);
                            $i++;
                        }                    
    
                        $i++;
                    }
                }
    
                // where
                if ( $tokens[$i] == 'where' )
                {
                    $k = $i+1;                
                    while( $k < count($tokens) )
                    {
                        if ( $tokens[$k] != 'AND' and $tokens[$k] != 'OR'  )
                        {
                            // stack conditions rpn
                            // expr boolean
                            $conditions[] = $tokens[$k];
                            $conditions[] = $tokens[$k+2];
                            $conditions[] = $tokens[$k+1];
                            $k++;$k++;
    
                        }
                        else
                        {                        
                            $conditions[] = $tokens[$k];
                        }
                        $k++;
                    }
                    echo 'Debug Conditions, Stack RPN '.join(' ', $conditions).'\n';
                }
            }
        }
    
        $out = array();
        // program
        foreach( $methods as $k => $method )
        {        
            if (  $method[0] == 'url' )
            {
                $local_vars['time'] = time();
                $local_vars['source'] = file_get_contents( str_replace(''','', $method[1] ) );
                // get title
                if (  preg_match('|<title>(.*?)</title>|is', $local_vars['source'] , $cap ) )
                {
                    $local_vars['title'] = $cap[1];
                }            
            }
    
            $select_vars = array();
    
            if (  $local_vars )
            {
                foreach( $vars as $var )
                {
                    if ( isset($local_vars[ $var[0]  ])  )
                    {
                        if ( isset( $var[1] )  )
                        {
                            $select_vars[ $var[1] ]  = $local_vars[ $var[0]  ];
                        }
                        else
                        {
                            $select_vars[ $var[0] ]  = $local_vars[ $var[0]  ];
                        }
                    }
                    else
                    {                    
                        $select_vars[ $var[1] ]  = $var[0];
                    }
                }
            }
            $out[] = $select_vars;
    
        }
    
        return $out;
    }
    
    $querys[] = 'select time, title , source_content 'my_cat_tom' as my_pet , 20 as years  from url 'http://pyparsing.wikispaces.com/' retry 3 where title = 'pyparsing_Parser' OR time > 1000';
    $querys[] = 'select title, source from url 'http://pyparsing.wikispaces.com/' retry 3 , url 'http://www.wikipedia.com/' retry 3 ';
    
    foreach( $querys as $k => $query )
    {
        print_r( parse_query( $query ) );
    }
    ?>



I have the following written in python with pyparsing, It does not work with my grammar.



    #!/usr/bin/env python
    from pyparsing import *
    import os, sys, getopt, string
    import time
    # Grammar
    
    S_QUOTE = Literal(''').suppress()
    D_QUOTE = Literal(''').suppress()
    DOT = Literal('.').suppress()
    COMMA = Literal(',').suppress()
    SEMI = Literal(';').suppress()
    COLON = Literal(':').suppress()
    EQUAL = Literal('=')
    LANGLE = Literal('<').suppress()
    LBRACE = Literal('[').suppress()
    LPAREN = Literal('(')
    PLUS = Literal('+').suppress()
    RANGLE = Literal('>').suppress()
    RBRACE = Literal(']').suppress()
    RPAREN = Literal(')')
    
    s_select = CaselessKeyword('select')
    s_from = CaselessKeyword('from')
    s_where = CaselessKeyword('where')
    s_and = CaselessKeyword('and')
    s_as = CaselessKeyword('as').suppress()
    
    t_val = Word(alphanums + '_')
    num = Word(nums + '+-.')
    var = Word(alphas + '%')
    entry = Group(Optional(t_val + DOT).setResultsName('table') + \
            t_val.setResultsName('name')).setResultsName('entry')
    pseudo = Optional(s_as) + (t_val).setResultsName('pseudo')
    table = Group(t_val.setResultsName('name') + Optional(pseudo)).setResultsName('table')
    column = (delimitedList(entry) ^ delimitedList(entry + pseudo) ^ delimitedList(entry + s_as + pseudo))
    proc = entry + LPAREN + Optional(Suppress(SkipTo(RPAREN))) + RPAREN
    
    expr = Group(Optional(LPAREN).suppress() + entry + oneOf('= > <').setResultsName('type') + \
           Optional(D_QUOTE) + (proc | entry | var | num) + Optional(D_QUOTE) + \
           Optional(RPAREN).suppress()).setResultsName('expr')
    
    select_q = Group(s_select + Group(delimitedList(column)).setResultsName('what') + \
                s_from + Group(delimitedList(table)).setResultsName('whence') + s_where + \
                Group(delimitedList(expr, s_and)).setResultsName('where')).setResultsName('select')
    
    
    test1 = '''
    select time, title , source_content 'my_cat_tom' as my_pet , 20 as years  from url 'http://pyparsing.wikispaces.com/' retry 3 where title = 'pyparsing_Parser' OR time > 1000
    '''
    print test1
    tokens = select_q.parseString( test1 )



then if you can help me a bit, I just need a little we understand the code for my grammar (just something functional), the rest of the interpretation of the tokens I do.

Very Thanks
firecore

#### 2011-09-01 23:44:36 - ptmcg
Wow, where to start?  For comparison, look at the parser in the Examples page, .

First off, set your Python code aside for a minute. Write yourself a BNF definition of just what your SELECT grammar will contain, and how the different bits will work together. When I wrote the parser on the Examples page, I worked very closely with the grammar definition in the link in the header of that code. This does a couple of things. For one, it let's you focus purely in grammar space for a bit, to capture all of the grammar features and syntax you want to support, without worrying about implementation details. For something as complex as an SQL parser, you really must do this, no matter what you are implementing the parser in. Having an example in another language makes it tempting to skip this step, but please don't. Think of the example as a jumpstart and potential validator of your BNF once you have finished it. (The BNF doesn't have to be super formal or rigorous, just get the syntax down in some notation that helps you capture what kind of statements you want to recognize as valid SELECT statements.) Lastly, the BNF serves as a checklist, so that you know when you have finished implementing all the expressions/constructions defined in it.

Second, start small.  Your first test case should not be the one as you have listed in test1.  I think you have crammed just about every aspect of your grammar into a single statement.  Start with:


    select title from titletable where title > 'A'


Then add an alias: 


    select title 'Title' from titletable where title > 'A'


Then use a numeric literal:


    select 20 as years from titletable where title > 'A'


Then select multiple columns:


    select title 'Title', 20 as years from titletable where title > 'A'


and so on. For one thing, this will give you some self-encouragement that you are making some progress. It will also help you identify when you accidentally break something when you add a new aspect to the grammar, or when something in your grammar turns out to be ambiguous.

Looking at your PHP code, I see the following:
- I don't see any support for quoted strings. Add them later.
- the 'where' clause is optional, your code makes it mandatory
- conditions are simple 'x > 1' or 'y = z'; it will be nice to support more elaborate expressions, you can look at the posted example to see how that can be done
- the where clause can have AND and OR; again, see how this is done in the example
- there is no 'retry' syntax allowed

Here is a stab at a BNF from what I grasp of your PHP example:



    select_stmt :: 'select' column_list 'from' select_source ['where' where_clause]
    column_ref :: (identifier | numeric_constant) ['as' column_alias]
    column_alias :: identifier | quotedString
    column_list :: column_ref (',' column_ref)*
    select_source :: 'url' quotedString
    where_clause :: condition (('and' | 'or') condition)*
    condition :: expression comparison_operator expression
    expression :: identifier | numeric_constant
    comparison_operator :: '<' | '=' | '>'
    identifier :: word of printable characters
    numeric_constant :: word of digits


Even this BNF goes beyond the PHP code (at least as I understand it), as I started to add in some knowledge of quoted strings.

The next step is to start converting the BNF into pyparsing expression definitions. For instance, the opening line can be written using pyparsing as:



    select_stmt = 'SELECT' + column_list + 'FROM' + select_source + Optional('WHERE' + where_clause)


BNF's are typically written top-down, but pyparsing grammars have to be bottom up (since you are actually defining Python variables, and you have to have the lower-level expressions defined before you can use them in the higher-level ones). So this definition of select_stmt will actually be one of the last lines in your parser.

When you add results names, please use the newer 'call' syntax, as the verbose .setResultsName calls really break up the readability of your grammar:



    select_stmt = 'SELECT' + column_list('columns') + 'FROM' + select_source('table') + 
        Optional('WHERE' + where_clause('where'))


Also, when you translate your BNF to pyparsing, you can use some shortcuts using pyparsing helpers.  Instead of:


    column_list = column_ref + ZeroOrMore(',' + column_ref)


you can use the delimitedList helper:


    column_list = delimitedList(column_ref)


Get this much working. Start small. Then you can start to add complexity. Add more comparison operators. Add support for the optional 'retry' clause. Flesh out expression to support more complex arithmetic (again, consult the example for some details). As you add features, try to go back to your BNF and keep it up to date - it will be a nice documentation resource later.

On an implementation level, let me mention one item - I found in doing my parser that the SQL syntax is not completely unambiguous as to when you might encounter a keyword vs. an identifier.  So before parsing a token as an identifier, you will first have to rule out that you have actually encountered a keyword. For example in 'select time from timetable' (if you permit 'as alias' to be written without the 'as'), you have to determine whether 'from' is an alias for the time column or not.  So the definition of identifier has to start with a lookahead to see if we are really about to parse a keyword.  The posted example creates an expression of all of the SQL keywords, and then defines identifier as '~keyword + Word(alphas,alphanums+'_')'. You will probably have to do something similar in your parser.

Well, that is a lot to chew on, please write back and let us all know how you are getting on in your class.

-- Paul

---
## 2011-08-29 17:31:29 - adilh - parsing words with underscore
Hello, I think that I'm making a mess. My grammar has words that can be alphanumeric or numbers or quoted words or words with underscores. I can parse the words, but in the case of the underscore I have to be careful in how I order the grammar. E.g.



    import pyparsing
    iw = pyparsing.alphanums + '*$/%_'
    w = pyparsing.Word(iw)
    a = 'SOME_NAME'
    w.parseString(a)


This will correctly parse the string and identify the SOME_NAME as a word. But, if I now extend my grammar to include the other types of words:



    w = pyparsing.Word(pyparsing.alphanums) | pyparsing.Word(pyparsing.nums) | pyparsing.Word(iw)
    w.parseString(a)


I only get the partial string 'SOME'returned. If I reorder the statement to put the word with the greatest variation first I get the correct parsing:



    w = pyparsing.Word(iw) | pyparsing.Word(pyparsing.alphanums) | pyparsing.Word(pyparsing.nums)
    w.parseString(a)


Is this the acceptable way of doing things? I get the concern that order shouldn't matter and that the parser should look for the greediest match? Perhaps I should not use the OR, but some other construct?

#### 2011-08-29 22:41:54 - ptmcg
This is exactly how pyparsing works. Congratulations, this is often a difficult concept for new pyparsing users, and you have described it perfectly.

Pyparsing has 2 different classes for describing alternative parsers: Or and MatchFirst.  MatchFirst is not so much a 'greedy' parser as it is an 'eager' one; the first expression in its list to match the input will be used, as you have observed.  The Or class is more thorough, and evaluates all of its defined alternatives, returning the one that matches the longest string.

The '|' operator generates MatchFirst expressions; the '^' operator generates Or expressions.

Why 2 different operators? Well mostly for performance, I guess.  Imagine that at some point within a larger grammar that you had 4 different alternative commands: 'GIVE', 'TAKE', 'PUSH', or 'PULL'.  If I defined these with an Or, then a match of the string 'GIVE' would test all 4 alternatives, even though there is no chance that a match of 'GIVE' might also match 'TAKE', 'PUSH', or 'PULL'. So a MatchFirst expression is perfectly all right here.  If I had comparison operators '<', '>', '=', '<=' and '>=', then there *is* a chance of accidentally reading the leading '<' of '<=' as just the '<' operator, and then getting stuck on some unexpected '=' sign.  I *could* use an Or expression to keep things straight, but again, this is simple enough to just reorder the alternatives to check for '<=' before matching '<'.  There is similar logic in the oneOf helper method, but in that case, the method itself looks at the provided alternatives, and does the reordering/disambiguation for you. (The expression created by oneOf is actually neither an Or nor a MatchFirst, but a Regex!)

If you like, you could just use '^' instead of '|' operators, but there are cases (typically involving recursive grammars with Forward's) where Or's can take exponentially long to finally resolve themselves - in those cases, you'd really need to use MatchFirst.

Hope that helps,
-- Paul
#### 2011-08-30 01:54:14 - adilh
Hello Paul,
Thanks for the response! Very useful. BTW I think that your book is very useful too!
hope all is well,
adil

---
## 2011-09-02 03:46:41 - MattWarren2 - New to PyParsing, parsing a simple text structure
Hi. I have just discovered pyparsing and I am trying to use it to parse two files with subtle differences.

I have a grammar defined that succesfully parses file A, but cannot get a slightly altered version to parse the slightly-different file B.


The grammar I have for file A, and File A follow;

Grammar for file A


    TOPOLOGY_DEFINITIONS_HEADER=Literal('TOPOLOGY DEFINITIONS').suppress()
        STORAGE_SUBSYSTEMS_HEADER=Literal('STORAGE SUBSYSTEM').suppress()
    
        ##Final 'headers' declaration
        SECTION_HEADER=TOPOLOGY_DEFINITIONS_HEADER+\
                       STORAGE_SUBSYSTEMS_HEADER
    
        VALUE=Word(printables) 
        SUPPRESS_VALUE=VALUE.suppress()
        SUPPRESS_REST_OF_LINE=restOfLine.suppress()
    
        #define the type_block
        DEFAULT_TYPE_BLOCK=Group(Literal('Default type:')+VALUE)
    
        #define default block 
        DEFAULT_GROUP_BLOCK=Literal('Default Group')
    
        #define the host_group block
        #a host_groups block consist of a list of host blocks
        #so define a host block first
        HOST_HEADER=Literal('Host:')+VALUE
    
        SUPPRESS_HOST_TYPE=Literal('Host type:').suppress()
        HOST_TYPE_HEADER=SUPPRESS_HOST_TYPE+SUPPRESS_REST_OF_LINE
    
        SUPPRESS_INTERFACE_TYPE=Literal('Interface type:').suppress()
        INTERFACE_TYPE_HEADER=SUPPRESS_INTERFACE_TYPE+SUPPRESS_REST_OF_LINE
    
        HOST_PORT_IDENTIFIER_LINE=Literal('Host port identifier:')+VALUE
    
        SUPPRESS_ALIAS=Literal('Alias:').suppress()
        ALIAS_LINE=SUPPRESS_ALIAS+SUPPRESS_VALUE
    
        #define the host_block
        HOST_BLOCK=Group(HOST_HEADER+\
                         HOST_TYPE_HEADER+\
                         INTERFACE_TYPE_HEADER+\
                         Group(ZeroOrMore(HOST_PORT_IDENTIFIER_LINE+ALIAS_LINE)))
    
        #Now we can define a host group block
        HOST_GROUP_HEADER=Literal('Host Group:')+VALUE
        HOST_GROUP_BLOCK=Group(HOST_GROUP_HEADER+Group(ZeroOrMore(HOST_BLOCK)))
    
    
        #Now define the file structure in terms of its headers
        #type block
        #default info block
        #
        #then any of host_block's or host_group_blocks
    
        TOPOLOGY_DEFINITIONS_STRUCTURE=SECTION_HEADER+\
                       DEFAULT_TYPE_BLOCK+\
                       DEFAULT_GROUP_BLOCK+\
                       ZeroOrMore(HOST_BLOCK)+\
                       ZeroOrMore(HOST_GROUP_BLOCK)+\
                       ZeroOrMore(HOST_BLOCK)+\
                       ZeroOrMore(HOST_GROUP_BLOCK)+\
                       ZeroOrMore(HOST_BLOCK)+\
                       ZeroOrMore(HOST_GROUP_BLOCK)+\
                       ZeroOrMore(HOST_BLOCK)+\
                       ZeroOrMore(HOST_GROUP_BLOCK)+\
                       ZeroOrMore(HOST_BLOCK)+\
                       ZeroOrMore(HOST_GROUP_BLOCK)+\
                       ZeroOrMore(HOST_BLOCK)+\
                       ZeroOrMore(HOST_GROUP_BLOCK)+\
                       ZeroOrMore(HOST_BLOCK)+\
                       ZeroOrMore(HOST_GROUP_BLOCK)+\
                       ZeroOrMore(HOST_BLOCK)+\
                       ZeroOrMore(HOST_GROUP_BLOCK)+\
                       ZeroOrMore(HOST_BLOCK)+\
                       ZeroOrMore(HOST_GROUP_BLOCK)+\
                       ZeroOrMore(HOST_BLOCK)+\
                       ZeroOrMore(HOST_GROUP_BLOCK)


I call fileParse on TOPOLOGY_DEFINITIONS_STRUCTURE 
and this succesfully parses the following file, and returns a usable list structure

file A


    TOPOLOGY DEFINITIONS
                   STORAGE SUBSYSTEM
                      Default type:                   DEFAULT
                      Default Group
                      Host Group:                     DR-SVC-01
                         Host:                        DR-SVC01_Node1
                            Host type:                IBM TS SAN VCE
                            Interface type:           Fibre Channel
                               Host port identifier:  50:05:07:68:01:40:53:97
                               Alias:                 DR-SVC-01_Node1_p1
                               Host port identifier:  50:05:07:68:01:30:53:97
                               Alias:                 DR-SVC-01_Node1_p2
                               Host port identifier:  50:05:07:68:01:10:53:97
                               Alias:                 DR-SVC-01_Node1_p3
                               Host port identifier:  50:05:07:68:01:20:53:97
                               Alias:                 DR-SVC-01_Node1_p4
                         Host:                        DR-SVC-01_Node2
                            Host type:                IBM TS SAN VCE
                            Interface type:           Fibre Channel
                               Host port identifier:  50:05:07:68:01:40:52:1b
                               Alias:                 DR-SVC-01_Node2_p1
                               Host port identifier:  50:05:07:68:01:30:52:1b
                               Alias:                 DR-SVC-01_Node2_p2
                               Host port identifier:  50:05:07:68:01:10:52:1b
                               Alias:                 DR-SVC-01_Node2_p3
                               Host port identifier:  50:05:07:68:01:20:52:1b
                               Alias:                 DR-SVC-01_Node2_p4
                         Host:                        DR-SVC-01_Node3
                            Host type:                IBM TS SAN VCE
                            Interface type:           Fibre Channel
                               Host port identifier:  50:05:07:68:01:40:51:1e
                               Alias:                 DR-SVC-01_Node3_p1
                               Host port identifier:  50:05:07:68:01:30:51:1e
                               Alias:                 DR-SVC-01_Node3_p2
                               Host port identifier:  50:05:07:68:01:10:51:1e
                               Alias:                 DR-SVC-01_Node3_p3
                               Host port identifier:  50:05:07:68:01:20:51:1e
                               Alias:                 DR-SVC-01_Node3_p4
                         Host:                        DR-SVC-01_Node4
                            Host type:                IBM TS SAN VCE
                            Interface type:           Fibre Channel
                               Host port identifier:  50:05:07:68:01:40:51:ba
                               Alias:                 DR-SVC-01_Node4_p1
                               Host port identifier:  50:05:07:68:01:30:51:ba
                               Alias:                 DR-SVC-01_Node4_p2
                               Host port identifier:  50:05:07:68:01:10:51:ba
                               Alias:                 DR-SVC-01_Node4_p3
                               Host port identifier:  50:05:07:68:01:20:51:ba
                               Alias:                 DR-SVC-01_Node4_p4
                         Host:                        DR-SVC-01_Node5
                            Host type:                IBM TS SAN VCE
                            Interface type:           Fibre Channel
                               Host port identifier:  50:05:07:68:01:40:a1:91
                               Alias:                 DR-SVC-01_Node5_P4
                               Host port identifier:  50:05:07:68:01:10:a1:91
                               Alias:                 DR-SVC-01_Node5_P1
                               Host port identifier:  50:05:07:68:01:30:a1:91
                               Alias:                 DR-SVC-01_Node5_P3
                               Host port identifier:  50:05:07:68:01:20:a1:91
                               Alias:                 DR-SVC-01_Node5_P2
                         Host:                        DR-SVC-01_Node6
                            Host type:                IBM TS SAN VCE
                            Interface type:           Fibre Channel
                               Host port identifier:  50:05:07:68:01:30:a1:89
                               Alias:                 DR-SVC-01_Node6_P3
                               Host port identifier:  50:05:07:68:01:10:a1:89
                               Alias:                 DR-SVC-01_Node6_P1
                               Host port identifier:  50:05:07:68:01:40:a1:89
                               Alias:                 DR-SVC-01_Node6_P4
                               Host port identifier:  50:05:07:68:01:20:a1:89
                               Alias:                 DR-SVC-01_Node6_P2
                         Host:                        DR-SVC-01_Node7
                            Host type:                IBM TS SAN VCE
                            Interface type:           Fibre Channel
                               Host port identifier:  50:05:07:68:01:40:b1:3e
                               Alias:                 DR-SVC-01_Node7_p4
                               Host port identifier:  50:05:07:68:01:10:b1:3e
                               Alias:                 DR-SVC-01_Node7_p1
                               Host port identifier:  50:05:07:68:01:20:b1:3e
                               Alias:                 DR-SVC-01_Node7_p2
                               Host port identifier:  50:05:07:68:01:30:b1:3e
                               Alias:                 DR-SVC-01_Node7_p3
                         Host:                        DR-SVC-01_Node8
                            Host type:                IBM TS SAN VCE
                            Interface type:           Fibre Channel
                               Host port identifier:  50:05:07:68:01:30:b1:1a
                               Alias:                 DR-SVC-01_Node8_p3
                               Host port identifier:  50:05:07:68:01:40:b1:1a
                               Alias:                 DR-SVC-01_Node8_p4
                               Host port identifier:  50:05:07:68:01:20:b1:1a
                               Alias:                 DR-SVC-01_Node8_p2
                               Host port identifier:  50:05:07:68:01:10:b1:1a
                               Alias:                 DR-SVC-01_Node8_p1
                      Host Group:                     DR-USPV-01_1G_2G
                         Host:                        DR-USPV-01_1G_2G
                            Host type:                Linux
                            Interface type:           Fibre Channel
                               Host port identifier:  50:06:0e:80:15:31:40:06
                               Alias:                 DR-USPV-01_1G
                               Host port identifier:  50:06:0e:80:15:31:40:16
                               Alias:                 DR-USPV-01_2G
                      Host Group:                     DR-USPV-01_1H_2H
                         Host:                        DR-USPV-01_1H_2H
                            Host type:                Linux
                            Interface type:           Fibre Channel
                               Host port identifier:  50:06:0e:80:15:31:40:07
                               Alias:                 DR-USPV-01_1H
                               Host port identifier:  50:06:0e:80:15:31:40:17
                               Alias:                 DR-USPV-01_2H
                      Host Group:                     DR-USPV-01_3G_4G
                         Host:                        DR-USPV-01_3G_4G
                            Host type:                Linux
                            Interface type:           Fibre Channel
                               Host port identifier:  50:06:0e:80:15:31:40:26
                               Alias:                 DR-USPV-01_3G
                               Host port identifier:  50:06:0e:80:15:31:40:36
                               Alias:                 DR-USPV-01_4G
                      Host Group:                     DR-USPV-01_3H_4H
                         Host:                        DR-USPV-01_3H_4H
                            Host type:                Linux
                            Interface type:           Fibre Channel
                               Host port identifier:  50:06:0e:80:15:31:40:27
                               Alias:                 DR-USPV-01_3H
                               Host port identifier:  50:06:0e:80:15:31:40:37
                               Alias:                 DR-USPV-01_4H


Now, I wrap grammar A in a class, subclass it, and override some of the defined constants with the following;

grammar B



    SECTION_HEADER=IBMDS345GrammarParser.TOPOLOGY_DEFINITIONS_HEADER
    
        HOST_PORT_IDENTIFIER_LINE=Literal('Host Port:')+IBMDS345GrammarParser.VALUE
        DEFAULT_TYPE_BLOCK=Group(Literal('Default type:').suppress()+IBMDS345GrammarParser.SUPPRESS_REST_OF_LINE)
        DEFAULT_GROUP_BLOCK=Group(Literal('DEFAULT GROUP')+\
                            DEFAULT_TYPE_BLOCK+\
                            Group(ZeroOrMore(HOST_PORT_IDENTIFIER_LINE)))
    
        SUPPRESS_HOST_TYPE=Literal('Type:').suppress()
        HOST_TYPE_HEADER=SUPPRESS_HOST_TYPE+IBMDS345GrammarParser.SUPPRESS_REST_OF_LINE    
    
        HOST_PORT_GROUP=ZeroOrMore(HOST_PORT_IDENTIFIER_LINE+IBMDS345GrammarParser.ALIAS_LINE+HOST_TYPE_HEADER)
    
        #define the host_block
        HOST_BLOCK=Group(IBMDS345GrammarParser.HOST_HEADER+\
                         Group(HOST_PORT_GROUP))
    
        #Now we can define a host group block
        HOST_GROUP_HEADER=Literal('HOST GROUP')+IBMDS345GrammarParser.VALUE
        HOST_GROUP_BLOCK=Group(HOST_GROUP_HEADER+Group(ZeroOrMore(HOST_BLOCK)))
        HOST_GROUP_BLOCK.setDebug(True)
    
        SINGULAR_HOST_HEADER=Literal('HOST')+IBMDS345GrammarParser.VALUE
        SINGULAR_HOST_BLOCK=Group(SINGULAR_HOST_HEADER+ZeroOrMore(HOST_PORT_GROUP))
        #Now define the file structure in terms of its headers
        #type block
        #default info block
        #
        #then any of host_block's or host_group_blocks
    
        TOPOLOGY_DEFINITIONS_STRUCTURE=SECTION_HEADER+\
                       DEFAULT_GROUP_BLOCK+\
                       Group(ZeroOrMore(HOST_GROUP_BLOCK))+\
                       Group(ZeroOrMore(SINGULAR_HOST_BLOCK))


This attempts to parse the following file (note the HOST {hostname} structures after the 'Host Group' structures, and the slightly different default_group structure (and missing default_type section)



    TOPOLOGY DEFINITIONS
          DEFAULT GROUP
             Default type:      Windows 2000/Server 2003/Server 2008 Non-Clustered
             Host Port:         50:01:43:80:03:ba:6f:24
             Host Port:         50:01:43:80:03:ba:7e:98
          HOST GROUP ESX_DMZ
             Host:              HP_LAME16
                Host Port:      50:01:43:80:03:ba:67:58
                   Alias:       HP_LAME16
                   Type:        LNXCLVMWARE
                Host Port:      50:01:43:80:03:ba:67:5a
                   Alias:       HP_LAME16_FCS2
                   Type:        LNXCLVMWARE
          HOST GROUP ESX_PRD
             Host:              HP_LAME12
                Host Port:      50:01:43:80:03:ba:7f:40
                   Alias:       HP_LAME12
                   Type:        LNXCLVMWARE
                Host Port:      50:01:43:80:03:ba:7f:42
                   Alias:       HP_LAME12_FCS2
                   Type:        LNXCLVMWARE
             Host:              HP_LAME8
                Host Port:      50:01:43:80:03:ba:65:ac
                   Alias:       HP_LAME8
                   Type:        LNXCLVMWARE
                Host Port:      50:01:43:80:03:ba:65:ae
                   Alias:       HP_LAME8_FCS2
                   Type:        LNXCLVMWARE
             Host:              HP_LAME10
                Host Port:      50:01:43:80:03:ba:6e:8c
                   Alias:       HP_LAME10
                   Type:        LNXCLVMWARE
                Host Port:      50:01:43:80:03:ba:6e:8e
                   Alias:       HP_LAME10_FCS2
                   Type:        LNXCLVMWARE
             Host:              HP_LAME13
                Host Port:      50:01:43:80:03:ba:6e:92
                   Alias:       HP_LAME13_FCS1
                   Type:        LNXCLVMWARE
                Host Port:      50:01:43:80:03:ba:6e:90
                   Alias:       HP_LAME13_FCS0
                   Type:        LNXCLVMWARE
             Host:              HP_LAME1
                Host Port:      50:01:43:80:03:ba:63:e6
                   Alias:       HP_LAME1_FCS2
                   Type:        LNXCLVMWARE
                Host Port:      50:01:43:80:03:ba:63:e4
                   Alias:       HP_LAME1
                   Type:        LNXCLVMWARE
             Host:              HP_LAME2
                Host Port:      50:01:43:80:03:ba:6d:b6
                   Alias:       HP_LAME2_FCS2
                   Type:        LNXCLVMWARE
                Host Port:      50:01:43:80:03:ba:6d:b4
                   Alias:       HP_LAME2
                   Type:        LNXCLVMWARE
             Host:              HP_LAME4
                Host Port:      50:01:43:80:03:ba:65:ca
                   Alias:       HP_LAME4_FCS2
                   Type:        LNXCLVMWARE
                Host Port:      50:01:43:80:03:ba:65:c8
                   Alias:       HP_LAME4
                   Type:        LNXCLVMWARE
             Host:              HP_LAME3
                Host Port:      50:01:43:80:03:ba:6e:8a
                   Alias:       HP_LAME3_FCS2
                   Type:        LNXCLVMWARE
                Host Port:      50:01:43:80:03:ba:6e:88
                   Alias:       HP_LAME3
                   Type:        LNXCLVMWARE
             Host:              HP_LAME9
                Host Port:      50:01:43:80:03:ba:99:c2
                   Alias:       HP_LAME9_FCS2
                   Type:        LNXCLVMWARE
                Host Port:      50:01:43:80:03:ba:99:c0
                   Alias:       HP_LAME9_FC1
                   Type:        LNXCLVMWARE
             Host:              HP_LAME7
                Host Port:      50:01:43:80:03:ba:6e:6e
                   Alias:       HP_LAME7_FCS2
                   Type:        LNXCLVMWARE
                Host Port:      50:01:43:80:03:ba:6e:6c
                   Alias:       HP_LAME7
                   Type:        LNXCLVMWARE
          HOST GROUP ESX_DEV
             Host:              HP_LAME5
                Host Port:      50:01:43:80:03:ba:7f:10
                   Alias:       HP_LAME5
                   Type:        LNXCLVMWARE
                Host Port:      50:01:43:80:03:ba:7f:12
                   Alias:       HP_LAME5_FCS2
                   Type:        LNXCLVMWARE
             Host:              HP_LAME6
                Host Port:      50:01:43:80:03:ba:67:d4
                   Alias:       HP_LAME6
                   Type:        LNXCLVMWARE
                Host Port:      50:01:43:80:03:ba:67:d6
                   Alias:       HP_LAME6_FCS2
                   Type:        LNXCLVMWARE
          HOST GROUP HP_LAME11
             Host:              HP_LAME11
                Host Port:      50:01:43:80:03:ba:6f:54
                   Alias:       HP_LAME11_FC1
                   Type:        LNXCLVMWARE
                Host Port:      50:01:43:80:03:ba:6f:56
                   Alias:       HP_LAME11_FCS2
                   Type:        LNXCLVMWARE
          HOST ESXBKP02
             Host Port:         10:00:00:00:c9:87:22:08
                Alias:          ESXBKP02_FCS0
                Type:           LNXCLVMWARE
          HOST ESXBKP01_FCS0
             Host Port:         10:00:00:00:c9:89:92:b8
                Alias:          ESXBKP01_FCS0
                Type:           LNXCLVMWARE


But it returns the error:
Expected 'HOST GROUP' (at char 4563), (line:108, col:7)

Firstly;  I have a feeling my problem is to do with expression operators and not being sure of the correct ones to use. (SEE:  the repeated sections of TOPOLOGY_DEFINITION_STRUCTURE in grammar A.  I am trying to find the expression that means 'zero or more of the following definitions in any order' - I have tried multiple combinations of '|' and '&' etc.. but cant seem to get it to work for grammar B and file B.)

If anyone could take a look at why grammar B fails on file B and give me any pointers, that would be much appreciated. This is my first attempt at using pyparsing so any general hints to improve my approach would also be much appreciated.

Matt.

#### 2011-09-02 03:52:17 - MattWarren2
Just to add;

references to IBMDS345GrammarParser in grammar B, this is refering to the class I wrapped grammar A in.  Grammar b lives in a  class that inherits from IBMDS345GrammarParser.
#### 2011-09-02 05:07:57 - ptmcg
Did you try `ZeroOrMore(HOST_BLOCK | HOST_GROUP_BLOCK)` ?
#### 2011-09-02 05:46:27 - MattWarren2
I didn't for grammar A, for grammar B I get the same error if I use

    ZeroOrMore(HOST_GROUP_BLOCK|SINGULAR_HOST_BLOCK)

in TOPOLOGY_DEFINITIONS_STRUCTURE

I was expecting that to work, which then led to my confusion of the expression operators (I also then tried & which did not work )

had that worked, I would have refactored grammar A to use the same method.

I can't see why the parser is giving the error 'expecting HOST GROUP' where the code states



    TOPOLOGY_DEFINITIONS_STRUCTURE=SECTION_HEADER+\
                       DEFAULT_GROUP_BLOCK+\
                       Group(ZeroOrMore(HOST_GROUP_BLOCK))+\
                       Group(ZeroOrMore(SINGULAR_HOST_BLOCK))


I would understand that, when 'HOST GROUP' does not match where expected, it would then look for a match on 'HOST {somestring}' (the first tokens defined in the SINGULAR_HOST_BLOCK chain)
#### 2011-09-02 07:29:38 - ptmcg
First, I think you left out the repetition on HOST_PORT_GROUP in HOST_BLOCK:



    HOST_BLOCK=Group(IBMDS345GrammarParser.HOST_HEADER+\
                     Group(ZeroOrMore(HOST_PORT_GROUP)))


Second, please don't use a shotgun approach of sprinkling ZeroOrMore's all over your grammar. You will end up parsing things you don't intend to parse, or more likely, you will get yourself into an infinite parse loop, as you hit a structure that continues to look for ZeroOrMore of something that isn't there, so it just keeps looking and looking.

Define your basic expressions as they are if they were present.  Not:



    HOST_PORT_GROUP=ZeroOrMore(HOST_PORT_IDENTIFIER_LINE+IBMDS345GrammarParser.ALIAS_LINE+HOST_TYPE_HEADER)


but just:



    HOST_PORT_GROUP=(HOST_PORT_IDENTIFIER_LINE+IBMDS345GrammarParser.ALIAS_LINE+HOST_TYPE_HEADER)


Use the ZeroOrMore in the higher-level expressions where this is referenced. This might take some of the mystery out of your parsing process.

I also see that you are using some setDebug() calls - that is good!  Unfortunately, you are probably seeing some cryptic default strings for expressions, like:



    Group:({'Host:' W:(0123...) Group:([{'Host Port:' W:(0123...) 'Alias:' empty Re:('.*') Suppress:('Type:') Suppress:(Re:('.*'))}]...)})


Add these lines for expressions you want to debug (and you can leave them in after debugging and you'll get nicer looking exceptions):



    HOST_PORT_GROUP.setName('HOST_PORT_GROUP')
    HOST_BLOCK.setName('HOST_BLOCK')


#### 2011-09-05 07:07:59 - MattWarren2
Thanks for the tips! much appreciated as I get used to using pyparsing. I resolved the error by starting again and methodically building up a cleaner implementation. this let me get it right first time, rather than scrabbling around in the code I had 'left over' from the first couple of attempts.

thanks, matt.

---
## 2011-09-02 14:27:05 - tdris01 - Hello World example? 
The hello example produce the following exception:

    from pyparsing import Word, alphas
    greet = Word( alphas ) + ',' + Word( alphas ) + '!' # <-- grammar defined here
    hello = 'Hello, World!'
    print hello, '->', greet.parseString( hello )

    Syntax Error:             except ParseException, err:: C:\Python32\lib\site-packages\pyparsing.py, line 248034
    File 'C:\Users\sysadmin\untitled-1.py', line 1, in <module>
      from pyparsing import Word, alphas
    File 'C:\Python32\lib\site-packages\pyparsing.py', line 2480, in ?
      except ParseException, err:

Any suggestions?

#### 2011-09-24 10:29:15 - FireJet
Just had the same issue while installing on Windows with Python 3.2. I checked the site-packages folder, and the Windows installer actually installs the Py2.6 version of pyparsing instead. 
Remove it, and use the .zip to install instead
#### 2016-09-22 09:01:19 - lhughes42
This is surely my stupidity but I thought I would mention. I am trying to parse out '__' (I am aware of dblSlashComment) . If I do it this way things are fine:

    pat = Literal('_')
    result = pat.parseString(simple_str)
    print result
--
But if i do it this way with a setDebug:

    result = pat.parseString(simple_str).setDebug(True)

I get 'TypeError: 'str' object is not callable'

I don't get this with other literal patterns only with double back slash.  Tore my hair for awhile till I traced it to the setDebug
#### 2016-09-22 09:08:44 - lhughes42
Sorry wrong place to post. Above had issues but can't find way to remove. See posting in 'help' section. Please delete my posts here.

---
## 2011-09-07 07:41:36 - MattWarren2 - Matching whitespace and preserving the token
Hi,

I have a line in a file that could read either;



    Product ID: something\n


or



    Product ID: \n


Initially I used the following to match the line,



    PRODUCT_ID_LINE=Literal('Product ID:')+Word(printables)


When using the ParseResults as a list, there would be list elements like



    ['Product ID:','something']


However, this fails to parse the second form, where the product ID is effectivly blank.

I have since tried changing Word(printables) to restOfLine in combination with keepOriginalText as ways of trying to parse the 'blank' product ID line, and preserving the fact it was blank, IE:  hoping to get list elements in the ParseResult like;



    ['Product ID',''] 
    or
    ['Product ID',' ']


but any approach I have tried has failed so far.

How should I correctly be trying to match the product id line, such that when it is blank I still see



    ['Product ID:','']
    or
    ['Product ID:',' ']


in the results list.

Thanks,

Matt.

#### 2011-09-07 09:23:56 - MattWarren2
I have resolved this now. Problem was somewhere between the chair and the keyboard.
#### 2011-09-07 11:07:29 - ptmcg
Glad you were able to work it out - you posted the 2,100th message!

-- Paul

---
## 2011-09-11 06:24:09 - almoni - Prioritization
Hey.

I have encountered what seems to be a weird bug. For example:

    num = Word(nums)
    vec = num + ZeroOrMore(',' + num)
    mat = vec + ZeroOrMore(';' + vec)
    exp = num ^ mat
    
    num.setParseAction(lambda s, l, tok: 'num')
    mat.setParseAction(lambda s, l, tok: 'mat')
    
    exp.parseString('1') # prints (['num'], {})
    exp.parseString('''
    1
    ''') # prints (['mat'], {})

I am looking for some method of prioritization; that is, I would like to parse the string as a number if possible, and as a matrix if not.

Note that a number is also a matrix, but I would like to distinguish these cases. I thought setting the expression to be 'num ^ mat' will take care of it, but for some reason the raw string is parsed differently.

Do you have any solution, or another method for prioritization?

Thanks in advance.

#### 2011-09-11 08:42:39 - ptmcg
I think you still have some thinking to do here.  You have said:



    1


is a num, and



    1,2,3;4,5,6


is a mat.

What are:


    1;2;1;2 - is this a mat of nums or a mat of vecs?
    1,2,3,1 - is this a vec or a mat?


By your definitions, all of these examples would match mat, but you would prefer, if possible, to treat things as a simpler num.

The typical way to express prioritization is to use '|' instead of '^' to express alternatives. a | b generates a MatchFirst, as if you had written MatchFirst([a,b]) and if a matches, then b is not even tested for a match.  ('^' generates an Or, which tests all alternatives and returns the *longest* match.)  However, you have to be careful in case the leading part of b could be mistaken for a in this case.  At this point, I usually suggest that you 'Be The Parser'.  If you were looking at the leading 1 of 1,2,3 or 1;2;3, how do you decide that it is not just a plain num?  From just looking at 1, you can't tell.  You have to look past the 1 to see if it is followed by a vec or mat delimiter - only if it isn't is it just a bare num.

I would leave your definition of num just as it is, to be used as the repeated element of vec, but define a special just_a_num expression to use as a preferred alternative to a mat:



    just_a_num = num + ~FollowedBy(oneOf(', ;'))
    exp = just_a_num | mat


I think this will give you what you are looking for.

Also, please consider using delimitedList as a simplifier for your vec and mat expressions. The structure:


    expr + ZeroOrMore(delimiter + expr)


is so common, pyparsing includes the helper method delimitedList.  delimitedList(expr, delim=',') gives back expr + ZeroOrMore(Suppress(delim) + expr).  The delimiters are needed during parsing, but usually just get in the way when working through the parsed output.  

You'll also want to use Group to retain the structure of your vecs and mats.  Since mat's are made up of vec's, you would define vec and mat as:


    vec = Group(delimitedList(num))
    mat = delimitedList(vec, delim=';')


The Group on vec will keep the nums separated by their respective vecs. You can Group mat too if you like, but since this is the top-level item, Group'ing it is probably unnecessary.
#### 2011-09-11 13:41:21 - almoni
Hey.

Thank you for you answer.

The example I gave was a very simplified version of what I am doing, and the problem I have with the '|' operator is that, indeed, 'num | mat' always returns a number, and ignores the rest of the matrix (or generates an error, as I usually use 'parseAll = True').

However, I may be able to modify the '~FollowedBy' solution to fit, so thanks again!

---
## 2011-09-11 22:55:12 - vsemionov - Possible bug in CharsNotIn
CharsNotIn does not skip leading whitespace.

EXAMPLE:

    >>> from pyparsing import CharsNotIn
    >>> CharsNotIn(' ').parseString(' hello')
    
    Traceback (most recent call last):
      File '<pyshell#3>', line 1, in <module>
        CharsNotIn(' ').parseString(' hello')
      File 'D:\Documents\work\wordbase\src\pyparsing.py', line 969, in parseString
        raise exc
      File 'D:\Documents\work\wordbase\src\pyparsing.py', line 959, in parseString
        loc, tokens = self._parse( instring, 0 )
      File 'D:\Documents\work\wordbase\src\pyparsing.py', line 837, in _parseNoCache
        loc,tokens = self.parseImpl( instring, preloc, doActions )
      File 'D:\Documents\work\wordbase\src\pyparsing.py', line 1855, in parseImpl
        raise ParseException(instring, loc, self.errmsg, self)
    pyparsing.ParseException: Expected !W:( ) (at char 0), (line:1, col:1)

#### 2011-09-11 22:58:15 - vsemionov
Sorry about the code snippet.

Here's the example:


    >>> from pyparsing import CharsNotIn
    >>> CharsNotIn(' ').parseString(' hello')
    Traceback (most recent call last):
      File '<pyshell#3>', line 1, in <module>
        CharsNotIn(' ').parseString(' hello')
      File 'D:\Documents\work\wordbase\src\pyparsing.py', line 969, in parseString
        raise exc
      File 'D:\Documents\work\wordbase\src\pyparsing.py', line 959, in parseString
        loc, tokens = self._parse( instring, 0 )
      File 'D:\Documents\work\wordbase\src\pyparsing.py', line 837, in _parseNoCache
        loc,tokens = self.parseImpl( instring, preloc, doActions )
      File 'D:\Documents\work\wordbase\src\pyparsing.py', line 1855, in parseImpl
        raise ParseException(instring, loc, self.errmsg, self)
    pyparsing.ParseException: Expected !W:( ) (at char 0), (line:1, col:1)


#### 2011-09-12 05:56:01 - ptmcg
This is a documentation error. CharsNotIn does not skip whitespace.  If you want to skip over whitespace before beginning your CharsNotIn, precede it with an 'empty':



    >>> (empty + CharsNotIn(' ')).parseString(' hello')
    (['hello'], {})


#### 2011-09-12 08:14:07 - vsemionov
Thanks!

---
## 2011-09-15 02:02:02 - jcronje - Newlines in quoted strings
Hi!

I am using the sexpParser.py example with one or two mods:

added: (ca line 77)


    comment = ';' + restOfLine
    
    sexp = Forward()
    sexpList = Group(LPAR + ZeroOrMore(sexp) + RPAR)
    sexpList.ignore(comment)
    sexp << OneOrMore( string_ | sexpList )


Then I try to use it on data like this:



    ;; HMF V1.25 TEXT
    ;; (Microsoft Win64 X64)  HOOPS 19.10-1
    (Alias '?include library' '/include library')
    (Alias '?locater' '?picture')
    (Alias '?keyboard' '?picture')
    (Alias '?style library' '/style library')
    (Alias '?null' '?driver/null')
    (Alias '?home' '/')
    (Alias '?driver' '/driver')
    (Color_Name 'apricot' '' '' 'r=1.0 g=0.76 b=0.65')
    (Color_Name 'aqua, aquamarine' '' '' 'r=0.0 g=0.95 b=0.84')
    (Color_Name 'bittersweet' '' '' 'r=0.81 g=0.04 b=0.0')
    (Color_Name 'black' 'dimmer, dim, darker, dark, blacker' 'darkish, blackish' 'h
    =0.0 l=0.0 s=0.0')
    (Color_Name 'blue green' '' '' 'r=0.0 g=0.71 b=0.66')
    (Color_Name 'blue grey' '' '' 'r=0.16 g=0.46 b=0.65')


The problem I have is that it only reads to the line containing 'bittersweet' (line 12). The next line is truncated at char 80 (as are all lines in the file). How to handle this situation -- I thought newlines are ignored? If I modify the file and join line 13 and 14, it works.

#### 2011-09-15 05:39:09 - ptmcg
The qstring definition:


    qString = Group(Optional(decimal,default=None).setResultsName('len') + 
                            dblQuotedString.setParseAction(removeQuotes)).setParseAction(verifyLen)

uses the builtin 'dblQuotedString', which does not span newlines.  You can get multiline quoted strings by replacing 'dblQuotedString.setParseAction(removeQuotes)' with 'QuotedString(''',multiline=True)'.  You can also add this parse action to it:


    lambda t : t[0].replace('\n','')

to automatically reassemble the quoted strings that are broken across line boundaries.

HTH,
-- Paul
#### 2011-09-15 05:47:56 - jcronje
Thanks a mil!

---
## 2011-09-25 18:58:24 - bramsc - Whitespaces
I have a recursive grammar which appears to almost be working correctly, and was hoping that someone might be able to point out what I might be doing wrong.

A very simplified version of the language allows for macros to be called in text. So, for example, an input string might be:


    #i{This is italic, and this is #b{bold}\n\foobar\n\nbaz.}


I use the following code to parse the input:



    from pyparsing import *
    
    class Macro(object):
        def __init__(self, name, block):
            self.name = name
            self.block = block
    
        def __repr__(self):
            return '#%s %s' % (self.name, self.block)
    
    class Block(object):
        def __init__(self, content):
            self.content = content
    
        def __repr__(self):
            return '{ %s }' % (''.join([str(e) for e in self.content]))
    
    lbrace = Literal('{')
    rbrace = Literal('}')
    decorator = Literal('#')
    
    block = Forward()
    macro = decorator + Word(alphas).setResultsName('name') + block.setResultsName('block')
    macro.setParseAction(lambda t: Macro(t.name, t.block))
    
    text = Regex('[^{}#]+')
    content = macro ^ text
    
    block << lbrace + ZeroOrMore(content).setResultsName('content') + rbrace
    block.setParseAction(lambda t: Block(t.content))
    
    test = '#i{italic text #b{bolded}\n  \nfoobar\n\nbaz}'
    
    print macro.parseString(test)


and I get the following output:



    [#i { italic text #b { bolded }foobar
    
    baz }]


The problem is that there should be '\n\n' before 'foobar' just like there is afterwards. I suspect the problem is that all whitespace is stripped after the macro and before the next text.

As an aside, I've also noticed strange behavior. If I change 'text' to:



    text = OneOrMore(~lbrace | ~rbrace | ~decorator)


I get an infinite loop, which is similar behavior I saw when I was trying to use NestedExpr with 'OneOrMore' for the content.

#### 2011-09-26 05:46:54 - ptmcg
I think your OneOrMore(~lbrace ...) expression is on the right track, although the flaw here is that ~expr is simply a negative lookahead - it doesn't actually consume any text to advance the parser, so if you are at a place that doesn't match any of the 3 lookaheads, the parser will just stay there forever.

But take a look at CharsNotIn.  The Regex you have has the default whitespace-skipping behavior, that before it tries to match, pyparsing will skip over leading whitespace.  So you could try modifying your Regex by using leaveWhitespace, as in:


    text = Regex('[^{}#]+').leaveWhitespace()

Or you could use CharsNotIn, which is one of the few pyparsing constructs that does not skip leading whitespace:


    text = CharsNotIn('{}#')

CharsNotIn acts like sort of an anti-Word, so, unlike your OneOrMore expression, CharsNotIn *does* consume characters from the input string, as long as they don't match one of the given characters in the expression.

CharsNotIn is a little off the beaten path in the family of pyparsing classes, but I think it fits perfectly here.

Interesting parser, could you post it to the Feedback or one of the public pages?

-- Paul
#### 2011-09-26 08:17:39 - bramsc
Awesome, both methods work (though looking through the debug trace CharsNotIn looks to be a bit more efficient). Thanks for the help, and being so quick to respond!

I'd be happy to put the code up. It's a *very* pared down version of a grammar I'm working on for documentation preparation system. I'm not sure how useful the system is to anyone else, but was written to fill a need I've had for a while of taking technical notes without having to use LaTeX (great for writing papers, not so much for notes). The idea is that you can write a quick macro in Python and then invoke it in your document. For example:



    #doc(type='html') {
    I have some important equation I need to display:
    
    #eqn(inline=False, label='eqn1') {:
    E = mc^2
    :}
    
    You will should already be familiar with equation #ref('eqn1').
    }
    
    #table {
      #hline
      #row { | {a} | {b} | }
      #hline
    }


In this case, '#table', '#eqn', and '#document' are predefined macros, but it's easy enough to write a '#vennDiagram' macro if you needed it. If this does interest anyone, the code is currently available at:



though I should warn the code is still in its infancy and only the code from the repo is worth considering.

---
## 2011-09-28 18:25:41 - stalwarts - Help on improving performance ...

I have a 10 MB file which has around 60,000 lines. Here is the sample of few lines 



    [a30]ZTimer          [Jul 07 12:23:36] SUBS     ZSubscriptionManager::onTimeout() =>   Closure:  3
    [133c]ZReporting-Pro [Jul 07 12:23:36] HTTP     ZHTTPTunnelStream::openRequestStream Opening HTTP session to: http://dir-g.kontiki.com:80/zbbpreq/1310041416546875
    [a28]KDPSched        [Jul 07 12:23:36] PEER_KDP ZPeerKDPMonCast::reschedule starting
    [a30]ZTimer          [Jul 07 12:23:36] SUBS     ZSubscriptionManager::onTimeout() Priority: #   3
    [133c]ZReporting-Pro [Jul 07 12:23:36] HTTP     Connecting to http://dir-g.kontiki.com:80/zbbpreq/1310041416546875
    [a28]KDPSched        [Jul 07 12:23:36] PEER_KDP ZPeerKDPMonCast::reschedule finished
    [a30]ZTimer          [Jul 07 12:23:36] SUBS     ZSubscriptionManager::resolve()
    [133c]ZReporting-Pro [Jul 07 12:23:36] HTTP     Opening http://dir-g.kontiki.com:80/zbbpreq/1310041416546875
    [a30]ZTimer          [Jul 07 12:23:36] SUBS     ZSubscriptionManager::processList() -> Begin
    [a30]ZTimer          [Jul 07 12:23:36] SUBS     ZSubscriptionResolution() -> Res #5


This logs follows a 4 Field pattern [Thread Name] [Date Time] [LOG CHANNEL] [Text..].

I have the following grammar defined. 

    
    # Grammar to parse the [Date Time] Field
    month = Word(alphas, ax=3)
    day = Word(nums,max=2)
    timeDigit = Word(nums,max=2)
    time =  Combine (timeDigit + ':' + timeDigit + ':' + timeDigit)  
    # delimited List will ignore spaces in between elements
    dateTime = Combine (delimitedList('[' + month +  day +  time +']'),adjacent=False) # This will match [Jul 07 12:23:36]

    threadName = SkipTo(dateTime) # First field in the Log , lets skip to the Date ..everything in between is the thread name

    logChannel = Word(alphas+'_') # this will match the log Channel , like SUBS or PEER 

    EOL = SkipTo('\n') # Once we match LOG CHANNEL , from here till New line is the actual Log Text

    logLine =  delimitedList(threadName + dateTime + logChannel + EOL) # A Combined parser which returns a List of Lists
    


Here is the code which parses the data from file



    def parseLog(self,filename):
          logList = []
          errorLogFile = open(filename,'rU')
          for line in errorLogFile:
            if line.strip():
              try:
                parsedLine = self.logLine.parseString(line)
                logList.append([parsedLine[0],parsedLine[1],parsedLine[2],parsedLine[3]]) 
              except ParseException:   # if there is an exception in parsing , I add an empty list, this is to capture lines which will not have a thread/date and channel information , an example is given below
                logList.append(['','','',line])
                continue
          return logList



#### #################
Example mentioned in the above code


    [123c]ZPeerServerRea [Jul 07 12:23:39] DMSMGR   ***** begin AutoLogResolveKIDFromHostFailureg **********
                                                    !!!!!!!!!!!Unable to getResolverInfo!!!!!!!!!!!!
                                                    ***** end AutoLogResolveKIDFromHostFailureg **********
    [123c]ZPeerServerRea [Jul 07 12:23:39] DMSMGR   ResolveKID urn:kid:utc:moid:0 failed - 80004005

#### #################


Now , here is the question: for a 10 MB file which has around 60,000 lines it takes ~60 seconds to parse this log, is there any way I can improve this performance  ? Can someone suggest a better grammar - perhaps don't use certain grammar compared to the other etc. 
I'm fairly new to PyParser and I like it a lot for its ease of use. Any help would be greatly appreciated ..

#### 2011-09-28 23:30:57 - ptmcg
This won't be a huge gain, but might save you 10% or so of performance. I've found that building up simple terminals and simple primitives with Combine, Word, etc. (like realNumber = Combine(Word(nums) + '.' + Word(nums) + Optional(oneOf('e E') + Word(nums)) really works better using a Regex, without too much compromising of grammar maintainability.  In your grammar, what kind of improvement do you get if you change dateTime to:


    dateTime = Regex(r'\[[A-Z][a-z]{2} \d\d \d\d:\d\d:\d\d\]')


Only do this though to replace expressions you build up with Combine - if you have just simple Word or oneOf expressions, these will already create regular expressions internally, so Regex won't be much of a win. Also, you could try using the provided restOfLine in place of SkipTo('\n').

-- Paul
#### 2011-09-29 11:13:19 - stalwarts
Paul,

Thank you for the suggestion, surprisingly i ended up having a 50% improvement by just changing the dateTime in to Regex. 

So i took your second suggestion and used 'restOfLine' and that gave another 50% improvement from the previous test ..

So now it takes 15 seconds instead of 60.
Thanks a lot, hope people can learn from this ...

---
## 2011-09-29 23:20:26 - stalwarts - parser help
I have the following sting and i want to extract 'node' from it , i don't want to use the python url parser. Can someone help ... 

node is followed by Alpha Nums which also includes '/' and '+' which has a min length of 65 chars ...

Here is the line


    
    NETMGR   contacting net mgr: https://web-g.kontiki.com/zodiac/servlet/zodiac?action=DCI&client_moid=fbacaf04-de48-e0f8-8563-c264dfffa091&arena=0&timestamp=20110512052008Z&node=qZtX8rVLJYi4T%2bMQYbrTsNle3sofefeyy%2fdHmPdy2HT584xdsT6dgCEj6lKZ108g
    


here is the code i have



    
     nodeIDStart = Literal('&node=')
     nodeID = Word(alphanums+'/+',min=65)
     seldNodeID = nodeIDStart + nodeID
    



#### 2011-09-29 23:35:14 - stalwarts
False alarm ... missed '%' in nodeID String
#### 2011-09-30 03:01:31 - ptmcg
The Examples page on this wiki includes a pyparsing version of urlparse. Once you get the query string, you can convert it to a dict using:


    def parse_query(qstring):
        if qstring:
            queryparam = Word(printables, excludeChars='&=') + Optional(Suppress('=') + Word(printables, excludeChars='#&'))
            queryExpression = Dict(delimitedList(Group(queryparam), delim='&'))
            return queryExpression.parseString(qstring, parseAll=True).asDict()
        else:
            return {}


-- Paul

---
## 2011-09-30 08:25:58 - fidlej - Invalid inputs
I'm confused by the following usage note:
'it is possible that your defined matching patterns may accept invalid inputs. Use pyparsing to extract data from strings assumed to be well-formatted.'

Does it refer to a limitation of pyparsing?
I understand how to define a grammar.
Is pyparsing accepting also an input not valid in the grammar?

Is it safe to use pyparsing for validation of user inputs?
For example, to parse search queries.

Thanks for the library. It is expressive enough for my needs.

#### 2011-10-01 07:47:23 - ptmcg
It is possible to define an overly-permissive grammar. For instance, you might define a yy/mm/dd date simply using Word(nums+'/'), but this would also accept many strings that don't look anything like a date. This is not a limitation of pyparsing, it is just a caution to grammar writers - it is common to have unexpected assumptions about data affect our view of the inbound data, but the grammar needs all of these assumptions to be made explicit, and that is the challenge in writing good grammars/parsers.

Search query parsing is becoming a more and more common application for pyparsing - a sample query parser can be found on the Examples page of this wiki, and you can also find a better-packaged module in the Whoosh library.

Glad you are finding pyparsing enjoyable!

-- Paul
#### 2012-05-13 18:21:34 - Relsqui
Just wanted to mention that I joined to ask this exact same question--my project partner and I came across that snippet and were worried about whether pyparsing would be safe to use for our purposes. I'm glad to see this reassurance, but perhaps consider rewording the line to something which matches the more verbose caution you gave here?

---
## 2011-10-05 11:11:40 - graingert - Can't seem to get parseaction working on a originalTextFor token

<pre>
    #Parsing the date
    abrWeekday = oneOf('Mon Tue Wed Thu Fri Sat Sun')
    abrMonth = oneOf('Jan Feb Mar Apr May Jun Jul Aug Sept Oct Nov Dec')
    dayOfMonth = Word(nums)
    hour24 = Word(nums)
    minute = Word(nums)
    second = Word(nums)
    timeZoneName = Word(alphas)
    yearWithCent = Word(nums)

    #Parse the date, use datetime to convert into a UTC iso string - as 
    #json requires all date/times to be in UTC
    #Yep that's right two different date formats! (this one doesn't have a timezone either)
    global date_tkn
    date_tkn = originalTextFor(
        (abrWeekday + abrMonth + dayOfMonth + hour24 + ':' + minute + ':' + second + timeZoneName + yearWithCent) |
        (abrWeekday + dayOfMonth + abrMonth + hour24 + ':' + minute + ':' + second + yearWithCent)
    ).addParseAction(lambda s,l,t:  dateutil.parser.parse(t[0]).isoformat())
</pre>


<pre>

>>>parselog.date_tkn.parseString('Tue Oct 4 18:23:38 BST 2011')</li></ul></ul></ul>Traceback (most recent call last):
  File '<stdin>', line 1, in <module>
  File '/usr/lib/python2.6/site-packages/pyparsing-1.5.6-py2.6.egg/pyparsing.py', line 1021, in parseString
    loc, tokens = self._parse( instring, 0 )
  File '/usr/lib/python2.6/site-packages/pyparsing-1.5.6-py2.6.egg/pyparsing.py', line 921, in _parseNoCache
    tokens = fn( instring, tokensStart, retTokens )
  File '/usr/lib/python2.6/site-packages/pyparsing-1.5.6-py2.6.egg/pyparsing.py', line 675, in wrapper
    return func(*args[limit[0]:])
TypeError: <lambda>() takes exactly 3 arguments (0 given)
</pre>

#### 2011-10-05 13:03:22 - ptmcg
(You have to use [[code]] tags on this wiki, instead of <pre>.)

Okay, you correctly used addParseAction instead of setParseAction.  I suspect that you may be getting some exception in your lambda - did you try calling dateutil.parser.parse('Tue Oct 4 18:23:38 BST 2011').isoFormat() directly, to make sure your lambda will process correctly?  Also, originalTextFor returns a string, not a list, so you might want to just pass t instead of t[0].

-- Paul
#### 2011-10-05 15:18:00 - graingert
yep that fixes it - thanks!

I had to change to dateutils 1.5

---
## 2011-10-05 11:26:57 - rmayr - Location of parsed tokens
Hi,
I would like to annotate location information to tokens returned by PyParsing. Monkey patching a decorator for postParse() did work.
But is there a nicer way? Any suggestions?

Thanks
Roland

#### 2011-10-05 12:58:17 - ptmcg
Do you want this on *every* set of matched tokens, or just on particular ones?  You could define your own subclass of TokenConverter that contains your postParse logic, and then just wrap any expression with your subclass.  In fact, that sounds like such a clean solution, I think I'll add that to the to-do list for the next release...  If you want it on all matched tokens, I would say that your monkeypatch is the 'cleanest'.
#### 2011-10-05 15:25:24 - rmayr
Thanks Paul!

---
## 2011-10-11 02:01:33 - zukifoo - nestedExpr() and newlines howto?
one liners are working fine with this code:


    quotedString.setParseAction(removeQuotes)
    pattern = Literal('depends=') + nestedExpr(ignoreExpr=quotedString)


how would I extend to handle multi-line expressions? like this:

    depends=('foo1' 'foo2' 'foo3' 'foo4')

#### 2011-10-11 05:21:05 - ptmcg
This parsed fine for me, I get:


    ['depends=', ['foo1', 'foo2', 'foo3', 'foo4']]


What version of pyparsing are you using?
#### 2011-10-11 12:10:48 - zukifoo
ah, you're right.

sorry, it was something other than pyparsing causing this.

---
## 2011-10-14 19:45:58 - cadourian - Handling carriage return
Hello,

I have the following parser definition:

    S-ESCAPE =  '\?' | '\n' | '\r' | '\t' | '\v' 

Notice the \r, \n escape characters
I am unable to write a parser for this that works

Tried using the following line:
ParserElement.setDefaultWhitespaceChars(' \t')

but it didn't help.

The example I'm working from is:

    from pyparsing import *

    ParserElement.setDefaultWhitespaceChars(' \t')

    statement = (Literal('foobar') | Word(nums)) + LineEnd().suppress()
    statements = ZeroOrMore(statement)
    document = StringStart() + statements + StringEnd()

   test = '5498\n\r foobar'
    print test, '->', document.parseString(test)

Any Ideas?

#### 2011-10-14 20:15:49 - cadourian
Hi,

I improved things a little bit by using the following code:

    ParserElement.setDefaultWhitespaceChars('')
    S_ESCAPE= Word('\'\'\?\\\a\b\f\n\r\t\v',exact=1)

This code successfully works for all the characters in;

    ['\'','\'','?','\\','\a','\b','\f','\n','\r','\t','\v']

except for '\t'

#### 2011-10-15 00:25:56 - ptmcg
I'm not completely clear on what S_ESCAPE is supposed to do here. In your first message, it looks like a definition of escaped whitespace expressions. But in your second message, it is expanded to include things like escaped quotation marks too, so now it looks like escaped expressions you might find inside a quoted string. So I'm not exactly sure how to interpret where the S_ESCAPE is supposed to be found in your test string, making it hard to write a parser.

For instance, I got your example to work just by changing your setDefaultWhitespaceChars line to:


    ParserElement.setDefaultWhitespaceChars(' \t\r')


But I don't know how that relates to S_ESCAPE. What is S_ESCAPE for in your larger grammar?

Also, if you are in fact parsing escaped characters from a larger string, and you expect to see the backslashes in your input, be sure to use raw string literals, like r'\n\t\r'. And lastly, if you are trying to create an expression with whitespace characters, consider using the White class instead of Word.

-- Paul
#### 2011-10-17 07:25:41 - cadourian
Hi Paul,

thanks for the info. You have a point in terms of S_ESCAPE being incoherent with the rest. I have copied the definition of S_ESCAPE from a grammar definition. After some exploration, I came to the conclusion that the following
['\'','\'','?','\\','\a','\b','\f','\n','\r','\t','\v']

was actually meant to be interpreted 'raw' like you suggested. So, we have to use r'\'', r'\''...
This makes more sense.

I am however not clear on the meaning of 

    ParserElement.setDefaultWhitespaceChars(' \t\r')

Its my understanding that this assumes that the following are token separators ' ', '\t' and '\r'. Is that what the setDefaultWhiteSpaceChars does?

thanks
#### 2011-10-18 01:00:38 - ptmcg
I wouldn't exactly call them 'separators', as there is no requirement that they be inserted between tokens. Instead, think of them as 'ignorable characters'.  For instance, to parse a word and an integer, I can write:


    word = Word(alphas)
    integer = Word(nums)
    grammar = word + integer


grammar can now parse 'ABC 123' into ['ABC','123'].  But you'll get the same result from parsing 'ABC123'.  There doesn't *have* to be whitespace between the word and the integer, the word naturally stops when it hits the first non-alpha character. On the other hand, if my grammar were:


    grammar = OneOrMore(word) + integer


'ABC123' would still parse the same, but if I wanted to parse a second word 'DEF' before the integer, there would have to be some whitespace after 'ABC' to terminate the matching of the first word before starting the second. 

By telling pyparsing what the default whitespace characters are, you don't have to write:


    whitespace = Suppress(' \t\n\r')
    grammar = OneOrMore(word + Optional(whitespace)) + Optional(whitespace) + integer



---
## 2011-10-15 13:51:30 - projetmbc - Fist steps with pyparsing
Hello, for my first use of pyparsing I would like to parse expressions like :

<ul class="quotelist"><li>f(x+y**'Some text with (parenthesis)')</li></ul>

The tree structure of this is the following one :

<ul class="quotelist"><li>f<ul class="quotelist"><li>() I need to know that parenthesis has been used<ul class="quotelist"><li>+<ul class="quotelist"><li>x</li><li>**<ul class="quotelist"><li>[]<ul class="quotelist"><li>y-5</li><li>'Some text with (parenthesis)'</li></ul></li></ul></li></ul></li></ul></li></ul></li></ul>

#### 2011-10-15 16:07:09 - ptmcg
If this is your first effort at pyparsing, then I suggest the first step is to *not* start writing code, but to write out a good definition of the grammar you plan to implement. Your expression looks like normal 5-function arithmetic, but the inlcusion of a quoted string as what looks like an exponent is definitely unexpected. It's also not clear where 'y-5' comes from in your input string. This gives a roadmap to work from in defining your pyparsing constructs, it helps you think through how you might have conflicts or ambiguities in your grammar, and it gives an implementation checklist so you know when you are done.  Then as you start writing test cases, don't jump right to the most complex expression possible, but build up to it, like:


    f(x)
    x+y
    y**'test'
    f(x+y)
    f(y**'test')
    ... and so on ...


You can find a sample 5-function infix parser at this wiki's Examples page, (titled fourFn.py). Since you are including exponentiation, I suggest that you *not* use the operatorPrecedence examples (since repeated exponentiation should be evaluated right-to-left, but operatorPrecedence parses it with left-to-right evaluation.
#### 2011-10-16 02:22:50 - projetmbc
Thanks for the example, I'll try to adapt it to my example.

PS : You're right with y-5, I've forgotten to change it...

---
## 2011-10-18 07:56:02 - almoni - NotAny Question
Hey.

I parse code that contains variables of the form /[a-zA-Z_][a-zA-Z0-9_]*/, and some reserved keywords. I do:

    keywords = oneOf('button ...')
    v = ~keywords + Word(alphas+'_', alphanums+'_')

But then variable names such as 'button_text' or 'buttons' do not work. How can I reserve the keywords in the list only, and not anything that contains them as a substring?

Thanks.

#### 2011-10-18 07:57:21 - almoni
I missed the formatting comment. Read:



    keywords = oneOf('button ...')
    v = ~keywords + Word(alphas+'_', alphanums+'_')


#### 2011-10-18 19:07:20 - ptmcg
Instead of oneOf, which is a short cut for defining a bunch of Literals, try Keyword:


    keywords = map(Keyword, 'button ...'.split())


#### 2011-10-19 04:49:25 - almoni
I do:



    keywords = map(Keyword, 'button ...'.split())
    v        = ~keywords + Word(alphas+'_', alphanums+'_')


PyParsing complains:



    bad operand type for unary ~: 'list'


To make it clear, what I want is to avoid vairable names (anything v parseStrings) from being words in the list keywords. How can I do this?

Thanks.
#### 2011-10-19 05:46:08 - ptmcg
Sorry, that was dumb. What I meant to do was:

- split 'button ...' into a list

- create a Keyword from each list element (as opposed to a Literal, which is the equivalent of what oneOf does)

- use the list to create a MatchFirst (as if you manually did 'expr | expr | expr | etc.')

So my missing bit was:


    keywords = MatchFirst(map(Keyword, 'button ...'.split()))

or since the list is so small, just do:


    keywords = Keyword('button') | Keyword('...')


#### 2011-10-19 08:59:23 - almoni
Perfect. Thank you.

---
## 2011-10-21 18:03:40 - cadourian - Extracting a hierarchical tree out of a parsed result
Hello,

I wrote the grammar using pyparsing for a programming language called Modelica. The parser is working. i.e. it can (in general) correctly identify the code.

Once I have parsed the code, I need to explore the parseResults and extract the tree of rules that have been detected.

For example assume the following rules
A := B + C
B := b1 + b2
C := C1 + C2 | C3 + C4 + C5

If I use 
p = A.parseString(str) 
to get the parse results, I want to then have a tree that reflects what was detected.

I have tried using 'setResultsName()' to name the various tokens and grammar rules, I have used asDict() to convert the parseresults to a dictionary and to some extent this works. 

However, I have many circular rules like:

    A = B + A

and they seem to cause trouble.

Anyone has a good example of a solution?

For example the following return value would be good:

    res= [A : {B: B_val, C: C_val}]

where

    B_val = [B1: B1_val, B2: B2_val]
    C_val; = [C3: C3_val, C4: C4_val, C5: C5_val]

Thanks

Chah

#### 2011-10-21 18:07:26 - cadourian
Maybe the examples here:


Any other suggestions?

Thanks
#### 2011-10-21 18:59:39 - cadourian
I wanted to provide more information as to where I am. For example, I can print the following information:

<ul><ul><li>Input: 5+10</li></ul></ul>
<ul><ul><ul><ul><li>(key: value) = ( arithmetic_expression : ['5', '+', '10'] , type: <class 'pyparsing.ParseResults'> )</li><li>(key: value) = ( term : ['10'] , type: <class 'pyparsing.ParseResults'> )</li><li>(key: value) = ( add_op : + , type: <type 'str'> )</li><li>(key: value) = ( primary : 10 , type: <type 'str'> )</li><li>(key: value) = ( factor : ['10'] , type: <class 'pyparsing.ParseResults'> )</li></ul></ul></ul></ul><strong>SS</strong> Result = SUCCESS with parse result: ['5', '+', '10']

Where I have an input '5+10' and I parse it. Each rule has an attached ResultsName, then I print the dictionary keys and values. 

the problem is that the result is flat. I want to nest the results based on the grammar rule nesting that was detected and printed

Chahe
#### 2011-10-22 01:14:49 - ptmcg
I don't really see what setResultsName would do for you here, except to give you the lhs and rhs parts of the rule definition itself. If you want to extract a hierarchical tree, then look at using Group or operatorPrecedence.

Here is a simple parser for your test rules using operatorPrecedence:



    from pyparsing import *
    
    
    tests = '''\
     A := B + C
     B := b1 + b2
     C := C1 + C2 | C3 + C4 + C5'''.splitlines()
    
    var = Word(alphas, alphanums)
    
    expr = operatorPrecedence(var, 
      [
      ('+', 2, opAssoc.LEFT),
      ('|', 2, opAssoc.LEFT),
      ])
    
    ruleExpr = var('lhs') + ':=' + expr('rhs')
    
    for t in tests:
        rule = ruleExpr.parseString(t)
        print rule.dump()
        print
    


prints:



    ['A', ':=', ['B', '+', 'C']]
    - lhs: A
    - rhs: ['B', '+', 'C']
    
    ['B', ':=', ['b1', '+', 'b2']]
    - lhs: B
    - rhs: ['b1', '+', 'b2']
    
    ['C', ':=', [['C1', '+', 'C2'], '|', ['C3', '+', 'C4', '+', 'C5']]]
    - lhs: C
    - rhs: [['C1', '+', 'C2'], '|', ['C3', '+', 'C4', '+', 'C5']]
    


-- Paul
#### 2011-10-31 13:44:03 - cadourian
Hi Paul,

I needed to get an object instead of a string. The closest way was to use asXML() and then create a nested dictionary list from it. Instead I created a modified version of asXML() which I called asDict2() and inserted into the pyparsing.py. Here it is if its of any use to others:

    def asDict2( self, doctag=None, namedItemsOnly=False, indent='', formatted=True ):
        '''Returns the parse results as a nested Dictionary. Dictionary is created for tokens and lists that have defined results names.'''
        nl = '\n'
        out = []
        namedItems = dict( [ (v[1],k) for (k,vlist) in self.`__tokdict.items()
                                                            for v in vlist ] )
        nextLevelIndent = indent + '  '
        # collapse out indents if formatting is not desired
        if not formatted:
            indent = ''
            nextLevelIndent = ''
            nl = ''

        selfTag = None
        if doctag is not None:
            selfTag = doctag
        else:
            if self.__`name:
                selfTag = self.`__name

        if not selfTag:
            if namedItemsOnly:
                return ''
            else:
                selfTag = 'ITEM'

        outVal =[]
        worklist = self.__`toklist
        for i,res in enumerate(worklist):
            if isinstance(res,ParseResults):
                if i in namedItems:
                    docTag = namedItems[i]
                else:
                    docTag = None

                outVal.append(res.asDict2(docTag,
                                    namedItemsOnly and doctag is None,
                                    nextLevelIndent,
                                    formatted))
            else:
                # individual token, see if there is a name for it
                resTag = None
                if i in namedItems:
                    resTag = namedItems[i]
                if not resTag:
                    if namedItemsOnly:
                        continue
                    else:
                        resTag = 'ITEM'
                xmlBodyText = _xml_escape(_ustr(res))
                outVal.append({resTag : xmlBodyText})

        outDict={selfTag : outVal}
        return outDict

---
## 2011-10-28 16:07:02 - stalwarts - XML Parser Help
I have two variants of an XML Snippet repeated multiple times in a log file. Here are the two types.

Type 1:


    <ZStatus endtime='2011-Oct-03 22:11:13' starttime='2011-Oct-03 22:10:54' />


In the Above example the XML is ended with />

Type 2:


    <ZStatus endtime='2011-Oct-03 19:45:14' starttime='2011-Oct-03 19:45:04' >
                                                             <zserver endtime='9' starttime='4' />
                                                             <zserver endtime='9' starttime='4' />
                                                             </ZStatus>
    


In the Above example the XML is ended with </ZStatus>

Here is the parser Im using and it has shortcomings 



    zStatusParser = Literal('<ZStatus') + SkipTo('</ZStatus>')


What the parser currently doing.

1.    It matches the Type 2. 
2.    It doesnt match the type 1 exclusively instead it will return 



    
    <ZStatus endtime='2011-Oct-03 22:11:13' starttime='2011-Oct-03 22:10:54' />
    Other lines from log
    Other lines from log
    Other lines from log
    Other lines from log
    <ZStatus endtime='2011-Oct-03 19:45:14' starttime='2011-Oct-03 19:45:04' >
                                                             <zserver endtime='9' starttime='4' />
                                                             <zserver endtime='9' starttime='4' />
                                                             </ZStatus>


The Result according to the parser is correct, it matches the ZStatus from type1 and it skips to the </ZStatus> in type 2 by including unwanted error log lines.

Is it possible to write a single expressing which will match only the XML and not return the result above.

I want a parser which can match 'Type 1' and 'Type 2'.

#### 2011-10-29 09:06:44 - ptmcg
SkipTo has an optional argument failOn which should cause the SkipTo to fail if the given expression is found while skipping. You want your 'skip to the end tag' to really read 'skip to the end tag, except if I find a start tag first' - that's what failOn is for. So try SkipTo with failOn=Literal('<ZStatus').

Also, please check out the helper method makeXMLTags, which will give you much better matching and results names for attributes and their values.

Here's another approach you might take, using makeXMLTags, and explicit testing for empty tags (XML tags with no body and no separate closing tag are 'empty' tags):



    from pyparsing import *
    
    sample = '''<ZStatus endtime='2011-Oct-03 22:11:13' starttime='2011-Oct-03 22:10:54' />
    Other lines from log...
    Other lines from log...
    Other lines from log...
    Other lines from log...
    <ZStatus endtime='2011-Oct-03 19:45:14' starttime='2011-Oct-03 19:45:04' >
                                                             <zserver endtime='9' starttime='4' />
                                                             <zserver endtime='9' starttime='4' />
                                                             </ZStatus>'''
    
    zstatus, endZstatus = makeXMLTags('ZStatus')
    def mustNotBeEmpty(tokens):
        if tokens.empty:
            raise ParseException('empty tag')
    zstatus.setParseAction(mustNotBeEmpty)
    
    def mustBeEmpty(tokens):
        if not tokens.empty:
            raise ParseException('not an empty tag')
    emptyZStatus = zstatus.copy().setParseAction(mustBeEmpty)
    
    zstatusWithBody = zstatus + SkipTo(endZstatus) + endZstatus
    
    for match in (zstatusWithBody | emptyZStatus).searchString(sample):
        print match.dump()
        print


#### 2011-11-01 13:01:03 - stalwarts
Thanks a lot !

I was able to use this filter 



    
    zStatusParser = Literal('<ZStatus') + (SkipTo('/>',failOn='zserver') | SkipTo('</ZStatus>')) + restOfLine
    
    instead of zStatusParser = Literal('<ZStatus') + SkipTo('</ZStatus>').
    



---
## 2011-10-31 04:13:18 - marcmolla - Sequencial parsing
Hi all!

I'm trying to do a 'sequential parsing' like for example:

    word = Word(alphas)
    number = Word(nums)
    
    test_string = 'word 234 anotherWord'

If I parse the string with *word*:

    >>>word.parseString(test_string)
    (['word'], {})

There is any method to obtain the rest of the string for continuing the parsing? In this example: '234 anotherWord'

Thanks!

#### 2011-10-31 05:59:47 - ptmcg
Yes, you need to define an expression which will consume the rest of the input string. You used this expression:


    word

So all you got was the first word in the string.

Here are a couple of expressions that would read the whole thing:


    word + number + word
    word + OneOrMore(word | number)
    OneOrMore(word | number)
    word + restOfLine


Depending on what you want to get for the rest of the input string, choose one of these expressions.

-- Paul
#### 2011-10-31 08:25:35 - marcmolla
Thanks! 

restOfLine function is the best solution for my problem.

---
## 2011-11-01 05:12:16 - freshkiwi - doing something wrong
My code:


    from pyparsing import *
    
    text = Word(printables)
    
    script = OneOrMore(Group(Literal('DATA_') + text + Literal('=') + text))
    
    parsed = script.parseString(open('data.txt', 'r').read())
    
    print parsed[[code]]
    
data.txt:

    DATA_1 = Hello World
    DATA_MyFile1 = Another string
    DATA_01=A

I want it to output this:

    ['DATA_', '1', '=', 'Hello World']
    [DATA_', 'MyFile1', '=', 'Another string']
    [DATA_', '01', '=', 'A']

#### 2011-11-01 16:58:51 - ptmcg
Try changing script to:


    script = OneOrMore(Group(Literal('DATA_') + text + Literal('=') + restOfLine))


#### 2011-11-01 22:09:53 - freshkiwi
OK, what if I'm not sure if each will be on a new line and not be on a single line?
#### 2011-11-01 22:42:21 - ptmcg
The problem with your original expression was that 'text' reads in only a single word, and 'Hello World' is two words.  So your expression matched the leading 'DATA_1 = Hello', but then the next word was 'World', which does not start with 'DATA_'.

Now if you have multiple entries on one line, I guess it might be something like this:


    DATA_1 = Hello World DATA_MyFile1 = Another string DATA_01=A


You can still look at that and pick out the keys and values, but ask yourself how you know that? How do you know that 'World' is still part of the first value, but 'DATA_MyFile1' is the next key? Well, one way to tell is that 'World' doesn't start with 'DATA_' but 'DATA_MyFile1' does. So you could define your values as 'one or more words that don't start with 'DATA_''.  And that would look like this:



    key = 'DATA_' + Word(printables)
    text = ~key + Word(printables)
    key_value = key + '=' + OneOrMore(text).setParseAction(lambda t : ' '.join(t))
    script = OneOrMore(Group(key_value))


Note how we had to be more careful to exclude the '=' as a valid key character, since your last key-value pair had no whitespace around the '=' sign; otherwise your third key-value would have been included as part of the second value string. We also had to support repetition of the 'text' expression, so as to read in 'Hello World' as a valid value. I also added the parse string to rejoin the separately parsed words, with separating spaces.

Another way you might interpret that line is that any word that is immediately followed by an '=' sign must be the key to a new key-value pair. That would look like this:


    key = 'DATA_' + Word(printables, excludeChars='=')
    text = Word(printables, excludeChars='=') + ~FollowedBy('=')
    key_value = key + '=' + OneOrMore(text).setParseAction(lambda t : ' '.join(t))
    script = OneOrMore(Group(key_value))


Now we have to exclude '=' from value strings too.

This issue of lookahead is a common one for new pyparsing writers. Pyparsing does not do any forward matching/lookahead, the way regular expressions do for instance.  If you write 'OneOrMore(Word(printables))' in your grammar, there is a good chance that one expression will read the entire rest of your input string.  More likely, you would want something like 'OneOrMore(~specialWordMarkingTheStartOfTheNextGroup + Word(printables))', so that pyparsing knows when a word is really the beginning of the next structure to be parsed.

---
## 2011-11-03 13:00:51 - spastor2 - RightMost match or last match
Hi, 
I am new to Pyparsing and so far i ve been very much impressed (especially because i managed to handle very tortured case, and this, easily, thanks to PyParsing!)
Now i ve hit an issue, that might be quite easy to handle ... but i did not manage how to.
Here is the thing : 

I am trying to isolate : <airport from> and <airport to> from a sentence built that way : 
<airport from> a <airport to>


What i did so far is this : 



    upperCase=Word(string.uppercase)
    separator=WordStart()+oneOf(['a'])+WordEnd()+FollowedBy(upperCase)
    airportLabelFrom=(OneOrMore(~separator+Word(printables)))('airportLabelFrom')
    airportLabelTo=(OneOrMore(Word(printables)))('airportLabelTo')
    whole=airportLabelFrom+separator+airportLabelTo
    


it works well with : 

    'Nice (Terminal 2) a Paris Orly (Terminal Sud)'

But will not work with 

    'Nice a Toto (Terminal 2) a Paris Orly (Terminal Sud)'

How could i specify that i want to match only the rightmost (last match) only ?
as in i want the separator 'a' located near 'Paris'  and not the a of 'a Toto'

Thanks in advance and thanks for this tool.

Seb

#### 2011-11-04 03:38:02 - spastor2
Quick update ... I ve got the feeling that my problem resides in NotAny stopping sooner than what i would want. Is there a way to tell NotAny to act greedy? Or am i going completely in a wrong way ?

Thanks for any help
#### 2011-11-04 06:23:27 - ptmcg
Your hunch is correct, but it really is not about greediness. Pyparsing is a pure left-to-right parser, processing input as it goes through the grammar, with just as much or as little lookahead as you define in your parser.

I think what you've done is a reasonably good attempt to resolve some ambiguity in your airport names, using a negative lookahead to avoid having your airport name consume the intervening 'a' between departure and arrival - unfortunately 'a' followed by a capital is not sufficiently specific, as your 'Nice a Toto' case shows.

So yes, pyparsing is pretty much doing just what you've told it, that airport labels are word groups of printables as long as they are not the word 'a', and when reading 'Nice a Toto', that 'a' matches your given criteria of what the separator looks like.  Pyparsing does not try multiple paths to see if there is a later 'a' that will allow it to match more of the input.

I notice that both from and to airports also include gate information in ()'s.  Your parser would be better able to discriminate airports if you knew there would always be this parenthesized gate reference.  Then instead of airport names being OneOrMore(match-any-group-of-printables), you could make a tighter match on something like airportReference + gateReference, and then it would only look for 'a' after seeing a gateReference.

Here is a rework of your parser with some other pyparsing tips built in:


    airportName = Combine(OneOrMore(Word(printables, excludeChars='(')), ' ', adjacent=False)
    gateRef = QuotedString('(', endQuoteChar=')')
    
    airportGate = Group(airportName('airport') + gateRef('gate'))
    flightSpec = airportGate('depart') + Keyword('a') + airportGate('arrival')
    
    for t in tests:
        flightData = flightSpec.parseString(t)
        print flightData.dump()


Now you can access individual fields of the flights as flightData.depart.gate.

I am afraid, though, that you won't always get this gate info in ()'s, in which case your parser is going to have to get a lot smarter to include 'a' in the departure airport as long as there is a later 'a' in the input string. That will take a little more thinking, and is likely to be a slow parser to process.
#### 2011-11-04 06:29:09 - ptmcg
Ok, given my last comment, I cooked up this definition of separator which is more consistent with your first approach.  It works pretty much just like the prose description at the end of my last comment:


    separator = Keyword('a') + ~FollowedBy(SkipTo(Keyword('a')))


To come up with this, I kind of had to 'be the parser', and think what was it that really distinguished the first 'a' from the second, and that is the first 'a' had a later 'a' in the input string.

Unfortunately, this parser will also fail when you try to parse the return trip from Paris Orly back to Nice a Toto. As long as your separator is also indistinguishable from the content of an airport name, you are going to have this problem.
#### 2011-11-07 01:34:37 - spastor2
Hi Paul, 

Thanks very much for your detailed answers and your tips in them.
You are very right... as long as i cannot deal with a more distinguishable separator (i realized even a 'last match' approach would sadly fail anyway ...) it will be impossible to get a reliable parser. I just need to go another route for this i guess. (a list of reliable airport labels might be necessary here).

Thanks again and thanks for PyParsing!

Seb

---
## 2011-11-04 16:44:00 - chandreas - Need help with NotAny
Hi everyone, 

I started playing with pyparsing, but I have trouble understanding the NotAny expression.

Here is my code:



    word = Word(alphas)
    doe = Keyword('doe')
    
    expr = word + NotAny(doe)
    rev_expr = NotAny(doe) + word
    
    print expr.searchString('john doe homer simpson')
    
    print rev_expr.searchString('john doe homer simpson')


From what I understood, 'expr' should find any word not followed by 'doe' and 'rev_expr' any word not preceeded by 'doe'.

However, the result for 'rev_expr' is:



    [['john'], ['oe'], ['homer'], ['simpson']]


instead of:



    [['doe'], ['homer'], ['simpson']]


So what am I doing wrong ? Thanks in advance !

#### 2011-11-05 08:06:52 - ptmcg
The problem you are having is that you are using NotAny in isolation, combined with searchString's scanning behavior.

`NotAny` is good as part of a larger parser, often used in language parsers to avoid accidentally parsing keywords as variable or function names.  These parsers are run using parseString, which expects to have a definition for the full input string.

In contrast, searchString works through the input string looking for any substring match, character by character.  We see your negative lookahead working when pyparsing is positioned at the leading 'd' in 'doe' - the NotAny correctly fails on 'doe' and advances to the next position.  The confusion is that we humans tend to scan through a sentence word by word, so you would think the next postion is 'homer', but the next position is the 'o' in 'doe'.  At this point, the NotAny does *not* match, so the remaining 'oe' is returned as the matching text.

You can get a little more insight into this by setting debug on rev_expr (rev_expr.setName('rev_expr').setDebug()) before calling searchString.

---
## 2011-11-22 23:40:15 - jcronje - Where to begin to see why a parser is slow?
Hi!

I use a modified version of the S-expression parser:


    def __defineParser(self):
      LPAR, RPAR, LBRK, RBRK, LBRC, RBRC, VBAR = map(Suppress, '()[]{}|')
      decimal = Word('0123456789',nums).setParseAction(lambda t: toInt(t[0]))
      bytes = Word(printables)
      raw = Group(decimal.setResultsName('len') + Suppress(':') + bytes).setParseAction(self.__verifyLen)
      token = Word(alphanums + '-./_:*+=')
      base64_ = Group(Optional(decimal,default=None).setResultsName('len') + VBAR 
          + OneOrMore(Word( alphanums +'+/=' )).setParseAction(lambda t: b64decode(''.join(t)))
          + VBAR).setParseAction(self.__verifyLen)
      hexadecimal = ('#' + OneOrMore(Word(hexnums)) + '#')\
                      .setParseAction(lambda t: toInt(''.join(t[1:-1]),16))
      qString = Group(Optional(decimal,default=None).setResultsName('len') + 
                              QuotedString(''',multiline=True)).setParseAction(self.__verifyLen)
      simpleString = raw | token | base64_ | hexadecimal | qString
      real = Regex(r'[+-]?\d+\.\d*([eE][+-]?\d+)?|\d+[eE][+-]?\d+').setParseAction(lambda tokens: float(tokens[0]))
      token = Word(alphanums + '-./_:*+=!<>')
      simpleString = real | decimal | raw | token | base64_ | hexadecimal | qString
      display = LBRK + simpleString + RBRK
      string_ = Optional(display) + simpleString
      comment = ';' + restOfLine
      self.__parser = Forward()
      sexpList = Group(LPAR + ZeroOrMore(self.__parser) + RPAR)
      sexpList.ignore(comment)
      self.__parser << OneOrMore( string_ | sexpList )

The problem I have is that the parser is incredibly slow, it takes ~ 13 mins to parse a ~ 450kB file. I suspect it is due to the number of levels of nesting, but not really being a parsing expert, I do not know where to begin to search for why it is so slow. Any hints? I can supply a test data file if need be.

#### 2011-11-23 00:54:22 - jcronje
Never mind, it seems the program that I use is the culprit, running the same code in native python GUI is fast (~ 15 sec).
#### 2011-11-23 02:24:35 - ptmcg
Thanks for posting - I revisited this parser and found that there are a few errors, particularly in the parsing of base64 and raw elements - please get the latest version from the Examples page.

-- Paul

---
## 2011-11-23 07:49:04 - masura-san - Issue with grammar with empty line support
Hi,

Im having trouble defining a grammer that supports empty lines (multiple).
The grammar in the code sample states that every action should have a result.

When I parse specs1 I get a clean exception:
pyparsing.ParseException: Expected 'result' (at char 17), (line:2, col:1)

When I parse specs2 I get a doubtful exception
pyparsing.ParseException: Expected end of text (at char 33), (line:4, col:1)


I suspect that there is something wrong with my empty line specification.
Can you guys point out what I doing wrong?



    from pyparsing import *
    
    # Grammar specifications
    # ======================
    
    # set spaces and tabs as default whitespace characters
    ParserElement.setDefaultWhitespaceChars(' \t')
    
    # punctuation marks
    colon = Suppress(':')
    comma = Suppress(',')
    
    # lines
    lineEnd = LineEnd().suppress()
    emptyLine = lineEnd
    zeroOrMoreEmptyLines = ZeroOrMore(emptyLine)
    
    # other
    word = Word(alphanums)
    name = word('name')
    type = word('type')
    
    # result 
    result = Suppress('result') + colon + type + lineEnd
    
    # action
    typeAndName = Suppress('action') + colon + name + lineEnd
    action =  zeroOrMoreEmptyLines + typeAndName + zeroOrMoreEmptyLines + result
    
    # grammer
    grammar = OneOrMore(action | emptyLine)
    grammar.setDebug()
    
    # define and parse specs
    specs1 = '''\
    action:actionOne
    '''
    
    specs2 = '''\
    action:actionOne
        result:int
    
    action:actionTwo
    '''
    
    parsedSpecs = grammar.parseString(specs1, parseAll=True)



#### 2011-11-30 03:10:48 - ptmcg
Masura-san,

Sorry not to reply sooner. The problem you are describing (unexpected error location reported in the ParseException) has nothing to do with your empty line definition, but is a direct result of pyparsing's strict left-to-right parsing.

Here is a high level step-by-step of how pyparsing's nested evaluation works with your input:


    action:actionOne
        result:int
    action:actionTwo
    
    <match OneOrMore(action)>
        <match action>
            action:actionOne
                result:int
        <action match successful>
        <match action>
            action:actionTwo
        <action match failed, no result part>
    <match OneOrMore(action) successful, one action found>
    <match StringEnd, parseAll=True>
        action:actionTwo
    <match StringEnd failed, raise exception>


This has been a fundamental problem with pyparsing since the beginning, that constructs like OneOrMore, ZeroOrMore, and Optional will accept a ParseException assuming that the exception has been caused by reaching the end of the repetition, and so moves on to the next expression in the parser to match the next bit of text.

Here is a simpler case, a grammar consisting of one or more pairs of letters and numbers:


    letter = oneOf(list('abcdefghijklmnopqrstuvwxyz'))
    number = oneOf(list('0123456789'))
    pair = Combine(letter + number)
    pairs = OneOrMore(pair)
    
    print pairs.parseString('a1 b2 c3 d4 e')


This code parses fine, and does not raise an exception because at least one pair was found, but only 'a1' thru 'd4' are valid pairs, so the parser stops at 'e'.  If I change the parseString call to add parseAll=True, then pyparsing starts at the end of the last match, and sees if it has in fact parsed the whole string by testing to see if it is at the end of the input text. And the answer is 'no', because there is still more text, the trailing 'e', so pyparsing reports that the end of text was expected.

Sometimes this is sufficient, but sometimes, we want a more specific error, that in fact the problem was that there was no number after the letter 'e'.  About 2 years ago I added an ErrorStop syntax that raises a special syntax error exception, that causes pyparsing to stop parsing immediately, without continuing on with any repetition, optionals, or whatever.  In this case, once we read a letter, there *has* to be a following number. The easiest way to indicate this is to use a '-' instead of '+' when defining the pair expression:



    pair = Combine(letter - number)


Leaving everything else the same, when I parse the same input string with parseAll=True, I get this new exception:


    pyparsing.ParseSyntaxException: Expected Re:('[0123456789]') (at char 13), (line:1, col:14)


If I take the trouble to name the number expression using setName:


    number = oneOf(list('0123456789')).setName('number')


I get the even nicer-looking:


    pyparsing.ParseSyntaxException: Expected number (at char 13), (line:1, col:14)


Now, going back to your parser, I made this change in my version:


    action =  typeAndName - zeroOrMoreEmptyLines + result


That is, once a `typeAndName` are found, a result *must* follow or this is a syntax error.

You would think that every '+' should just be replaced with '-', but let's go back to our letter-number case.  What if we had another kind of pair that might be followed by '.'?  Then we would need to parse various alternatives beyond just letter-number, we wouldn't want a failed letter-number parse of 'x.' to terminate all parsing.  In this case, to add the '-' error stop, we would actually have to refactor our expressions so that after a letter, we *must* find either a number or a '.'.

Hope this was helpful, again, so sorry for taking this long to respond.

-- Paul

---
## 2011-11-30 01:16:14 - jimcollum - New user of pyparser
I've just discovered pyparser, and have been going through examples learning. I'm getting the basics, but am stuck when dealing with some more involved whitespace issues. How something like this

    Ip Address................................ 192.168.10.1

be broken up into two tokens (having whitespace defined as 'two . or more'

         jim

#### 2011-11-30 01:18:11 - jimcollum
... so the result would be [ 'Ip Address' , '192.168.10.1']
#### 2011-11-30 03:31:12 - ptmcg
Jim -

I wouldn't try to define your repeated '.'s as whitespace, but you do have a couple of alternatives.

One would be to just match it like any other token, and use suppress() to keep the parsed dots from actually being returned:



    s = 'Ip Address................................ 192.168.10.1'
    
    num = Word(nums)
    ip_address = Combine(num + ('.'+num)*3)
    
    # using the new tuple multiplication to specify 2 or more
    dots = (Literal('.')*(2,)).suppress()
    ip_address_entry = CaselessLiteral('Ip Address') + dots + ip_address
    
    print ip_address_entry.parseString(s)


Will print:



    ['Ip Address', '192.168.10.1']


You can get something very close to treating the dots as whitespace by telling pyparsing to ignore the dots expression wherever it might appear. Then you can define ip_address_entry as just:



    ip_address_entry = CaselessLiteral('Ip Address') + ip_address
    ip_address_entry.ignore(dots)


And again:



    print ip_address_entry.parseString(s)


Will print:



    ['Ip Address', '192.168.10.1']



Now let's say you have a whole bunch of these data elements, with a text label and some value, with intervening dots.  Adding dots to every expression is definitely something we want to avoid, as we could easily leave them out by accident.  But adding ignore(dots) to every expression is not much better.  Fortunately, we can just define ignore(dots) at a higher level in the parser, and pyparsing will ignore dots at all component expressions that make up that expression:



    # define lots of different types of entries, leaving out the dots
    entry1 = CaselessLiteral('entry1') + data1
    entry2 = CaselessLiteral('entry2') + data2
    ...
    ip_address_entry = CaselessLiteral('Ip Address') + ip_address
    
    # define an expression for all entries
    configuration = OneOrMore(entry1 | entry2 | ...and so on... | ip_address_entry)
    
    # ignore any dots
    configuration.ignore(dots)


Good luck with pyparsing!
-- Paul
#### 2011-12-01 00:31:34 - jimcollum
thank you!!! this had me blocked for a day trying to figure it out... it's also given me a better understanding of organizing the parsing!

jim
#### 2011-12-01 05:37:18 - ptmcg
Jim -

I was just re-reading my post. I think you will have faster parsing if you define dots as `Word('.', min=2)` instead of all those Literals.  Plus, repetition of the Literals would also allow intervening whitespace, which I don't think you want to permit. With Word, the '.' have to be contiguous.

Glad you are making headway,

-- Paul
#### 2011-12-01 16:21:07 - jimcollum
I tried the `Word('.',min=2)` since your first post and found it had worked.. didn't know why or if it was a better choice though... 

The parsing is being done on a very large string (output from a router's cli command showing some status). i only need a few dozen values from a few hundred, so rather than create that many entries individually, i created a list of the first words (used in SkipTo), and looped thru that extracting the corresponding values



    tokenToFind = Combine(secondWord ^ ipAddr ^ macAddr)
    AP_GENERAL=[
    'one',
    'two',
    'three'..... ]
    
    for each in AP_GENERAL:
        try:
            toParse = SkipTo(each).suppress() + word + dots + \
            tokenToFind + SkipTo(StringEnd()).suppress()
        apDict[apName].update({ toParse.parseString(moreApInfo)[0] : \
                toParse.parseString(moreApInfo)[1]})
    

(please bear with me.. i've been programming in C since the 80's.. just learning Python.. :)

I'm filling in various dictionaries with value pairs extracted this way.. and all has been going fine until I ended up with a '.' in the first word, the actual line being

      802.1x............................ Enabled

I've increased the scope of 'word' to


    word = Word(alphanums + ' /:-.')
    


(the others ' /:-.'  having been added for entries like

     Network Name (SSID).....................networkA

and are working fine. I suspect that the issue is a conflict with . in '802.1x'  and dots
#### 2011-12-01 22:46:25 - ptmcg
Jim -

Rather than try to spec out every label and value, maybe you can just use the delimiting dots, and the line-orientation of your data.  Here is a parser that pretty much just parses the data as 'stuff before the dots + dots + stuff after the dots up to the end of the line':



    from pyparsing import *
    from pprint import pprint
    
    source = '''\
    802.1x..................................Enabled
    Network Name (SSID).....................networkA
    Manufacturer............................Linksys'''
    
    
    dots = Word('.', min=2).suppress()
    entry = Group(SkipTo(dots) + dots + restOfLine)
    
    entries = OneOrMore(entry)
    data = entries.parseString(source)
    pprint(data.asList())


Giving:


    [['802.1x', 'Enabled'],
     ['Network Name (SSID)', 'networkA'],
     ['Manufacturer', 'Linksys']]


Note how the data is structured as nice key-value pairs.  You could easily make this a dict by just passing data to the dict constructor.



    print dict(data.asList())


which prints:



    {'Network Name (SSID)': 'networkA', '802.1x': 'Enabled', 'Manufacturer': 'Linksys'}



Pyparsing's ParseResults object also has dict-like properties and access modes. And there is a class named Dict that will dynamically assign the key-value data of a series of grouped tokens. We just take entries as it was before, and wrap it with Dict:



    entries = Dict(OneOrMore(entry))
    data = entries.parseString(source)
    print data.dump()
    print data.keys()
    print data['802.1x']
    print data.Manufacturer


prints:



    [['802.1x', 'Enabled'], ['Network Name (SSID)', 'networkA'], ['Manufacturer', 'Linksys']]
    - 802.1x: Enabled
    - Manufacturer: Linksys
    - Network Name (SSID): networkA
    ['Network Name (SSID)', '802.1x', 'Manufacturer']
    Enabled
    Linksys


And of course, you could just extract as a dict too.



    pprint(data.asDict())


Which gives us a dict much like the one before:


    {'802.1x': 'Enabled',
     'Manufacturer': 'Linksys',
     'Network Name (SSID)': 'networkA'}


-- Paul

---
## 2011-12-14 07:58:16 - techtonik - Split\/join without the loss of formatting
I wonder if it is possible to parse some text (config file, actually) and then assemble it back preserving all formatting, comments etc.?

For example, I want to edit nested 'name' field in the following PHP file. There are many lines in file that contain 'name' => 'value' string, so regexp parsing doesn't help here and a more structured approach is required. I don't want to lose comments (they are valuable) or formatting (readability counts, and also easier to compare).


    <?php
    -- 
    ```
    { Header
    **
     * some irrelevant stuff
     */
    -- }
    ```
    
    
    $config = array
    (
        ** automatically updated version of this configuration file */
        'version' => '$Rev$',
    
        'nested_settings' => array(
            'name' => 'value'
        ),
    );
    
#### 2011-12-14 18:51:01 - ptmcg
transformString is the method call that will do selected substitutions, as defined in a pyparsing expression and attached parse actions.  See the comments in the code below:
    
    
    
        from pyparsing import Literal, quotedString, originalTextFor
        
        source = '''\
        <?php
         { Header
         /
         * some irrelevant stuff
         */
         }
        
        $config = array
         (
         / automatically updated version of this configuration file */
         'version' => '$Rev$',
        
        'other_settings' => array(
         'name' => 'valueX'
         ),
        
        'nested_settings' => array(
         'name' => 'valueY'
         ),
         );'''
        
        
        # define the nested_settings expression
        ARROW = Literal('=>')
        LPAR,RPAR = map(Literal,'()')
        name_value_defn = Literal(''name'') + ARROW + quotedString('value')
        nested_settings_defn = originalTextFor(Literal(''nested_settings'') + ARROW + 
            'array' + LPAR + name_value_defn + RPAR, asString=False)
        
        # attach a parse action to convert the value quoted string
        def convert_value(t):
            old_value = t.value
            # implement code to here to convert old value to new value
            new_value = old_value.upper()
            return t[0].replace(old_value, new_value)
        nested_settings_defn.addParseAction(convert_value)
        
        # use transformString to parse for matches, replacing/suppressing tokens
        # as defined in the pyparsing expression
        print nested_settings_defn.transformString(source)


prints



    <?php
     { Header
     /
     * some irrelevant stuff
     */
     }
    
    $config = array
     (
     / automatically updated version of this configuration file */
     'version' => '$Rev$',
    
    'other_settings' => array(
     'name' => 'valueX'
     ),
    
    'nested_settings' => array(
     'name' => 'VALUEY'
     ),
     );


Note that transformString only changes the matched text - everything else passes through to the output string unchanged.

---
## 2011-12-14 23:03:30 - techtonik - [patch] ParseResults in API doc
Just a small patch to link ParseResults in API reference.



    
    Index: pyparsing.py
    ===================================================================
    --- pyparsing.py    (revision 213)
    +++ pyparsing.py    (working copy)
    @@ -793,7 +793,7 @@
                C{fn(loc,toks)}, C{fn(toks)}, or just C{fn()}, where:
                 - s   = the original string being parsed (see note below)
                 - loc = the location of the matching substring
    -            - toks = a list of the matched tokens, packaged as a ParseResults object
    +            - toks = a list of the matched tokens, packaged as a C{L{ParseResults}} object
                If the functions in fns modify the tokens, they can return them as the return
                value from fn, and the modified list of tokens will replace the original.
                Otherwise, fn does not need to return any value.
    @@ -3306,7 +3306,7 @@
            string containing the original parsed text.  
    
            If the optional C{asString} argument is passed as C{False}, then the return value is a 
    -       C{ParseResults} containing any results names that were originally matched, and a 
    +       C{L{ParseResults}} containing any results names that were originally matched, and a 
            single token containing the original matched text from the input string.  So if 
            the expression passed to C{L{originalTextFor}} contains expressions with defined
            results names, you must set C{asString} to C{False} if you want to preserve those



#### 2011-12-15 00:42:40 - ptmcg
Thanks!
#### 2011-12-22 10:48:28 - techtonik
Is it committed? My SVN checkout doesn't show that the change is integrated and I already have another fix in queue.
#### 2011-12-22 23:45:35 - ptmcg
Yes, changes are checked in now.  Please look over the other doc strings, I have extended your suggestion to a number of other links too.  Thanks again for posting.
-- Paul
#### 2011-12-23 03:09:41 - techtonik


    --- pyparsing.py    (revision 214)
    +++ pyparsing.py    (working copy)
    @@ -3383,7 +3383,7 @@
    
     def replaceWith(replStr):
         '''Helper method for common parse actions that simply return a literal value.  Especially
    -       useful when used with C{L{transformString}()}.
    +       useful when used with C{L{ParserElement.transformString}}.
         '''
         def _replFunc(*args):
             return [replStr]


It is a fix for epydoc warning when testing docs with:


    $ PYTHONPATH=. epydoc -o ../doc --html pyparsing -v


There is another warning, but I don't know how to fix it.
#### 2011-12-23 09:14:52 - ptmcg
Epydoc allows you to leave the shortened string as the actual displayed text, and add the fully qualified string in <>'s, like this: L{transformString<ParserElement.transformString>}.  I made this correction in both error places.  I've also checked into SVN the genEpydoc.bat file that I use to generate the htmldoc directory that ships with the pyparsing source distribution.  The command I use has a few different switches from the one you have been using.

Thanks for the assistance with docs. Would you care to take a stab at the docs for the wiki Examples page? This is an important page for newcomers to pyparsing, and I've never been able to properly organize it.

-- Paul
#### 2011-12-23 11:11:14 - techtonik
That's great. It will surely help to improve API references further. In particular I'd like to see return values documented using @return notation from  because it seems that at least scanString() and parseString() return results are incompatible.
#### 2011-12-23 11:11:48 - techtonik
I am not sure I can do anything about wiki examples. Still learning pyparsing API, so there is a little chance I can organize something I don't understand.

Perhaps a table, but again - you need some test data/test cases to prove these examples work as expected.
#### 2011-12-23 17:29:55 - ptmcg
As for the incompatibility of the results from scanString and parseString, I thought the docstrings for each made this pretty clear.  To recap, here are the 4 main API calls:
- parseString - returns a ParseResults built up from the parsed input text
- scanString - is a generator function that yields a 3-element tuple for each found match; the first element of each is a ParseResults of the matched tokens, the second element is the start location of the match, and the third element is the end location of the match
- searchString is a convenience wrapper around the scanString results, returning a list comprehension of the first element of each yielded 3-tuple
- transformString returns a string created by merging the results from each scanString match with the intervening unmatched text
#### 2011-12-28 05:11:48 - techtonik
Thank for the explanation.

These four would be nice to be grouped together and explicitly explained in the main page of API reference as an entrypoint for newcomers. The biggest problem that they are hidden inside ParseElement docs.

It will be much more clear to me if pyparsing process was first described as two-fold - first you define the grammar with (...). Then use that grammar with (...) to do operations on strings. The problem that these two functions are blended and its hard to see a separation just by reading the code. I suspect that in case of string transformation the code is even more intertwined.

---
## 2011-12-22 02:44:02 - denj - Transform embedded HTML to latex and replace newline
Hi,
I'm trying to convert some text with embedded HTML tags to latex code.

But I have no idea how to convert the '\n' into the latex equivalent '\\\\'.

Example-Code:



    # -*- coding: utf-8 -*-
    
    from pyparsing import *
    
    # define here in order to access this object in processHtmlTag()
    htmlTag = Forward()
    
    notes = u'''HTML-Example
    Next line of text.
    
    List:
    
    <ul>
        <li>t1</li>
        <li>t2</li>
        <li>t3</li>
        <li><b>bold</b></li>
    </ul>
    
    <b>bold</b>
    <i>iterative</i>
    <u>underlined</u>
    <b><i><u>mixed</u></i></b>
    
    Several lines
    of
    text...'''
    
    def convertList(token):
        listingScel = '''\\begin{itemize}
    %s
    \\end{itemize}'''
    
        # prfe Listenelemente auf html-Tags
        l = ['\\item %s' % htmlTag.transformString(t.entry) for t in token.entries]
        s = '\n'.join(l)
        return listingScel % s
    
    
    def makeHtmlParseObjects(tagName, parseAction):    
        startTag, endTag = makeHTMLTags(tagName)
        contents = SkipTo(endTag).setResultsName('contents')
        htmlTag = startTag + contents + endTag
        htmlTag.setParseAction(withAttribute(src=withAttribute.ANY_VALUE))
        htmlTag.setParseAction(parseAction)
    
        return htmlTag
    
    
    def processHtmlTag(st, locn, token):
        processedStr = ''
        formatStr = '%s'
        isMatch = False
    
        if token.tag == 'b':
            isMatch = True
            formatStr = '\\textbf{%s}'
        elif token.tag == 'i':
            isMatch = True
            formatStr = '\\textit{%s}'
        elif token.tag == 'u':
            isMatch = True
            formatStr = '\\underline{%s}'
        else:
            pass    
    
        if isMatch:
            # process nested tags recursively
            processedStr = htmlTag.transformString(token.contents)
    
        return formatStr % processedStr
    
    
    # convert lists
    ulTag, endUlTag = map(Suppress, makeHTMLTags('ul'))
    liTag, endLiTag = map(Suppress, makeHTMLTags('li'))
    liBody = SkipTo(endLiTag).setResultsName('entry')
    listEntries = OneOrMore(Group(liTag + liBody + endLiTag)).setResultsName('entries')
    
    listTag = ulTag + listEntries + endUlTag
    listTag.setParseAction(convertList)
    
    # convert basic html-Tags
    supportedHtmlTags = ('a', 'b', 'i', 'u')
    for tagName in supportedHtmlTags:
        htmlTag |= makeHtmlParseObjects(tagName, processHtmlTag)
    
    parser = listTag | htmlTag
    
    
    if __name__ == '__main__':
        processedStr = parser.transformString(notes)
        print processedStr



#### 2011-12-22 02:54:39 - denj
The '\n' should only be replaced if it appears in the text, but not in the HTML tags.

The output should look like this:



    HTML-Example\\
    Next line of text.\\
    
    List:
    
    \begin{itemize}
    \item t1
    \item t2
    \item t3
    \item \textbf{fett}
    \end{itemize}
    
    \textbf{bold}
    \textit{iterative}
    \underline{underlined}
    \textbf{\textit{\underline{mixed}}}
    
    Several lines\\
    of\\
    text...\\


Regards,
denj
