## Pyparsing Wikispaces Discussion - 2008

_[Note: these entries are fairly old, and predate many new features of pyparsing,
and are predominantly coded using Python 2.
They are captured here for historical benefit, but may not contain
the most current practices or features. We will try to add editor
notes to entries to indicate when discussions have been 
overtaken by development events.]_

[2008-01-02 00:08:32 - piranha - Matching start and end of word](all_wiki_discussion_toc_2008.md#2008-01-02-000832---piranha---matching-start-and-end-of-word)  
[2008-01-03 10:12:21 - wcbarksdale - The evils of a << b | c](all_wiki_discussion_toc_2008.md#2008-01-03-101221---wcbarksdale---the-evils-of-a--b--c)  
[2008-01-13 09:03:34 - bardaa1 - Raising an exception when an optional string is not found](all_wiki_discussion_toc_2008.md#2008-01-13-090334---bardaa1---raising-an-exception-when-an-optional-string-is-not-found)  
[2008-01-16 17:05:00 - Rich_Fletcher - setResultsName problem even considering copy behavior](all_wiki_discussion_toc_2008.md#2008-01-16-170500---rich_fletcher---setresultsname-problem-even-considering-copy-behavior)  
[2008-01-22 11:21:53 - michael_ramirez44 - ZeroOrMore](all_wiki_discussion_toc_2008.md#2008-01-22-112153---michael_ramirez44---zeroormore)  
[2008-02-07 00:25:49 - jpj1138 - Handling line ends with continuation characters](all_wiki_discussion_toc_2008.md#2008-02-07-002549---jpj1138---handling-line-ends-with-continuation-characters)  
[2008-02-07 09:15:31 - carterson2 - i need a C languager preprocessor written in python](all_wiki_discussion_toc_2008.md#2008-02-07-091531---carterson2---i-need-a-c-languager-preprocessor-written-in-python)  
[2008-02-07 10:56:43 - ruedi-w - parsing VBA variable declarations](all_wiki_discussion_toc_2008.md#2008-02-07-105643---ruedi-w---parsing-vba-variable-declarations)  
[2008-02-08 06:43:37 - michael_ramirez44 - Adding to parse string during parsing](all_wiki_discussion_toc_2008.md#2008-02-08-064337---michael_ramirez44---adding-to-parse-string-during-parsing)  
[2008-02-08 10:38:50 - propell - Tips for making a BibTeX parser faster?](all_wiki_discussion_toc_2008.md#2008-02-08-103850---propell---tips-for-making-a-bibtex-parser-faster)  
[2008-02-09 12:09:58 - carterson2 - I need a C preprocessor written in python](all_wiki_discussion_toc_2008.md#2008-02-09-120958---carterson2---i-need-a-c-preprocessor-written-in-python)  
[2008-02-11 12:10:39 - wcbarksdale - What is matched by outer patterns?](all_wiki_discussion_toc_2008.md#2008-02-11-121039---wcbarksdale---what-is-matched-by-outer-patterns)  
[2008-02-13 04:07:12 - plomlund - No exception on parse error](all_wiki_discussion_toc_2008.md#2008-02-13-040712---plomlund---no-exception-on-parse-error)  
[2008-02-14 14:32:35 - michael_ramirez44 - Keywords matched as Identifier](all_wiki_discussion_toc_2008.md#2008-02-14-143235---michael_ramirez44---keywords-matched-as-identifier)  
[2008-02-21 12:35:51 - scripteaze - i think i might be doing this the long way.](all_wiki_discussion_toc_2008.md#2008-02-21-123551---scripteaze---i-think-i-might-be-doing-this-the-long-way)  
[2008-02-21 15:27:41 - dang42 - help removing ordering constraint](all_wiki_discussion_toc_2008.md#2008-02-21-152741---dang42---help-removing-ordering-constraint)  
[2008-02-25 04:10:33 - murk8 - Command and arg on the same line - ?](all_wiki_discussion_toc_2008.md#2008-02-25-041033---murk8---command-and-arg-on-the-same-line---)  
[2008-02-29 09:27:57 - crmccreary - Trouble with this grammar](all_wiki_discussion_toc_2008.md#2008-02-29-092757---crmccreary---trouble-with-this-grammar)  
[2008-03-02 18:15:35 - alito - enablePackrat speeds up parsing of simple expression by a factor of >10000](all_wiki_discussion_toc_2008.md#2008-03-02-181535---alito---enablepackrat-speeds-up-parsing-of-simple-expression-by-a-factor-of-10000)  
[2008-03-05 01:53:57 - nagaraj1 - requesting the code format](all_wiki_discussion_toc_2008.md#2008-03-05-015357---nagaraj1---requesting-the-code-format)  
[2008-03-08 17:02:13 - jenhl - Odd dict output](all_wiki_discussion_toc_2008.md#2008-03-08-170213---jenhl---odd-dict-output)  
[2008-03-11 02:22:11 - jenhl - LineEnd confusion](all_wiki_discussion_toc_2008.md#2008-03-11-022211---jenhl---lineend-confusion)  
[2008-03-12 22:32:44 - taknev - Important Pyparsing webpage not rendering nicely](all_wiki_discussion_toc_2008.md#2008-03-12-223244---taknev---important-pyparsing-webpage-not-rendering-nicely)  
[2008-03-13 22:17:59 - alito - Combining Forward and operatorPrecedence](all_wiki_discussion_toc_2008.md#2008-03-13-221759---alito---combining-forward-and-operatorprecedence)  
[2008-03-14 16:04:00 - ricedaddy - parse string and extract certain text](all_wiki_discussion_toc_2008.md#2008-03-14-160400---ricedaddy---parse-string-and-extract-certain-text)  
[2008-03-18 11:08:49 - Poldy - Getting the REAL position of the error](all_wiki_discussion_toc_2008.md#2008-03-18-110849---poldy---getting-the-real-position-of-the-error)  
[2008-03-22 20:50:38 - ecir-hana - FAQ: How to get pyparsing to parse the entire input string?](all_wiki_discussion_toc_2008.md#2008-03-22-205038---ecir-hana---faq-how-to-get-pyparsing-to-parse-the-entire-input-string)  
[2008-04-07 01:43:02 - lolob - Extract HTML table](all_wiki_discussion_toc_2008.md#2008-04-07-014302---lolob---extract-html-table)  
[2008-04-07 13:15:29 - kenpierce - nestedExpr and preserving whitespace](all_wiki_discussion_toc_2008.md#2008-04-07-131529---kenpierce---nestedexpr-and-preserving-whitespace)  
[2008-04-08 15:52:18 - MatTourne - ParseResults and location](all_wiki_discussion_toc_2008.md#2008-04-08-155218---mattourne---parseresults-and-location)  
[2008-04-16 10:38:10 - MatTourne - Error recovery](all_wiki_discussion_toc_2008.md#2008-04-16-103810---mattourne---error-recovery)  
[2008-04-17 00:43:48 - propell - QuotedString problems with escChar](all_wiki_discussion_toc_2008.md#2008-04-17-004348---propell---quotedstring-problems-with-escchar)  
[2008-04-17 15:30:03 - michael_ramirez44 - Optional Ordering](all_wiki_discussion_toc_2008.md#2008-04-17-153003---michael_ramirez44---optional-ordering)  
[2008-04-29 07:47:43 - dminor14 - Can't get alternative keywords to work](all_wiki_discussion_toc_2008.md#2008-04-29-074743---dminor14---cant-get-alternative-keywords-to-work)  
[2008-05-04 15:59:48 - ecir-hana - setParseAction, Optional and Combine](all_wiki_discussion_toc_2008.md#2008-05-04-155948---ecir-hana---setparseaction-optional-and-combine)  
[2008-05-04 23:02:26 - dminor14 - transformString question](all_wiki_discussion_toc_2008.md#2008-05-04-230226---dminor14---transformstring-question)  
[2008-05-13 11:53:40 - MatTourne - ignore rule](all_wiki_discussion_toc_2008.md#2008-05-13-115340---mattourne---ignore-rule)  
[2008-05-27 04:52:05 - jstanforth - WordStart syntax question](all_wiki_discussion_toc_2008.md#2008-05-27-045205---jstanforth---wordstart-syntax-question)  
[2008-05-28 16:08:24 - sli1que - pyparsing usage](all_wiki_discussion_toc_2008.md#2008-05-28-160824---sli1que---pyparsing-usage)  
[2008-06-03 18:51:12 - MatTourne - "Visiting" the ParseResults](all_wiki_discussion_toc_2008.md#2008-06-03-185112---mattourne---visiting-the-parseresults)  
[2008-06-06 12:20:37 - skinny_00 - parsing a simple language](all_wiki_discussion_toc_2008.md#2008-06-06-122037---skinny_00---parsing-a-simple-language)  
[2008-06-09 11:57:40 - MatTourne - bug? ignore and maximum recursion excedeed](all_wiki_discussion_toc_2008.md#2008-06-09-115740---mattourne---bug-ignore-and-maximum-recursion-excedeed)  
[2008-06-09 22:44:40 - alxtoth - question on nestedExpr ](all_wiki_discussion_toc_2008.md#2008-06-09-224440---alxtoth---question-on-nestedexpr)  
[2008-06-10 12:52:56 - maxstylus - skipping lines that don't fit grammar](all_wiki_discussion_toc_2008.md#2008-06-10-125256---maxstylus---skipping-lines-that-dont-fit-grammar)  
[2008-06-13 20:29:48 - eleybourn - I need to match a word unless it is a keyword](all_wiki_discussion_toc_2008.md#2008-06-13-202948---eleybourn---i-need-to-match-a-word-unless-it-is-a-keyword)  
[2008-06-26 20:04:41 - akineko - SkipTo()'s unexpected behaviour](all_wiki_discussion_toc_2008.md#2008-06-26-200441---akineko---skiptos-unexpected-behaviour)  
[2008-06-27 09:37:53 - akineko - process ifdef ... endif](all_wiki_discussion_toc_2008.md#2008-06-27-093753---akineko---process-ifdef--endif)  
[2008-07-01 13:15:49 - akineko - pyparsing aid tool](all_wiki_discussion_toc_2008.md#2008-07-01-131549---akineko---pyparsing-aid-tool)  
[2008-07-17 10:58:53 - agustingianni - Problems with recursive grammars](all_wiki_discussion_toc_2008.md#2008-07-17-105853---agustingianni---problems-with-recursive-grammars)  
[2008-07-24 06:36:26 - jkozak - space complexity](all_wiki_discussion_toc_2008.md#2008-07-24-063626---jkozak---space-complexity)  
[2008-07-24 14:12:55 - 3rdangle - Inquiry Regarding Programatic Setting of setResultsName](all_wiki_discussion_toc_2008.md#2008-07-24-141255---3rdangle---inquiry-regarding-programatic-setting-of-setresultsname)  
[2008-07-27 23:27:21 - mayapower - Read .txt file like .py file](all_wiki_discussion_toc_2008.md#2008-07-27-232721---mayapower---read-txt-file-like-py-file)  
[2008-08-01 16:21:52 - robinsiebler - New to PyParsing](all_wiki_discussion_toc_2008.md#2008-08-01-162152---robinsiebler---new-to-pyparsing)  
[2008-08-02 06:35:38 - ecik - Comments in parsed code](all_wiki_discussion_toc_2008.md#2008-08-02-063538---ecik---comments-in-parsed-code)  
[2008-08-04 11:13:25 - robinsiebler - Text that spans multiple lines](all_wiki_discussion_toc_2008.md#2008-08-04-111325---robinsiebler---text-that-spans-multiple-lines)  
[2008-08-05 13:51:41 - robinsiebler - Another newbie question](all_wiki_discussion_toc_2008.md#2008-08-05-135141---robinsiebler---another-newbie-question)  
[2008-08-06 11:15:45 - efaherty - Recursive terminals](all_wiki_discussion_toc_2008.md#2008-08-06-111545---efaherty---recursive-terminals)  
[2008-08-09 06:06:11 - efaherty - nestedExpr question question](all_wiki_discussion_toc_2008.md#2008-08-09-060611---efaherty---nestedexpr-question-question)  
[2008-08-12 05:58:08 - chris_laws - regular expression extracting groups](all_wiki_discussion_toc_2008.md#2008-08-12-055808---chris_laws---regular-expression-extracting-groups)  
[2008-08-14 16:05:06 - ecir-hana - OT: LPEG](all_wiki_discussion_toc_2008.md#2008-08-14-160506---ecir-hana---ot-lpeg)  
[2008-09-02 18:49:22 - luanzhu - Word match and searchString (newbie question)](all_wiki_discussion_toc_2008.md#2008-09-02-184922---luanzhu---word-match-and-searchstring-newbie-question)  
[2008-09-05 06:48:43 - Leonidas-from-XIV - Handle \r\n like \n?](all_wiki_discussion_toc_2008.md#2008-09-05-064843---leonidas-from-xiv---handle-rn-like-n)  
[2008-09-18 00:13:57 - chrisdew - adding autocomplete to pyparsing](all_wiki_discussion_toc_2008.md#2008-09-18-001357---chrisdew---adding-autocomplete-to-pyparsing)  
[2008-09-19 02:23:16 - kib2 - problem with the given pgn parser](all_wiki_discussion_toc_2008.md#2008-09-19-022316---kib2---problem-with-the-given-pgn-parser)  
[2008-09-20 16:35:57 - pilsn - Dict: why are int converted to string?](all_wiki_discussion_toc_2008.md#2008-09-20-163557---pilsn---dict-why-are-int-converted-to-string)  
[2008-09-22 05:33:29 - nnathan - Adding exception/parse failures in semantics](all_wiki_discussion_toc_2008.md#2008-09-22-053329---nnathan---adding-exceptionparse-failures-in-semantics)  
[2008-09-24 21:55:50 - dminor14 - What is your license](all_wiki_discussion_toc_2008.md#2008-09-24-215550---dminor14---what-is-your-license)  
[2008-09-29 00:57:59 - chrisdew - completion - questions on extending pyparsing for autocompletion](all_wiki_discussion_toc_2008.md#2008-09-29-005759---chrisdew---completion---questions-on-extending-pyparsing-for-autocompletion)  
[2008-09-30 03:29:06 - hgaudecker - nestedExpr with multiple characters in opener/closer](all_wiki_discussion_toc_2008.md#2008-09-30-032906---hgaudecker---nestedexpr-with-multiple-characters-in-openercloser)  
[2008-09-30 05:35:02 - onto - converting a yapps 1.1 program to pyparsing](all_wiki_discussion_toc_2008.md#2008-09-30-053502---onto---converting-a-yapps-11-program-to-pyparsing)  
[2008-09-30 06:45:46 - hgaudecker - Newbie Question on LineStart + Whitespace](all_wiki_discussion_toc_2008.md#2008-09-30-064546---hgaudecker---newbie-question-on-linestart--whitespace)  
[2008-10-01 14:25:48 - pilsn - copying a parser](all_wiki_discussion_toc_2008.md#2008-10-01-142548---pilsn---copying-a-parser)  
[2008-10-01 19:25:24 - eleybourn - SQL Parser](all_wiki_discussion_toc_2008.md#2008-10-01-192524---eleybourn---sql-parser)  
[2008-10-07 11:45:13 - TheGrudge - C++ #include parser](all_wiki_discussion_toc_2008.md#2008-10-07-114513---thegrudge---c-include-parser)  
[2008-10-09 02:50:49 - rhattersley - Recording position of match](all_wiki_discussion_toc_2008.md#2008-10-09-025049---rhattersley---recording-position-of-match)  
[2008-10-12 03:03:10 - steven-d - Ignoring trailing spaces](all_wiki_discussion_toc_2008.md#2008-10-12-030310---steven-d---ignoring-trailing-spaces)  
[2008-10-12 03:29:10 - steven-d - Escaping characters](all_wiki_discussion_toc_2008.md#2008-10-12-032910---steven-d---escaping-characters)  
[2008-10-16 09:00:00 - dozpav - Recursive Element with parseActions](all_wiki_discussion_toc_2008.md#2008-10-16-090000---dozpav---recursive-element-with-parseactions)  
[2008-10-16 17:10:33 - teenwag - pyparsing.py:378: undefined name 'j'](all_wiki_discussion_toc_2008.md#2008-10-16-171033---teenwag---pyparsingpy378-undefined-name-j)  
[2008-10-18 19:33:20 - oyster - need help with my BASIC-like language](all_wiki_discussion_toc_2008.md#2008-10-18-193320---oyster---need-help-with-my-basic-like-language)  
[2008-10-21 15:52:10 - steven-d - Parsing line by line](all_wiki_discussion_toc_2008.md#2008-10-21-155210---steven-d---parsing-line-by-line)  
[2008-10-22 02:02:31 - oyster - nested for and more](all_wiki_discussion_toc_2008.md#2008-10-22-020231---oyster---nested-for-and-more)  
[2008-10-23 15:31:15 - ptmcg - I've been busy with work...](all_wiki_discussion_toc_2008.md#2008-10-23-153115---ptmcg---ive-been-busy-with-work)  
[2008-10-30 00:11:07 - oyster - be carefulwith enablePackrat](all_wiki_discussion_toc_2008.md#2008-10-30-001107---oyster---be-carefulwith-enablepackrat)  
[2008-10-30 19:30:29 - oyster - invisible visible var](all_wiki_discussion_toc_2008.md#2008-10-30-193029---oyster---invisible-visible-var)  
[2008-10-31 14:03:04 - DanFelts - parsing HTML Input ](all_wiki_discussion_toc_2008.md#2008-10-31-140304---danfelts---parsing-html-input-)  
[2008-11-01 07:20:13 - oyster - how to get a real name for this case?](all_wiki_discussion_toc_2008.md#2008-11-01-072013---oyster---how-to-get-a-real-name-for-this-case)  
[2008-11-03 01:06:41 - oyster - did I misunderstand Optional?](all_wiki_discussion_toc_2008.md#2008-11-03-010641---oyster---did-i-misunderstand-optional)  
[2008-11-03 06:36:35 - oyster - reconstruct expressions from operatorPrecedence results](all_wiki_discussion_toc_2008.md#2008-11-03-063635---oyster---reconstruct-expressions-from-operatorprecedence-results)  
[2008-11-03 08:38:39 - cpennington - Line based parsing and empty lines: possible bug?](all_wiki_discussion_toc_2008.md#2008-11-03-083839---cpennington---line-based-parsing-and-empty-lines-possible-bug)  
[2008-11-03 11:28:36 - cpennington - New failOn parameter for SkipTo doesn't work](all_wiki_discussion_toc_2008.md#2008-11-03-112836---cpennington---new-failon-parameter-for-skipto-doesnt-work)  
[2008-11-04 18:54:00 - oyster - be careful with setResultsName and setParseAction](all_wiki_discussion_toc_2008.md#2008-11-04-185400---oyster---be-careful-with-setresultsname-and-setparseaction)  
[2008-11-04 19:04:00 - oyster - operatorPrecedenceWithName](all_wiki_discussion_toc_2008.md#2008-11-04-190400---oyster---operatorprecedencewithname)  
[2008-11-09 03:00:45 - oyster - infinite loops](all_wiki_discussion_toc_2008.md#2008-11-09-030045---oyster---infinite-loops)  
[2008-11-14 04:14:26 - steven-d - Line-oriented parsing, with goodies](all_wiki_discussion_toc_2008.md#2008-11-14-041426---steven-d---line-oriented-parsing-with-goodies)  
[2008-11-14 21:00:58 - oyster - setDefaultWhitespaceChars bug](all_wiki_discussion_toc_2008.md#2008-11-14-210058---oyster---setdefaultwhitespacechars-bug)  
[2008-11-17 14:45:24 - steven-d - Should ParseSyntaxException be a subclass of ParseException?](all_wiki_discussion_toc_2008.md#2008-11-17-144524---steven-d---should-parsesyntaxexception-be-a-subclass-of-parseexception)  
[2008-12-09 12:00:04 - ajone825 - how to input a file (Newb)](all_wiki_discussion_toc_2008.md#2008-12-09-120004---ajone825---how-to-input-a-file-newb)  
[2008-12-13 13:05:38 - orestis82 - Error tolerance (needed for syntax highlighting?)](all_wiki_discussion_toc_2008.md#2008-12-13-130538---orestis82---error-tolerance-needed-for-syntax-highlighting)  
[2008-12-16 09:02:02 - mUogoro - Callback execution inside grammar rules](all_wiki_discussion_toc_2008.md#2008-12-16-090202---muogoro---callback-execution-inside-grammar-rules)  
[2008-12-18 05:40:54 - mayama - Nested elements](all_wiki_discussion_toc_2008.md#2008-12-18-054054---mayama---nested-elements)  
[2008-12-19 03:03:21 - chrisi_g - nestedExpr: unexpected behavior](all_wiki_discussion_toc_2008.md#2008-12-19-030321---chrisi_g---nestedexpr-unexpected-behavior)  
[2008-12-27 23:28:33 - pgurumur - Having some trouble parsing.](all_wiki_discussion_toc_2008.md#2008-12-27-232833---pgurumur---having-some-trouble-parsing)  


---
## 2008-01-02 00:08:32 - piranha - Matching start and end of word
Hi, I'm trying to write parser for a Wiki syntax, but I'm getting some troubles.



To match bold and italic, I have started with (QS is simplified version for calling QuotedString)





    m_strong = QS('*').setParseAction(convert_to_html('strong'))
    m_em = QS('_').setParseAction(convert_to_html('em'))



and it works perfectly except that it matches even in middle of word. I tried to switch to Regex (because I have not found approach to go with simple pyparsing symbols):





    m_strong = Regex(r'\b\*.*\*\b').setParseAction(convert_to_html('strong', '*'))
    m_em = Regex(r'\b_.*_\b').setParseAction(convert_to_html('em', '_'))



Interesting is that emphasis works, but not strong. I've played in shell and discovered that from this two expressions first does not match and second does:





    >>> re.search(r'\b\*(.*)\*\b', 'test *bold* hehe')
    >>> re.search(r'\b\\*(\.*)\\*\b', 'test *bold* hehe')
    <_sre.SRE_Match object at 0x014C9D20>
    >>>



But changing regex in m_strong drops pyparsing into endless loop (or something like that, I have not waited more than 30 seconds :D).



Any suggestions what can I do? Maybe I can somehow forget regexps? Maybe there are some wiki parsers implementations on pyparsing (I have tried to look at some standalone wiki parsers, but they are hard to extends and poorly written in most of cases).



Thanks a lot in advance.

#### 2008-01-02 00:41:02 - piranha
Another idea:



    m_em = ~oneOf(alphanums) + QS('_').setParseAction(convert_to_html('em')) + ~oneOf(alphanums)



matches line 'some_python_func_name' as 'some<em>python</em>func_name'. Does not suit my needs. :-(
#### 2008-01-02 01:34:17 - ptmcg
<sigh> It seems that there are a number of wiki markup designers out there these days, and I'm not confident that pyparsing is completely up to the task yet.  The bold and italic markup are not too difficult, but what about this case:



    *This is bold _and italic_ too*



I think I've found a reasonable solution using nestedExpr for these cases, but then the next item is bulleted lists - since these are whitespace-sensitive, it is really quite a struggle to have pyparsing process them suitably.



To answer your specific question though, no, there is no lookbehind operation to test for whitespace or word boundaries (although probably not overly difficult to write).  To do this with pyparsing as it currently exists, you'll need to work around the problem with another expression to mask this case.



I assume that you are using transformString, or possibly scanString with your own processing when token matches are found.  Let's look at some dummy text:





    This is _something to be emphasized._
    This is *a strong statement.*
    But this is neither*strong*nor_emphatic_.



Building on your sample code:



    m_strong = QS('*').setParseAction(convert_to_html('strong'))
    m_em = QS('_').setParseAction(convert_to_html('em'))



I would guess that somewhere you have an overall expression:



    wikiMarkup = m_strong | m_em



The most straightforward workaround is define an expression for a word with embedded '*' or '_' characters, and then do nothing with it.  That is:



    word_with_embedded_quotes = Word(printables.replace('*','').replace('_',''), printables)



and then have that precede the valid markup in wikiMarkup:



    wikiMarkup = word_with_embedded_quotes | m_strong | m_em



Now you won't match m_strong or m_em with any embedded '*' or '_' characters, since the word_with_embedded_quotes will match those first.  And since there is no parse action attached to word_with_embedded_quotes, then there will be no transformation for it.



I've given thought in the past to a PrecededBy expression, but there are some problems with how deterministic and unambiguous such an expression could be, so I haven't done it yet.  However, I think most of the times that this has come up has been in detecting word boundaries.  Since re's have found it necessary to define word boundary markers, I would guess it would not be unreasonable for pyparsing to have similar constructs.  Maybe the best would be to extend the positional expressions such as LineStart/End and StringStart/End, by implementing a WordStart/End pair of expression classes.  Then you could define your wiki markup as:





    m_strong = WordStart() + QS('*').setParseAction(convert_to_html('strong')) + WordEnd()



Let me know if you are interested in testing out such a thing, and I'll try to cook something up in the next few days.



-- Paul
#### 2008-01-02 02:05:41 - piranha
<ul class="quotelist"><li>I think I've found a reasonable solution using nestedExpr for these cases, but then the next item is bulleted lists - since these are whitespace-sensitive, it is really quite a struggle to have pyparsing process them suitably.</li></ul>

Ugh... Yeah, I've thought about lists, but not too much guessing that I'll understand how to do them in the future... But your words are pretty scaring - looks like I guess wrong? :) That's really bad, because the only two parsers that have understandable and flexible enough syntax from my POV is pyparsing and parsec (this one is Haskell thing, so I don't really thought about using it because of my almost zero Haskell knowledge).



And about nestedExpr... I should have thought about nesting earlier. :( Need to play with it, because currently I have no ideas how to use it properly... 



<ul class="quotelist"><li>and then have that precede the valid markup in wikiMarkup:</li></ul>

Order does not really matter, I have inserted in start and in end - both cases work.



<ul class="quotelist"><li>Let me know if you are interested in testing out such a thing, and I'll try to cook something up in the next few days.</li></ul>

Of course, I'm interested. :-) This would be very nice and useful addition.
#### 2008-01-02 08:37:35 - ptmcg
According to this reference (), `\b` matches at the boundary between a `\w` and a `\W`, and the set of characters used in a `\w` are alphas and numbers.  Pyparsing's interpretation of word boundaries is usually tied to the definition of whitespace, so I think we'll need a constructor argument for WordStart and WordEnd, with the default=alphanums or the default=printables.  Having printables be the default is more consistent with pyparsing's concepts.



Here would be my proposed docstring for WordStart (with WordEnd being similar in concept):



Matches if the current position is at the beginning of a Word, and is not preceded by any character in a given set of wordChars (default=printables).  To emulate the \b behavior of regular expressions, use WordStart(alphanums).  WordStart will also match at the beginning of the string being parsed, or at the beginning of a line.



-- Paul
#### 2008-01-02 08:46:27 - piranha
Yup, exactly what I've thought. :-)
#### 2008-01-02 20:29:20 - ptmcg
Ok, I just checked in an updated version into the SourceForge SVN repository.  Here is the test code I used:



    from pyparsing import WordEnd, WordStart, oneOf
    
    ws = WordStart()
    we = WordEnd()
    vowel = oneOf(list('AEIOUY'))
    consonant = oneOf(list('BCDFGHJKLMNPQRSTVWXZ'))
    
    leadingVowel = ws + vowel
    trailingVowel = vowel + we
    leadingConsonant = ws + consonant
    trailingConsonant = consonant + we
    
    bnf = leadingVowel | trailingVowel
    
    tests = '''\
    ABC DEF GHI
      JKL MNO PQR
    STU VWX YZ  '''.splitlines()
    
    for t in tests:
        print t
        print leadingConsonant.searchString(t)
        print leadingVowel.searchString(t)
        print trailingConsonant.searchString(t)
        print trailingVowel.searchString(t)
        print bnf.searchString(t)
        print



Please try these out and see if they fit your application.  Also, if there are other test cases you can think of, please give them a try and let me know how they work.



-- Paul
#### 2008-01-03 01:22:50 - piranha
Perfectly!



This code on this string:



    ws = WordStart(alphanums + '*[]_^')
    we = WordEnd(alphanums + '*[]_^')
    m_strong = ws + QS('*').setParseAction(convert_to_html('strong')) + we
    m_em = ws + QS('_').setParseAction(convert_to_html('em')) + we
    
    text = '''
     _italics_. And longer
     _and italic_
     some_python_func_name
    '''



works perfectly. Thanks! Next step is discovering nestedExpr to get '_*bold-italic*_' working right. Is there any possibility to send result to processing again without explicitly declaring every case? Because there will be a lot of similar cases, like that:





    *_bold-italic_*
    _*italic-bold*_
    ^^*super-bold*^^
    [\[url|*name*]]



---
## 2008-01-03 10:12:21 - wcbarksdale - The evils of a << b | c
I've already done `a << b | c` more than once.  I was wondering what you thought of having `Forward.__lshift__` just return None, or maybe at least something that warns you if you try to use a troublesome operator like | or +.  The disadvantage is that you couldn't write something like



    a = Forward()
    d = (a << b) | c

but this seems like poor style to me anyway.  From the perspective of a new user at least it's a small loss to reduce the confusion I imagine most people will have at least once.

#### 2008-01-03 10:15:45 - wcbarksdale
I should note that I have done the same thing in C++ plenty of times (cout << 2 + 2), but getting an error right away saves a lot of time.
#### 2008-01-03 11:09:11 - ptmcg
Good idea, thanks!  I also cleaned up those places where a syntax warning is printed saying 'you can't combine a <blah> object with a ParserElement', to also return None.



This was an unfortunate operator choice, I should have used '<<=' instead of '<<'.  But I'm pretty certain `<< b + c` does the right thing, since '+' is evaluated before '<<'.



I'll post this to the SourceForge SVN repository later today.



-- Paul
#### 2008-01-03 11:26:46 - wcbarksdale
You're right, I was thinking of &.  Why not support both '<<' and '<<='?  I think I'd rather write <<= in my own code because it's more clearly a statement.
#### 2008-02-12 08:34:37 - ptmcg
'<<' operator returns None as of pyparsing 1.4.11, released this past weekend.



-- Paul

---
## 2008-01-13 09:03:34 - bardaa1 - Raising an exception when an optional string is not found


Hello, 

the following code snippet ignores WHO being 'bar'
and simply prints the HELLO part. I admit it is desirable
in some situations but what I'd like pyparsing to do right now
is to raise an exception when an optional keyword is not found
in the text. I guess I need an OptionalButNotAnyOtherThenAWhitespace class ;-) Can you please suggest how I can achieve it?

Thanks for help!


    from pyparsing import Keyword, Optional
    
    HELLO = 'hello'
    WHO = Keyword('foo', caseless=True)
    
    grammar = HELLO + Optional(WHO)
    print grammar.parseString('hello bar')
    
    # And here's the output
    
    ['hello']



#### 2008-01-13 09:45:02 - ptmcg
Well, there are a lot of possibilities, but I don't fully understand your question.  If you want the absence of the WHO expression to cause an exception, then it doesn't sound like it is very optional.



Or is the issue that there is something ('bar') following where the WHO is supposed to be, and <em>that</em> is supposed to raise the exception?



Try adding a StringEnd() to the end of your grammar.  That way, if the grammar does not fully match the input string, you will get an exception.



If that does not address your problem, write back.



-- Paul
#### 2008-01-13 10:20:16 - bardaa1
> Or is the issue that there is something ('bar') following
> where the WHO is supposed to be, and that is supposed to raise the
> exception?

Yes, that was exactly the case (sorry for not being clearer in the
previous post) and adding StringEnd() did the trick.

Thanks a bunch!

---
## 2008-01-16 17:05:00 - Rich_Fletcher - setResultsName problem even considering copy behavior
I'm running into a problem with setResultsName, and I don't see why it isn't working despite (I think) dealing with the copy behavior.  I'm using a variant of the IDL parser on this site. I'm going to try to avoid spamming with the (long) entire grammar and (long, proprietary) IDL document, but don't know if it'll be enough context:





    valuetypeDef = Group( Optional( abstract_ ) + valuetype_ + \
        identifier + Optional( colon + delimitedList( typeName ) ) + \
        Optional( supports_ + delimitedList( typeName ) ) + \
        lbrace + OneOrMore( valuetypeItem ) + rbrace + semi )
    interfaceItem = Group( constDef | exceptionDef | attributeDef | operationDef )
    interfaceDef = Group( interface_ + identifier  + \
        Optional( colon + delimitedList( typeName ) ) + lbrace + \
        ZeroOrMore( interfaceItem ) + rbrace + semi )
    moduleItem = ( interfaceDef | exceptionDef | constDef | \
        typedefDef | moduleDef | enumDef | valuetypeDef)
    moduleDef << Group( module_ + identifier + lbrace + OneOrMore( moduleItem ) + rbrace + semi )



The only time I can get any named results is when I add a setResultsType to the moduleDef expression:



    moduleDef << Group( module_ + identifier + lbrace + OneOrMore( moduleItem ) + rbrace + semi )('module')



..this can be later referred to.  But trying to name an earlier definition, say:



    valuetypeDef = Group( Optional( abstract_ ) + valuetype_ + \
        identifier + Optional( colon + delimitedList( typeName ) ) + \
        Optional( supports_ + delimitedList( typeName ) ) + \
        lbrace + OneOrMore( valuetypeItem ) + rbrace + semi )('valuetype')



...I cannot get to work, even though I clearly have matching valuetype expressions.  My asList(), for example, prints out just fine after parsing.  Hints?  Need more info?  I'm degraded to list parsing, which is what I was hoping to avoid with a real parser.

#### 2008-01-16 18:49:37 - ptmcg
Rich -



Well, writing a full IDL parser is very impressive, congratulations!



There are a couple of ways to go from here, now that you are parsing the IDL successfully.  One way is as you are doing, by assigning results names to different elements in the IDL.  Another is to attach parse actions and have them perform some processing or return some type of component-descriptive object.



I'm not sure why the results names aren't working, unless the Forward << operator is getting in the way.  This is a long shot, but try modifying your code to read:





    moduleDef << Group( module_ + identifier + lbrace + OneOrMore( moduleItem ) + rbrace + semi )
    moduleDef = moduleDef('module')



But even if you use results names, you may end up having to walk the parsed list.  By default, results names only keep the last parsed tokens.  You can override this, but you'll need to use the setResultsName method call, as in:





    moduleDef << Group( module_ + identifier + lbrace + OneOrMore( moduleItem ) + rbrace + semi )
    moduleDef = moduleDef.setResultsName('module',listAllMatches=True)



What is your intention with this parser?  Are you generating server or proxy code? documentation? transforming the IDL?  I think your next step will depend on what you are trying to accomplish.



-- Paul
#### 2008-01-16 23:41:50 - Rich_Fletcher
Somewhat away from the code at the moment, but I'll give a quick response.



To clarify: I didn't write the parser [from scratch]; I'm modifying the IDL parser given at  , which wasn't an exact fit but was a useful starting point.



Pretty sure I've the 'moduleDef = moduleDef('module')' syntax, but I'll give it a shot later.  I am aware of the listAllMatches flag--I'm glad you clarified that it can't be set to True unless you use the more verbose/original syntax, because I couldn't see a way to do it otherwise and wondered if it was possible.



Of course, if I got only one match, then I would have simply used the other syntax. But I'm not getting any.



I might noodle around with ParseActions; some of my debugging went that direction.  FWIW, I'm doing a bit of boilerplate code generation  (valuetype implementation) to save a good bit of typing on my part.



Thanks!
#### 2008-01-17 06:00:40 - ptmcg
If you are doing something with valuetype, then this might be an easy way to get started:



1. Attach a dummy parse action to valuetype, `processValueType(tokens)`, and give it a single pass statement as the body.

2. Decorate this method with `@traceParseAction` (defined in pyparsing)

3. After defining valuetype, call `valuetype.setParseAction(processValueType)`.



This should give you a trace message for every match of a valuetype, and show you the tokens you are getting. Or you can omit the decorator, and have the body of processValueType be `print tokens.dump()`. Then, as you add results names to the expressions in valuetype, you will start to see what kind of values are available to you in the parse action. Then you can write a proper parse action that does the code generation that you want.



Good luck,

-- Paul

---
## 2008-01-22 11:21:53 - michael_ramirez44 - ZeroOrMore
Are these equal





    ZeroOrMore( express1 ^ express2 ^ express3 )



vs





    ZeroOrMore( express1 ) ^  ZeroOrMore(express2 ) ^ ZeroOrMore( express3 )



I'm trying to match express 1,2 and 3 in any order.

#### 2008-01-22 12:45:47 - ptmcg
How about using Each?





    Each( [express1, express2, express3] ) 



or





    express1 & express2 & express3





Each is not used a lot, but it seems like the exact thing you need for this application.



-- Paul

---
## 2008-02-07 00:25:49 - jpj1138 - Handling line ends with continuation characters
Hi everyone,



I know that skipping white space is the way to go with Pyparsing, but I've come across a few cases where my input is already very neatly delimited by new lines.  E.g, reading shell scripts:





    shellcmd -arg val -arg2 val2 \
    -arg3 val3



As the example above shows, I need to ignore the continuation character '\'.  It seems the Pyparsing function <em>ignore</em> was meant for exactly this, but I haven't been able to get it to work.  I'll keep at it, but if anyone has done this before I'd appreciate any pointers you can give me.



Thanks,



Jason

#### 2008-02-07 02:18:46 - ptmcg
Yes, ignore is just what this is for.  Try this:





    from pyparsing import *
    
    cmd = r'''shellcmd -arg val -arg2 val2 \
    -arg3 val3'''
    
    option = Word('-',alphanums)
    arg = ~Literal('-')+Word(printables) | quotedString
    command = Word(alphas) + ZeroOrMore(Group(option+Optional(arg)))
    
    continuation = '\\' + LineEnd()
    command.ignore(continuation)
    
    print command.parseString(cmd)



prints:



    ['shellcmd', ['-arg', 'val'], ['-arg2', 'val2'], ['-arg3', 'val3']]



Here is a more advanced version, that will return the command options in pseudo-dict form:



    continuation = '\\' + LineEnd()
    option = Word('-',alphanums)
    option.setParseAction(lambda tokens:tokens[0][1:])
    arg = ~Literal('-')+Word(printables)
    command = Word(alphas) + Dict(ZeroOrMore(Group(option+Optional(arg))))
    command.ignore(continuation)
    
    commandOptions = command.parseString(cmd)
    print commandOptions.arg2
    print commandOptions.dump()



prints:



    val2
    ['shellcmd', ['arg', 'val'], ['arg2', 'val2'], ['arg3', 'val3']]
    - arg: val
    - arg2: val2
    - arg3: val3



Write back if you want more details on just what is going on here.



-- Paul
#### 2008-02-07 09:51:11 - jpj1138
Ah, pretty straightforward.  I was trying something much simpler:





    Cmd = Literal('shellcmd') + SkipTo(LineEnd())
    Continuation = '\\' + LineEnd()
    Cmd.ignore(Continuation)



I don't care what the options are for this particular application so I was just trying to grab everything to the end of the line.  So, I guess SkipTo() doesn't ignore the arguments of <em>ignore</em>.



Thanks again for your quick and helpful response.



Jason
#### 2008-10-23 01:48:21 - steven-d
I've tried following this, and I can't get it to work. If I do an exact copy and paste of the above code, it works fine, but when I adapt if for my own use, it fails. What have I done wrong?





    text = r'''1 a line
    2 another line
    3 this is a very long line which is \
    continued on the next physical line
    4 another line'''
    
    from pyparsing import *
    
    ws = ' \t'
    ParserElement.setDefaultWhitespaceChars(ws)
    
    continuation = '\\' + Optional(ws) + LineEnd()
    line = Word(alphanums + ws) + LineEnd().suppress()
    parser = OneOrMore(line) + StringEnd()
    parser.ignore(continuation)
    
    try:
        for i, item in enumerate(parser.parseString(text)):
            print 'Line %d: %s' % (i+1, item)
    except ParseException, err:
        print err.line
        print ' '*(err.column-1) + '^'
        print err



prints:





    3 this is a very long line which is \
    ^
    Expected end of text (at char 24), (line:3, col:1)


#### 2008-10-24 08:11:43 - ptmcg
Steven -



I just looked over your post - thanks for making it easy to copy your simple example out so it is easy to work with.



The first thing I did was to make sure that the continuation expression was being recognized.  To do this, I added the statement:



    continuation.setDebug()

right after the definition of continuation.  setDebug will write a message every time the expression is about to be parsed, and shows the success or failure of that parse.  (You can provide your own debug actions if you like using setDebugActions.)  Running your parser with setDebug, I got the following:



    Match {'\' ['     '] LineEnd} at loc 0(1,1)
    Exception raised:Expected '\' (at char 0), (line:1, col:1)
    Match {'\' ['     '] LineEnd} at loc 8(1,1)
    Exception raised:Expected '\' (at char 8), (line:1, col:1)
    Match {'\' ['     '] LineEnd} at loc 9(2,1)
    Exception raised:Expected '\' (at char 9), (line:2, col:1)
    Match {'\' ['     '] LineEnd} at loc 9(2,1)
    Exception raised:Expected '\' (at char 9), (line:2, col:1)
    Match {'\' ['     '] LineEnd} at loc 23(2,1)
    Exception raised:Expected '\' (at char 23), (line:2, col:1)
    Match {'\' ['     '] LineEnd} at loc 24(3,1)
    Exception raised:Expected '\' (at char 24), (line:3, col:1)
    Match {'\' ['     '] LineEnd} at loc 24(3,1)
    Exception raised:Expected '\' (at char 24), (line:3, col:1)
    Match {'\' ['     '] LineEnd} at loc 60(3,37)
    Matched {'\' ['     '] LineEnd} -> ['\\', '\n']
    Match {'\' ['     '] LineEnd} at loc 62(4,1)
    Exception raised:Expected '\' (at char 62), (line:4, col:1)
    Match {'\' ['     '] LineEnd} at loc 62(4,1)
    Exception raised:Expected '\' (at char 62), (line:4, col:1)
    Match {'\' ['     '] LineEnd} at loc 24(3,1)
    Exception raised:Expected '\' (at char 24), (line:3, col:1)
    3 this is a very long line which is \
    ^
    Expected end of text (at char 24), (line:3, col:1)



Still not the right answer at the end, but we can see at location 60 that the continuation at end of line *does* get found.



Then I went back to look a bit closer at your grammar.  Here is the problem:



    line = Word(alphanums + ws) + LineEnd().suppress()



I see that you are reading the entire line using a Word expression, including all printables and non-newline whitespace.  Unfortunately, this is not quite the way to work with an ignoreable expression.



Pyparsing refers to its list of ignorables *between* expressions.  Like this quasi-definition of a Python function definition:



    func_defn = 'def' + Word(alphas) + '(' + Optional(arglist) + ')' + ':'

So an ignorable expression could occur between 'def' and the function name, or between the function name and the opening '(', or between the args in the arglist, etc.  But it can't be ignored inside the implicit repetition done by the Word - a Word is supposed to be a contiguous group of characters without any intervening whitespace or ignorables.  (It always feels weird to see a Word expression defined to include whitespace characters as valid body characters.  Strictly speaking this is not illegal, and I've seen people write parsers where it worked just as they wanted, but it was not my intention for this class.)



So to read in a line of arbitrary characters, while allowing for ignorables, you need to do some repetition of expressions that will allow pyparsing to detect the continuation, and then keep on parsing the current line.  Something like this:



    line = OneOrMore(Word(alphanums)) + LineEnd().suppress()

Between each iteration of OneOrMore, pyparsing will check to see if there are any ignorables or whitespace, such as your continuation, skip over them, and then continue parsing for 'or-more's.



This expression just gives us a list of tokens, so we'll need to put some spaces back in, maybe something like this parse action:



    line.setParseAction(lambda tokens: ' '.join(tokens))



This isn't perfect, it may add only single spaces where multiple spaces were there.  Here is another approach, to preserve the original whitespace:



    line = OneOrMore(White(ws) | Word(alphanums)) + LineEnd().suppress()
    line.setParseAction(lambda tokens: ''.join(tokens))



I think this last version is pretty close to what you want.



So in general, I think you can look at ignorables like 'super-whitespace'.  They are something pyparsing will skip over *between* expressions.



-- Paul

(and now back to our grindstone...)

---
## 2008-02-07 09:15:31 - carterson2 - i need a C languager preprocessor written in python
Hi,



'not sure what you have here.

I wanna preprocess C files. 

-regards,  
jim pruett

#### 2008-02-09 09:45:28 - ptmcg
What kind of preprocessing did you have in mind?  There is a macroProcessor.py example that simulates a subset of C preprocessing, it is one of the more advanced examples of pyparsing though.



When people say they want to preprocess C files, this could mean a wide variety of things.  Can you give more details?



-- Paul

(Sorry for not answering sooner, I don't usually check the discussion tab on this page of the wiki.  The discussion tab on the home page is where most of the message action takes place.  We can certainly continue this thread, but you'll get a quicker response to future messages if you post them on the main home page discussion.)

---
## 2008-02-07 10:56:43 - ruedi-w - parsing VBA variable declarations
Hi, I just started with pyparsing and try to parse some VBA variable declarations, e.g.:



    Dim j
        Dim j As Integer
        Dim i() As Integer, IM(3) As Integer, IP(4,5) As Integer
        Dim i() As Integer, IM(3,5 To 9) As Integer, IP As Integer  ' a comment
        Dim WG(0 To (N + 1) / 2) As Double, WGK(0 To N) As Double, XGK(0 To N) As Double
        Dim j(0 To (N+1)/2, (0 To (N-1)/2)) As Integer

the problem appears to be that

<ul><li>'(),' are used in different contexts and the parser gets confused</li><li>I dont want to parse the expressions in the range declarations, I'd rather get back <em>(N-1)/2))</em> in one piece</li></ul>

Here is my attempt, it completely fails to recognize the arrays:



    examples = '''
        Dim j
        Dim j As Integer
        Dim i() As Integer, IM(3) As Integer, IP(4,5) As Integer
        Dim i() As Integer, IM(3,5 To 9) As Integer, IP As Integer  ' a comment
        Dim WG(0 To (N + 1) / 2) As Double, WGK(0 To N) As Double, XGK(0 To N) As Double
        Dim j(0 To (N+1)/2, (0 To (N-1)/2)) As Integer
        '''
    from pyparsing import *
    
    varname = Word(alphas+'_', alphanums+'_')
    datatype = Word(alphas)
    comment = Literal(''') + restOfLine
    
    anything = Regex('.*')
    range = Optional(anything + Keyword('To')) + anything 
    shape = '(' + delimitedList(range) + ')'
    
    decl = varname + Optional(shape)+Optional(Suppress('As ')+ datatype)
    
    dim = Literal('Dim') + delimitedList(decl) + Optional(comment) #+ StringEnd()
    
    for i in examples.strip().splitlines():
        i = i.strip()
        print
        print i
        try:
            print dim.parseString(i)
        except Exception, e:
            print 'got error', e
    print '-'*44

Strange? Any help would be wellcome!

#### 2008-02-08 06:45:03 - ptmcg
I think this is a very good first cut at the problem.  Since the array range spec uses parenthetical nesting, I changed your shape definition to:



    shape = nestedExpr('(',')')



(Strictly speaking, the left and right paren arguments are the default values and could be left off, but for the tutorial sake I've explicitly shown them - read this as 'a nested expression using '(' and ')' for nesting'.)

(Also, be sure you are using the latest 1.4.10 version - prior versions had a bug in nestedExpr.)



This will actually parse the array definitions, but only for parenthesis nesting - it wont do anything to identify upper and lower bounds, or number of dimensions or anything.  So you get back gibberish like:



    []
    ['3']
    ['4,5']
    []
    ['3,5', 'To', '9']
    ['0', 'To', ['N', '+', '1'], '/', '2']
    ['0', 'To', 'N']
    ['0', 'To', 'N']
    ['0', 'To', ['N+1'], '/2,', ['0', 'To', ['N-1'], '/2']]



mixed in with your other tokens.  So you could suppress the data (which you already know how to do), or you could have pyparsing preserve the original text using the parse action keepOriginalText:



    shape = nestedExpr('(',')')
    shape.setParseAction(keepOriginalText)



Now you just get back the original parenthesized string, which you can reparse later if you want.  But since you said you just want the original text, this will do the trick.



With this one change, your program now returns:



    Dim j
    ['Dim', 'j']
    
    Dim j As Integer
    ['Dim', 'j', 'Integer']
    
    Dim i() As Integer, IM(3) As Integer, IP(4,5) As Integer
    ['Dim', 'i', '()', 'Integer', 'IM', '(3)', 'Integer', 'IP', '(4,5)', 'Integer']
    
    Dim i() As Integer, IM(3,5 To 9) As Integer, IP As Integer  ' a comment
    ['Dim', 'i', '()', 'Integer', 'IM', '(3,5 To 9)', 'Integer', 'IP', 'Integer', 
        ''', ' a comment']
    
    Dim WG(0 To (N + 1) / 2) As Double, WGK(0 To N) As Double, XGK(0 To N) As Double
    ['Dim', 'WG', '(0 To (N + 1) / 2)', 'Double', 'WGK', '(0 To N)', 'Double', 
        'XGK', '(0 To N)', 'Double']
    
    Dim j(0 To (N+1)/2, (0 To (N-1)/2)) As Integer
    ['Dim', 'j', '(0 To (N+1)/2, (0 To (N-1)/2))', 'Integer']

(You can also drop the definitions of 'anything' and 'range'.)



Here are some other suggestions:

<ol><li>Group - Since you can have multiple variables within a single Dim statement, wrap the expression decl in a Group, so that it is clear where the tokens for one variable stop and the next one starts.  Also simplifies iteration over the returned tokens.</li><li>results names - You have some optional fields in your grammar, which means that getting at the parsed tokens can be tricky.  As is, you'll end up writing some smelly code like `"if tokens[i].startswith("("): # then treat it as shape, otherwise it's the data type"`.  This is what results names are for in pyparsing.  If you assign 'name', 'shape', and 'type' results names to the components of the decl expression, you'll be able to access them directly within each grouped set of tokens.  If you implement Group and results names, you will be able to write this code to process the results:</li></ol>

    vars = dim.parseString(i)
        for var in vars[1:]:  # skip over leading 'Dim' token
            print var.name
            print var.type
            print var.shape

<ol><li>default datatype - Since VB allows you to declare 'Dim i', I assume that the default datatype is 'Integer'.  If so, you can specify this as the default with something like `Optional(Suppress("As")+datatype, default="Integer")`.</li><li>comments - You are lucky for now that your declarations are all on the same line, but if they span lines, you may have comments intruding anywhere within a decl.  Instead of narrowly defining that comment can come only after the trailing variable type, use ignore, as in `decl.ignore(comment)`.  Now a comment can appear anywhere within a statement, not just after the variable type.</li></ol>#'As ' vs. 'As' - this is another case where multiline decls could break.  If a programmer wrote 'As SomeVeryLongUserDefinedObjectTypeName', they might chose to break up the line after 'As', which would fail to match 'As '.  Better to just suppress('As') and let pyparsing hassle with the whitespace.



Since you are in learning mode, I'll leave you to implement these suggestions for yourself at your convenience/discretion/inclination/whimsy.  If you want more help with them, write back.



-- Paul
#### 2008-02-08 10:35:10 - ruedi-w
Paul



Thanks for your fast and precise answer! I'm following the 'tutorial programme' you outlined and (after reading ...) came accross two minor issues:



> Actually I wanted to parse the shape deeper
> I.e. split it first at `,` and then split at the {To} with applying a default for the first argument. Ok, it is quite trivial to do this later by hand, but it spreads grammar information accross the code.



> Interaction of default values and names  
> If I use `Optional( Suppress('As') + dtype, default='Variant' )('type')` I get `type: ['Double']`. This is just one level too much, but works with the default. 

On the other hand, if I use `Optional( Suppress('As') + dtype('type'), default='Variant' )`, my name is as I want it (`type: 'Double'`), but doesnt pick up the default value. 

Logically this is correct, but it's not nice.



Do you have an elegant solution to these (maybe the second one <em>should not</em> result in a unclean workaround. I'd rater use `var.shape[0]` than to work with some VBA-like 'flexibility'.



--Ruedi
#### 2008-02-08 13:08:30 - ruedi-w
Oups, my typing was faster than my brain: the <u>Interaction of default values and names</u> thing is solved tru an almost trivial parseAction:

    name = Word(alphas+'_', alphanums+'_')

    dtype = Optional( Suppress('As') + Word(alphas), default='Variant' )
    dtype.setParseAction(lambda t: t[0])  # <<<--- THIS MOVES TOKEN 1 LEVEL UP
    
    comment = ''' + restOfLine('comment')

    shape = nestedExpr()
    shape.setParseAction(keepOriginalText)

    decl = Group(
        name('name')
        + Optional(shape('shape'))
        + dtype('type')
        )
    decl.setParseAction(create_variable)

    dim = Suppress('Dim') + delimitedList(decl) + Optional(comment)



--Ruedi
#### 2008-02-09 10:42:25 - ptmcg
To parse the shape deeper, you'll need to handle any of the following (just from your examples):


    '()'
    '(3)'
    '(4,5)'
    '(3,5 To 9)'
    '(0 To (N + 1) / 2)'
    '(0 To N)'
    '(0 To (N+1)/2, (0 To (N-1)/2))'



Here is a mini-BNF for the simplest cases:



    dimension :: '(' integer TO integer ')' | integer TO integer | integer
    shape :: '(' [dimension [, dimension]...] ')'



This will cover all but the shapes that use arithmetic expressions instead of integers (an integer of course is just a very simple arithmetic expression with only a single term).



Arithmetic expressions used to be very complicated to compose, but the operatorPrecedence helper method simplifies things greatly.  To use operator precedence, first define a pyparsing expression for the most simple operand that will be found in the expression.  In this case, it will either be an integer or a variable name:



    operand = integer | varname



Now we define the operators, and create a list of tuples for each.  Each tuple defines:

- the operator or operators

- whether they are binary or unary

- whether they are left or right associative

- (optional) any parse action you would like to attach to the internal pyparsing expression for this operator level



The tuples are defined in a list in ascending order of precedence.  Here is an arithmetic expression lifted almost verbatim from simpleArith.py in the pyparsing examples directory:



    arithExpr = operatorPrecedence( operand,
        [('^', 2, opAssoc.RIGHT),
         (oneOf('+ -'), 1, opAssoc.RIGHT),
         (oneOf('* /'), 2, opAssoc.LEFT),
         (oneOf('+ -'), 2, opAssoc.LEFT),]
        )



Note that no ()'s are shown here - grouping with ()'s is implicit in the operatorPrecedence method.



As before, this may give you more detail than you need from the resulting text.  operatorPrecedence will return a structured list of tokens, with sublists that reflect grouping by ()'s and precedence of operations.  In your case, I don't think you want any of that detail, so you probably can use the same keepOriginalText trick that we used previously.



    arithExpr.setParseAction(keepOriginalText)



So finally, here is the pyparsing code to parse your variable shapes:



    integer = Word(nums)
    operand = integer | varname
    arithExpr = operatorPrecedence( operand,
        [('^', 2, opAssoc.RIGHT),
         (oneOf('+ -'), 1, opAssoc.RIGHT),
         (oneOf('* /'), 2, opAssoc.LEFT),
         (oneOf('+ -'), 2, opAssoc.LEFT),]
        )
    arithExpr.setParseAction(keepOriginalText)
    def setDefaultLowerBound(t):
        t['lowerBound'] = '1'
    dimension = Group(
        ('(' + arithExpr('lowerBound') + 'To' + empty + arithExpr('upperBound') + ')' ) | 
        (arithExpr('lowerBound') + 'To' + empty + arithExpr('upperBound')) | 
        arithExpr('upperBound').setParseAction(setDefaultLowerBound,keepOriginalText))
    shape = '(' + Optional(delimitedList(dimension)) + ')'



I had one tiny wrinkle when actually writing this code.  I had to insert `empty` expressions before the second `arithExpr` - to see what this does, try removing them and see the difference in the upperBound values.



With this in place, I modified the result-displaying code to:



    vars = dim.parseString(i)
            for var in vars[1:]:
                print var.name, 
                print var.type[0], 
                #~ print var.shape
                if var.shape:
                    for vardim in var.shape[1:-1]: # skip leading and trailing parens
                        print '%(lowerBound)s..%(upperBound)s' % vardim, 



And for your examples, this prints out:



    Dim j
    j Integer
    
    Dim j As Integer
    j Integer
    
    Dim i() As Integer, IM(3) As Integer, IP(4,5) As Integer
    i Integer
    IM Integer 1..3
    IP Integer 1..4 1..5
    
    Dim i() As Integer, IM(3,5 To 9) As Integer, IP As Integer  ' a comment
    i Integer
    IM Integer 1..3 5.. 9
    IP Integer
    
    Dim WG(0 To (N + 1) / 2) As Double, WGK(0 To N) As Double, XGK(0 To N) As Double
    WG Double 0..(N + 1) / 2
    WGK Double 0..N
    XGK Double 0..N
    
    Dim j(0 To (N+1)/2, (0 To (N-1)/2)) As Integer
    j Integer 0..(N+1)/2 0..(N-1)/2



Cheers,

-- Paul
#### 2008-02-12 10:29:08 - ruedi-w
I needed a while to understand why you parse the expressions - indeed, I do not want then parsed, the keepOriginalText action is perfect. I just had the hope to get away without writing an expression grammar. But it is simple enough.



Thanks for that great code!

-- Ruedi

---
## 2008-02-08 06:43:37 - michael_ramirez44 - Adding to parse string during parsing
Is it possible to add to the parse string during parsing. My grammar has include statements that I want to resolve during parsing.

#### 2008-02-08 06:54:17 - ptmcg
Sorry, but no.  I suggest you use an initial pass to resolve include statements, including recursion and loop-guarding (in case A includes B and B includes A), then a second pass to parse for content.



-- Paul

---
## 2008-02-08 10:38:50 - propell - Tips for making a BibTeX parser faster?
I am currently having some fun writing a parser for parsing BibTeX data () I now have a fully working parser. Unfortunately it is a bit slow when parsing large files. Here is the code:





    from pyparsing import *
    
    import pprint,string
    
    def filter_str(instr,removechars):
        for c in removechars:
            instr = instr.replace(c,'')
        return instr
    
    
    '''
    Simple BNF for parsing a BibTeX file.
    Based on: http://www.gerg.ca/software/btOOL/doc/bt_language.html
    
    <comment> :: '%' - end of line ;
    <bibfile> :: <entry>*;
    <entry>   :: <bib_entry> | <macro_entry> | <preamble_entry> | <comment_entry>
    <bib_entry>   :: '@' <type> ('{' <key> ',' <fields> '}') |
                            ('(' <key> ',' <fields> ')') ;
    <macro_entry>   :: '@' <type> ('{' <field> '}') | ('(' <field> ')');
    <preamble_entry> :: '@' <type> ('{' <value> '}') | ('(' <value> ')');
    <comment_entry> :: '@' <type> <string>;
    
    <fields>  :: <field> (',' fields);
    <field>   :: <name> '=' <value>;
    <value>   :: <simple_value> ('#' <simple_value>)*;
    <simple_value> :: <string> | <number> | <name> ;
    
    <name>    :: 'a-zA-Z0-9' '!$&*+-./:;<>?[]^_`|'
    <key>     :: <name> | <number>
    <string>  :: <quoted_string> | <braces_string>
    <type>    :: <name>
    '''
    
    
    
    # basic punctation
    COMMA, LBRACK, RBRACK, EQUAL, AT, HASH,LPAR,RPAR = map(Suppress,',{}=@#()')
    
    comment = '%' + restOfLine
    
    name = Word(filter_str(printables,'{}, ='))
    key = Optional(name,'')('key')
    number = Word(nums)
    entrytype = Word(alphanums)('entrytype')
    
    quoted_string = QuotedString(''',multiline=True,escChar='\\')
    braced_string = (nestedExpr('{', '}'))
    
    bstring = (quoted_string | braced_string)('string')
    
    bstring.setParseAction(keepOriginalText)
    # remove braces/quote chars
    bstring.addParseAction(lambda tokens: tokens[0].strip()[1:-1])
    
    simplevalue = (bstring('string') | number('number') | name('macro'))
    value = Group(simplevalue + ZeroOrMore(HASH + simplevalue))('value')
    field = (name('fieldname') + EQUAL+ value)('field')
    fields = Group(Group(field) + ZeroOrMore(COMMA+Group(field)))
    
    bib_entry = (AT + entrytype + ((LBRACK+key+COMMA+ fields('fields')+RBRACK) | \
                   (LPAR+key+COMMA+ fields('fields')+RPAR)))('bib_entry')
    macro_entry = (AT + CaselessKeyword('string')('entrytype') + \
                 ((LBRACK + field + RBRACK) | (LPAR + field + RPAR)))('macro_entry')
    comment_entry = (AT + CaselessKeyword('comment')('entrytype') + \
                     bstring)('comment_entry')
    preamble_entry = (AT + CaselessKeyword('preamble')('entrytype') + \
                       value)('preamble_entry');
    entry = bib_entry | macro_entry | comment_entry | preamble_entry
    
    #entry.ignore(comment)
    if __name__ == '__main__`:
        teststrings = []
        teststrings.append('''
    @BOOK{texbook,
       author = {Donald E. Knuth},
       title= {The {{\TeX}book}},
       publisher = 'Addison-Wesley',
       year = 1984,
       note = jan # {test}
       }
    ''')
        teststrings.append('''@STRING{jan = 'january' }''')
        teststrings.append('''@comment{some text}''')
        for s in teststrings:
            print entry.parseString(s).asXML()
    



The parser works very well and was fun to write. Here is the output from the above code:





    <bib_entry>
      <entrytype>BOOK</entrytype>
      <key>texbook</key>
      <fields>
        <field>
          <fieldname>author</fieldname>
          <value>
            <string>Donald E. Knuth</string>
          </value>
        </field>
        <field>
          <fieldname>title</fieldname>
          <value>
            <string>The {{\TeX}book}</string>
          </value>
        </field>
        <field>
          <fieldname>publisher</fieldname>
          <value>
            <string>Addison-Wesley</string>
          </value>
        </field>
        <field>
          <fieldname>year</fieldname>
          <value>
            <number>1984</number>
          </value>
        </field>
        <field>
          <fieldname>note</fieldname>
          <value>
            <macro>jan</macro>
            <string>test</string>
          </value>
        </field>
      </fields>
    </bib_entry>
    
    <macro_entry>
      <entrytype>string</entrytype>
      <fieldname>jan</fieldname>
      <value>
        <string>january</string>
      </value>
    </macro_entry>
    
    <comment_entry>
      <entrytype>comment</entrytype>
      <string>some text</string>
    </comment_entry>
    



The parser is unfortunately quite slow when parsing large bib files. Parsing the following file:







with 1685 entries takes 50 sec on my three year old laptop:





    data = open('graphs.bib').read()
    entries = [ent for ent in entry.searchString(data)]



Are there any tricks I can try to speed up the parser?



- Kjell Magne Fauske

#### 2008-02-10 10:38:44 - ptmcg
Very interesting application!  There are a number of folks who have taken a stab at generic TeX parsing (mathtext module of matplotlib, for one), but things get hairy quickly!



I downloaded your code and sample file, and found a few items to clean up.  First of all, you have these errors in your input file:



    %article at line 5288 should be @article
    %article at line 5302 should be @article
    %mastersthesis at line 6293 should be @mastersthesis
    %techreport at line 6465 should be @techreport
    } at end of line 9500 should be ,
    Mismatched '}' at line 1247
    Mismatched '}' at line 3842
    Mismatched '}' at line 5454
    Mismatched '}' at line 5976
    Mismatched '}' at line 7559
    Mismatched '}' at line 9448
    Mismatched '}' at line 9454
    Mismatched '}' at line 9459
    Mismatched '}' at line 9518



Unfortunately, you can't use psyco with this parser, since the keepOriginalText method call to inspect seems to result in a pysco exception. (I tried various methods documented for psyco to tiptoe around this method, but could get none of them to work.  Even just calling `import psyco` seems to break the code.)



The only substantive performance change that I could see was in changing the approach to ignoring of comments, but it would require a reduction in functionality.  In your input file, all of your comments are complete line comments, that is, they all look like:



    % The next section is ...
    % The general format is ...



None of them are actually embedded within any bib entries.  By using `entry.ignore(comment)`, you allow comments to appear anywhere within an entry, which is certainly a good comprehensive approach.  But it comes with a cost: pyparsing must first stop and look for comments between any two pyparsing expressions, and this takes some time.  Using `entry.ignore(comment)` (and a corrected input file), this script takes about 53 seconds on my laptop.



Instead of using `entry.ignore(comment)`, I then changed the definition of entry to:



    entry = bib_entry | macro_entry | comment_entry | preamble_entry | comment.suppress()



Now comments cannot be embedded within an entry, but your complete line comments are just fine, and you can choose to suppress them or not.  Making this change cuts the run time for me down to 42 seconds, about 20% faster.  But you'll have to decide if this functionality restriction is acceptable/worth the improvement.



I also looked into rearranging the order of expressions defined in entry.  Since entry is a MatchFirst, it may be worth putting more frequently occurring expressions to the front (taking care that shorter or more general expressions don't mask longer or more specific expressions).  I did this by adding a tally to the grammar, with parse actions used to attach a key to each expression.  Here is how that is done:



First set up a tally dictionary.  I'm using a defaultdict here, but I could have just used a normal dict and initialized it to the 4 known entry types:



    from collections import defaultdict
    typetally = defaultdict(int)
    def tally(s):
        def pa():
            typetally[s] += 1
        return pa



Note that `tally` is <em>not</em> itself a parse action, but it is a little parse action factory method.  It is called with an argument to help qualify the specific entry type the parse action is supposed to tally, and `tally` then returns a function that will do this correctly.



Then I attach tallying parse actions to each expression:



    macro_entry.setParseAction(tally('macro'))
    comment_entry.setParseAction(tally('comment'))
    preamble_entry.setParseAction(tally('preamble'))
    bib_entry.setParseAction(tally('bib'))



Now when the parser runs, as each entry type is found, its tally in the tally dict will get incremented.  After the parsing is over, I can then just print out the results:



    for k in typetally.keys():
            print k,typetally[k]



And for your data file, I get these values:



    macro 232
    bib 1463

(which fortunately adds up to the 1685 number you expected!)



As it turned out, this frequency information didn't really help me much in improving the performance of your grammar, but I thought it might be a useful technique sometime in the future.



-- Paul
#### 2008-02-10 11:51:44 - propell
Thank you Paul for the thorough analysis. I really appreciate you taking the time to answer. I am sorry for posting input data with errors. I have posted a cleaned up version here: 



You are right about the comments. Including them in the grammar makes the parser slower. For my application I can ignore the comments because they will never appear inside an entry. In fact everything outside an entry can be ignored.  



I just discovered that one of the the main bottlenecks in the code is the snippet





    bstring = (quoted_string | braced_string)('string')
    
    bstring.setParseAction(keepOriginalText)
    # remove braces/quote chars
    bstring.addParseAction(lambda tokens: tokens[0].strip()[1:-1])
    



Removing the keepOriginalText parse action reduces the parse time to approx. 20 sec! That gives me something to work on. For my application I can probably puzzle together the bstring pieces myself without any loss of information. 



The application I have in mind is to parse BibTeX files generated by the program JabRef. The output from Jabref is always well formed and don't use some of BibTeX' optional syntax. I can probably simplify and speed up the parser by removing unused elements from the grammar. 



- Kjell Magne Fauske
#### 2008-02-10 15:09:23 - ptmcg
Ah! This is very helpful!  Here are some more things to think on, and a proposal that cuts parse time in half!



1. keepOriginalText is only required for a nested braced string, it is not needed for quoted_string.

2. Looking at your data, there are <em>many</em> braced strings that are not nested at all.  I thought there might be a performance gain in trying to match a simple unnested braced string first, and only if nested braces were found would we need to drop back and use the nested form, followed by keepOriginalText.  A key point is that the unnested string does <em>not</em> need to call keepOriginalText, and is also much simpler to parse.



Combining these notes, I end up with this code fragment:



    unnested_braced_string = Combine('{' + CharsNotIn('{}') + '}')
    nested_braced_string = nestedExpr('{', '}').setParseAction(keepOriginalText)
    
    bstring = (quoted_string | unnested_braced_string | nested_braced_string)('string')



Now I can parse your input file in only 17.5 seconds!
#### 2008-02-11 00:09:38 - propell
Indeed! The nested and unnested version optimization works like a charm. Now the parser is fast enough for most purposes. It is not often I need to parse files with thousands of entires.  



Writing the BibTeX parser has taught me a lot about Pyparsing. Thank you for the help!



- Kjell Magne Fauske

---
## 2008-02-09 12:09:58 - carterson2 - I need a C preprocessor written in python
Hi,



I want to get rid of #define and #include and then see what variables are left. Then I am going to categorize variables into const, local, global.



This is part of a project I wish you would join in. Its new.







I have a bunch of DSP C code that we are converting.



Q: Can you point me to any appropriate tools?



thanks

jim

#### 2008-02-11 06:05:14 - carterson2
speechless.
#### 2008-02-12 11:46:28 - carterson2
Q: How do I clean up this output. (ignore the THIS)

(boy do I wish I lived in Austin...)

-jim



--------prints out----------------------------------

    BOOL sub_1(THIS, [['int', 'k', ','], ['int', 'j']] ) 
    {
        i=6;
    }

----------------------code-----------------------

    from pyparsing import *
    
    sample = '''
    BOOL sub_1(int k, int j)
    {
        i=6;
    }
    '''
    ident = Word(alphas, alphanums+'_')
    datatype = oneOf('void int char float BOOL')
    variableGroup= Group( datatype + ident + Optional(',') )
    functionArgs= OneOrMore( variableGroup )
    functionVar = datatype('type') + ident('fnname') + '(' + functionArgs('args') + ')' + '{'
    
    def transformExtern(toks):
        return '%(type)s %(fnname)s(THIS, %(args)s ) \n{' % toks
    functionVar.setParseAction(transformExtern)
    
    print functionVar.transformString(sample)

#### 2008-02-13 07:03:56 - ptmcg
Pyparsing provides a decorator for debugging parse actions, called `traceParseAction`.  Use it like this in your code:

    @traceParseAction
    def transformExtern(toks):
        return '%(type)s %(fnname)s(THIS, %(args)s ) \n{' % toks



You are getting groups because of the Group expressions in your grammar.  You can either do something like `" ".join(toks)` or use the keepOriginalText parse action to revert any parsed groups back to the original input text.  Since functionArgs is the expression that is not working properly, this is where I would add keepOriginalText:



    functionArgs= OneOrMore( variableGroup ).setParseAction(keepOriginalText)



This gets us pretty close, but the text is still wrapped as a single-element list.  Another parse action can do the unwrapping for us:



    ungroup = lambda toks: toks[0]
    functionArgs= OneOrMore( variableGroup ).setParseAction(keepOriginalText,ungroup)



I'm sorry if this is looking like black magic, please be patient.



-- Paul



(Also, please use the wiki markup to set off you code in code blocks - this preserves indentation, which is so critical to Python.  There is a link labeled 'help on how to format text' in the lower right corner below the Reply box, that should show you how, and also a banner comment at the top of the 'Discussion' tab listing.)
#### 2008-02-14 09:40:36 - carterson2
Thanks for the continued support. Without pyparsing, I would have abandoned this effort....



I need to support 'extern int i,j,k on different lines' See below:

(I have two commented lines below that I tried, but they crash)

-jim







    from pyparsing import *
    
    sample= '''
    extern int i,
                j;
    void fn(int ii, int *j) {
        i = 6;
    }
    '''
    
    externs = []
    externRef = Forward()
    varsUsed = []
    
    ident = Word('*'+alphas, alphanums+'_')
    datatype = oneOf('void short int char float BOOL')
    fnVariableGroup= Group( datatype + ident + Optional(',') )
    exVariableGroup= Group(            ident + Optional(',') )
    ungroup= lambda toks: toks[0]
    functionArgs= OneOrMore( fnVariableGroup ).setParseAction(keepOriginalText,ungroup)
    externArgs  = OneOrMore( exVariableGroup ).setParseAction(keepOriginalText,ungroup)
    externFn    =            datatype('type') + ident('fnname') + '(' + functionArgs('args') + ')' + '{'
    externVar   = 'extern' + datatype('type') + ident('varname')
    #externVar  = 'extern' + datatype('type') +                         externArgs('args') + ';'
    
    @traceParseAction
    def transformExtern(toks):
        return 'struct this { %(type)s %(varname)s; } _this' % toks
    #    return 'struct this { %(type)s %(args)s;    } _this' % toks
    externVar.setParseAction(transformExtern)
    
    def addExternVarnameToList(toks):
        externs.append(toks.varname)
        externRef         << MatchFirst(map(Keyword,externs))
    externVar.setParseAction(addExternVarnameToList, transformExtern)
    
    def convertExternRef(toks):
        varsUsed.append(toks[0])
        return '_this->' + toks[0]
    externRef.setParseAction(convertExternRef)
    
    def transformFunction(toks):
        return '%(type)s %(fnname)s(THIS, %(args)s ) \n{' % toks
    externFn.setParseAction(transformFunction)
    
    print (externFn | externRef | externVar).transformString(sample) 


#### 2008-02-14 13:59:18 - carterson2
Hi Paul,



I put out a project using pyparsing.

Let me know your thoughts.





-jim
#### 2008-02-15 06:43:13 - ptmcg
Ok, I've downloaded it, I'll take a look at it in the next few days.



-- Paul
#### 2008-02-25 06:40:28 - ptmcg
Jim -



Sorry not to look at this code earlier, you got pushed too far down my interrupt stack.



Just a style point on the cleanSlate method:



    def cleanSlate():
        #bug: I dont know how to zero-out these puppies????
        #~ list([externs.pop() for z in xrange(len(externs))])
        #~ list([varsUsed.pop() for z in xrange(len(varsUsed))])
    
        # list comps in place of for or while loops is not good form
    
        # instead try this
        while externs:
            externs.pop()
    
        # even better, del a slice which contains the whole list
        del externs[:]
        del varsUsed[:]



Your method names are not PEP-8 compliant either, which will garner you shame and derision amongst the Python faithful (as has pyparsing - I always thought method_names_like_this went out with Tcl/Tk).



The first step in recognizing 'extern int i,j,k;' is to update the externVar expression:



    externVar = 'extern' + datatype('type') + ident('varname')

becomes



    externVar = 'extern' + datatype('type') + delimitedList(ident)('varname')



Then in the extern var parse action that extracts the variable names, do:



    externs.extend( vn.strip('* ') for vn in tokens.varname )



I noticed that you added '*' as a possible leading character, as in:



    ident = Word('*'+alphas, alphanums+'_')



This is ok for *i, but fails for **i, or ******k.  Isn't C fun?!  Instead try:



    ident = Combine(Optional(Word('*')) + Word(alphas, alphanums+'_'), adjacent=False)



I think you will need to take two passes at your input file, to handle the case where there are multiple extern refs.  In this way, you can handle the 'extern int i,j,k;' items too.  In pass 1, just look for the extern refs, and collect the variable declarations (not just the names, you'll need the types too).  Then in pass 2, prefix the output with 'struct {' + ';'.join(previouslyFoundVarDecls) + '} _this;', strip the extern refs (just use suppress() to filter them out), and perform the insertion of '_this->' at every symbol reference.
#### 2008-02-25 22:01:31 - carterson2
Thanks very much for that. I will put your suggestions in tomorrow.



One other C style I would like to handle is:

    extern int i,
               j,
               k;

As you can tell, my Python skills 'float like a duck', so perhaps you could help me with that one. The other one I had (which I think I solved) is:



    extern int *ptr[N];



thanks

jim
#### 2008-02-26 01:05:00 - ptmcg
If you get to parsing



    extern int i,j,k;

then the same parser will handle



    extern int i,
               j,
               k;



Pyparsing will just skip over the newlines as whitespace.



For the trailing '[N]', just add another Optional expression to the varname expression to handle it.  Note that '[N][M]' is also possible, as is '[N+1]'.  For now, let's assume you *don't* need to handle nested []'s, then your array definition could be:



    arrayDim = '[' + SkipTo(']') + ']'
    arrayDefn = Combine(OneOrMore(arrayDim), adjacent=False)



And then add this optional element to your externVar:



    externVar = 'extern' + datatype('type') + \
        delimitedList(Group(ident+Optional(arrayDefn)))('varname')



Now the elements for each variable in varName will be one- or two-element lists instead of just plain strings, so you'll need to adjust your code accordingly.  Perhaps give names to each field:



    externVar = 'extern' + datatype('type') + \
        delimitedList(Group(ident('name')+
                            Optional(arrayDefn)('arrayDef')))('vars')



To access these elements, iterate over the items returned by the delimitedList:



    for v in results.vars:
        print v.name, v.arrayDef



As you can see, parsing C variable declarations gets complicated once you include pointers and arrays.



-- Paul
#### 2008-02-26 12:51:51 - carterson2
If possible, I need to ignore functions like

    extern int fn();

#### 2008-02-26 12:52:17 - carterson2
oops,



    extern int fn();
#### 2008-02-26 14:30:55 - ptmcg
What is it about fn that it should be ignored?

<ul><li>that it is a function of any kind?</li><li>that it is a function with no arguments?</li><li>that it is a function that starts with 'f'?</li></ul>

If all functions are to be ignored, then define something like:



    externFuncRef = 'extern' + datatype + ident + nestedExpr('(', ')') + ';'



By 'ignore' do you mean 'leave as is'?  If so then just change your call to transformString to insert externFuncRef to be tested before any other expression, and don't transform it at all.



    print (externFuncRef | externFn | externRef | externVar).transformString(sample)

(This might require adding the line 'externFuncRef.setParseAction(keepOriginalText)'.)



If by 'ignore' you mean 'remove from the output', then just add this instead:



    print (externFuncRef.suppress() | externFn | externRef | externVar).transformString(sample)



At this point, Jim, this program feels like it is getting out of control.  We started with some simple examples, then added some more cases, then some more.  It's not clear to me if we have addressed all of the requirements, nearly all of them, or have we just scratched the surface?



I think that instead of adding any more bits to the list of what to change and what to leave, and how the changes should be done, it is probably best to <strong>stop</strong>, write down all the cases you want to test for, what should be done with each one, and what special cases should be skipped over.  We've done some good experimentation so far, but now I think we should look at <em>all</em> the transforms that should be done, and address them as a whole.  Then we can also work through the first-pass/second-pass logic, since we will have a clear idea of what should/could be done in one pass vs. the other, how much data must be captured and retained about any given variable or function from the first pass to be properly represented in the second pass, etc.



I think <em>you</em> probably understand the problem space, but to me it is unfolding only one little bit at a time, and the program is just getting hacked together one patchwork piece at a time (add '*' support; now add array refs; this kind of function should be changed; that kind should be left alone; this other kind should just be removed; etc.).



The other advantage is that we will be able to write some test cases, <em>and</em> we'll have a pretty good idea of when we are done.
#### 2008-02-10 11:40:17 - ptmcg
I followed your web link, and found this example of the kind of transform you want to make in this C code:



    --------------before---------------------------------- 
    extern int b; 
    int subroutine() { b=5; } 
    --------------after----------------------------------- 
    struct this { int b; } _this; 
    int subroutine(struct this *_this) { _this->b= 5; } 
    ------------------------------------------------------ 



Since you are writing a transformation utility, let me get you in the right mindset as far as how pyparsing will do this for you.  Most of the examples you will see in parsing read like:



    expr1 = expr1a | expr1b
    allExprs = OneOrMore(expr1)
    results = allExprs.parseString(inputText)



This goes through some input text, which is composed entirely of text matching either expr1a or expr1b.



In your case, you aren't really trying to parse all the C code, just trying to find some patterns in it and doing some transforms on them.  For this kind of application, pyparsing provides transformString.  transformString works by allowing you to define data conversions in parse actions, and transformString will insert any modified tokens in place of the original matched ones.  Here's an example from the examples directory (excerpted from scanExamples.py):



    # simulate some C++ code
    testData = '''
    #define MAX_LOCS=100
    #define USERNAME = 'floyd'
    #define PASSWORD = 'swordfish'
    
    a = MAX_LOCS;
    CORBA::initORB('xyzzy', USERNAME, PASSWORD );
    
    '''
    
    ident = Word(alphas, alphanums+'_')
    
    
    # convert C++ namespaces to mangled C-compatible names
    scopedIdent = ident + OneOrMore( Literal('::').suppress() + ident )
    scopedIdent.setParseAction(lambda t: '_'.join(t))
    
    print '(replace namespace-scoped names with C-compatible names)'
    print scopedIdent.transformString( testData )



Prints out:



    (replace namespace-scoped names with C-compatible names)
    
    #define MAX_LOCS=100
    #define USERNAME = 'floyd'
    #define PASSWORD = 'swordfish'
    
    a = MAX_LOCS;
    CORBA_initORB('xyzzy', USERNAME, PASSWORD );



So the first part of your parser is to locate and transform the 'extern'-defined variables.



    ident = Word(alphas, alphanums+'_')
    datatype = oneOf('int char float')
    externVar = 'extern' + datatype('type') + ident('varname')



The transforming parse action looks like:



    def transformExtern(toks):
        return 'struct this { %(type)s %(varname)s; } _this' % toks



And we connect it to externVar using setParseAction:



    externVar.setParseAction(transformExtern)



With this definition of externVar, we can use your 'before' code as the input sample, and write this code:



    sample = '''
    extern int b; 
    int subroutine() { b=5; } 
    '''
    
    print externVar.transformString(sample)



And we get:



    struct this { int b; } _this; 
    int subroutine() { b=5; } 



Now matching the 'b' in 'b=5' is a little harder.  We can take some hints from the macroProcessor.py example, in which we define a self-modifying grammar, containing a Forward expression.  As other expressions execute attached parse actions when recognizing a macro definition, the Forward expression gets modified to recognize the macro names, and substitute the macro definition for the macro name itself.  Here we will do something very similar looking for variable names instead of macro names.



First we'll create a list of the variable names we are to look for, and a Forward that will contain the expression of all possible variable names:



    externs = []
    externRef = Forward()



Next, we define the transforming parse action, which prefixes the variable name with '_this->', and attach it to subExternRef:



    def convertExternRef(toks):
        return '_this->' + toks[0]
    externRef.setParseAction(convertExternRef)



Lastly, we create a second parse action to get run when we first encounter extern vars, so that the varname will be added to the list of all extern vars seen so far, and update the content of the subExternRef Forward.  Note that we don't just look for Literal's of the var names - in this case we don't want to transform <em>every</em> 'b' in the source, just those b's that are standalone keywords:



    def addExternVarnameToList(toks):
        externs.append(toks.varname)
        externRef << MatchFirst( [ Keyword(e) for e in externs ] )



We want this parse action to get run before the previously-defined transformExtern, we can do this in a single setParseAction call:



    externVar.setParseAction(addExternVarnameToList, transformExtern)



We now have two things to look for while transforming our input file, externVar's, and externRef's:



    print (externRef | externVar).transformString(sample)



This gives us:



    struct this { int b; } _this; 
    int subroutine() { _this->b=5; } 



Can you try to run with this for a bit?  I'm not sure I can be very active on your project, as I have a paying job that I really have to put ahead of my open source hobbies.



But please write back if you want more help.



-- Paul



Here is the entire program, so you don't have to piece it together from amongst the above ramblings:



    from pyparsing import *
    
    sample = '''
    extern int b; 
    int subroutine() { b=5; } 
    '''
    
    ident = Word(alphas, alphanums+'_')
    datatype = oneOf('int char float')
    externVar = 'extern' + datatype('type') + ident('varname')
    
    def transformExtern(toks):
        return 'struct this { %(type)s %(varname)s; } _this' % toks
    externVar.setParseAction(transformExtern)
    
    print externVar.transformString(sample)
    
    externs = []
    externRef = Forward()
    
    def convertExternRef(toks):
        return '_this->' + toks[0]
    externRef.setParseAction(convertExternRef)
    
    def addExternVarnameToList(toks):
        externs.append(toks.varname)
        externRef << MatchFirst(map(Keyword,externs))
    externVar.setParseAction(addExternVarnameToList, transformExtern)
    
    print (externRef | externVar).transformString(sample)



---
## 2008-02-11 12:10:39 - wcbarksdale - What is matched by outer patterns?


    >>> one = OneOrMore(Literal('a') | Literal('b')).setResultsName('one')
    >>> one.parseString('a a b a b')
    (['a', 'a', 'b', 'a', 'b'], {'one': [((['a', 'a', 'b', 'a', 'b'], {}), -1)]})
    >>> two = (one | Literal('c')).setResultsName('two')
    >>> two.parseString('a a b a b')
    (['a', 'a', 'b', 'a', 'b'], {'two': [('a', 0)], 'one': [((['a', 'a', 'b', 'a', 'b'], {}), -1)]})



Why does the results name 'two' refer only to ('a') rather than everything that 'one' refers to?

#### 2008-02-11 13:21:58 - ptmcg
Urk!  Looks like a bug.  I'll try to look at it later today or tomorrow.



-- Paul

---
## 2008-02-13 04:07:12 - plomlund - No exception on parse error
I have tried to rewrite an old config file parser I once wrote in yacc to Python using pyparsing. The parser works nicely, except it never gives me a parse error, no matter what it encounters! The parser just stops and returns whatever it has parsed up until the faulty input. I cannot figure out what is going on, and would really appreciate if somebody could point out where the logic is flawed. A stripped down version of the parser, including the input, is given below. If you uncomment the first line it returns nothing, otherwise it returns precisely what it is supposed to. 





    from pyparsing import *
    
    def tester(str):
            prtable = alphanums+r'!$%&*+-./<>?@^_|~'
    
            kstr=Word(prtable) ^ quotedString.setParseAction(removeQuotes)
            name = Word(alphas+'_',alphanums+'_')
    
            section=Forward()
            keyword = name + '=' + kstr
            section << name + '{'+ ZeroOrMore(section|keyword) + '}'  
    
            parser=ZeroOrMore(section|keyword) 
            parser.ignore(pythonStyleComment)
            print parser.parseString(str)
    
    input='''
    # Uncommenting this line makes the parser return [], without raising an exception
    bool=off        
    str='May the foo be with you'
    real=1.0
    int=42
    sect1 { 
            sect2 {    
                    key2=bar   
            }
            key1=foo
    }
    '''
    tester(input)



#### 2008-02-13 06:34:11 - ptmcg
If you expect the end-of-string after all of your sections or keywords, add a StringEnd() at the end of your grammar:



    parser=ZeroOrMore(section|keyword) + StringEnd()



-- Paul
#### 2008-02-13 07:51:22 - plomlund
Brilliant! I was getting desperate ;)



.j.
#### 2008-02-13 08:11:11 - ptmcg
You might also check out the configParse.py example () for some other ideas, such as using Dict, or dictOf to auto-define results names.



-- Paul

---
## 2008-02-14 14:32:35 - michael_ramirez44 - Keywords matched as Identifier
How do I match an identifier only if its not a Keyword?

#### 2008-02-15 06:24:40 - ptmcg
You didn't post any code, so I'll have to guess.  Look over the following code and see if the comments are helpful.



-- Paul





    from pyparsing import *
    from pprint import pprint
    
    if_ = Keyword('if')
    then_ = Keyword('then')
    else_ = Keyword('else')
    endif = Keyword('endif')
    true_ = Keyword('true')
    false_ = Keyword('false')
    
    ident = Word(alphas, alphanums+'_')
    
    test = '''
        if if_condition() then
            if if_condition2() then
                if_part()
            endif
        else
            else_part()
        endif
    '''
    
    stmt = Forward()
    boolExpr = true_ | false_ | Combine(ident+'()')
    if_stmt = if_ + boolExpr + then_ + stmt + \
                Optional(else_ + stmt) + endif
    stmt << Group( Combine(ident + '()') | if_stmt )
    
    # keywords are not confused with idents, since they are not
    # matchable as general statements
    pprint( OneOrMore(stmt).parseString(test).asList() )
    
    # this retrieves all 'idents' even keywords - this is not desirable
    print ident.searchString(test) 
    
    anyKeyword = if_ | then_ | else_ | endif | true_ | false_
    ident.ignore(anyKeyword)
    
    # now keywords will be skipped - better!
    print ident.searchString(test) 
    
    # this approach does not work with searchString/scanString, but may be 
    # useful in a parser
    ident = ~anyKeyword + Word(alphas, alphanums+'_')
    



Prints:



    [['if',
      'if_condition()',
      'then',
      ['if', 'if_condition2()', 'then', ['if_part()'], 'endif'],
      'else',
      ['else_part()'],
      'endif']]
    [['if'], ['if_condition'], ['then'], ['if'], ['if_condition2'],
        ['then'], ['if_part'], ['endif'], ['else'], ['else_part'], 
        ['endif']]
    [['if_condition'], ['if_condition2'], ['if_part'], ['else_part']]



---
## 2008-02-21 12:35:51 - scripteaze - i think i might be doing this the long way.
end result will be the ipaddress and version outputed to an html page.for right now though, im just wondering if what ive done so far is valid.

i do not want the code done for me, just want someone to point me in the right direction.

thanks in advance

here is normal output from the nmap scan

    Starting Nmap 4.53 (  ) at 2008-02-21 14:33 CST
    Interesting ports on 51.72.188.74:
    PORT   STATE    SERVICE VERSION
    80/tcp filtered http


    
    import os,string,sys
    from pyparsing import Word, Combine, Suppress, nums, delimitedList, SkipTo
    
    def scanA():
    
        # variables for parsing
    
        integer = Word(nums)
        ip = delimitedList(integer, '.', combine=True)
    
        header = Suppress('Starting Nmap')
        cstTAG = Suppress('CST')
    
        start  = Suppress('Interesting ports on')    
        middle = Suppress('PORT   STATE    SERVICE VERSION')
        rport  = Suppress('80/tcp')
    
        # havent figured this part out yet
        status = ''
        ver    = ''
    
        # define sub pattern
    
        intro  = header + SkipTo(cstTAG) + cstTAG
        line1  = start + ip.setResultsName('ipaddy') + ':'
        line2  = middle
        line3  = rport + status + ver
    
        # define pattern
    
        ptrn   = intro + '\n' + line1 + '\n', line2 + '\n', line3
    
        # run scan, then read into list
    
        scantype   = 'nmap -sV -T3 -P0 -PN -iR 1 -p80'
        result     = os.popen(scantype).read()
    
    
    
        for scan, startscan, endscan in ptrn.scanString(result):
            print scan.ipaddy

    scanA()
    

the output to this is currently scrolling one word per line like so. which i will work out later

    S
    t
    a
    r
    t
    i
    n
    g
    
    N
    m
    a
    p
    
    4
    .
    5
    3
    
    (
    
    h
    t
    t
    p
    :
    /
    /
    i
    n
    s
    e
    c
    u
    r
    e
    .
    o
    r
    g
    
    )
    
    a
    t
    
    2
    0
    0
    8
    -
    0
    2
    -
    2
    1
    
    1
    4
    :
    3
    1
    
    C
    S
    T
    
    
    I
    n
    t
    e
    r
    e
    s
    t
    i
    n
    g
    
    p
    o
    r
    t
    s
    
    o
    n
    
    1
    5
    2
    .
    2
    4
    9
    .
    1
    8
    .
    4
    :
    
    
    P
    O
    R
    T
    
    
    
    S
    T
    A
    T
    E
    
    
    
    
    S
    E
    R
    V
    I
    C
    E
    
    V
    E
    
    
    S
    I
    O
    N
    
    
    8
    0
    /
    t
    c
    p
    
    f
    i
    l
    t
    e
    r
    e
    d
    
    h
    t
    t
    p
    
    
    
    
    S
    e
    r
    v
    i
    c
    e
    
    d
    e
    t
    e
    c
    t
    i
    o
    n
    
    p
    e
    r
    f
    o
    r
    m
    e
    d
    .
    
    P
    l
    e
    a
    s
    e
    
    r
    e
    p
    o
    r
    t
    
    a
    n
    y
    
    i
    n
    c
    o
    r
    r
    e
    c
    t
    
    r
    e
    s
    u
    l
    t
    s
    
    a
    t
    
    h
    t
    t
    p
    :
    /
    /
    i
    n
    s
    e
    c
    u
    r
    e
    .
    o
    r
    g
    /
    n
    m
    a
    p
    /
    s
    u
    b
    m
    i
    t
    /
    
    .
    
    N
    m
    a
    p
    
    d
    o
    n
    e
    :
    
    1
    
    I
    P
    
    a
    d
    d
    r
    e
    s
    s
    
    (
    1
    h
    o
    s
    
    
    u
    p
    )
    
    s
    c
    a
    n
    n
    e
    d
    
    i
    n
    
    0
    .
    2
    6
    3
    
    s
    e
    c
    o
    n
    d
    s

#### 2008-02-21 13:54:15 - ptmcg
Overall, you are on the right track.  I can't explain why the output is one-char/line, unless popen('lsjfdlj').read() is doing this.



Here are few notes on your pyparsing content:



1. In intro, instead of SkipTo(cstTag), I think you just want to read the rest of the line, use restOfLine.



2. There are a couple of problems with:



    ptrn   = intro + '\n' + line1 + '\n', line2 + '\n', line3

a. you have commas where you really want to have '+'s

b. whitespace literals will not match when pyparsing is using the default whitespace character set - use LineEnd()'s instead.  Or even just try leaving them out altogether, they are just whitespace after all.



3. To get this to work, I had to fake in something for status and ver, like this:



    status = oneOf('filtered unfiltered raw')
        ver    = oneOf('http https ftp nntp')

Else I got a compile time error about creating Literal('') expressions, which is not allowed.



4. There is one more error in your pattern matching.  I wont tell you what it is, but here is how to do some checking on how your grammar is working - use setDebug().  Do this after defining ptrn:



    intro.setDebug()
        line1.setDebug()
        line2.setDebug()
        line3.setDebug()



This will echo out 'Matching ...'/'Matches...' or 'Matching...'/'Exception' pairs for each expression that has been set for debug.  You can also define your own debugging output routines.



-- Paul
#### 2008-02-21 17:23:46 - scripteaze


    ptrn   = intro + '\n' + line1 + '\n', line2 + '\n', line3

tyvm, must have over looked the commas

<hr />


using 'restOfLine'



would this be something like:



    intro  = header + restOfLine + cstTAG



i guess im trying to use what i come across and should be learning more of whats availbale and how to utilize it.

<hr />


I will be looking into setDebug()



thanks alot for your time on my project and for not holding anything against me for you know what.



I will be posting here again on next learing project.
#### 2008-02-21 18:45:25 - ptmcg
restOfLine just reads to the next newline, so you wouldn't even need the cstTag.  It would just be:



    intro = header + restOfLine



I think there is some advantage to this, since CST might change to CDT during daylight savings time, or EST or MST or PST depending on where the input data might come from.



Yes, please look over the htmldoc directory that comes with pyparsing, and skim through the examples.  There are some nuggets here and there.  You might even read through the CHANGES file, to see what features were added later, often for the sake of convenience in defining grammars.  But I'll admit that there are *many* features of pyparsing, and it's easy to overlook some.



And don't worry about you know what - we're good.



-- Paul

---
## 2008-02-21 15:27:41 - dang42 - help removing ordering constraint
Hi,

I have a parser for lines of name=value pairs built with pyparsing. I'm trying to require that two names in particular ('ts' and 'event') occur, but for some reason I'm also getting a constraint on their order relative to each other, which I don't want.

my grammar:

    notSpace = CharsNotIn(' \n')
    name = Word(alphanums +'-_.')
    eq = Literal('=').suppress()
    value = (QuotedString(''', escChar=chr(92), unquoteResults=False) ^ OneOrMore(notSpace))
    ts = Group(Literal('ts') + eq + value)
    event = Group(Literal('event') + eq + value)
    nv = ZeroOrMore(Group(name + eq + value))
    required = ts & event
    nvp = Each([ts, event, nv]) + White('\n').suppress() + StringEnd() 

my issue: both 's' and 's2', which differ only by the order of the required elements 'event' and 'ts', should be OK, but s2 has an error.

    >>> s
    'x=12 ts=2007-08-16T13:18:02.576034+01:30 id=10234 event=e4\n'
    >>> nlParser.parseLine(s)
    {'event': 'e4', 'x': '12', 'ts': 1187264882.5760341, 'id': '10234'}
    >>> s2
    'x=12 event=e4 ts=2007-08-16T13:18:02.576034+01:30 id=10234\n'
    >>> nlParser.parseLine(s2)
    Traceback (most recent call last):
      File '<stdin>', line 1, in <module>
      ...
    PyParsing error is: Missing one or more required elements (Group:({'ts' Suppress:('=') {quoted string, starting with ' ending with ' ^ {!W:( 
    )}...}})) (at char 0), (line:1, col:1)


Can someone point me in the right direction?

Thanks,

-Dan

#### 2008-02-21 19:16:28 - ptmcg
I'm not sure what nlParser or parseLine are, but I tried out your code using nvp.parseString(s), and I got the results you did.



The thing to look at are these 3 expressions:



    ts = Group(Literal('ts') + eq + value)
    event = Group(Literal('event') + eq + value)
    nv = ZeroOrMore(Group(name + eq + value))



Since `name` is defined as `Word(alphanums +'-_.')`, then it would be easy for a `ts` or `event` to be misread as an `nv` element (since 'ts' and 'event' would both match `name`).



The simplest way to make sure that 'ts' and 'event' are not mistakenly read as `name` expressions, you can define name as:



    name = ~oneOf('ts event')+Word(alphanums +'-_.')



This should get your grammar parsing properly.



One last comment - I think your grammar is a good candidate for using the Dict class to automatically define results names in the parsed data.  Here is the change you would have to make - change:



    nvp = Each([ts, event, nv]) + White('\n').suppress() + StringEnd()

to:



    nvp = Dict(Each([ts, event, nv])) + White('\n').suppress() + StringEnd()



Then you could write code like this with the parsed data:



    data = nvp.parseString(s)
    print data.ts
    print data.event
    print data.dump()

which prints



    [['x', '12'], ['ts', '2007-08-16T13:18:02.576034+01:30'], ['id', '10234'], ['event', 'e4']]
    2007-08-16T13:18:02.576034+01:30
    e4
    [['x', '12'], ['ts', '2007-08-16T13:18:02.576034+01:30'], ['id', '10234'], ['event', 'e4']]
    - event: e4
    - id: 10234
    - ts: 2007-08-16T13:18:02.576034+01:30
    - x: 12



-- Paul
#### 2008-02-22 07:38:25 - dang42
Thanks, that worked perfectly!

---
## 2008-02-25 04:10:33 - murk8 - Command and arg on the same line - ?


Hello, I'm currently trying to write a parser for a simple assembly language and I want commands and their (optional) arguments appear on the same line. How to declare that? Thank you :-)

#### 2008-02-25 06:02:24 - ptmcg
There are two ways to approach this: tokenizing and parsing.  The tokenizing approach is to define a generic format for all commands, like:



    cmdKeyword = Word(alphas,alphanums)
    arg = Word(alphanums)
    args = delimitedList(arg)
    command = cmdKeyword + Optional(args) + lineEnd

which will parse any of the following:



    MV A,ACC
    AHOY THERE, MATEY
    THIS IS,A,DUMB,PARSER,IT,WILL,ACCEPT,JUST,ABOUT,ANYTHING



Once you run these 'commands' through the command grammar, then you will get lists of tokens that you can then analyze with follow-on code:



    if tokens[0]=='MV':
        if len(tokens)<3:
            raise NotEnoughArgumentsException()
        if tokens[1]=='ACC':
            ...
    elif tokens[0] == 'STORE':
        etc.
    else:
        raise InvalidCommandException()



In this case, pyparsing is not doing much more than ','.split() would do, and you end up writing a lot of validation code to make sure that proper command words are given, and that commands that take arguments are supplied with the right number and type of arguments.



Pyparsing can do much more than this, if you incorporate these requirements into the grammar itself. To do this define an expression for each command:



    registerRef = oneOf('A0 A1 A2 ACC')
    memoryRef = '#' + Word(hexnums,exact=4)
    hexInteger = '0' + Word(hexnums)
    sourceRef = registerRef | memoryRef | hexInteger
    destRef = registerRef | memoryRef
    
    moveCommand = 'MV' + sourceRef + ',' + destRef + lineEnd
    storeCommand = 'STORE' + ( sourceRef + ',' + destRef | destRef ) + lineEnd
    addCommand = 'ADD' + sourceRef + lineEnd
    
    grammar = moveCommand | storeCommand | addCommand



Now this grammar will match your specific assembler commands, and will ensure that a minimum amount of junk will get through your parser.  The lists of tokens that you get should be validated already, so you wont have to test for invalid commands, or incorrect number of arguments.



And once the parser is written that correctly reads this assembly language, then you can extend it using parse actions to create objects that will be useful in creating executable machine code, or in running the assembly within a simulator, or whatever you choose.  For instance:



    class MoveCommand(object):
        def __init__(self,tokens):
            # no need to verify that tokens[0] is 'MV'
            # this could would not have been called unless it is
            if len(tokens) == 2:
                self.source = 'ACC'
                self.dest = tokens[1]
            else:
                self.source = tokens[1]
                self.dest = tokens[2]
    
    moveCommand.setParseAction( MoveCommand )



Now, instead of getting back lists of token strings, you will get command objects that can be processed in turn to do whatever you want.



Please do some more reading on pyparsing, you can follow the links on the wiki Documentation page.  I recommend you read the section in the Short Cut on creating a Search Query language, or the presentation on creating an Adventure game.  (The parse action code would be cleaner-looking if results names were used - I recommend them, they are described in the other docs as well.)



I hope these examples are sufficient to get you started. Please give it a try with your own language definitions (I was just guessing in the code above), and let me know how it goes.
#### 2008-02-25 07:54:39 - murk8
But still, what if a programmer provide such source:



    
      add
      ax
    

which is a command with an argument on two separate lines? I want the parser to treat this code as invalid. It might be similar to most scripting language interpreters' behaviour when the end of line marks end of 'statement'. I don't get how to achieve it. Thank you for your help very much:-)
#### 2008-02-25 08:09:59 - ptmcg
The simplest way is to remove '\n' as valid whitespace, so that whitespace skipping does not step over newlines.  Do this using:



    ParserElement.setDefaultWhitespaceChars(' \t')

Then, explicitly add `lineEnd` at the end of each command.



Now add and ax will have to be on the same line, or you will get a parsing exception.



-- Paul
#### 2008-02-25 08:27:50 - ptmcg
(forgot to mention)

Call setDefaultWhitespaceChars as soon as possible after importing pyparsing, definitely before defining any of your own grammar expressions.
#### 2008-02-25 08:45:43 - murk8


Ok. It works :-) Thank you for your help & your good software.





Maybe an additional question. I also want to introduce 'labels' in this language and I want to allow it to appear either on the same line with a command or on separate lines as well.





    
    xxx:
      add ax          ; ok
    
    
    yyy:    add ax    ; ok
    



Now should I use here 'ZeroOrMore(_newlines_)' or there is a way to change whitespace behaviour only for commands and not for other symbols?
#### 2008-02-25 09:11:53 - ptmcg
Give ZeroOrMore(lineEnd) a try, but see what happens with a line of whitespace.  You might need to define an expression for a blank line:



    blankLine = lineStart + Optional(White(' \t')) + lineEnd



Then allow ZeroOrMore(lineEnd | blankLine) between the label and the command.



You can get very specific with whitespace skipping using setWhitespaceChars() to define the whitespace characters an expression is allowed to skip over before matching, or leaveWhitespace() to specify that no whitespace should be skipped at all.  These are instance methods, so you can narrow their scope to individual expressions (vs. setDefaultWhitespaceChars, which is a class method on the base ParserElement class).



The reason I would suggest *not* changing whitespace behavior only for commands is that you might get input like this:



    yyy:   add ax,
                bx   ; not ok
    
    zzz:   mv acc
              , 0x100 ; not ok, and ugly too

If you only suppress newlines after the command, they might crop up someplace else.



So I think if you have a parser that is mostly single line at a time, then remove newline from the default whitespace set, and explicitly include it where it might crop up.
#### 2008-02-25 09:30:58 - murk8
Thanks a lot, Paul! :-)
#### 2008-02-25 10:25:52 - murk8
Another question (but related). I tried to run this code with use of 'setWhitespaceChars()' on 'command' symbol:





    from pyparsing import *
    
    ident = Word(alphas, alphanums).setResultsName('ident')
    number = Optional(oneOf('+ -')) + Word(nums)
    label = ident + ':'
    arg = ident | number
    command = Or([Keyword(w) for w in 'nop add'.split()]) + Optional(arg) + lineEnd
    command.setWhitespaceChars(' \t')
    lineOfCode = Optional(label) + command
    prog = ZeroOrMore(lineOfCode) + stringEnd
    
    text = '''\
    
      nop
    
    xxx:
      add ax
    
    '''
    
    print prog.parseString(text)
    



I get exception 

    pyparsing.ParseException: Expected stringEnd (at char 3), (line:2, col:3)



It seems like parser tries to read 'nop xxx' as a command and then doesn't know what to do with the ':'. What's wrong with my declaration of command.setWhitespaceChars(' \t')



Or it is probably my mistake somewhere in rules definitions. I'll be glad to read your comment on it. Thank you.
#### 2008-02-25 12:56:29 - ptmcg
command.setWhitespaceChars defines the whitespace characters that can be skipped over before parsing command.  Didn't you want



    command.setWhitespaceChars(' \t\n')

??



And [[code]] only works as a tag when it is on a line by itself.  To highlight inline source code write {{comand.setWhitespaceChars()}}



Some other tips:



number will match '- 123', and return ['-','123'] as the matched tokens.  To enforce no whitespace between the leading sign and the integer digits, write:



    number = Combine(Optional(oneOf('+ -')) + Word(nums))

or



    number = Word(nums+'+-',nums)



I see you are starting to use results names - this is a good practice!  If you are using pyparsing version 1.4.8 or later, you can use the abbreviated syntax:



    ident = Word(alphas, alphanums).setResultsName('ident')

becomes



    ident = Word(alphas, alphanums)('ident')

This will become more useful when you add results names as part of a complex expression (as in the following).



Now that you have shown me the NOP and ADD commands, here is a (I hope) clearer example of combining tokenizing and parsing into a single definition.



    ident = Word(alphas, alphanums).setResultsName('ident')
    number = Optional(oneOf('+ -')) + Word(nums)
    label = ident + ':'
    arg = ident | number
    
    # define structure for each command
    NOP,ADD = [Keyword(w) for w in 'nop add'.split()]
    NOP_cmd = NOP('cmd')
    ADD_cmd = ADD('cmd') + arg('addArg')
    
    # command represents any defined command structure
    command = NOP_cmd | ADD_cmd
    
    # this is the same for all commands
    lineOfCode = Group( Optional(label('label')) + 
                            Group(command)('statement') | 
                        label('label') ) + lineEnd.suppress()
    
    prog = ZeroOrMore(lineOfCode) + stringEnd



The beginning and ending parts of this definition are the same as what you sent.  What I changed was the generic 'a command can be a label, and a keyword, and zero or more args, depending on which command it is' to 'a NOP is the keyword 'nop' and nothing else', 'an ADD is the keyword 'add' followed by an arg', and 'a command is a NOP or an ADD'.



Now you wont have to filter out invalid syntax like:



    nop ax

with code like:



    if commandTokens[0]=='nop' and len(commandTokens) > 1:
        raise InvalidSyntaxException('you dummy! nops don't take args!!!')



When you use this prog definition to parse your input text, you can then write this code:



    cmds = prog.parseString(text)
    for c in cmds:
        print c.statement.dump()
        if c.label:
            print ':'+c.label.ident.upper()
        print c.statement.cmd.upper()
        if c.statement.cmd == 'add':
            print '+', c.statement.addArg
        print

and get



    ['nop']
    - cmd: nop
    NOP
    
    ['add', 'ax']
    - addArg: ax
    - cmd: add
    - ident: ax
    :XXX
    ADD
    + ax


#### 2008-02-25 19:40:59 - murk8
Oh! Very useful tips. Thank you. And this code is much clearer! My question also was: if I set-whitespaces-chars to ' \t' (excluding '\n') for commands then shouldn't the parser stop at the moment it reads 'nop' and end-of-line? Why does it ignore end-of-line character as if it was a whitespace? (Now I see that I actually haven't asked this question but it was in my mind.) Maybe I misunderstand some basic thing about parsing?
#### 2008-02-25 21:28:32 - ptmcg
In your code, you had setWhitespaceChars on command to ' \t'. 



    command = Or([Keyword(w) for w in 'nop add'.split()]) + Optional(arg) + lineEnd
    command.setWhitespaceChars(' \t')

However, there was no setting of whitespace characters on arg, so after reading in 'nop', the grammar continued reading over whitespace and newlines to see if there was an arg - and it found one, 'xxx'. I think that if you had done arg.setWhitespaceChars(' \t') things would have been more like you expected them. 



For debugging and troubleshooting, you can also use setDebug, as in arg.setDebug(). This will print out messages when an arg is about to be tested for a match, and then if the match succeeds or fails, and if it succeeds, what the matching text was. If you leave your original code as it was, and add arg.setDebug(), you will see the various matches and failures.
#### 2008-02-26 12:15:05 - murk8
M. Now I see)

---
## 2008-02-29 09:27:57 - crmccreary - Trouble with this grammar
Presently, I'm parsing the following grammar with r.e.'s:

    *command1,attribute1,attribute2=123,attribute3=abcdef
    data
    data
    **comment
    data
    *command2,attribute1
    data
    *sub command,attribute1
    data
    *command3
    data
    ...

Commands start with an asterisk `(*)`  
Comments start with two asterisks `(**)`  
Command attributes are separated by commas and may have parameters  
Some commands have sub commands (the only way to identify sub commands is to know a priori that some commands have certain sub commands)  

As you can imagine, parsing using r.e.'s is very cumbersome. How would I attack this using pyparsing? I'm having trouble with the multiline nature of the data following the commands and the sub commands.

#### 2008-02-29 12:17:07 - ptmcg
Well, pyparsing by default is blind to newlines, treating them just like any other whitespace.  We can disable that by defining what the set of whitespace characters are, and leave out the \n:



    ParserElement.setDefaultWhitespaceChars(' \t')



pyparsing includes a built-in for detecting newlines, called LineEnd.  Since we need to be explicit about them now, we define a symbol to make it easy to insert newlines wherever they are supposed to go:



    NL = LineEnd().suppress()

(If we didn't suppress the parsed newline, we'd end up with '\n's garbaging up our parsed output.)



Then the rest is pretty much like a standard pyparsing application, showing where we want to test for newlines.  Here is a first cut at this application, which parses the example that you pasted in.  Note that we can access many of the parsed fields by name, as if they were object attributes of the parsed results.



I had to guess at a number of the field types and parsing formats, but I hope you can take this as a starting point and go with it (adding things like real numbers or date-time stamps for attrVals, etc.)



-- Paul





    datafile = '''\
    *command1,attribute1,attribute2=123,attribute3=abcdef
    data
    data
    
    **comment 
    data
    *command2,attribute1
    data
    *subcommand,attribute1
    data
    *command3
    data
    
    '''
    from pyparsing import ParserElement,Suppress,LineEnd,Word,\
        alphas,alphanums,nums,Combine,Optional,printables,quotedString,\
        SkipTo,Dict,Group,delimitedList,StringEnd,restOfLine,ZeroOrMore,\
        OneOrMore
    # don't skip over newlines
    ParserElement.setDefaultWhitespaceChars(' \t')
    
    # define punctuation and basic types
    STAR,COMMA,EQ = map(Suppress,'*,=')
    NL = Suppress(LineEnd())
    cmd = Word(alphas,alphanums)
    attr = Word(alphas,alphanums)
    
    integer = Combine(Optional('-')+Word(nums))
    integer.setParseAction(lambda tokens: int(tokens[0]))
    attrStringChars = printables.replace(',','')
    string = Word(attrStringChars)
    attrVal = integer | quotedString | SkipTo(COMMA|NL)
    
    # define each type of line
    cmdLine = STAR + cmd('command') + Optional(COMMA + 
                Dict(
                    delimitedList(Group(attr + Optional(EQ + attrVal)))
                    )('attributes')
                )
    dataline = ~STAR + ~StringEnd() + Optional(restOfLine)
    comment = Suppress(STAR + STAR + Optional(restOfLine))
    
    # define a total command entry
    cmdEntry = Group(cmdLine + NL + ZeroOrMore(
                                comment + NL | dataline + NL 
                                )('data'))
    
    # parse input for one or more command entries
    cmds = OneOrMore(cmdEntry).parseString(datafile)
    
    # iterate over parsed commands, and display results
    for c in cmds:
        # access named fields with parsed command entry
        print 'Command:',c.command
        print 'Attributes:',c.attributes.keys() if c.attributes else '<none>'
        if c.attributes and 'attribute2' in c.attributes:
            print '  Attribute2:', c.attributes.attribute2
        # display all parsed fields in command entry
        print c.dump()
        print



Prints out:



    Command: command1
    Attributes: ['attribute2', 'attribute3', 'attribute1']
      Attribute2: 123
    ['command1', [['attribute1'], ['attribute2', 123], 
        ['attribute3', 'abcdef']], 'data', 'data', '', 'data']
    - attributes: [['attribute1'], ['attribute2', 123], ['attribute3', 'abcdef']]
      - attribute1: 
      - attribute2: 123
      - attribute3: abcdef
    - command: command1
    - data: ['data', 'data', '', 'data']
    
    Command: command2
    Attributes: ['attribute1']
    ['command2', [['attribute1']], 'data']
    - attributes: [['attribute1']]
      - attribute1: 
    - command: command2
    - data: ['data']
    
    Command: subcommand
    Attributes: ['attribute1']
    ['subcommand', [['attribute1']], 'data']
    - attributes: [['attribute1']]
      - attribute1: 
    - command: subcommand
    - data: ['data']
    
    Command: command3
    Attributes: <none>
    ['command3', 'data', '']
    - command: command3
    - data: ['data', '']
    


#### 2008-02-29 12:37:21 - crmccreary
I was able to get something that worked once I grokked the concept. Now that I have seen your solution, I think that I will use it as a framework.

---
## 2008-03-02 18:15:35 - alito - enablePackrat speeds up parsing of simple expression by a factor of >10000
There seems to be something wrong with the use of operatorPrecedence without using enablePackrat, or maybe that's the best that can be done and it isn't pretty.  What kind of side-effects exactly preclude the use of enablePackrat()?  Do they need to modify the parser definition?  I can see from the docstrings that matchPreviousLiteral and matchPreviousExpr are out, and probably modifying parser globals would be a no-no, but what else?

The following summarises the results:
(The code is at the bottom, but basically, this is a bog-standard boolean expression parser, very similar to what appears in simpleBool.py, but with more operators, which are therefore increasing the node-explosion pain)

    $time python parser.py 'for (a or (b and (c and (d or not (e and f)))))'
    Packrat enabled
    
    real    0m0.179s
    user    0m0.088s
    sys     0m0.012s
    
    $time python parser.py -nopackrat 'for (a or (b and (c and (d or not (e and f)))))'
    ^C (gave up, backtrace snipped)
    real    83m2.602s
    user    75m3.425s
    sys     0m3.552s



    import sys, os
   
    from pyparsing import *
    
    def createParser():    
        otherNameCharacters = '_-!@#~%'
    
        name = sglQuotedString.setParseAction(removeQuotes) | Word(alphanums + otherNameCharacters)
    
        #suppressed keywords
        forKeyword = CaselessKeyword('for').suppress()
    
        #logic
        andKeyword = CaselessKeyword('and')
        orKeyword = CaselessKeyword('or')
        notKeyword = CaselessKeyword('not')
        exceptKeyword = CaselessKeyword('except')
        allKeyword = CaselessKeyword('all')
        comma = Literal(',')
        lparen = Literal('(')
        rparen = Literal(')')
    
    
        #statements
        literalName = name('name')
        castName = name('nameType') + lparen + name('name') + rparen
        fullName = castName | literalName
    
    
        restrictedName = fullName
    
        criteria = operatorPrecedence(restrictedName,[
                                  (notKeyword, 1, opAssoc.RIGHT),
                                  (andKeyword, 2, opAssoc.LEFT),
                                  (orKeyword, 2, opAssoc.LEFT),
                                  (exceptKeyword, 2, opAssoc.LEFT),
                                  (comma, 2, opAssoc.LEFT)])
    
        selectionCriteria = forKeyword + criteria('criteria')
    
        description = stringStart.suppress() + selectionCriteria + stringEnd.suppress()
    
        def parse(statement):
            results = description.parseString(statement)
            return results
        return parse
    
    def main(args):
        parser = createParser()
        parser(' '.join(args))
        return 0
    
    if __name__ == '__main__`:
        args = sys.argv[1:]
    
        if '-nopackrat' in args:
            args.remove('-nopackrat')
        else:
            print 'Packrat enabled'
            ParserElement.enablePackrat()
    
        sys.exit(main(args))
    




---
## 2008-03-05 01:53:57 - nagaraj1 - requesting the code format

 hi all,

 Can someone please tell me the instruction format for the operation like this-

    var= group of many statements but not in order

say suppose

    var = stmt_1 + stmt_2 + stmt_3 
    
but order of stmt_1,_2,_3 being different, like in some files its stmt_1,_2,_3 and in some stmt_2,_3,_1 ... something like that...

all the statements are of interest, so can someone please help me out? u can reach me at  and thanks in advance.

regards  
nagaraj

#### 2008-03-05 08:53:57 - ptmcg


    var = stmt_1 & stmt_2 & stmt_3



uses the Each class, as in 'one of each,' regardless of order.



-- Paul

---
## 2008-03-08 17:02:13 - jenhl - Odd dict output
Firstly, thank you for producing pyparsing and being so responsive to user queries!



My query is about some odd (to me, at least) output using result names. My BNF grammar is:





    binary_features ::= binary_feature+
    binary_feature ::= 'Binary Feature', feature_name, EOL, plus_list, minus_list
    feature_name ::= [A-Za-z]+
    plus_list ::= 'plus:', char_list
    minus_list ::= 'minus:', char_list
    char_list ::= char, (',', char)*, EOL
    char ::= Unicode IPA character



And my pyparsing code (simplified with respect to the IPA characters used) is:





    char = Word(alphas, exact=1)
    char_list = delimitedList(char).setName('list of IPA characters')
    plus_list = Literal('plus:').suppress() + char_list
    minus_list = Literal('minus:').suppress() + char_list
    binary_feature = Literal('Binary Feature').suppress() + Word(alphas) + 
        LineEnd().suppress() + plus_list.setResultsName('plus_list') +
        minus_list.setResultsName('minus_list')
    binary_features = Group(OneOrMore(binary_feature))
        .setResultsName('binary_features')
    print binary_features.parseString('''Binary Feature voiced
      plus: b, d
      minus: a, c, e''').asDict()



From which I get:





    {'binary_features': (['voiced', 'b', 'd', 'a', 'c', 'e'],
    {'minus_list': [((['a', 'c', 'e'], {}), 3)],
    'plus_list': [((['b', 'd'], {}), 1)]})}



What are the integers there, and why are 'plus_list' and 'minus_list' values so nested?



Thanks!

#### 2008-03-08 18:10:06 - ptmcg
Jenhl -



Those dicts and numbers are an artifact of the display of a ParseResults's contents.  If you are trying to view the structure of your parsed data, I recommend using dump() in place of asDict():



    res = binary_features.parseString('''Binary Feature voiced
      plus: b, d
      minus: a, c, e''')
    
    print res.dump()



This will print:



    [['voiced', 'b', 'd', 'a', 'c', 'e']]
    - binary_features: ['voiced', 'b', 'd', 'a', 'c', 'e']
      - minus_list: ['a', 'c', 'e']
      - plus_list: ['b', 'd']



The purpose of dump() is to show you at a glance how the tokens are structured and what keys are available for accessing groups and subgroups.  From this output, I see that I can write:



    print res.binary_features.minus_list[2]



I rarely find that asDict() is all that useful beyond just using the results itself.  I think I implemented it mostly as a parallel option to asList().



-- Paul
#### 2008-03-08 18:42:14 - jenhl
Thanks for that Paul! I'll use dump() in future for checking the parse output.

---
## 2008-03-11 02:22:11 - jenhl - LineEnd confusion
I do not understand how LineEnd() works, or how it interacts with other elements. Given the following data:





    data = '''a
    b
    '''



the following grammar parses it fine:





    list = delimitedList(Word(alphas))
    g = list + list
    results = g.parseString(data)
    print results.dump()
    
    ['a', 'b']



But if I add in a LineEnd() to the end of the list definition, it fails:





    list = delimitedList(Word(alphas)) + LineEnd()
    g = list + list
    results = g.parseString(data)
    print results.dump()
    
    pyparsing.ParseException: Expected end of line (at char 2),
    (line:2, col:1)



If I then add a second element to the first list, it goes back to working:





    data = '''a, b
    b
    '''
    
    list = delimitedList(Word(alphas)) + LineEnd()
    g = list + list
    results = g.parseString(data)
    print results.dump()
    
    ['a', 'b', '\n', 'b']



What am I not understanding that explains why the second case fails to parse?



Thank you!

#### 2008-03-12 02:19:31 - jenhl
From reading some other threads here, it appears that what I should do is:





    ParserElement.setDefaultWhitespaceChars(' \t')



and then it will work.



However, I'm still confused as to why there is the discrepency between the behaviour when there is one item in the list and when there is more than one, in the original code.
#### 2008-03-15 04:54:03 - ptmcg
By default, end-of-line is treated like ignorable whitespace.  The only time you need to use LineEnd is if you have suppressed \n as valid whitespace.



-- Paul
#### 2008-03-15 05:23:27 - jenhl
Right, but I'm unsure why that causes a different behaviour in the two cases (a list of one item, and a list with more than one). Not that it matters, I guess - I'm just curious.

---
## 2008-03-12 22:32:44 - taknev - Important Pyparsing webpage not rendering nicely
Hi,



This page does not render well on IE7, Mozilla2.x/3beta and Safari2 

( mac/windows)

The page is:





If  you look under any category - for instance the Classes section, I see either '??' or weird numbers. 



As a result it is hard to tell what heading is and the relavant sub-section is talking about.



Thanks.

#### 2008-04-16 08:14:29 - weofij
Not even correctly i'd say. I see only numbers between some symbols that render as either question signs on FF or square boxes on IE or some strange glyph in Opera..



It would be very nice to read the article with actual names they're referring to instead of nonsense numbers !
#### 2008-08-09 18:55:59 - ptmcg
Are things still not displaying properly for this page?  What about the other pyparsing pages?  I just fill in the wikispaces templates, so I really can't explain why these pages are not formatting well for these browsers.



-- Paul

---
## 2008-03-13 22:17:59 - alito - Combining Forward and operatorPrecedence
I'm having trouble using recursive operatorPrecedence constructs.  First an introduction:

I'm trying to write a parser for something like the following:

words are joined by 'and' and 'or' (and a few others), but they can optionally have properties, here let's say they are numbers (they can be differentiated at the tokenisation point in any case), which can also be 'and' and 'or'ed.  The keyword 'that' denotes the property bit.  So you get sentences like:

    something that 5 or 6 and otherwise that 8 or 9

which would be split like:

    [('something','that',['5','or','6']) 'and' ('otherwise','that',['8','or','9'])]

This can be done with something like:


    number = Word(nums)
    letter = Word(alphas)
    andKeyword = CaselessKeyword('and')
    orKeyword = CaselessKeyword('or')
    thatKeyword = CaselessKeyword('that')
    numberCriteria = operatorPrecedence(number,[
        (andKeyword, 2, opAssoc.LEFT),
        (orKeyword, 2, opAssoc.LEFT)])
    fullLetter = letter + Optional(thatKeyword + numberCriteria)
    letterCriteria = (operatorPrecedence(fullLetter,[
        (andKeyword, 2, opAssoc.LEFT),
        (orKeyword, 2, opAssoc.LEFT)]))
    statement = stringStart + letterCriteria + stringEnd

Till then, all good.  Now, I want to handle something like the following:

    (something or other) that (5 or 6)

So I need the 'that' binding to apply to the full operatorPrecedence, not just the letter.  I thought I could do it with a Forward, something like the following:

    number = Word(nums)
    letter = Word(alphas)
    andKeyword = CaselessKeyword('and')
    orKeyword = CaselessKeyword('or')
    thatKeyword = CaselessKeyword('that')
    numberCriteria = operatorPrecedence(number,[
        (andKeyword, 2, opAssoc.LEFT),
        (orKeyword, 2, opAssoc.LEFT)])
    letterCriteria = Forward()
    fullLetter = letterCriteria + Optional(thatKeyword + numberCriteria)
    letterCriteria << (operatorPrecedence(fullLetter,[
        (andKeyword, 2, opAssoc.LEFT),
        (orKeyword, 2, opAssoc.LEFT)]))
    statement = stringStart + letterCriteria + stringEnd

but that goes into an infinite recursion.  
What's the right way to do it?  Should I stop using the operatorPrecedence crutch and do it 'by hand'?

Thanks

#### 2008-03-15 04:52:55 - ptmcg
I am currently at an Internet cafe in London, so I can't consider this problem in depth.  My first guess is that you should add 'that' as another operator level in the operatorPrecedence definition, probably ahead of AND.



I'll be back in my office on Tuesday late, and can look at this further then.



-- Paul
#### 2008-03-16 17:36:34 - alito
Thanks for the reply.  Adding 'that' to operatorPrecedence criteria seems wrong to me since I don't want sentences like 



'something that other'



to be accepted.  ie the bit on the right-hand side of the 'that' is of a different nature to the bit on its left-hand side.  (It doesn't seem to help with the infinite recursion either).



I'll post if I come up with something.
#### 2008-03-16 20:16:35 - alito
Ahh, I misunderstood.  I see what you mean now: put the whole 

'that <whatever>' as an operator in the letterCriteria operatorPrecedence as a unary left-associative.  



Works too.  Thanks a lot.

---
## 2008-03-14 16:04:00 - ricedaddy - parse string and extract certain text
I've made a mess of my last post so please ignore that one. I am newb at python so please bear with me. This is my situation:



I want to extract certain infomation from a text file. Right now, I have the users define the containing elements (opening and closing) and it parses the stuff in the middle as a result.





    
    testData = 'Short Display: MON ONT Long Display: Montreal'
    
    startTerm = 'Short Display'
    endTerm = 'Long Display'
    



So I want all the text inside the start and end terms which in this case is 'MON ONT'





the grammer: It looks for the starTerm, then assigns the next item(s) as the value since it is what we are looking for, then it looks for the end term.





    searchTerm1 = startTerm + ':' + OneOrMore(Word(alphas)).setResultsName('value') + endTerm
    
    for t,s,e in searchTerm1.scanString(testData):
        print t.value
    



this doesn't work! t.value is not showing anything. Is my grammer incorrect? Is there another way to do this? please help, thanks

#### 2008-03-15 04:48:43 - ptmcg
Unfortunately, your repetition term `OneOrMore(Word(alphas))` also matches the 'Long' and 'Display' in 'Long Display:', so you never match the end.  pyparsing does not do any type of lookahead unless you explicitly ask it to do so.



Try this instead:



    searchTerm1 = startTerm + ':' + OneOrMore(~endTerm + Word(alphas)).setResultsName('value') + endTerm

or



    searchTerm1 = startTerm + ':' + SkipTo(endTerm).setResultsName('value') + endTerm



-- Paul
#### 2008-03-17 09:53:12 - ricedaddy
Thanks a lot! they both work.



In the first suggestion you listed, can you explain how it works? what does the addition of  ~endTerm  enforce the type of lookahead you talked about?
#### 2008-03-17 09:57:51 - ricedaddy
I think I spoke too soon; the 2nd suggestion gives me this error:





    bad operand type for unary ~: 'unicode'


#### 2008-03-17 09:59:05 - ricedaddy
Sorry I meant the first suggestion. 





    searchTerm1 = startTerm + ':' + OneOrMore(~endTerm + Word(alphas)).setResultsName('value') + endTerm



Causes the error 





    TypeError: bad operand type for unary ~: 'unicode'


#### 2008-03-18 12:41:36 - ptmcg
Ooops, should have been `~Literal(endTerm)`.



-- Paul

---
## 2008-03-18 11:08:49 - Poldy - Getting the REAL position of the error
Hi!



I have a problem with a quite complex grammar parsing an even more complex file. The to be parsed file looks somehow like this (very simplified):





    entity:
    
      generic:
        some generic stuff here
      end generic;
    
      optional:
        some optional stuff here
      end optional;
    
    end entity;



The grammar looks like this (totally simplified):





    optional = #something#
    generic = #something#
    entity = 'entity:' + generic + Optional( optional ) + 'end entity;'



So, when there is a syntax error in the optional section pyparsing returns the start of the optional section as the pint of error.



From pyparsing's point of view this makes sense: after the generic section either an optional section or the end. Since the optional section doesn't match (because of the error) the end would be expected.



But the user of my application will just say: heck, why do I get an error there?



Any ideas on how to get the REAL position of the error? Even the most stupid suggestion is welcome, I'm desperate... ;)



Thomas

#### 2008-03-18 12:37:21 - ptmcg
Thomas -



Unfortunately, I don't have a good answer for you.  I've tried in the past to improve the precision of error locations, but as you can see, Optional's (and Zero/OneOrMore's) give pyparsing some difficulty in nailing these issues down.  What makes this difficult is pyparsing's use of the call stack to keep the grammar state, and raising ParseExceptions to move back up the stack when the current grammar expression wont match.



As you state, the exception location is best interpreted as 'pyparsing was able to get this far successfully with the grammar, but then could not match any more complete expressions.'



The only way I have found to give more precise errors is only valid when detecting a semantic error, such as matching opening and closing tags in something like XML.  If parsing '<XYZ>sldkjflskdjf</ZYZ>', a parse action on the closing tag to match the opening tag can raise a ParseFatalException.  A PFE differs from the ordinary ParseException in that it stops all parsing immediately.



I've tackled this in the past, and when I get time, it is on my list of 'significant warts' in pyparsing.



-- Paul
#### 2008-03-18 22:06:58 - ptmcg
I got a similar inquiry on the pyparsing mailing list, and I'm mulling over some ideas on how to implement an ExceptionMonitor class that can be used to identify the 'furthest' parse exception, presumably the one that causes the actual parsing failure.  I'll post more info when I get past some experiments.



-- Paul
#### 2008-03-19 00:13:19 - Poldy
First, thank you for your really quick response!



I came up with an idea for extending pyparsing and would like to discuss it:



add a new parameter to Optional, 'identifiedby' (there might be better names though). This parameter would take an expression, that if matches the Optional would be taken as matching. Example:



Take the example from above, with the following change



    optional = Keyword( 'optional' ) + '(' ....... + ';'
    optional_identified = Keyword( 'optional') + '('
    
    entity = 'entity:' + generic + Optional( optional, identifiedby = optional_identified ) + 'end entity;'



I don't know, if this would be easy to implement. It definitely adds a new source for errors, when the 'identifiedby' and the 'real' expression don't match.



Thomas
#### 2008-03-19 06:27:04 - ptmcg
Hmm, I don't <em>quite</em> follow your description, but my first impression is that this could be done with current pyparsing using a parse action.



For instance, here is a way to qualify a generic integer if < 128:



    integer = WordStart() + Word(nums) + WordEnd()
    
    def acceptOnlyIfLessThan128(t):
        if int(t[0]) >= 128:
            raise ParseException('integer >= 128')
    
    integer.setParseAction( acceptOnlyIfLessThan128 )
    
    print integer.searchString('128 122 7 99 3.14 100')

prints



    [['122'], ['7'], ['99'], ['100']]



integer is defined to be any sequence of consecutive digits.  By adding the parse action, we can insert additional logic that would be a bit more difficult to implement in the parser.  You can see how this could be modified to accept only integers that were odd, or integers that were evenly divisible by 7, etc.



Reading your message again, and the leading thread, is this a way to 'force' pyparsing to get past the 'optional (' lead in?  If so, it feels unwieldy - one must repeat part of the original optional expression, which as you already pointed out is a bug opportunity.



Can you elaborate on how you would use this feature?  I'm just not getting it.



-- Paul
#### 2008-03-20 08:33:38 - Poldy
Before I try to explain what I mean, with thousands of examples and such: here is an implementation of it. Not nice nice and probably not bug-free, but it works for me.



This is a bit different from what I suggested above, in that the optional 'identifiedBy' parameter is perpended before the other expression. This way the possible error of these two expressions not matching up is eliminated.



-Thomas





    class Optional(ParseElementEnhance):
        '''Optional matching of the given expression.
           A default return string can also be specified, if the optional expression
           is not found.
        '''
        def __init__( self, expr, default=_optionalNotMatched, identifiedBy=None ):
            super(Optional,self).__init__( expr, savelist=False )
            self.defaultValue = default
            self.mayReturnEmpty = True
            self.identifiedBy = identifiedBy
    
        def parseImpl( self, instring, loc, doActions=True ):
            if self.identifiedBy:
                try:
                    loc, tokens = self.identifiedBy._parse( instring, loc, doActions, callPreParse=False )
                except (ParseException,IndexError):
                    if self.defaultValue is not _optionalNotMatched:
                        tokens = [ self.defaultValue ]
                    else:
                        tokens = []
                    return loc, tokens
                loc, tokens = self.expr._parse( instring, loc, doActions)
                return loc, tokens
            else:
                try:
                    loc, tokens = self.expr._parse( instring, loc, doActions, callPreParse=False )
                except (ParseException,IndexError):
                    if self.defaultValue is not _optionalNotMatched:
                        tokens = [ self.defaultValue ]
                    else:
                        tokens = []
                return loc, tokens
    
        def __str__( self ):
            if hasattr(self,'name'):
                return self.name
    
            if self.strRepr is None:
                self.strRepr = '[' + _ustr(self.expr) + ']'
    
            return self.strRepr



My grammar (shortened) :





    
    port_clause = '(' + ........ + ')'
    
    entity = 'entity' + '(' + \
        Optional( port_clause, identifiedBy=Keyword('port') ) + \
        ')'



the to be parsed file (shortened of course):



    entity (
        port (
          #a lot of stuff here#
        )
    )



Now, if there is an error in the 'lot of stuff' part, the correct position is reported.
#### 2008-05-12 00:23:23 - ptmcg
It turns out that this issue affects many parts of pyparsing, not just Optional.  The root problem actually occurs in the And class, in that if a succession of expressions does not parse completely, than a routine ParseException is raised.  For example, in your grammar, you found the need to modify Optional because you did not get the desired error location from:





    port_clause = '(' + ...body of port definition... + ')'
    
    entity = Literal('entity') + '(' + \
        Optional( Keyword('port') + port_clause ) + \
        ')'



ParseException is 'routine' because it is a way for any expression to indicate that no match occurred, and other alternatives should be tried.



However, in this case, we want non-routine behavior.  If the parser reads 'port' and it is not followed by '(' and the other interesting port items, then the parser should stop immediately.  This is a different flavor of And - when 'port' is read, you know that the next items in the string should be the port data, and if it isn't then this is a syntax error.



Since normal And sequencing is defined using '+' signs, I'm trying to insert the syntax error trapping using another operator.  The logical choice for this operator would be '-'; it is equal to '+' in precedence, and it is visually intuitive as a sequence connector.  The distinction will be that, if a parser error occurs after passing the '-' operator, then this error will be flagged immediately as a syntax error.  (I am adding the exception class ParseSyntaxException, derived from ParseFatalException.)



In your case, your code would become:



    port_clause = '(' + ...body of port definition... + ')'
    
    entity = Literal('entity') + '(' + \
        Optional( Keyword('port') - port_clause ) + \
        ')'



The syntax would be the same if Optional were replaced with ZeroOrMore, OneOrMore, or any of the other repetition classes.



It is possible now to have a lot of control over just where syntax errors get signaled.  You could define an expression as:



    expr = A + B + C - D + E + F

and any parsing mismatch after having matched A, B, and C would be raised as a syntax error, and parsing would stop immediately.



Your proposed code was very helpful, I used it to validate my implementation of the error stopping in And (for instance, I had overlooked the IndexError case).  Although this does not exactly match your proposal, I hope you find this implementation agreeable.



-- Paul

---
## 2008-03-22 20:50:38 - ecir-hana - FAQ: How to get pyparsing to parse the entire input string?
Hi,

I would like to kindly ask you very newbie question, please, what is the reason the following code doesn't throw exception? I found in FAQ how to make it throw but I would like to understand the workings.





    from pyparsing import *
    digit = oneOf('0 1 2 3 4 5 6 7 8 9')
    number = digit + digit + digit
    number.parseString('123---')



Is this how 'Parsing expression grammar' works? (If it is related, anyway.) Is this how Pyparsing works? What kind of parser wouldn't need to explicitly end 'number' with '+ StringEnd()'?



Thanks a lot in advance!

#### 2008-03-23 01:43:26 - ptmcg
The grammar doesn't raise an exception because it successfully parses to the completion of the grammar.  You and I know that there is more text after the initial 3 numeric digits, but the grammar as you have defined it does not know or care this.  That is how pyparsing works - its behavior is driven by the defined grammar, as it processes through the provided input text.



In practice, I think just about every grammar that is used to call parseString is expected to parse the entire input.  I implemented parseString to *not* assume this, and added StringEnd() as the object to indicate where the end of the input string could be valid to occur.  



But this expectation is so common, I think I might add an optional argument to parseString, something like parseAll that if set to True would do the same as the user appending StringEnd() to the given grammar.  To preserve the current behavior, I would have parseAll default to False.  In some future version, something like a 2.0, say, I'd probably change the default to True.



-- Paul
#### 2008-03-23 05:50:41 - ecir-hana
Thank you for the explanation! 



I, for one, would very welcome the optional argument 'parseAll' or even 'parseAll=True'.
#### 2008-03-23 06:40:17 - ecir-hana
Also, I have to thank you for Pyparsing, it is extremly useful.
#### 2008-05-11 23:58:12 - ptmcg
'parseAll=False' has been checked into SVN, and will be in the next release of pyparsing.
#### 2008-05-12 15:05:40 - ecir-hana
That's great, thanks!
#### 2008-06-03 12:38:38 - pboucher
Hi!



First off: Good work on PyParsing.



Then:

Could / should ParserElement.parseFile also include the parseAll=True argument and pass it on to its parseString call?



Cheers,

Patrick
#### 2008-06-03 13:43:29 - pboucher
Hmmm... Yeah.

That should have read:

parseAll=False
#### 2008-06-03 15:35:15 - ptmcg
Sure, that makes sense.



Just for completeness, the other parsing methods (scanString, searchString, and transformString) really do different modes of parsing, and parseAll would not fit their parsing model.



I'll check this in to SVN when I get a chance.  It will look pretty much just as you suggest.



-- Paul

---
## 2008-04-07 01:43:02 - lolob - Extract HTML table
Hello everyone..I'm new in python world...and I'm looking for good tutorial/example how to extract data from HTML table..I find this pyparsing is quite interesting..I also found one example from [[ |ONLamp.com ]] about extracting data from html table..but I don't really get it actually...for example how to deal with the html tags, the attributes and so on..

#### 2008-04-07 06:33:07 - ptmcg
This example is a bit old, and a simple one too.  On this particular web page, the tags don't have any attributes, so they can be matched with simple Literal's:

    tdStart = Literal('<td>')
    tdEnd   = Literal('</td>')

You ask a very good question, though - how to handle the tags, attributes, etc., which can have some very unpredictable forms in terms of upper/lower case, attributes, order of attributes, and whether the opening tag is also the closing tag (of the form `&lt;TAG_WITH_NO_BODY /&gt;`).  Pyparsing handles all of these using a helper method called makeHTMLTags or makeXMLTags if you are parsing XML - there are some slight differences between the two).

To use makeHTMLTags, you start by replacing the above two lines with the single line:

    tdStart,tdEnd = makeHTMLTags('td')

But this does a lot more than just map to the two Literal definitions.  Since HTML does not enforce upper/lower case matching, the generated expressions will match `<TD>`, `<td>`, `<Td>`, etc.  The generated expressions also recognize that the opening tag may contain attributes.  Attributes can be of the form `attrib=value`, `attrib="value"`, or `attrib='value'`.

Once the tag is parsed, the attributes are accessible using the dict-like access of ParseResults - you can get all the attribute names using keys(), for instance.

To mark whether an opening tag is also the closing tag (that it, the tag has an empty body), there is an extra boolean attribute added, named 'empty'.

You can see an example of using makeHTMLTags here: .

Here is another one, that parses table cells and extracts the cell value (also handles <FONT> tags), and uses the new `withAttribute` parse action to limit matching to only those tags that have the given attribute values: 

-   <small>Apr 7, 2008</small>

#### 2008-04-07 19:04:24 - lolob
Thanks ptmcg for the explanation, it helps a lot..



I receive the following callback error when I try to run 'withAttribute.py' file..but the 'makeHTMLTags.py' file is okay..





    
    azzam:~/Desktop azzam$ python withAttribute.py 
    Traceback (most recent call last):
      File 'withAttribute.py', line 18, in ?
        realNum = Combine( Word(nums) + '.' + Word(nums) ).setParseAction(lambda t:float(t[0]))
      File '/System/Library/Frameworks/Python.framework/Versions/2.3/lib/python2.3/site-packages/pyparsing.py', line 773, in setParseAction
        self.parseAction = map(self._normalizeParseActionArgs, list(fns))
      File '/System/Library/Frameworks/Python.framework/Versions/2.3/lib/python2.3/site-packages/pyparsing.py', line 739, in _normalizeParseActionArgs
        tmp.__name__ = f.__name__
    TypeError: readonly attribute
    azzam:~/Desktop azzam$ 
    


#### 2008-04-07 20:58:20 - ptmcg
Dang!  I think this is a Python 2.3 thing, because this runs fine on 2.5.  But I'm trying to be compatible back to 2.3, so I'll wrap this code in a try/except block to do a better job.



As a workaround, try changing:



    realNum = Combine( Word(nums) + '.' + Word(nums) ).setParseAction(lambda t:float(t[0]))



to 



    realNum = Combine( Word(nums) + '.' + Word(nums) ).setParseAction(lambda s,l,t:float(t[0]))



This will bypass the _normalizeParseActionArgs routine.



-- Paul
#### 2008-04-10 20:54:48 - lolob
Yes you're right,thanks..



At this moment, I have successfully read some of the data from website(html-table)..



Now, I want the output to look like a table-form..So,how do I control within the 'for' loop?..





    from pyparsing import *
    import urllib
    
    # define basic text pattern
    tdS,tdE = makeHTMLTags('TD')
    anchorS,anchorE = makeHTMLTags('A')
    fontS,fontE = makeHTMLTags('FONT')
    realNum = Word(alphanums+'./-: ')
    patt = tdS + anchorS + fontS + realNum('value')+ fontE + anchorE + tdE
    patt2 = tdS + fontS + realNum('value')+ fontE + tdE
    
    # get list of time servers
    nistTimeServerURL = 'http://infobanjir.water.gov.my/perlisw.htm'
    serverListPage = urllib.urlopen( nistTimeServerURL )
    serverListHTML = serverListPage.read()
    serverListPage.close()
    
    tdS.setParseAction( withAttribute(align='center') )
    for srvrtokens,startloc,endloc in patt.scanString( serverListHTML):
        print srvrtokens.value
    for srvrtokens2,startloc2,endloc2 in patt2.scanString(serverListHTML):
        print srvrtokens2.value #+ srvrtokens2.value        





One more thing, can somebody explain about the code below? Because normal 'for' loop look like this 'for(i=0;i<10<i++)'.Thanks





    for srvrtokens,startloc,endloc in patt
        print patt


#### 2008-04-11 03:20:02 - ptmcg


> How do I make this data look like a table?

The first thing I did was visit the web page you are downloading, and viewed the content.  Here is a complete table row from that page:



    <tr>
        <td align='center' class='tdcontent1acc' width='84'>
        <a href='../photos/pls/pls06.htm' title='Station Photo'>
        <font
        color='#0000FF'>.......</font></a></td>
        <td width='204' class='tdcontent1acc'>
        <a href='../trend/Pls06_06.htm?state=PLS&station_id=133' title='River Level Graph' 
            target='blank'>
        <font color='#0000FF'>Temiang</font></a></td>
        <td align='center' class='tdcontent1acc' width='104'><a href='../trend/PLS06_06.txt' 
        Title='Last 7 Days Data'><font color='#0000FF'>Sg.Arau</font></a></td>
        <td align='center' class='tdcontent1acc' width='223'><font 
            color='#000000'>               11/04/2008-16:00   </font></td>
        <td align='center' class='tdcontent1acc' width='223'>
            <font color='#000000'>-99.99</font></td>
        <td align='center' class='tdcontent1acc' width='107'>
            <font color='#000000'>12.45</font></td>
        <td align='center' class='tdcontent1acc' width='104'>
            <font color='#000000'>12.85</font></td>
        <td align='center' class='tdcontent1acc' width='101'>
            <font color='#000000'>13.25</font></td>
      </tr>



Your code tries to parse any <TD>...</TD> field with these patterns:



    realNum = Word(alphanums+'./-: ')
    patt = tdS + anchorS + fontS + realNum('value')+ fontE + anchorE + tdE
    patt2 = tdS + fontS + realNum('value')+ fontE + tdE



I couldn't figure out why you were defining realNum to include '/' and ':' characters until I viewed the page.  Now I see that you are minimally adapting the NTP web page scraper to fit your web page, but you should really do a bit more work with this.  If you want to access this data by rows, you will have to parse the rows, not just the individual cells.



So to extract the data from this table, it would be best to define a pattern for an entire row (from <TR> to </TR>), not just the <TD> tag contents.  I see your row of data looks like this:



    linked_text  linked_text  linked_text  date_time  real  real  real  real



The table headings for these columns are:

- StationID   

- Station Name 

- River Basin 

- Last Update Time

- River Level 

- Normal level

- Alert Level

- Danger Level

We will use these later for results names.



Now lets define patterns for linked_text, date_time, and realNum:



    realNum = Word(alphanums+'-',alphanums+'.')
    linkedText = anchorS + fontS + SkipTo(fontE) + fontE + anchorE
    date_time = Combine( Word(nums,exact=2) + '/' + Word(nums,exact=2) + '/' + Word(nums,exact=4) + '-' + Word(nums,exact=2) + ':' + Word(nums,exact=2) )



Let's also suppress all of your TD, TR, and FONT tags - they are useful for matching, but just get in the way when we are trying to process the output (leave A tags alone, we might want the link):



    trS,trE         = map(Suppress, makeHTMLTags('TR'))
    tdS,tdE         = map(Suppress, makeHTMLTags('TD'))
    fontS,fontE     = map(Suppress, makeHTMLTags('FONT'))
    anchorS,anchorE = map(Suppress, makeHTMLTags('A'))



Now we can now define a complete row of data (I've added results names for each column):



    datarow = ( trS
                + Group( tdS + linkedText + tdE )('stationID')
                + Group( tdS + linkedText + tdE )('stationName')
                + Group( tdS + linkedText + tdE )('riverBasin')
                + Group( tdS + Optional(fontS) + date_time + Optional(fontE) + tdE )('lastUpdateTime')
                + Group( tdS + Optional(fontS) + realNum   + Optional(fontE) + tdE )('riverLevel')
                + Group( tdS + Optional(fontS) + realNum   + Optional(fontE) + tdE )('normalLevel')
                + Group( tdS + Optional(fontS) + realNum   + Optional(fontE) + tdE )('alertLevel')
                + Group( tdS + Optional(fontS) + realNum   + Optional(fontE) + tdE )('dangerLevel')
                + trE
              )
    datarow.ignore(pTag)

(I had to make the fontS and fontE tags optional, as they were not consistently used.  There were also <P> tags here and there - easiest to just ignore them.)



Now when we parse your web page, here is what one row's worth of data looks like:



    [['6602490'], ['Sg.Pelarit di Kaki Bukit'], ['Sg.Perlis'], ['11/04/2008-17:00'], 
        ['35.41'], ['35.60'], ['38.30'], ['39.00']]
    - alertLevel: ['38.30']
    - dangerLevel: ['39.00']
    - lastUpdateTime: ['11/04/2008-17:00']
    - normalLevel: ['35.60']
    - riverBasin: ['Sg.Perlis']
      - text: Sg.Perlis
    - riverLevel: ['35.41']
    - stationID: ['6602490']
      - text: 6602490
    - stationName: ['Sg.Pelarit di Kaki Bukit']
      - text: Sg.Pelarit di Kaki Bukit



From this you should be able to pick out the items you want to print, and the indented list shows how you would access that value:



    print data.stationID.text, data.stationName.text, data.riverBasin, data.lastUpdateTime, data.riverLevel



Here is a nicer-looking version, using an interpolated string:



    print '%s %-40s %-12s %s %8s' % \
            (data.stationID.text, data.stationName.text, data.riverBasin[0], 
             data.lastUpdateTime[0], data.riverLevel[0])



This prints out a nice tabular list:



    6602490 Sg.Pelarit di Kaki Bukit                 Sg.Perlis    11/04/2008-17:00    35.41
    6602480 Sg.Jarum di Kg.Masjid                    Sg.Perlis    11/04/2008-17:00    29.64
    6602481 Kolam Air di Timah Tasuh Dam             Sg.Perlis    11/04/2008-17:00    28.72
    6502480 Sg.Korok di Hilir Timah Tasuh Dam        Sg.Perlis    11/04/2008-17:00    18.53
    6402480 Sg.Arau di Kg. Kuala Tunggang            Sg.Arau      11/04/2008-17:00    12.77
    ....... Temiang                                  Sg.Arau      11/04/2008-17:00   -99.99



Here is the complete program:



    from pyparsing import *
    import urllib
    
    # get web page data
    url = 'http://infobanjir.water.gov.my/perlisw.htm'
    page = urllib.urlopen( url )
    html = page.read()
    page.close()
    
    trS,trE          = map(Suppress, makeHTMLTags('TR'))
    tdS,tdE          = map(Suppress, makeHTMLTags('TD'))
    fontS,fontE      = map(Suppress, makeHTMLTags('FONT'))
    anchorS, anchorE = map(Suppress, makeHTMLTags('A'))
    pTag             = map(Suppress, makeHTMLTags('P'))[0]
    
    realNum = Word(alphanums+'-',alphanums+'.')#.setParseAction(lambda t:float(t[0]))
    linkedText = anchorS('link') + fontS + SkipTo(fontE)('text') + fontE + anchorE
    date_time = Combine( Word(nums,exact=2) + '/' + Word(nums,exact=2) + '/' + 
                Word(nums,exact=4) + '-' + Word(nums,exact=2) + ':' + Word(nums,exact=2) )
    
    datarow = ( trS
                + Group( tdS + linkedText + tdE )('stationID')
                + Group( tdS + linkedText + tdE )('stationName')
                + Group( tdS + linkedText + tdE )('riverBasin')
                + Group( tdS + Optional(fontS) + date_time + Optional(fontE) + tdE )('lastUpdateTime')
                + Group( tdS + Optional(fontS) + realNum   + Optional(fontE) + tdE )('riverLevel')
                + Group( tdS + Optional(fontS) + realNum   + Optional(fontE) + tdE )('normalLevel')
                + Group( tdS + Optional(fontS) + realNum   + Optional(fontE) + tdE )('alertLevel')
                + Group( tdS + Optional(fontS) + realNum   + Optional(fontE) + tdE )('dangerLevel')
                + trE
              )
    datarow.ignore(pTag)
    
    for data,startloc,endloc in datarow.scanString(html):
        print '%s %-40s %-12s %s %8s' % \
            (data.stationID.text, data.stationName.text, data.riverBasin[0], data.lastUpdateTime[0], data.riverLevel[0])





> Because normal 'for' loop look like this 'for(i=0;i<10<i++)'.</li></ul></ul>

You must be confusing Python with some programming language that starts with 'C'. :)

Python for loops look like this:

    for (value) in (iterable):
        do stuff


The iterable will return items one at a time until StopIteration is raised, or the code block executes a 'break' statement.



Are you familiar with tuple-based assignment (called 'tuple unpacking') in Python?  It looks like this:



    a,b = 1,100
    min,max = 0, 1000
    a,b = b,a # Python version of in-place value swap



Well the code you are asking about:



    for srvrtokens,startloc,endloc in patt.scanString( serverListHTML):
        print srvrtokens.value

uses an iterable which is a Python generator `patt.scanString( serverListHTML)`, and this returns a 3-item tuple for each iteration.  Maybe this more-explicit version would help you to see what is happening:



    for parseDataTuple in patt.scanString( serverListHTML):
        srvrtokens,startloc,endloc = parseDataTuple
        print srvrtokens.value



For each iteration, the generator returns a tuple, and this then gets unpacked into the 3 variables srvrtokens, startloc, and endloc.



-- Paul
#### 2008-04-13 22:49:31 - lolob
Thanks Paul,



I'll study the examples thoroughly..

---
## 2008-04-07 13:15:29 - kenpierce - nestedExpr and preserving whitespace
Hi all,



I got into pyparsing today and (as is often the case with python) got close to where I wanted very quickly. I want to parse input data for a little text-replacement script that I'm writing. 



The input data is a series of rules. Each rule has a name and a single quoted string.





    reality: 'I reject your <s> and substitute my own.'



Each string can also contain zero or more references to other rules, contained within angle brackets. So far, I have the following:





    name = Word(alphas + nums + '_')
    elem = ...
    rule = name + Literal(':').suppress() + elem



I'm having trouble with the definition of elem. I want to maintain all the characters, whitespace etc. in the single-quoted string, since it will be printed out with the references replaced. I began looking at nestedExpr, but I can't get my head around preserving whitespace, e.g. the following is too simplistic.





    elem = OneOrMore(Word(alphanums) ^ nestedExpr())



Can anyone point me in the right direction?



Cheers,

Ken Pierce.

#### 2008-04-07 16:37:28 - ptmcg
Ken -

Welcome to pyparsing!



Perhaps the built-in defined `sglQuotedString` is what you are looking for?



If you want to automatically trim the quotations marks after parsing, then use:



    sglQuotedString.setParseAction(removeQuotes)



If you want to allow single or double quoted strings (like the way Python does), use:



    quotedString.setParseAction(removeQuotes)



-- Paul
#### 2008-04-07 16:44:28 - ptmcg
Also, it sounds like it would be helpful to you to look through the example on the Examples page (also in the examples directory that comes with pyparsing).



BTW, did you use easy_install or the Windows installer to install pyparsing?  If so, download either the source or doc release from SF, this will include more detailed HTML documentation and a copy of the examples directory.



-- Paul
#### 2008-04-08 14:54:51 - kenpierce
Thanks for the help, I'm getting to where I wanted to be now. I haven't had a lot of time to play, but hopefully I won't have further problems.



I used the Windows installer, but I also downloaded the examples. I hadn't spotted the macro one, I'll look into it.



Cheers,

Ken.
#### 2008-04-10 13:28:51 - kenpierce
Hi Again,



My script is coming on well, but I find myself back at the nestedExpr question. I am now at a point where I have a list of strings to parse and have built the following definition:





    defn = OneOrMore(CharsNotIn('<>') ^ nestedExpr('<','>'))
    input = 'I reject your <s> and substitute my own.'



This gives the following output for the test string (which is what I'm aiming for):





    (['I reject your ', (['s'], {}), ' and substitute my own.'], {})



The only problem is that I'd like to be able to escape angle brackets, e.g. 





    input = 'I reject your <s> and <blah> substitute my own.' 



This doesn't work. I added ignoreExpr=Literal('<'), but I'm not sure I'm using it correctly. Perhaps my naive definition using CharsNotIn is the problem.



Can anyone offer any guidance, please?



Cheers,

Ken.
#### 2008-04-10 14:40:27 - ptmcg
I didn't really have escaped delimiters in mind when I wrote `nestedExpr`, I was thinking more of expressions with nested parentheses, braces, etc.



`nestedExpr` takes an optional 3rd argument, defining the expression you expect to find within the nested characters.  If no expression is defined, then `nestedExpr` makes up one that is mostly a `CharsNotIn(nestingCharacters)`.  Since you want to support escaped nesting characters as valid nesting content, I thought I'd try defining what such content would be.  Also, this same content would be used in the overall `defn` expression, so I captured it as a separate expression, called `nonTag`:



    nonTag = Combine(OneOrMore(CharsNotIn(r'<>') | r'<' | r'>' ))



Now the overall definition looks like:



    defn = OneOrMore(nonTag | nestedExpr('<','>', nonTag))

(I'm also switching from the '^' version of alternation (Or) to the '|' version of alternation (MatchFirst) - since there is no chance of confusing a CharsNotIn(r'<>') with a '<' or a '>', then I don't have to exhaustively check every alternative, which is what Or does.)



With this change, I can now parse your input string (which I've extended a bit to test how well the repeated nesting works:



    input = r'I reject your <s> and <blah> substitute my own <s2>.'
    
    print defn.parseString(input)

prints:

    ['I reject your ', ['s'], ' and \<blah\> substitute my own ', ['s2'], '.']

But I have to ask a couple of questions:

- Will `<s>` tags every contain other `<tags>` as in `<s<s2>>`?  If not, then nestedExpr is not the best choice, and I would suggest QuotedString instead.

- This looks like some form of templating application.  If so, you might want to have pyparsing do the whole tag-recognition-and-substitution job for you, by defining a parse action for tags, and using transformString instead of parseString.  See the following:





    from pyparsing import QuotedString
    import random
    
    ilities = 'reality/truth/unreality/theory/philosophy/irrationality/dogma'.split('/')
    
    # define the expression for a tag, and ignore escaped <'s
    tag = QuotedString('<',endQuoteChar='>')
    tag.ignore('<')
    
    # add a parse action that will replace <s1> and <s2> tags with
    # the contents of variable s1 and s2
    tag.setParseAction(lambda t: globals()[t[0]])
    
    input = r'I reject your <s1> and <blah> substitute my own <s2>.'
    for i in range(5):
        # define the substitution words
        s1 = random.choice(ilities)
        s2 = random.choice(ilities) 
    
        # transform the template string
        print tag.transformString(input)



Prints (of course, your own results will differ):



    I reject your reality and <blah> substitute my own unreality.
    I reject your reality and <blah> substitute my own philosophy.
    I reject your unreality and <blah> substitute my own irrationality.
    I reject your dogma and <blah> substitute my own truth.
    I reject your theory and <blah> substitute my own irrationality.



-- Paul
#### 2008-04-10 15:39:57 - kenpierce
Hi Paul,



The script is a little nonsense generator that parses sentences with placeholders and inserts words chosen randomly from word lists. The input is a set of recursivley defined rules. 



I wrote the recursive algorithm/data structure first, so I just need the parsing. This also means however, that the only thing that can appear in a tag is a rule name, so QuotedString it is.



I'm going to switch to '%' instead of the angle brackets to simplify things. Attempting to modify the previous example doesn't quite work. 





    nonTag = Combine(OneOrMore(CharsNotIn(r'\%')))
    defn = OneOrMore(nonTag | QuotedString(r'%',escChar=r'\\'))
    input = r'I reject your %s1% and \%blah\% substitute my own %s2%.'
    
    >>> defn.parseString(input)
    (['I reject your ', 's1', 'and '], {})



What am I missing?



Cheers,

Ken.
#### 2008-04-10 19:05:37 - ptmcg
Ken -



The purpose of the escChar argument is to escape occurrences of the quote char inside the quoted string.  In your case, that would allow you to define a tag that contains a '%', like '%s1\%%' -> 's1%'  But I don't think you really need to have % signs in your tags, so you can drop the escapeChar arg entirely.



You also need to expand the definition of nonTag to include r'\%'.





    input = r'I reject your %s1% and \%blah\% substitute my own %s2%.'
    nonTag = Combine(OneOrMore(CharsNotIn(r'\%') | r'\%' ))
    defn = OneOrMore(nonTag | QuotedString(r'%'))
    print defn.parseString(input)

prints:



    ['I reject your ', 's1', ' and \\%blah\\% substitute my own ', 's2', '.']



-- Paul
#### 2008-04-11 06:38:30 - kenpierce
Paul,



Excellent, that works perfectly. Thanks for you help.



I have another question, but I'll save that for a new thread later today.



Cheers,

Ken.



'I reject your didactic procedure and substitute my own.'

---
## 2008-04-08 15:52:18 - MatTourne - ParseResults and location
It would be good to have the location of each matched token.



I'm applying 'rules' according to the data that I've parsed. This is done in several ParseActions at various depths in the grammar. In order to display coherent warning messages when rules are not followed, having the location for each matched token would be nice.



Are their plans to had this feature ?



PS : What's in the FAQ for getting the line number of each token is not really relevant to me.

#### 2008-04-08 17:43:44 - ptmcg
What form of argument list are you using to call your parse actions?  Are you passing just the `(tokens)`, or the full `(string, location, tokens)` argument list?  If you use the full argument list, you could estimate the location by searching within the input string using the location as a starting point.



You could also add a synthesized results name in a parse action, like this:



    def parseActionCapturingLocation(string,locn,tokens):
        tokens['_location_'] = locn



Then add this parse action wherever you choose, and you can get the location of where the tokens were matched.

I currently don't have any plans to add this feature, but I don't think it would be a big deal to do so.  Note though, that this still does not save the location of each matched token, just the location of the beginning of the expression (likely to be the location of the first token).

Does this help?



-- Paul
#### 2008-04-08 18:04:09 - MatTourne
I was already aware of those options to get the locations I need, the process is just a little painful.

It'd be really awesome to have this feature :)
#### 2008-04-08 21:12:25 - ptmcg
Can you post an example of how this feature should look from the calling code's standpoint? Are you looking for pyparsing to just add these `<!-- ws:start:WikiTextRawRule:0:``__location__`` -->__location__<!-- ws:end:WikiTextRawRule:0 -->` values for all ParseResults, or something else?



For those tokens that are just strings, of course I can't just 
attach a `<!-- ws:start:WikiTextRawRule:1:``__location__`` -->__location__<!-- ws:end:WikiTextRawRule:1 -->` attribute, since those tokens are Python built-in objects. So to get the location of every parsed token, how do you envision this would look? Could you post a simulated code snippet showing how you would want to access the locations after parsing, say, 'Hello, World!'?



-- Paul
#### 2008-04-08 23:02:26 - MatTourne
I think that's what I'm looking for. Would it be possible to extend the python built-in objects for 
having a `__location__` attribute ?

    from pyparsing import Word, alphas
    
    # define grammar
    greet = Word( alphas ) + ',' + Word( alphas ) + '!'
    
    # input string
    hello = 'Hello, World!'
    
    # parse input string
    tokens = greet.parseString( hello )
    for i in tokens:
      print '%s : %s' % (i.getLocation(), i)



In this example, the getLocation() is a getter over `__location__`
#### 2008-04-08 23:36:17 - ptmcg
I could do that with ParseResults objects, but not for strings.  To handle all cases, I think I would have to store the locations with the parent ParseResults, like:



    tokens = greet.parseString(hello)
    for loc,token in zip(tokens.locations,tokens):
        print loc,token



That is, for each element in a ParseResults, have the ParseResults also store that element's location.



Not sure how much work that would be, I already do some legerdemain when building up a ParseResults object so the code is somewhat involved.  But how do you think this would look for you?



-- Paul
#### 2008-04-09 16:37:55 - MatTourne
It would be good to have the location directly for a token without going through all the tokens.locations list.



For the strings, there is no way to encapsulate a string inside another object and add locations ?
#### 2008-04-09 18:10:57 - ptmcg
This may be a good time to subclass a built-in class.  I could subclass str in a Pyparsing class solely to add a location attribute, but otherwise just like a string.



-- Paul
#### 2008-04-09 18:22:07 - MatTourne
Yeah that's what I've been thinking.



Can you give me updates if you finally implement it ?



I got around searching within the string from location, but having this feature would help cleaning my code



Thanks for all your answers, and for pyparsing which is a great tool :)

---
## 2008-04-16 10:38:10 - MatTourne - Error recovery
I've tried integrating an error recovery mechanism in my parser, but none of them are working as well as having a dedicated token like in bison :





I've tried re-launching the parse after the error using moreless parseString(string[err.loc + 1:] but by doing that, I need to keep the previously matched tokens, and the locations inside the string are not correct anymore if other errors raise an exception

#### 2008-04-16 10:58:12 - ptmcg
string[error.loc+1:] may have a problem if there are tabs in string.  The reported error location is given for a tab-expanded version of the input string by default.  Call expandtabs() on your string before slicing it.



-- Paul
#### 2008-04-16 11:35:39 - MatTourne
The problem is that err.loc + 1 becomes the new 0 of the second parsing. And having a cleaner way to manage error recovery would be a nice thing.

---
## 2008-04-17 00:43:48 - propell - QuotedString problems with escChar
Hi



I'm working on improving a parser for the Graphviz dot language. I need to parse statements like:





    a = '\n\nA \'quote\'';



To parse the quoted string I use





    from pyparsing import QuotedString
    
    s = r''\n\nA \'quote\'''
    
    qstring = QuotedString(''',multiline=True,escChar='\\')
    print qstring.parseString(s)



The problem is that pyparsing removes all the backslashes and I get:



    ['nnA 'quote'']

I need to preserve the \n chars and want to get output like:



    ['\\n\\nA 'quote'']



Is there something I can do to avoid this behavior?



Regards,

Kjell Magne Fauske

#### 2008-04-17 01:31:44 - ptmcg
Kjell -



Thanks for your note, this highlights a bug in QuotedString!



I was going to suggest you use `escQuote=r'\"'` instead of `escChar='\\'`, but it turns out that this still doesn't do the right thing.  QuotedString tries to generate a regex to do the matching, and it doesn't quite get it right.



For your case, the regex should be r'\'(?:(?:\\\')|[^'])*\''.  Since QuotedString is not generating this correctly, you can replace your use of QuotedString with 



    qstring = Regex(r'\'(?:\\\'|[^'])*\'', re.MULTILINE)

I think that will fix things.



-- Paul
#### 2008-04-17 02:05:04 - propell
Paul -



Thank you for your help! The Regex version of qstring works and the parser is finally passing all of my unit tests. 



- Kjell Magne
#### 2008-05-04 11:15:39 - propell
Paul,



I noticed a corner case where your qstring suggestion fails:





    from pyparsing import QuotedString,Regex
    import re
    s = r''\n\nA \'quote\'\\' abc 'def''
    
    qstring = Regex(r'\'(?:\\\'|[^'])*\'', re.MULTILINE)
    print qstring.parseString(s)



The output is in this case:



    [''\\n\\nA \\'quote\\'\\\\' abc '']

The expected output is:



    [''\\n\\nA \\'quote\\'\\\\'']



A fix seems to be:



    qstring2 = Regex(r'\'(?:\\\'|\\\\|[^'])*\'', re.MULTILINE)
    print qstring2.parseString(s)

which gives the expected output:



    [''\\n\\nA \\'quote\\'\\\\'']



Thanks again for the help. 



- Kjell Magne

---
## 2008-04-17 15:30:03 - michael_ramirez44 - Optional Ordering
My grammar fails if I change the order of my CLASS_MODIFIERS. When I move Optional(DYNAMIC) to the front of CLASS_MODIFIERS then it breaks.

    CLASS_MODIFIERS = Optional(FINAL) & Optional(INTERNAL ^ PUBLIC ^ PRIVATE ^ PROTECTED) & Optional(DYNAMIC)
    CLASS_DEFINITION = CLASS_MODIFIERS + CLASS + QUALIFIED_IDENTIFIER + Optional( CLASS_EXTENDS ) + Optional( CLASS_IMPLEMENTS ) + CLASS_BLOCK

I know it's failing when parsing the following string:

    public final class ButtonLabelPlacement

#### 2008-04-17 16:29:53 - ptmcg
Michael -



Thank you, this is a bug in the Each class!  The bug is caused by Each doing some premature expression grouping during the grammar construction phase, before the full Each expression has been completed.



I'll check in a patch to SVN, or drop me a note if you need me to send you a copy of the pyparsing.py file.



-- Paul
#### 2008-04-18 07:29:23 - michael_ramirez44
Paul,



Glad I found a bug for you :)

Can you send me a copy of the pyparsing.py file?
#### 2008-04-18 10:23:05 - ptmcg
I've been trying to update the SF SVN repository with the latest pyparsing.py, but about 3 weeks ago, SF changed all account passwords, and the password recovery process is either broken or just overwhelmed.



I will post the latest pyparsing.py file as a link from the Wiki home page in a few minutes.



-- Paul
#### 2008-04-18 10:30:03 - ptmcg
You can get the latest development version of pyparsing on the Wiki home page.



-- Paul

---
## 2008-04-29 07:47:43 - dminor14 - Can't get alternative keywords to work
I'm trying a simple test to parse (scan) c++ #ifdef and #ifndef, below is my code. It only parses #ifdef's, I'd appreciate if someone could tell me why. 

Thanks,

David





    import sys
    
    from pyparsing import *
    
    input_file = sys.argv[1]
    
    file_string = file(input_file,'r').readlines()
    
    def searchExpression(toks):
        print 'found', toks
    
    pre_identifier = Word(alphas, alphanums+'_')
    
    pre_ifdef_key = Keyword('#ifdef')
    pre_ifndef_key = Keyword('#ifndef')
    
    pre_def_key =  pre_ifdef_key | pre_ifndef_key
    
    print pre_def_key
    
    pre_def_expr = pre_def_key + pre_identifier.setResultsName('name') 
    
    pre_def_expr.setParseAction(searchExpression)
    
    print pre_def_expr
    
    
    for line in file_string:
        for t,s,e in pre_def_expr.scanString(line):
            print line[s:e] 
    



#### 2008-04-29 09:43:04 - ptmcg
David -



Could it be that your test files contain no #ifndefs?  I replaced your code to read in the input text with the following code that forces several test cases (I also tweaked your Keyword defs to add 'caseless=True' - I think #ifdef and #ifndef are case-insensitive, and you <em>never</em> know what a programmer will type in):



    file_string = '''
    #ifdef blah
    lsjdflsf
    lsdfl
    #ifndef blhaha
    sldf
    #ifdef2 lskdfljf // this should not match
    // test some caseless testing (using caseless=True argument to Keyword)
    #IFDEF blasaflaj
    lsjflfj
    #IFNDEF wlejr
    '''.splitlines()



And I got:



    {'#ifdef' | '#ifndef'}
    {{'#ifdef' | '#ifndef'} W:(abcd...,abcd...)}
    found ['#ifdef', 'blah']
    #ifdef blah
    found ['#ifndef', 'blhaha']
    #ifndef blhaha
    found ['#ifdef', 'blasaflaj']
    #IFDEF blasaflaj
    found ['#ifndef', 'wlejr']
    #IFNDEF wlejr



So it looks like 'ifndef's are matching, as far as I can tell.



-- Paul
#### 2008-04-29 21:48:31 - dminor14
Boy do I feel like an idiot! I kept messing with the test code at the same time I was messing with the parsing code and got confused. Thanks.

---
## 2008-05-04 15:59:48 - ecir-hana - setParseAction, Optional and Combine
Hi,

please, I have some troubles:



when Combine-ing just one match (Optionals don't match):

    Combine(Optional('...') + Literal('...') + Optional('...')).setParseAction(action)



'action' gets called twice. Why? Is this how it's supposed to be? Because when those Optionals get matched, everything's ok, 'action' gets called just once. What can I do to make it call just once in any case?



Thanks a lot in advance!

#### 2008-05-04 16:26:55 - ecir-hana
False alarm! My bad, I apologize!



One of those Optional('...') had another setParseAction attached to a global property.

---
## 2008-05-04 23:02:26 - dminor14 - transformString question
I hope this question is smarter than my last one! BTW, great package, I like your sense of aesthetics, it's definitely the easiest to use and read parser package I've found. Now the question. I've noticed that when transformString matches and I don't have an action attached to that match it strips off all blanks and runs the literals together. Is there a reason for this? I altered the code of transformString to do a join with spaces, ' '.join, and it seems to act more like I would expect with the side effect that it also inserts a space before each match even if it's at the beginning of a line. I've worked around the problem (in the unalters pyparsing.py) by adding addParseAction(keepBlanks) to most of my literals in order to add the blanks back in. But this results in tokens with blanks at the end. What's the best solution? Below I pasted my program, whose purpose is to change WIN32 defines to support 64 bit.



Regards,

David Minor





    import sys
    
    from pyparsing import *
    
    input_file = sys.argv[1]
    
    file_string = file(input_file,'r').readlines()
    
    
    def rewriteIfDefExpr(toks):
        return '#if (defined WIN32 || defined __WIN64)'
    
    def rewriteIfNDefExpr(toks):
        return '#if not (defined WIN32 || defined __WIN64)'
    
    
    def rewriteDefineExpr(toks):
        #print 'found toks', toks
        if  toks[1] == 'WIN32 ':
            #print 'found win32 tok'
            return '(defined WIN32 || defined __WIN64) '
        else :
            return ' '.join(toks)
    
    def keepBlanks(toks):
        return ' '.join(toks) + ' '
    
    identifier = Word(alphas, alphanums+'_')
    
    #keywords
    pre_if_def      = Keyword('#ifdef').addParseAction(keepBlanks)
    pre_if_ndef     = Keyword('#ifndef').addParseAction(keepBlanks)
    
    pre_if          = Keyword('#if').addParseAction(keepBlanks)
    pre_elif        = Keyword('#elif').addParseAction(keepBlanks)
    
    #operators
    pre_defined     = Literal('defined').addParseAction(keepBlanks)
    pre_not         = Literal('not').addParseAction(keepBlanks) | Literal('!')
    pre_or          = Literal('or') | Literal('|')
    left_paren      = Literal('(') 
    right_paren     = Literal(')') 
    
    
    #literals
    win32           = Literal('WIN32')
    
    #expressions
    #
    if_def_expr = (pre_if_def + win32) 
    if_def_expr.setParseAction(rewriteIfDefExpr)
    
    if_ndef_expr = (pre_if_ndef + win32)
    if_ndef_expr.setParseAction(rewriteIfNDefExpr)
    
    define_expr          = pre_defined + identifier.addParseAction(keepBlanks)
    define_expr.setParseAction(rewriteDefineExpr)
    pre_expression = ZeroOrMore(left_paren) + ZeroOrMore(pre_not) + define_expr + ZeroOrMore(right_paren)
    
    
    if_expression = (pre_if | pre_elif) + OneOrMore(pre_expression)
    
    all_expressions = if_def_expr | if_ndef_expr | if_expression
    
    all_expressions.ignore(cStyleComment)
    
    for line in file_string:
        print all_expressions.transformString(line)[:-1]
    
    
    



#### 2008-05-05 00:47:20 - ptmcg
David -



Thanks for the glowing praise!  Pyparsing is definitely taking on a life of its own - sometimes I feel like an author of a novel, who doesn't really know what his characters are going to do next.



Very interesting application, and I think it should be a good fit for transformString.  But I can see that there are some wrinkles in the system when trying to modify an element only if it is part of a larger expression.  transformString really works best if the expressions that are parsed have parse actions that return the full replacement string.



Here is a modified version of your program.  I've redone it so that you don't have to add keepBlanks to every Literal.  I've also used the pyparsing helper method replaceWith, to take the place of your first 2 parse actions.



The tricky case is when trying to match a nested boolean expression.  I've written sort of a hack to try to match these, short of writing a full boolean expression parser.  When an #if defined ... is found, I then use 2 parse actions: one to extract the original text with whitespace as-is, then a second to run a mini-transformString pass to do the WIN32 replacement.



I'm not sure I'd say this is a big step forward beyond what you already have done, but it might give you some other ideas to pursue.



-- Paul





    import sys
    
    from pyparsing import *
    
    input_file = sys.argv[1]
    
    file_string = file(input_file,'r').readlines()
    
    def rewriteDefineExpr(toks):
        #print 'found toks', toks
        if  toks[1] == 'WIN32':
            #print 'found win32 tok'
            return '(defined WIN32 || defined __WIN64)'
        else :
            return ' '.join(toks)
    
    defWin32 = Keyword('defined') + Keyword('WIN32')
    defWin32.setParseAction(replaceWith('(defined WIN32 || defined __WIN64)')
    def convertWIN32refs(t):
        return defWin32.transformString(t[0])
    
    identifier = Word(alphas, alphanums+'_')
    
    #keywords
    pre_if_def      = Keyword('#ifdef')
    pre_if_ndef     = Keyword('#ifndef')
    
    pre_if          = Keyword('#if')
    pre_elif        = Keyword('#elif')
    
    #operators
    pre_defined     = Literal('defined')
    pre_not         = Literal('not') | Literal('!')
    pre_or          = Literal('or') | Literal('|')
    left_paren      = Literal('(') 
    right_paren     = Literal(')') 
    
    
    #literals
    win32           = Literal('WIN32')
    
    #expressions
    #
    if_def_expr = (pre_if_def + win32) 
    if_def_expr.setParseAction(replaceWith('#if (defined WIN32 || defined __WIN64)'))
    
    if_ndef_expr = (pre_if_ndef + win32)
    if_ndef_expr.setParseAction(replaceWith('#if not (defined WIN32 || defined __WIN64)'))
    
    define_expr          = pre_defined + identifier
    define_expr.setParseAction(rewriteDefineExpr)
    
    #~ pre_expression = ZeroOrMore(left_paren) + ZeroOrMore(pre_not) + define_expr + ZeroOrMore(right_paren)
    pre_expression = delimitedList( define_expr | defWin32 | nestedExpr(), '|' )
    pre_expression.setParseAction(keepOriginalText, convertWIN32refs)
    
    if_expression = (pre_if | pre_elif) + pre_expression
    if_expression.setParseAction(lambda t: ' '.join(t))
    
    all_expressions = if_def_expr | if_ndef_expr | if_expression
    
    all_expressions.ignore(cStyleComment)
    
    for line in file_string:
        print all_expressions.transformString(line)[:-1]
    


#### 2008-05-05 00:48:49 - ptmcg
Oh, you should also change Literal('WIN32') to Keyword('WIN32'), else you will get false matches on things like '#ifdef WIN32_LEAN_AND_MEAN'



-- Paul

---
## 2008-05-13 11:53:40 - MatTourne - ignore rule
Could it be possible that the parser ignores a rule, e.g :

    SQL.ignore(cStyleComment) to ignore comments in _most_ cases



but also has the ability to do : 

    ALTER_STATEMENT = ALTER_TOKEN + Optional(Suppress(/*) + Word(alphas).setResultsName('metainfos') + Suppress(*/)) + ...



For some very particular cases, where we want to parse ignored rules.

#### 2008-05-14 19:21:23 - ptmcg
Mat -



I think this might actually require some sort of class like UnIgnore or something, in which the class's preParse method overrides the default ignore behavior.  This is completely untested, but how about something like this?





    class UnIgnore(ParseElementEnhance):
        def preParse( self, instring, loc ):
            if self.skipWhitespace:
                wt = self.whiteChars
                instrlen = len(instring)
                while loc < instrlen and instring[loc] in wt:
                    loc += 1
            return loc



Then use it like this:



    ALTER_STATEMENT = ALTER_TOKEN + \
        UnIgnore(Optional(Suppress('/*') + Word(alphas)('metainfos') + Suppress('*/')))



This is purely a wild guess, but might spark an idea for you.



-- Paul

---
## 2008-05-27 04:52:05 - jstanforth - WordStart syntax question
Hi Paul,



I have a question about the new WordStart/WordEnd syntax, which I must be using incorrectly given the error I'm seeing.  (Btw, I downloaded the 1.4.11 tar.gz yesterday, but the HowToUsePyparsing.html inside is still for 1.4.10, just fyi.)



In this case, given a scanString with random html or text content, I need to just grab all embedded `__$var__` variables.  Seems like it should be really easy with something like:





    scrbVarOpen = Literal('__$')
    scrbVarClose = Literal('__`)
    
    scrbVariable = WordStart + scrbVarOpen + \ 
                   Word(alphanums+'_.').setResultsName('var') + \
                   scrbVarClose + WordEnd
    
    scrbVariable.setParseAction( self.procContentVar )



... but that gives me a type error:



    TypeError: unsupported operand type(s) for +: 'NoneType' and 'type'



Of course, the expression without WordStart/WordEnd doesn't seem to match either, so maybe my problem is a level higher than these new positionals... ;-)



Btw, despite the very basic problem here, I'm using pyparsing in a few more complicated use-cases (with web-based templating) where it is just so incredibly fantastic that I can't imagine what it was like coding before this!  Thanks for creating such a brilliantly simple solution.



Cheers,

John

#### 2008-05-27 04:54:31 - jstanforth
Oops, forgot about wiki-formatting syntax breaking the variable name... :)  That should've been...





    __$var__


#### 2008-05-27 06:47:02 - ptmcg
WordStart and WordEnd are classes (note the leading capital letters).  Change your code to:





    scrbVariable = WordStart() + scrbVarOpen + \
                   Word(alphanums+'_.').setResultsName('var') + \
                   scrbVarClose + WordEnd()



and things should look better.
#### 2008-05-27 06:49:09 - ptmcg
... and thanks for the note on 'HowToUse...' being out of date.  In general, I think this doc needs some overhaul, but I'm lucky just to keep the feature descriptions current.



Cheers,

-- Paul
#### 2008-05-27 10:29:14 - jstanforth
Doh! How dumb... sorry, Paul. :)  I was more puzzled by it not matching and obviously didn't pay close enough attention to what I was typing.  Even with that fixed, though, it doesn't seem to match... Maybe there's a better way for me to approach this?



If I have a content block like this...





    asdfasdfasdfasdfasdfasdfasdf
    asdfasdf__$varAsdf__asdfasdf
    asdfasdfasdfasdfasdfasdfasdf
    asdf  <ssp print $a />  asdf
    asdfasdfasdfasdfasdfasdfasdf



... then, define a grammar like this to grab the variable and ssp tag from scanString()...





    scrbVarOpen = Literal('__$')
        scrbVarClose = Literal('__`)
        scrbVariable = WordStart() + scrbVarOpen + Word(alphanums+'_.').setResultsName('var') + scrbVarClose + WordEnd()
        scrbVariable.setParseAction( self.procContentVar )
    
        scrbContentTag = Suppress('<') + Suppress( Keyword('ssp') ) + SkipTo(Literal('/>')).setResultsName('tagData') + Suppress('/>')
        scrbContentTag.setParseAction( self.procContentTag )
    
        scrbContent = scrbVariable | scrbContentTag



... it doesn't seem to match either (firing neither ParseAction), though the same grammar seems to match other ssp tags in other content blocks when run elsewhere.



Is there something wrong with defining the grammar this way? or more importantly, is there a better way to accomplish what I'm attempting there?  As much as I love pyparsing, I'm clearly not familiar enough with it for solutions to be intuitive yet. :)



Thanks so much for your insight on this...



Cheers,

John
#### 2008-05-27 11:28:02 - ptmcg
John -



What method are you using to try to find these matches, parseString?  If I copy your code (cleaning up the stray 'self' reference to simplify the example), and use searchString, I at least get a match on the scrbContentTag expression.  So you <em>are</em> doing <em>something</em> right.



scrbVariable is a little trickier, for several reasons.  First of all, I'm not sure what you are trying to accomplish with WordStart() and WordEnd().  In the example you post, there are no word boundaries either before or after the '__$varAsdf__`, so you are immediately doomed to fail.  If I remove the WordStart and WordEnd, I am then left with:



    scrbVariable = scrbVarOpen + Word(alphanums+'_.').setResultsName('var') + scrbVarClose



This is still doomed to fail, but for a subtler reason - which may become clearer if I substitute back in your open and close expressions:



    scrbVariable = '__$var' + Word(alphanums+'_.').setResultsName('var') + '__`



The unfortunateness here is that the trailing '__` characters will be consumed in the parsing of Word(alphanums+'_.'), since the var name you define allows for '_' as part of the name - the parser will just continue reading the trailing '__` as part of the varname, and never actually find '__`.



Pyparsing is able to do lookahead with some effort, and in this case, the effort gets almost gruesome in form.  Lookahead is done using pyparsing's FollowedBy expression, but for us to get to use FollowedBy, we need to break open the implied repetition in the Word expression, and allow for '_'s only if not followed by a second '_':



    scrbVariable = '__$var' + \
        OneOrMore(Word(alphanums+'.')|'_' + ~FollowedBy('_')).setResultsName('var') + '__`



And even this is not sufficient, as we will eventually find that the individual pieces of var names with internal '_'s are being reported as lists of tokens, not a single contiguous string.  So we must also add Combine to take this final step:



    scrbVariable = '__$var' + \
        Combine(OneOrMore(Word(alphanums+'.')|'_' + ~FollowedBy('_'))).setResultsName('var') + '__`



At this point, I'd really recommend that a short regex might serve us better.  You can embed regexes within your grammar using the pyparsing Regex class:



    scrbVariable = Regex(r'__\$var(?P<var>[a-zA-Z0-9._]+)__`)



Since regexes implicitly <em>do</em> lookahead-type logic, then this expression will do what you like.  Note that I I have named the internal field by enclosing the var name in grouping '()'s, and using the '?P<<em>name</em>>' group prefix to give the group a name.  If groups within a regex are named, pyparsing's Regex class will automatically set the results name for you when the Regex is matched.



With this program:



    from pyparsing import *
    
    data = '''
    asdfasdfasdfasdfasdfasdfasdf
    asdfasdf__$varAsdf__asdfasdf
    asdfasdfasdfasdfasdfasdfasdf
    asdf  <ssp print $a />  asdf
    asdfasdfasdfasdfasdfasdfasdf
    '''
    
    @traceParseAction
    def procContentVar(tokens):
        print tokens.dump()
    
    @traceParseAction
    def procContentTag(tokens):
        print tokens.dump()
    
    scrbVarOpen = Literal('__$')
    scrbVarClose = Literal('__`)
    scrbVariable = Regex(r'__\$var(?P<var>[a-zA-Z0-9._]+)__`)
    scrbVariable.setParseAction( procContentVar )
    
    scrbContentTag = Suppress('<') + Suppress( Keyword('ssp') ) + \
        SkipTo(Literal('/>')).setResultsName('tagData') + Suppress('/>')
    scrbContentTag.setParseAction( procContentTag )
    
    scrbContent = scrbVariable | scrbContentTag
    
    scrbContent.searchString(data)
    

here is your output:



    >>entering procContentVar(line: 'asdfasdf__$varAsdf__asdfasdf', 38, ['__$varAsdf__`])
    ['__$varAsdf__`]
    - var: Asdf
    <<leaving procContentVar (ret: None)
    >>entering procContentTag(line: 'asdf  <ssp print $a />  asdf', 94, ['print $a '])
    ['print $a ']
    - tagData: print $a 
    <<leaving procContentTag (ret: None)



Don't get discouraged - I think you are really further along with pyparsing than you think!



Cheers,

-- Paul
#### 2008-05-27 11:57:32 - jstanforth
THANK YOU, Paul!  This wasn't an 'answer'... this was an education! :-)



The problem with underscores within variable names was even more confusing since there are two cases in my application, one within scripts (where variables are all $var, with clean word boundaries and no underscores), and one case with content blocks as you've seen here.  And since it works fine on the other side, I kept looking elsewhere for what I might be doing wrong.



That said, even if I'd diagnosed the problem, I still wouldn't have realized how well regex expressions worked within pyparsing.  Your explanation here really opens up some interesting opportunities to more effectively restructure some of my grammars elsewhere as well.



Thanks so much!

John
#### 2008-05-27 12:18:53 - ptmcg
Great!  I'm glad things are clearing up for you.



One last note: pyparsing already does some regex optimizations internally.  For instance, Word(alphas,alphanums) internally creates and uses a regex to do its parsing, and so does oneOf('+ - * /') - while still keeping pyparsing's verbosity/self-explanatory expression names in your code.  So don't feel you have to go overboard in replacing pyparsing expressions with Regex's.  I'd definitely agree that avoiding 'Combine(OneOrMore(blah | bleh), etc.)' is an obvious case for using Regex, though.



Please let me know when you publish your application, I can add you to the 'Who's Using Pyparsing' wiki page!



-- Paul
#### 2008-05-27 14:40:34 - jstanforth
Yeah, I came to pyparsing to get away from Regex (and definitely prefer pyparsing expressions!), so I'm not likely in danger of overusing Regex.   :)  Still, it has its uses, as you've noted, and it's neat to see how it improves edge-case options here.



Sure, I'll let you know when the app is finished... I'm basically building a Python version of a 12-year-old C++ web services platform, and using pyparsing to transform website templates into Python code.  So not only do I get the convenience of pyparsing on the dev side, but also, the speed of one-time parsing doesn't really matter in production, since web requests are served directly from pre-imported Python modules.



And coming from C++ to Python/pyparsing... wow... you can only imagine how quickly it's made me a raving fan of your work. :-)



Thanks again,

John

---
## 2008-05-28 16:08:24 - sli1que - pyparsing usage
I am new to pyparsing and I am having a problem setting up a grammer.

I have something like:

gram = A + B + C + D + E



The problem is that the B,C,D can be any order and the D is optional. How do I set this up on the final gram line.



Thanks

#### 2008-05-28 16:16:13 - sli1que
Nevermind. I found an existing example from the list that works.

    D = Optional()
    
    Each( [A, B, C, D, E] )
#### 2008-05-28 18:18:24 - ptmcg
You can also write that as:



    D = Optional(whatever)
    gram = A & B & C & D & E



Welcome to pyparsing! - be sure to post back here when you have more questions, or have developed some awesome parser to brag about! :)



-- Paul

---
## 2008-06-03 18:51:12 - MatTourne - "Visiting" the ParseResults
I'm trying to make some sort of visitor for my ParseResults. I need to be able to stop on any 'setResultsName', and have a default code that will walk all the ParseResults.



Currently I can only stop on some of them. I've tried modifying my code, but I can only make things worse.



Here is my visitor code, it will go into visit_queries, but not into visit_query:





    class Visitor(object):
      def _default(self, tokens):
        for token in tokens:
          if isinstance(token, pyp.ParseResults):
            self.visit(token)
    
      def visit_query(self, tokens):
         print tokens
         self._default(tokens)
    
      def visit_queries(self, tokens):
         print tokens
         self._default(tokens)
    
      def visit(self, tokens):
        namedItems = tokens.asDict().keys()
        print namedItems
        for namedItem in namedItems:
          res = tokens[namedItem]
          meth = getattr(Visitor, 'visit_' + namedItem, Visitor._default)
          meth(self, res)



Althought if I print the results with asXML() queries, and query seem the same. they are both 'pyp.Group(...).setResultName'





    <xml>
      <queries>
        <query>
          <metainfos>
            ....
          </metainfos>
          <statement>
            <query_type>alter</query_type>
            <table>Order</table>
            <operations>
              <operation>
                <op_type>modify</op_type>
                <column>ProductType</column>
                <column_definition>
                  <type>
                    <type_type>enum</type_type>
                    <list>
                      <ITEM>Foo</ITEM>
                      <ITEM>Bar</ITEM>
                    </list>
                  </type>
          </statement>
        <query>
          <statement>
           ...
          </statement>
        </query>
      </queries>
    </xml>
    



#### 2008-06-03 20:07:54 - ptmcg
Mat -

It would help if you would post the grammar and input text that is creating these ParseResults.

Off the top of my head, I have these comments:
1. tokens.asDict().keys() is unnecessary, and creates an extra throwaway dict; just use tokens.keys()

2. By default, results names store only 1 result value for a given name.  From your names 'queries' and 'query', it sounds like you might be trying to parse a collection named 'queries', each element of which has a results name of 'query'.  In the default case, the key 'query' will only give you the last element.  If you call setResultsName('query', listAllMatches=True), then results['query'] will return a list of elements that have the results name 'query'.

3. You might also try indexing through the ParseResults, and for these items that themselves are ParseResults, call 'getName' - this is pretty much how asXML works.

Not sure if this sparks any ideas, if not, please post back with a grammar and text that will show what you are trying to parse.

-- Paul

---
## 2008-06-06 12:20:37 - skinny_00 - parsing a simple language
Hi. I'm just starting to dig into using pyparsing for this project and want to make sure I'm on the right track and ask a few questions.

I have a simple language that I developed a few years ago for defining message sequence charts.

I currently use Perl and regular expressions to parse the language - but I have found this simple parsing to be somewhat lacking.

The language is currently a single line per command -


    cmd[(optional param(s))]: arg(s)

That's it. Simple enough.

Examples might be:


    decl(color=#FF0000): A
    msg: A, B, message text
    new: C, B, create
    ...



Note there is no white space at the front of each line. Each command has a specific number of ',' separated argument. Command parameters are optional and enclosed in '()'.

Here's my first attempt at parsing this kind of language:


    from pyparsing import *
    COLON      = Literal(':').suppress()
    COMMA      = Literal(',').suppress()
    EQUALS     = Literal('=').suppress()
    LPAREN     = Literal('(').suppress()
    RPAREN     = Literal(')').suppress()
    DECL       = Literal('decl')
    MSG        = Literal('msg')
    
    cmd_last_arg = restOfLine
    cmd_arg    = ZeroOrMore(Regex(r'[^,]+'))
    
    parm_label = Regex(r'[^,()=]')
    parm_value = Regex(r'[^,()=]')
    
    parm_arg   = parm_label + EQUALS + parm_value
    parm_args  = delimitedList(parm_arg,delim=',')
    parms      = LPAREN + parm_args + RPAREN
    
    decl_cmd   = Group(DECL + Optional(parms) + COLON + cmd_last_arg)
    msg_cmd    = Group(MSG +  COLON + cmd_arg + COMMA + cmd_arg + COMMA + cmd_last_arg)
    cmd        = (decl_cmd | msg_cmd | pythonStyleComment)
    
    content = raw_input('enter: ')
    try:
        a = cmd.parseString(content)
    except ParseException, pe:
        print pe
    else:
        cmd_text =  a[0][0]
        if cmd_text == 'msg':
            src = a[0][1]
            dst = a[0][2]
            txt = a[0][3]
            print 'message from ', src, ' to ', dst, ' with text = ', txt



These seems to work.



Now for some questions -



1. Is this the best approach for parsing this language?

2. On the result set - is it typical to parse out the results using array indexing?

3. The arguments are separated by ',' - what if I want to allow ',' within an argument? Is there a good way to escape these kind of separators?



So far pyparsing seems a great way to define this parsing. I'm very excited to find a module to powerful, yet easy to use.



Thanks.

#### 2008-06-06 13:00:33 - skinny_00
Did I create this post in the right place? I just noticed this was posted under 'HowToUsePyparsing'.
#### 2008-08-09 08:35:18 - ptmcg
1. Is this the best approach for parsing this language?

    Yes, I prefer this approach.  Some people try to write a single expression for parsing any command, and then do a lot of work in their post-parsing code to figure out just which command was given.  I prefer defining command-specific expressions - see further on this in the next question.

2. On the result set - is it typical to parse out the results using array indexing?

    I can't say what's typical, but I encourage people who are writing complex parsers *not* to use array indexing.  Indexing gets especially tricky when you define optional elements within a command.  Instead, I recommend using results names.  Since version 1.4.7 of pyparsing, the syntax for defining results names has gotten very simple.  Here is how I would add results names to your grammar:



    msg_cmd = Group(MSG +  COLON + cmd_arg + COMMA + cmd_arg + COMMA + cmd_last_arg)
    msg_cmd = Group(MSG +  COLON + cmd_arg('sender') + COMMA + cmd_arg('receiver') + COMMA + 
        cmd_last_arg('msgbody'))



Now I can access the individual fields by name instead of having to pick them out by index.



    print a.sender, a.receiver



By defining command-specific expressions, you can also specify very useful results names (rather than just generic 'arg1', 'arg2', etc. - which really is no better than indexing, after all).



3. The arguments are separated by ',' - what if I want to allow ',' within an argument? Is there a good way to escape these kind of separators?

    Are you referring to a quoted string containing a ','?  The way to do this would be define an argument as:



    parm_value = quotedString | Regex(r'[^,()=]+')



If you want to do more regex-like escaping, say treat ',,' as an escaped comma, then you could modify your regex:



    parm_value = Regex(r'([^,()=]|,,)+')
    parm_value.setParseAction(lambda t:t[0].replace(',,',','))



I hope these comments weren't too late for you - welcome to pyparsing!



-- Paul

---
## 2008-06-09 11:57:40 - MatTourne - bug? ignore and maximum recursion excedeed
Here is my grammar :





    _COMMENT_START = Suppress('--')
    
    _METAINFO_TAG = CaselessLiteral('meta:').suppress()
    
    _COMMENT_LINE = (_COMMENT_START
                     + ~(METAINFO_TAG)
                     + restOfLine)
    
    _SQLS.ignore(_COMMENT_LINE)



It was working perfectly well so far, but I've got something like that on the input :



    -------------------------------------------------------------------------------------------------
    -- test
    -------------------------------------------------------------------------------------------------



It crashes with a maximum recursion depth exceeded error

Here is what I've got in the debug output:



    ('Matched', '_COMMENT_LINE', '->', ['---'])
    ('Matched', '_COMMENT_LINE', '->', ['-----'])
    ('Matched', '_COMMENT_LINE', '->', ['-------'])
    ('Matched', '_COMMENT_LINE', '->', ['---------'])
    ('Matched', '_COMMENT_LINE', '->', ['-----------'])
    ('Matched', '_COMMENT_LINE', '->', ['-------------'])
    ('Matched', '_COMMENT_LINE', '->', ['---------------'])
    ('Matched', '_COMMENT_LINE', '->', ['-----------------'])
    ('Matched', '_COMMENT_LINE', '->', ['-------------------'])
    [...]
    ('Matched', '_COMMENT_LINE', '->', [' test'])



Is it a bug, or a problem on my side? It looks like restOfLine hasn't worked.

#### 2008-06-09 16:59:34 - ptmcg
Did you define your own version of restOfLine?  I tried to test your program, but got no error:





    from pyparsing import *
    
    _COMMENT_START = Suppress('--')
    _METAINFO_TAG = CaselessLiteral('meta:').suppress()
    _COMMENT_LINE = (_COMMENT_START
                     + ~(_METAINFO_TAG)
                     + restOfLine)
    
    #~ _SQLS.ignore(_COMMENT_LINE)
    text = '''\
    -------------------------------------------------------------------------------------------------
    -- test
    -------------------------------------------------------------------------------------------------'''
    
    print _COMMENT_LINE.searchString(text)



prints:





    [['-----------------------------------------------------------------------------------------------'], 
     [' test'], 
     ['-----------------------------------------------------------------------------------------------']]
    


#### 2008-06-09 17:52:28 - MatTourne
Try this one, consider that instead of _ERROR, there is (_SQL | _ERROR)





    from pyparsing import *
    
    FLAGS = flags.FLAGS
    
    _COMMENT_START = Suppress('--')
    _METAINFO_TAG = CaselessLiteral('meta:').suppress()
    _COMMENT_LINE = (_COMMENT_START
                     + ~(_METAINFO_TAG)
                     + restOfLine).setName('COMMENT')
    _COMMENT_LINE.setDebug(True)
    
    _METAINFOS = Group(_COMMENT_START + _METAINFO_TAG
                       + delimitedList(Word(alphas + '-'))
                       ).setResultsName('metainfos')
    
    _LINE_DELIMITER = Suppress(';')
    
    _ERROR = Group(
         SkipTo(_LINE_DELIMITER | StringEnd(),
                include=True)
         ).setResultsName('parse_error')
    
    
    _QUERY = Group(Optional(_METAINFOS) +
                   (_ERROR)
                   ).setResultsName('query')
    
    _SQLS = Group(OneOrMore(_QUERY)
                      + StringEnd()
                  ).setResultsName('queries')
    
    _SQLS.ignore(_COMMENT_LINE)
    text = '''\
    -------------------------------------------------------------------------------------------------
    -- test
    --meta: foo
    -------------------------------------------------------------------------------------------------'''
    
    def main(argv):
      print _SQLS.parseString(text).asXML()


#### 2008-06-09 20:09:38 - ptmcg
Ah! Okay, it turns out that this happens because the definition of _COMMENT_LINE includes some expression that is also part of the overall grammar.  So inside of _COMMENT_LINE, there is a sub-element that is... trying to ignore _COMMENT_LINE!  Try changing _COMMENT_LINE to:



    _COMMENT_LINE = (_COMMENT_START
                     + ~(_METAINFO_TAG.copy())
                     + restOfLine).setName('COMMENT')



This will get rid of the recursive calls that you are seeing.



I'm not sure what I'll do about this, though.  While this initially strikes me as an unusual case (defining a comment including a sub-expression from the non-comment grammar), your comment also includes a generic expression like restOfLine, which could easily be part of an element in the actual grammar.



For right now, I think all I can do is document this restriction.  Thanks for submitting a good code example.



-- Paul

---
## 2008-06-09 22:44:40 - alxtoth - question on nestedExpr 
Hi,



Trying to run the nestedExpr example from .



Wondering about the importance of whitespace. Pyparsing v 1.5.0 sometimes matches *C together:



    >>> print( mathExpr.parseString( '(( ax + by) *C) *(Z * (E*F) + D))') )
    [[['ax', '+', 'by'], '*C']]
    
    >>> print( mathExpr.parseString( '( ( ax + by)* C) * (Z * (E * F) + D)') )
    [[['ax', '+', 'by'], '*', 'C']]



And it does not seem to match the (Z * (E * F) + D), regardless of spacing



Any ideas?

-Alex

#### 2008-06-09 22:58:56 - ptmcg
nestedExpr *only* matches a single nested expression, by default one in ()'s.  If you do not specify an expression for the content you expect within the nesting, then nestedExpr will just return space-separated items, so '* C' will return the tokens '*','C', while '*C' will return the token '*C'.  nestedExpr really has no knowledge of what '*' means, it only tries to do the grouping/nesting based on the opening and closing nesting delimiters it finds.



In the example, mathExpr will only parse a single ()-nested expression; that is, a single expression that is *entirely* enclosed in parens.  To parse the expression you posted, this version might work better:



    mathExprInParens = nestedExpr()
    mathOperator = oneOf('+ - * / ^ & |')
    mathExpr = mathExprInParens + ZeroOrMore( mathOperator + mathExprInParens )
    print( mathExpr.parseString( '( ( ax + by)* C) * (Z * (E * F) + D)') )



But this would still not match '2 + 2', as there are no parenthesized terms here at all.  



If you really want to match arithmetic expressions with possible nested parentheses, then I suggest you look at operatorPrecedence, or the more explicit form defined in the fourFn.py example.



-- Paul
#### 2008-06-09 23:24:30 - alxtoth
Many Thanks!

---
## 2008-06-10 12:52:56 - maxstylus - skipping lines that don't fit grammar
Hello, 

I am attempting to parse a log file. I have the grammar defined for my most typical (and data rich) lines in the log file. However, I'm beginning to identify different types of lines with words such as: 

    warnings
    out of ammo
    no effect

My parser currently works great until I hit one of these lines - then it fails because it's looking for a particular token/grammar. I would like to be able to parse my file using my defined grammar, skipping the lines which contain the words/statements above. Can anyone give me some pointers on how to do that?

#### 2008-06-10 16:12:01 - ptmcg
Are you parsing your file line by line?  If so, then just add a try-except exception handler to your code, like this:



    for line in input_lines:
        try:
            results = grammar.parseString(line)
        except ParseException,pe:
            print 'Parsing exception:', pe.msg
            print line
            print ' '*(pe.col-1) + '^'



You can also try using scanString or searchString to skim through the input text looking for matches.  You may have to adjust your grammar a bit to avoid false matches starting in the middle of a line, but otherwise, these alternatives to parseString are designed specifically for those cases where a grammar only works with part of an input text.  (Since scanString is a generator, it will start returning results right away - searchString is a wrapper around scanString that returns all of the grammar matches in a single Python list, after processing the entire input text.)



Welcome to Pyparsing!  Post back if you need more help or ideas,



-- Paul

---
## 2008-06-13 20:29:48 - eleybourn - I need to match a word unless it is a keyword
I'll keep this simple. 



I'm trying to write a completely write a parser for SQL2003. One of the issues I have is specifing names.



The string:



    UPDATE foo SET bar=1;



The parser



    name = Word(alphanums + alphas8bit + '_')
    alias = Optional( Optional( CaselessKeyword('as') ) + name )
    tablename = name + Optional( alias )
    setClause = Group(delimitedList(Group(name + '=' + expression)))
    
    statement      << ( CaselessKeyword('update') + tablename + CaselessKeyword('set') + setClause +';') )



I need to have the alias syntax in there. But the 'set' keyword is being picked up as the alias. What I need is for something like.

    name = Word(alphanums + alphas8bit + '_') EXCEPT (CaselessKeyword('set')



Is something like this possible.

#### 2008-06-13 22:08:17 - ptmcg
Try this:



    SET = CaselessKeyword('set')
    identLeadChars = alphas + alphas8bit + '_'
    identBodyChars = identLeadChars + nums
    name = ~SET + Word(identLeadChars, identBodyChars)

(that is, only accept a word as a name if you first determine that it is NOT the keyword 'set').



I'm finding that more and more I'll predefine many of my grammar keywords using something like:



    SELECT,INSERT,UPDATE,DELETE,WHERE,AS,SET = map(CaselessKeyword,
        'select insert update delete where as set'.split())

and then just reference the all-caps expressions in the rest of the grammar. It just makes the grammar read more cleanly to me, without all the `CaselessKeyword`s breaking up the flow.  I'll also be more likely to reuse expressions this way, instead of having multiple `CaselessKeyword("as")` expressions cropping up here and there in my grammar.



Similarly, depending on the grammar, I'll define punctuation like:



    SEMI,COLON,EQ,DOT = map(Literal,';:=.')



Also, note my use of the 2-argument constructor of `Word`, specifying the allowable starting characters and then the allowable body characters - identifiers are not allowed to start with numbers.



I'm not sure why you have defined `statement` as a `Forward`, unless you are thinking of eventually supporting nested SELECT statements.  With what you have so far, though, there is no need to define `statement` as a `Forward` and then assign its contents using the << operator.



Are you working from a BNF that begins with `statement` and then defines finer and finer expressions?  This is often why people will define lots of `Forward`s in their pyparsing grammar, and then define them later.  I suggest you just work bottom up, start with the smallest tokens and building blocks, and then build up to `statement` - save the `Forward` definitions for those expressions that are truly recursive.



If you don't have a BNF for this project, I strongly encourage you to get one or write one out - it will really help you structure your grammar, and better lay out where you will need to define recursive elements and common sub-expressions, as well as see where you need negative lookaheads like `~SET` in this above example.



Lastly, do you have plans beyond just parsing these SQL statements?  If you just want to extract data from SQL, then you should map out where you want to add results names so that extracting the data during the post-parsing phase is not too tortuous.  Or you might consider where parse actions will be useful for doing parse-time conversions, validation, standards conformance, compiling, or whatever.



Good luck with this project, and post back if you need further help, or have a knock-em-dead SQL parser you want to share with the world!



-- Paul

---
## 2008-06-26 20:04:41 - akineko - SkipTo()'s unexpected behaviour
Hello,

I'm very new to pyparsing (several day old now).

It is quite powerful tool comparing to conventional tools, like yacc (bison). It is more Pythonic ;-).

Anyway, I'm having a problem with SkipTo().

My understanding is that SkipTo() eats any tokens until it encounters the token specified.

I tried to grab lines using ';' as a delimitor.

    v = StartLine() + SkipTo(Literal(';')) + SkipTo(LineEnd(), include=True)

This grammar should eat multiple lines until line with ';'.

It works fine with the following case:

    v.parseString('aaa ; bbb\n')
    
    => (['aaa ', (['; bbb', '\n'], {})], {})  

But it doesn't work with the following case:

    v.parseString(' aaa ; bbb\n')
    
    => pyparsing.ParseException: Expected start of line (at char 1)

In other words, my grammar works unless the line starts with whitespaces.

Is this behaviour something pyparsing intended?

What is the right way to deal with such possible inputs?

Any suggestions would be highly appreciated.

Thanks!  
Aki Niimura

#### 2008-07-01 01:21:10 - ptmcg
Aki -



Pyparsing's default skipping of whitespace is tripping you up here.  LineStart (not StartLine), like most other pyparsing expressions, skips over whitespace before attempting to match its grammar definition.  So before evaluating whether at the start of a line, LineStart advances past any whitespace, and then looks to see if 'aaa' is at the start of the line - which it isn't.



To suppress this behavior, change your grammar to:



    v = LineStart().leaveWhitespace() + 
            SkipTo(Literal(';')) + 
            SkipTo(LineEnd(), include=True)



This will now give you a successful parse, returning:



    ['aaa ', ['; bbb', '\n']]



However, note that the leading spaces have been stripped before 'aaa '.  Why?  Because SkipTo is also skipping over whitespace.  To truly get the text from the start of the line to the first semicolon, you must also suppress whitespace-skipping on the first SkipTo expression, as in:



    v = LineStart().leaveWhitespace() + 
            SkipTo(Literal(';')).leaveWhitespace() + 
            SkipTo(LineEnd(), include=True)



This now returns the tokens:



    [' aaa ', ['; bbb', '\n']]



I don't know which form is more appropriate for the program you are writing, but this should get you further along.



Welcome to the world of pyparsing!

-- Paul
#### 2008-07-01 09:52:56 - akineko
Hello Paul,



Thank you for your clear explanation on why my grammar failed if the line starts with white spaces.

Although I agree that it is a right design that all tokens eat preceeding white spaces before attempting to match, it is counter-intuitive for LineStart() to have such.

I cannot think of any situations where such skipping whitespace behaviour is necessary for LineStart().

Of course, that is not the case for LineEnd(), for which having such behaviour is useful and intuitive.



I can understand that you want to have consistency over the langauage architecture. But I feel intuitiveness is more important to get more people to become fluent in pyparsing.



Regardless of this, pyparsing is an amazing tool. That is no question about it.



Thanks,

Aki-

---
## 2008-06-27 09:37:53 - akineko - process ifdef ... endif
Hello,



I'm trying to process ifdef ... endif using pyparsing.



The ifdef syntax are:

    #ifdef macro st_A #else st_B #endif   (Case-1)

or

    #ifdef macro st_A #endif   (Case-2)



I tried the following pyparsing grammar:

    Keyword('#ifdef') + Word(alphas, alphanums+'_') + Optional(SkipTo(Keyword('#else'), include=True)) + SkipTo(Keyword('#endif'), include=True)



This grammar didn't work if it is applied to an input that contains a case that (Case-1) followed by (Case-2).

The reason why it didn't work was SkipTo() will eat the longest possible match even it contains #else.



The only work around I came up with was do (Case-1) first then do (Case-2), which is very crude and would like to avoid.



What is the right way to do ifdef parsing using pyparsing?



Any suggestions will be highly appreciated.



Thank you!  
Aki Niimura

#### 2008-06-27 09:44:42 - akineko
Hi,



I was wrong.

My crude workaround, do (Case-1) then (Case-2), didn't work.

It eats incorrectly (Case-2) while applying (Case-1) grammar.



Now, I really need a help.



Aki-
#### 2008-07-01 02:04:44 - ptmcg
Aki -



SkipTo is by its nature a greedy expression, and will read past many lines of text searching for a match.  If you are combining Optional and SkipTo, I think in general this is a recipe for trouble.



Still, I think we can address your immediate problem.  The key is to somehow have SkipTo accept only those strings that do not themselves contain an '#endif', because this would tell us we were in an '#ifdef' with no '#else' clause.  The most direct way to do this with pyparsing is to use a parse action.  See the sample code below:



    from pyparsing import *
    
    case1 = '#ifdef macro st_A #else st_B #endif'
    case2 = '#ifdef macro st_A #endif'
    case3 = '''#ifdef macro st_A #endif
                #ifdef macro st_A #else st_B #endif'''
    
    # original grammar - recreate unwanted behavior
    ifdef = Keyword('#ifdef') + Word(alphas, alphanums+'_') + \
        Optional(SkipTo(Keyword('#else'), include=True)) + \
        SkipTo(Keyword('#endif'), include=True)
    
    for c in (case1,case2,case3):
        print ifdef.searchString(c)
    
    # modified grammar - attach parse action to SkipTo expression to make 
    # sure we don't include an #endif in what we have skipped over
    matchElse = SkipTo(Keyword('#else'), include=True)
    def notContainsEndif(s,l,t):
        if '#endif' in t[0][0]:
            raise ParseException(s,l,'contains #endif')
    matchElse.setParseAction( notContainsEndif )
    
    ifdef = Keyword('#ifdef') + Word(alphas, alphanums+'_') + \
        Optional(matchElse) + \
        SkipTo(Keyword('#endif'), include=True)
    
    for c in (case1,case2,case3):
        print ifdef.searchString(c)



The first tests illustrate the problem as you described, printing out:



    [['#ifdef', 'macro', ['st_A ', '#else'], ['st_B ', '#endif']]]
    [['#ifdef', 'macro', ['st_A ', '#endif']]]
    [['#ifdef', 'macro', ['st_A #endif\n            #ifdef macro st_A ', '#else'],
     ['st_B ', '#endif']]]



The modified grammar does a better job, properly detecting that the first macro expression does not contain an #else clause, and reports that it finds 2 macro expressions in case 3:



    [['#ifdef', 'macro', ['st_A ', '#else'], ['st_B ', '#endif']]]
    [['#ifdef', 'macro', ['st_A ', '#endif']]]
    [['#ifdef', 'macro', ['st_A ', '#endif']], ['#ifdef', 'macro', ['st_A ', '#else'],
     ['st_B ', '#endif']]]





The trickiest part in writing this parse action was in teasing apart the tokens that would be passed to it, and determining that it was actually a nested list of tokens, in which I wanted to test the `tokens[0][0]` element.  You can add a print statement at the beginning of a parse action to display the tokens sent in, or you can use the @traceParseAction decorator provided with pyparsing.  To use @traceParseAction, just insert it before the definition of the parse action to be traced (like any ordinary Python decorator):



    @traceParseAction
    def notContainsEndif(s,l,t):
        if '#endif' in t[0][0]:
            raise ParseException(s,l,'contains #endif')



@traceParseAction will show the current line of text, the parse location, and the tokens passed to the parse action.  On exiting the parse action, @traceParseAction will show either the returned value or any exception raised in the parse action.  Here is the output shown by adding @traceParseAction to the code:



    >>entering notContainsEndif(line: '#ifdef macro st_A #else st_B #endif', 13, 
      [['st_A ', '#else']])
    <<leaving notContainsEndif (ret: None)
    [['#ifdef', 'macro', ['st_A ', '#else'], ['st_B ', '#endif']]]
    [['#ifdef', 'macro', ['st_A ', '#endif']]]
    >>entering notContainsEndif(line: '#ifdef macro st_A #endif', 13, 
      [['st_A #endif\n            #ifdef macro st_A ', '#else']])
    <<leaving notContainsEndif (exception: contains #endif (at char 13), 
      (line:1, col:14))
    >>entering notContainsEndif(line: '            #ifdef macro st_A #else st_B 
      #endif', 50, [['st_A ', '#else']])
    <<leaving notContainsEndif (ret: None)
    [['#ifdef', 'macro', ['st_A ', '#endif']], ['#ifdef', 'macro', ['st_A ', '#else'],
     ['st_B ', '#endif']]]



You can see that our SkipTo expression is sending a list containing a list with 2 strings, the text that was skipped, and the <em>skipped-to</em> '#else'.  That is why we reference the [0][0] element of the tokens argument passed into the parse action.



-- Paul
#### 2008-07-01 10:06:12 - akineko
Hello Paul,



Thank you for your reply showing how to workaround my problem.



As you pointed out, the problem was due to the greedness of SkipTo(). All grammar tool I know of always grab the longest match as a winner. But quite often that have caused a problem.



If SkipTo() can be instructed to be more modest, we can solve the problem elegantly, IMHO.

If SkipTo() returns the shortest match (rather than the longest), I think my original crude grammar works as I intended.



What do you think of such feature?

I'm not suggesting that pyparsing should be changed.

I'm wondering if such proposal does make sense.



Any comments?



BTW, I'm updating my grammar but having a problem as my real pyparsing program has much more complicated syntax. But I think I can fix the problem thanks to your replies. 



Aki-
#### 2008-07-01 15:05:44 - ptmcg
I have just prototyped adding an argument to the constructor for SkipTo, named 'failOn'.  If the string or pyparsing expression passed as the failOn argument is found within the skipped text, the SkipTo fails.



In your example, the original grammar was:



    ifdef = Keyword('#ifdef') + Word(alphas, alphanums+'_') + \
        Optional(SkipTo(Keyword('#else'), include=True)) + \
        SkipTo(Keyword('#endif'), include=True)



Instead of defining the parse action to do testing for inclusion of '#endif' in the skipped text, you could just enhance the SkipTo expression to define a failOn argument:



    ifdef = Keyword('#ifdef') + Word(alphas, alphanums+'_') + \
        Optional(SkipTo(Keyword('#else'), include=True, failOn='#endif')) + \
        SkipTo(Keyword('#endif'), include=True)



failOn is not limited to a string literal, it can be any pyparsing expression.  If you wanted to fail on the presence of any of several strings, define them as:



    ifdef = Keyword('#ifdef') + Word(alphas, alphanums+'_') + \
        Optional(SkipTo(Keyword('#else'), include=True, failOn=oneOf('#this #that #other'))) + \
        SkipTo(Keyword('#endif'), include=True)



I think this will help address the SkipTo 'greediness' problem.



-- Paul

(I'll check in this version to the SF SVN repository later today or this evening.)
#### 2008-07-01 19:50:55 - akineko
Hello Paul,



Thank you for having prototyped a new way to control the greedness of SkipTo().



I think it is more intuitive than setting an action to SkipTo (even the goal is the same).



Although my problem was caused by the greedness of SkipTo, the true cause of my problem was that I didn't do full grammar analysis.



Having said that, in many cases, you don't care about full grammar and don't want to spend time to construct full grammar.

Therefore, SkipTo() is a quite convenient construct to use.



I personally envisioned a different language enhancement.

I thought providing another construct that returns the shortest match may be more intuitive that cutting off the matching.

I think this is more like A | B versus A ^ B.



failOn is a great tool to fix the problem once the programmer realizes what the problem is. In my view, providing another construct provides programmers a means to limit the parsing range explicitly.



As I'm not an expert on language analysis, probably you have a better idea on this. So, I will leave the final say to you. 



Aki-
#### 2008-07-01 20:20:58 - ptmcg
When I said that SkipTo was a greedy expression, I didn't mean that it returns the longest match.  SkipTo('X') will stop at the very next incidence of 'X'.  The problem you were having was that SkipTo was reading past the expression after it in the grammar, and reading into the middle of a later expression.  That is, for this grammar:



    'A' + Optional(SkipTo('B') + 'B') + SkipTo('C') + 'C'



if you parse 'A123CA456B789C', the Optional(SkipTo('B')+'B') will match '123CA456B', even though you would want it to recognize that in 'A123C' there *is* no B term, while in the following 'A456B789C' there is a valid B term.



So by 'greedy' I meant that SkipTo will read until it finds a match or reaches the end of the input string and raises a ParseException, without any kind of lookahead, backtracking, or checking for surrounding expressions in the grammar.  SkipTo *always* returns the shortest match, but Optional(SkipTo(xxx)) may skip further than you want.  This is why failOn helps, by providing information to SkipTo to tell it when it has gone too far.



-- Paul
#### 2008-07-01 20:35:34 - akineko
Hello Paul,

Thank you for correcting my mis-understanding.  
Somehow, I thought SkipTo() would eat the longest match if multiple conditions are given.

ex.

    s = 'aaa ; bbb #'
    v = SkipTo(Literal(';') | Literal('#'))
    v.parseString(s)

I thought the above eat until '#' but as you said it only ate till ';'.

My bad. Sorry.

Aki-

---
## 2008-07-01 13:15:49 - akineko - pyparsing aid tool
Hello Paul,



I would like to thank you for helping me despite your busy schedule.

Now, my pyparsing program is working as I intended.

I was originally thinking of using flex/bison until I found pyparsing. I would say 10-times productivity gain against flex/bison.

Speed is probably 10-times slower but that is not the issue for my case ;-)



It is quite common to see surprises when you are working on syntax and grammar rules. That is probably true for most pyparser users as most python programmers are not grammar rule experts.



I thought it would be very helpful if a GUI tool is provided with which people can try out his/her grammar with test texts interactively and GUI tool shows which portion of the input is consumed by which grammar (coloring) (showing the extent it has consumed).



Of course, creating such tool is not an easy thing to do.

But if such tool becomes available, people can figure out his/her mistakes by oneself. Otherwise, you need to keep answering many novice questions, like mine.



Again, thank you very much for your help and have a nice day.



Aki-

#### 2008-07-01 14:21:29 - ptmcg
Aki -



Thanks for the (flattering!) update, I'm glad you have things working better.



I've also thought about some sort of pyparsing workbench GUI, but as you say, such a tool could become a significant project.  Perhaps this would be a good Google Summer of Code or Google Highly Open Participation project proposal.



-- Paul

---
## 2008-07-17 10:58:53 - agustingianni - Problems with recursive grammars
Hi, I am writting a parser to parse GDB/MI messages. So far it has been easy, but now i face a problem i can't solve.



The grammar is available here: 





    const = c_string
    result_class =     Literal('done') ^ Literal('running') ^ Literal('connected') ^ Literal('error') ^ Literal('exit')
    async_class = Literal('stopped')

    variable = quotedString ^ c_string    # works

    value = Forward()
    result = Forward()

    tuple_ = Literal('{}') ^ '{' + result + ZeroOrMore(',' + result) + '}'
    list_ =  Literal('[]') ^ '[' + value + ZeroOrMore(',' + value) + ']' ^ ('[' + result + ZeroOrMore( ',' + result ) + ']')

    value << const ^ tuple_ ^ list_
    result << variable + '=' + value

        # Example parse string.
    print result.parseString('''bkpt={number='1',type='breakpoint',disp='keep',enabled='y',addr='0x08048382',at='<main+14>',times='0'}''')
    



#### 2008-07-17 13:23:53 - ptmcg
Welcome to pyparsing, and congrats on tackling a recursive grammar!



Your first problem has to do with a poor design choice on my part when I selected '<<' to be the operator for inserting a Forward's expression content.  In your statement:



    value << const ^ tuple_ ^ list_



You clearly want value to take on the alternative of a const OR tuple_ OR list_.  Unfortunately, the << operator has a higher precedence than '^', so your expression becomes:



    (value << const) ^ tuple_ ^ list_



The fix is to enclose your alternatives in ()'s:



    value << ( const ^ tuple_ ^ list_ )



And if you do the same for list_, then things will start to look better.



(In version 1.4.11, the '<<' operator was changed to return None, so that at compile time, you get the following warning:



    750.py:18: SyntaxWarning: Cannot combine element of type <type 'NoneType'> with ParserElement
      value << const ^ tuple_ ^ list_



Some other bits:



1. The pattern for a comma-delimited list of entries:



    entrylist = entry + ZeroOrMore(',' + entry)

is *very* common.  Look into using the pyparsing helper function, `delimitedList` instead:



    entrylist = delimitedList(entry)



`delimitedList` also allows for other delimiter characters, or even delimited expressions.



2. Since you are defining a recursively hierarchical grammar, I suggest you add grouping to the parsed results, using the Group class:



    tuple_ = Group('{' + Optional(delimitedList(result)) + '}')
    list_ = Group('[' + Optional(delimitedList(result^value)) + ']')
    result << Group(variable + '=' + value)



This way your returned tokens will be nested in sublists.  You can see this if you use asList() method of ParseResults, and use the pprint module to print out the hierarchical list.  With these groups, your data ends up looking like:



    [['bkpt',
      '=',
      ['{',
       ['number', '=', ''1''],
       ['type', '=', ''breakpoint''],
       ['disp', '=', ''keep''],
       ['enabled', '=', ''y''],
       ['addr', '=', ''0x08048382''],
       ['at', '=', ''<main+14>''],
       ['times', '=', ''0''],
       '}']]]



Write back as you dig deeper into this grammar!



-- Paul
#### 2008-07-17 14:55:33 - agustingianni
Hello! Thanks a lot for the help.



This library it's totally awesome.



I'll post the entire parser for GDB/MI commands once I finish it.



And again, thank you for your help.



PS: This is how the code looks with the fixes. I had to replace the

definition of const because it seems that the 'offical' grammar

has some kind of mistake because gdb returns the values like this

'<main>' and if you do not take into account the '' and exception is raised.





    variable = quotedString ^ c_string    # works
    const = variable # HACK it was defined to be a c_string 

    value = Forward()
    result = Forward()

    tuple_ = Group('{' + Optional(delimitedList(result)) + '}')
    list_ = Group('[' + Optional(delimitedList(result^value)) + ']')

    value << (const ^ tuple_ ^ list_)
    result << Group(variable + '=' + value)

    print result.parseString('''bkpt={number='1',type='breakpoint',disp='keep',enabled='y',addr='0x08048382',at='<main+14>',times='0'}''')



---
## 2008-07-24 06:36:26 - jkozak - space complexity
I've developed a grammar for an obscure scripting language which works quite well for me.  It's quite simple, as I'm only interested in the gross structure (so for example I don't bother with operator precedence). 

Sadly, I've come across a case which breaks it:  an automatic tool generates 8 megabyte files consisting of a single fncall with rather a lot of numeric arguments.  Is there anything obviously pessimal I'm doing?

thanks for pyparsing, btw, it really is a pleasure to use.  

    ident0 = Word(alphas+'_',alphanums+'_')
    ident  = ident0+ZeroOrMore('.'+ident0)
    string = dblQuotedString
    stype  = oneOf('string float int image')

    expr   = Forward()
    fncall = ident+'('+Optional(expr+ZeroOrMore(','+Optional(stype+ident0+'=')+expr))+')'
    lexpr  = ident                  # +++
    assign = lexpr+'='+expr
    inum   = Word('+-'+nums,nums)

    fnum_a = inum+Optional('.'+Optional(Word(nums)))+Optional(CaselessLiteral('f')|(CaselessLiteral('E')+inum))
    fnum_b = Optional('+-')+'.'+Word(nums)+Optional(CaselessLiteral('E')+inum)
    fnum   = Combine(fnum_a|fnum_b)

    cvec   = '['+Optional(fnum+ZeroOrMore(','+fnum))+']'
    shkqnt = (fnum|cvec)+Optional('@'+fnum)
    const  = shkqnt|fnum
    shunt  = ident+ZeroOrMore('@@'+expr)
    atom   = fncall|const|shunt|'('+expr+')'

    mterm  = Forward()
    mterm << atom+ZeroOrMore(Word('*/',max=1)+atom)

    aterm  = Forward()
    aterm << ((Optional('-')+mterm+ZeroOrMore(Word('+-',max=1)+mterm))|('('+aterm+')'))

    relop  = oneOf('== >= <= < > !=')
    lterm  = Forward()
    rterm  = (aterm+ZeroOrMore(relop+aterm)) | ('('+lterm+')')

    logop  = oneOf('&& ||')
    lterm << (rterm+ZeroOrMore(logop+rterm)) 

    cond   = Forward()
    cond  << (lterm+'?'+expr+':'+expr)

    expr  << (cond|lterm|aterm|string)

    stmt   = Group(Optional(assign|fncall))+Literal(';').suppress()

    shake  = OneOrMore(stmt)+StringEnd()
    shake.ignore(dblSlashComment)


#### 2008-07-24 06:37:53 - jkozak
[sorry, pressed 'post' too early] Attempting to parse the 8 meg file mentioned soaks up 8 gigabytes of main memory then fails.
#### 2008-07-24 08:44:58 - ptmcg
I'm glad to hear pyparsing is making your lot easier!



When tuning up parsers like this, one of the lowest of low-hanging fruit is usually to change the definition of the floating point literal from some composition of pyparsing bits into a single Regex.  I know, this seems to fly in the face of pyparsing's whole philosophy, but there are times when practicality trumps purity.  So I drummed up this expression for fnum:



    fnum   = Regex(r'[+\-]?(\d+(\.\d*([fF]|[eE]\d+)?)?|\.\d+([eE]\d+)?)')



(Note also that there is a slight bug in inum - it will accept a single '+' or '-' character as a valid integer.)

You also have a plethora of parentheticals, more than you really need.  It is not necessary to repeat the ()'s of lower level terms at each level of your operator hierarchy.  For that matter, it is also not necessary for each level (atom, mterm, aterm, etc.) to be a Forward.  Only expr needs to be a Forward.

Lastly, look into doing some Group expressions, and maybe some results names (as I have named the 'args' sub element in the fncall expression).  Currently, this just parses the whole statement as a flat mess of token strings, which must be murder to actually work through at post-parse time.

The following is your original grammar, with selected lines commented out, and my suggestions inserted.  I've not done much testing, but I hope that reducing some of the complexity in the grammar furthers your cause.  (You can also try enabling packrat parsing, by calling ParserElement.enablePackrat().)

-- Paul


    ident0 = Word(alphas+'_',alphanums+'_')
    #~ ident  = ident0+ZeroOrMore('.'+ident0)
    ident  = delimitedList(ident0,'.',combine=True)

    string = dblQuotedString

    stype  = oneOf('string float int image')

    expr   = Forward()

    #~ fncall = ident+'('+Optional(expr+ZeroOrMore(','+Optional(stype+ident0+'=')+expr))+')'
    arg = Group(stype + ident0 + '=' + expr) | expr
    fncall = ident+'('+Group(Optional(delimitedList(arg)))('args')+')'

    lexpr  = ident                  # +++

    assign = lexpr+'='+expr

    inum   = Word('+-'+nums,nums)

    #~ fnum_a = inum+Optional('.'+Optional(Word(nums)))+Optional(CaselessLiteral('f')|(CaselessLiteral('E')+inum))
    #~ fnum_b = Optional('+-')+'.'+Word(nums)+Optional(CaselessLiteral('E')+inum)
    #~ fnum   = Combine(fnum_a|fnum_b)
    fnum   = Regex(r'[+\-]?(\d+(\.\d*([fF]|[eE]\d+)?)?|\.\d+([eE]\d+)?)')

    #~ cvec   = '['+Optional(fnum+ZeroOrMore(','+fnum))+']'
    cvec   = '['+Optional(delimitedList(fnum))+']'
    shkqnt = (fnum|cvec)+Optional('@'+fnum)
    const  = shkqnt|fnum

    shunt  = ident+ZeroOrMore('@@'+expr)

    atom   = fncall|const|shunt|'('+expr+')'
    # or possibly this, to preserve the nesting levels in the expression?
    #atom   = fncall|const|shunt|Group('('+expr+')')

    #~ mterm  = Forward()
    #~ mterm << atom+ZeroOrMore(Word('*/',max=1)+atom)
    mterm  = atom+ZeroOrMore(Word('*/',max=1)+atom)

    #~ aterm  = Forward()
    #~ aterm << ((Optional('-')+mterm+ZeroOrMore(Word('+-',max=1)+mterm))|('('+aterm+')'))
    aterm  = (Optional('-')+mterm+ZeroOrMore(Word('+-',max=1)+mterm))

    relop  = oneOf('== >= <= < > !=')

    #~ lterm  = Forward()

    #~ rterm  = (aterm+ZeroOrMore(relop+aterm)) | ('('+lterm+')')
    rterm  = (aterm+ZeroOrMore(relop+aterm))

    logop  = oneOf('&& ||')
    #~ lterm << (rterm+ZeroOrMore(logop+rterm)) 
    lterm  = (rterm+ZeroOrMore(logop+rterm)) 

    #~ cond   = Forward()
    #~ cond  << (lterm+'?'+expr+':'+expr)
    cond   = (lterm+'?'+expr+':'+expr)

    expr  << (cond|lterm|aterm|string)

    stmt   = Group(Optional(assign|fncall))+Literal(';').suppress()

    shake  = OneOrMore(stmt)+StringEnd()
    shake.ignore(dblSlashComment)


#### 2008-07-24 08:48:20 - ptmcg
Oh, and I forgot to mention.  This is not really any performance saver or optimization, but I really find that using 



    delimitedList(expr)

is more readable than the explicit/verbose



    expr + ZeroOrMore(Suppress(',') + expr)

especially if it crops up more than once or twice.



But this is more a matter of personal taste - YMMV.



-- Paul
#### 2008-07-27 02:26:58 - jkozak
Thanks for the prompt and helpful replies.



I've looked at this a bit more, and it's an issue with packrat, insofar as it doesn't happen with packrat disabled.  To experiment I hacked this code in (against pyparsing 1.4.11):





    _cacheFull = False
    def _myParseCache( self, instring, loc, doActions=True, callPreParse=True ):
        global _cacheFull
        lookup = (self,instring,loc,callPreParse,doActions)
        if lookup in ParserElement._exprArgCache:
            value = ParserElement._exprArgCache[ lookup ]
            if isinstance(value,Exception):
                if isinstance(value,ParseBaseException):
                    value.loc = loc
                raise value
            return (value[0],value[1].copy())
        else:
            try:
                value = self._parseNoCache( instring, loc, doActions, callPreParse )
                if (len(ParserElement._exprArgCache)<100000):
                    if len(ParserElement._exprArgCache)%1000==0:
                        print 'cachesize: %d'%len(ParserElement._exprArgCache)
                    ParserElement._exprArgCache[ lookup ] = (value[0],value[1].copy())
                elif not _cacheFull:
                    _cacheFull = True
                    print '** cache full'
                    ParserElement._parse = ParserElement._parseNoCache
                return value
            except ParseBaseException, pe:
                ParserElement._exprArgCache[ lookup ] = pe
                raise
    ParserElement._parseCache = _myParseCache



which seems to fix the space issue.  Interestingly, if I don't reassign ParserElement._parse when the cache is deemed full, I still get the fast storage growth, so presumably it's to do with the calls to value[1].copy?
#### 2008-07-27 10:45:41 - ptmcg
What if you just clear the cache when you reach _cacheFull?  You will lose some possible cache hits, but you will still get the packratting benefits.



-- Paul
#### 2008-07-27 11:48:38 - jkozak
thanks - that does seem like a decent compromise, barring flashier cache management.  Might something like this become a tweakable option?

---
## 2008-07-24 14:12:55 - 3rdangle - Inquiry Regarding Programatic Setting of setResultsName
I have been unable to discern any successful mechanism to set the value of 'setResultsName' other than by way of an explicit literal string between quotes. 



I am attempting to modify the JSON code such that jsonObject.setResultsName( <string equivilent to parsed jsonObject name> ).



Is this possible?



Thanks,

Andy

#### 2008-07-24 15:00:18 - ptmcg
setResultsName gets set on pyparsing expressions, long before parse time.  It defines the name to be given to the token(s) that match that expression.  At parse time, when pyparsing expressions are crunching away at the input string, building up ParseResults objects, they add names to the ParseResults if the expressions have been built with associated results names.



So when you are building your grammar, there is no way to say 'expr.setResultsName(parsedJSONobjectName)' because the parsed JSON object name does not exist yet.



BUT!!!  Have you worked with the JSON parser as it exists?  It turns out that that parser uses a magic pyparsing class called Dict.  Dict does the kind of parse time naming I think you are looking for.  Look at the test case given at the end of the parser.  The parser *dynamically assigns* results names using the object names in the input JSON string.



Does this help?  If not, please write back with a bit more description on just what it is you are trying to do.



-- Paul

---
## 2008-07-27 23:27:21 - mayapower - Read .txt file like .py file
I have a .txt file which actually is collection of some python commands. For example:



    Help='''
    Your help goes here.
    '''
    data = (100,
           200,
           300)



I would like to read this file using a method where first readline method should return complete command as a string as it is even if command is spilt into multiline. I have tested compiler module but some how I didn't able to get it. Somebody suggested to have a look on pyparsing but I never did this kind of stuff before.

#### 2008-07-28 22:24:36 - ptmcg
If you just want to execute the Python commands in this file, you can use the Python built-in exec function, and execute the body of the text file:



    filename = 'file_containing_python_commands.txt'
    exec file(filename).read()



Why do you want to parse this file?  Does it consist solely of Python assignments?  You don't give much to go on.



-- Paul
#### 2008-07-28 22:36:51 - mayapower
Well, it's a fixed set of commands and functions calls. They all work

together as a file format. I can't use execfile/exec file directly because of security reasons. I just need to know the lines and I'll process them whether they are the valid commands or not.



Hope this will help.
#### 2008-07-29 07:55:13 - ptmcg
Ok, here is what is frustrating (and it is not just you, this is really very common when less-experienced people post parsing questions).  You post some sample code and write 'I need to parse some statements like this.'  The sample code contains 2 simple assignment statements of the form '<name> = <literal string or integer>'.  Let's say I spend 15 minutes writing up a little assignment statement parser, plus another 1/2 hour to put some descriptive words around it.  Then you say 'Oh, wait, that was just the simple example - there could also be arithmetic expressions on the right-hand-side, and possibly lists or tuples or floating point numbers.'  Next time around, 'Oh, and also if-then-else statements,' followed by, 'And for statements and while loops and list comprehensions and method definitions and class definitions and imports,' etc.  By now (if I haven't caught on before this), I've killed about 6 hours of pro-bono pyparsing support, and you still have only a partial solution to your problem.  (Please forgive my ranting, but I don't think you realize the potential scope of your question.)



So <em>please</em>, describe your problem as completely as you can, more than just 'a collection of Python commands.'  Maybe post a copy of one of your text files to .  If you need to keep these contents secret for some reason, then you will have to invest more time in formulating your description of the problem.



If you truly need to process *any* Python command, and need a full-on complete Python parser, that is <em>possible</em> to write with pyparsing, but would be a major job even for an experienced developer.  If this is what you need, then I'd look into the PyPy project ().  It would certainly get you going faster than starting from scratch with a parser module.



What exactly are you trying to do, that you must process untrusted Python code?  That is certainly a more significant issue to address than 'how do I parse multiline Python statements'?



-- Paul
#### 2008-07-29 11:23:06 - mayapower
Ok, Here is something in detail. I am writing an application where an object is created using data from XML file. The format is fixed but data can be varied. I read the XML file, parse, convert values from string to required ones like tuples, float etc and then pass it to  functions. Later I gave a thought why not use the python commands directly to design the format. It can give me everything I need in two steps. First check any error related to syntax and then read the file line by line. Check the example of the format here:



    parameterfile=''
    comment=''
    author='Prashant'
    web='http://www.gmail.com'
    date='July 16th 2008'
    
    help='''
    Code is written by xteam.
    '''
    
    frame(label='Output', expand=1)
    
    points(1,
    2,
    3,
    4,
    5)
    
    separator()



There are some variables and function call but it's the only valid commands user can write. As I said before data can be varied. The reason for asking to read a line means, if a command/syntax is written over multiple line, I need it as string. I can find out

by validating that string that whether it's the valid command or not by matching it with a command database. Validation is quite simple, find it if the string starts with valid command or not. If it's not then user has given something wrong here. This is for security reasons and it won't allow user to write something that is harmful or not required.



In the example above there is command for 'frame'. it accepts only two parameters. if user passes something like this:



    frame(label='Output', expand=1, abc=1)

as per validation system it's a valid command because it starts with frame. I'll execute this command and it'll generate an error saying one extra parameter has found. it's fine, user has to correct his mistake.



'help' and 'points' are spread in multiple lines and the number of lines can be anything. I can write a simple readline utility to read data written into multiple lines but for that I need to define something like this



    HELP-BEGIN
    This is the help.
    in multiple lines.
    ok
    HELP-END



Instead I gave a try to parser and I thought it knows about line begins and ends and may be help me out.



The conclusion is give me the complete python command in one string, either is in a single line or across multiple lines.



hope this help.
#### 2008-07-29 12:48:01 - ptmcg
Ok, look this over, and follow the embedded comments.  In the `'if __name__ == __main__:` section, you can see the example code on how the parser is used, and how the parsed tokens are accessed.



-- Paul





    from pyparsing import *
    
    # basic scalar literal values
    integer = Word(nums)
    float_ = Regex(r'\d+\.\d+([Ee]([+\-])?\d+)?')
    quotedStr = QuotedString(''''',multiline=True) | quotedString.setParseAction(removeQuotes)
    
    # valid Python var name
    varname = Word(alphas+'_',alphanums+'_')
    
    # define basic punctuation bits - suppress them so that they don't clutter
    # up the returned tokens
    EQ,COMMA,LPAR,RPAR,LBRK,RBRK = map(Suppress,'=,()[]')
    
    # a right-hand-side value to an assignment can be any of the scalar types,
    # or a potentially nested list or tuple (no dicts in this parser).  Because
    # nesting is a possibility, define a recursive rhs using pyparsing Forward.
    rhs = Forward()
    list_ = LBRK + Optional(delimitedList(rhs)) + RBRK
    tuple_ = LPAR + Optional(delimitedList(rhs)) + RPAR
    rhs << (float_ | integer | quotedStr | list_ | tuple_)
    
    # assignments are a left-hand-side (lhs) = right-hand-side (rhs)
    assignment = varname('lhs') + EQ + rhs('rhs')
    
    # define a function argument as an assignment, or a lone rhs
    # (use Group to keep arg=default bits together)
    arg = Group(assignment) | rhs
    
    # a function call is a varname, and a potentially empty list of arguments
    func_call = varname('fname') + LPAR + Group(Optional(delimitedList(arg)))('args') + RPAR
    
    # statements are assignments or function calls
    statement = Group(assignment('assignment') | func_call('func_call'))
    
    # ignore comments
    statement.ignore(pythonStyleComment)
    
    if __name__ == '__main__`:
        test = '''
        parameterfile=''
        comment=''
        author='Prashant'
        web='http://www.gmail.com'
        date='July 16th 2008'
    
        help='''
        Code is written by xteam.
        '''
    
        # there might even be a comment in the input file...
        frame(label='Output', expand=1)
    
        points(1,
        2,
        3,
        4,
        5)
    
        separator()
        '''
    
        # parsing the input string, containing all statements, not just one at a time
        stmts = OneOrMore(statement).parseString(test)
    
        # for each statement, print out parsed bits
        for stmt in stmts:
            #~ print stmt.dump()
            if stmt.assignment:
                print 'ASSIGNMENT: ', repr(stmt.rhs), '->', stmt.lhs
            if stmt.func_call:
                print 'FUNCTION CALL: ', stmt.fname, 'args: ', stmt.args



prints:



    ASSIGNMENT:  '' -> parameterfile
    ASSIGNMENT:  '' -> comment
    ASSIGNMENT:  'Prashant' -> author
    ASSIGNMENT:  'http://www.gmail.com' -> web
    ASSIGNMENT:  'July 16th 2008' -> date
    ASSIGNMENT:  '\n    Code is written by xteam.\n    ' -> help
    FUNCTION CALL:  frame args:  [['label', 'Output'], ['expand', '1']]
    FUNCTION CALL:  points args:  ['1', '2', '3', '4', '5']
    FUNCTION CALL:  separator args:  []


#### 2008-07-29 12:52:43 - ptmcg
If the pyparsing style is confusing or mysterious to you, please work through some of the links on the Documentation or Examples pages.  It might even be worth springing for the $10 for the 'Getting Started with Pyparsing' e-book from O'Reilly.



(Also, the code I posted uses features introduced in pyparsing 1.4.7, so if you have an older version, you'll need to upgrade.)



-- Paul
#### 2008-07-30 03:55:48 - mayapower
Thanks a lot Paul. It really helped me out.
#### 2008-07-30 06:42:14 - mayapower
Hi,



I am done with my application, thanks again. Just in case, I did a test by adding a tuple/list in the test string.



    color = [1,2,3]
    color = (1,2,3)
    color = ('a','b','c')

But it's not taking these types of assignments. I was not able to figure it out. AFAIK this code is concern, it should work even on tuples or lists. Am I right?



    '''
    a right-hand-side value to an assignment can be any of the scalar types, or a potentially nested list or tuple (no dicts in this parser). Because nesting is a possibility, define a recursive rhs using pyparsing Forward.
    '''
        rhs = Forward()
        list_ = LBRK + Optional(delimitedList(rhs)) + RBRK
        tuple_ = LPAR + Optional(delimitedList(rhs)) + RPAR
        rhs << (float_ | integer | quotedStr | list_ | tuple_)



Prashant
#### 2008-07-30 07:07:38 - ptmcg
Yes, you are right.  I got lazy in my testing.  Here are the changes you must make.  Replace the lines you posted with:



    rhs = Forward()
    list_ = LBRK + Group(Optional(delimitedList(rhs))+Optional(COMMA)) + RBRK
    tuple_ = LPAR + Group(Optional(delimitedList(rhs))+Optional(COMMA)) + RPAR
    list_.setParseAction(lambda t: t.asList())
    tuple_.setParseAction(lambda t: tuple(t[0]))
    rhs << (float_ | integer | quotedStr | list_ | tuple_)



Even handles lists or tuples with trailing commas, like '(1,2,)'.



-- Paul
#### 2008-07-30 21:21:25 - mayapower
Yes, it's working now but one thing I have noticed that every element

of list or tuple is converting to string and this is by default for all the values on the right and side.



    color = [1,2,3]
    ASSIGNMENT:  color -> ['1', '2', '3']

Is this is beacuse of nature of parsing or this can be solved?



Prashant
#### 2008-07-31 03:24:14 - ptmcg
Actually Prashant, parsers return strings because that is what they are parsing.  So the values in your case aren't being <em>converted</em> to strings, they are strings that are <em>not</em> being converted into their respective types.



Fortunately, this is very easy to solve with pyparsing, using parse actions (just like we did with tuples and lists).  Have you had a chance to read any of the pyparsing documentation yet?  This is covered in most of the articles that I write, because it is one of the more-powerful features of pyparsing.



With a parse action, pyparsing will, at parsing time, call the action after successfully parsing the expression that the action is attached to.  That is how this works:



    tuple_.setParseAction(lambda t: tuple(t[0]))



When the tuple_ expression is parsed, the tokens are passed to the parse action (in this case it is just a simple lambda that takes the list of tokens that matched as variable 't').  Then pyparsing returns the result of the expression, instead of the parsed strings.  With parse actions, you can perform all kinds of data conversion, string modifications, even extra validation.



Converting the strings that match floats and integers into actual floats and integers is done with very similar parse actions:



    float_.setParseAction(lambda t: float(t[0]))
    integer.setParseAction(lambda t: int(t[0]))



There is even a parse action for quoted strings, to remove the quotation marks that mark them as strings.  This is such a common parse action that I include it as a built-in for pyparsing, called removeQuotes.



So add the lines shown above to your parser, and you should start getting integers and floats instead of just the parsed strings.  Sorry to have left this out earlier.



-- Paul

---
## 2008-08-01 16:21:52 - robinsiebler - New to PyParsing
I'm tired of using the brute-force hacy way, so I thought I'd try a better way. :)  However, I'm having a bit of trouble wrapping my brain around this.  I have a text file that contains several exported pieces of e-mail and I want to extract the date and the subject (among other things).  What would that look like in PyParsing?

    Received: from INGESTOR2SQA ([10.220.83.198]) by sqaserver300 with Microsoft SMTPSVC(6.0.3790.0);
         Thu, 31 Jul 2008 12:10:26 -0700
    mime-version: 1.0
    from: 'AVDN Ingestor' <sqatest@ictvsys.pvt>
    to: 
    date: 31 Jul 2008 17:53:40 +0000
    subject: Upload Status for test
    content-type: multipart/mixed; boundary=--boundary_2_09ab8836-ff06-41a6-94d6-59258539bf88
    Return-Path: 
    Message-ID: <SQASERVER300wQtcC6Z0000000e@sqaserver300>
    X-OriginalArrivalTime: 31 Jul 2008 19:10:26.0125 (UTC) FILETIME=[16D08FD0:01C8F341]

#### 2008-08-01 20:18:13 - ptmcg
This is a good 'getting started' project for learning pyparsing.



The first step I'd make is to extract a few lines that I feel represent the basic email header, preferably something that all of the messages have in common.  For this problem, I chose the following lines:



    Received: from INGESTOR2SQA ([10.220.83.198]) by sqaserver300 with Microsoft SMTPSVC(6.0.3790.0);
    Thu, 31 Jul 2008 12:10:26 -0700
    mime-version: 1.0
    from: 'AVDN Ingestor' <sqatest@ictvsys.pvt>
    to: sqatest@ictvsys.pvt
    date: 31 Jul 2008 17:53:40 +0000
    subject: Upload Status for test



I'll get a pyparsing parser started to process these lines, and then leave it to you to expand on it if you want more information.



Looking at this extract, I begin by identifying the types of common data formats that I want to look for.  It would be possible to have a generic header format that consists of:



    header_label : header_value



But I'll break this up into header-specific expressions.  I describe this set of headers as:



    received_hdr
    full date time
    mime_version_hdr
    from_hdr
    to_hdr
    date_hdr
    subject_hdr



I'll define pyparsing expressions for each of these, so that when they are all done, I can create one overall expression that looks like this:



    mail_header = received_hdr + fulldatetime + mime_version_hdr + \
        from_hdr + to_hdr + date_hdr + subject_hdr



Pyparsing provides basic expressions like Literal and Word to define the lowest level expressions.  For example, 'Word('0123456789')'  means that we want to match a 'word' composed of one or more numeric characters.  This would be a good way to define a match for an integer.  The set of numeric integers '0123456789' occurs so often in parser that pyparsing defines the variable 'nums' to represent it.  So the value corresponding to 'mime-version' can be defined as:



    versionNum = Word(nums) + '.' + Word(nums)



Pyparsing reports the separate elements as their own tokens, so '1.0' would get returned as ['1', '.', '0'].  By wrapping it in a pyparsing Combine:



    versionNum = Combine(Word(nums) + '.' + Word(nums))



it will concatenate the elements to return the string '1.0'.  Using this expression versionNum, we can now define the mime_version_hdr as:



    mime_version_hdr = Group('mime-version: ' + versionNum('version'))



By adding the ('version') name to the versionNum expression, we will be able to access that field by name instead of having to access it by token index.  This features makes parsers *much* more robust as data and data formats change over time.



Another important pyparsing built-in is restOfLine.  It will make it easy for us to parse some of the more freely formatted headers, like Received, from, to, and subject:



    received_hdr = Group('Received: ' + restOfLine('body'))
    from_hdr = Group('from: ' + restOfLine('body'))
    to_hdr = Group('to: ' + restOfLine('body'))
    subject_hdr = Group('subject: ' + restOfLine('body'))



Using these methods for defining small tokens and combining them into larger expressions, we can define all of the headers:



    monthAbbr = oneOf( list(calendar.month_abbr) )
    dayAbbr = oneOf( list(calendar.day_abbr) )
    timezone = Combine(oneOf('+ -') + Word(nums,exact=4))
    time = Combine((Word(nums,exact=2) + ':')*2 + Word(nums,exact=2)) + timezone('tz')
    date = Word(nums) + monthAbbr + Word(nums,exact=4)
    fulldatetime = Group(dayAbbr('day') + ',' + date('date') + time('time'))
    versionNum = Combine(Word(nums)+'.'+Word(nums))
    
    received_hdr = Group('Received: ' + restOfLine('body'))
    mime_version_hdr = Group('mime-version: ' + versionNum('version'))
    from_hdr = Group('from: ' + restOfLine('body'))
    to_hdr = Group('to: ' + restOfLine('body'))
    date_hdr = Group('date: ' + date + time)
    subject_hdr = Group('subject: ' + restOfLine('body'))



And these individual header expressions can be combined into the mail_header expression I gave above.



Now that we have an expression for mail_header, we can use it to search through the given sample text, using the searchString method:



    for msg_hdr in mail_header.searchString(sample):
        for hdr in msg_hdr:
            print hdr.dump()



For each mail_header that is found, we index through each header, and then call dump() on that token list.



This code gives this output:



    ['Received: ', 'from INGESTOR2SQA ([10.220.83.198]) by sqaserver300 with Microsoft SMTPSVC(6.0.3790.0);']
    - body: from INGESTOR2SQA ([10.220.83.198]) by sqaserver300 with Microsoft SMTPSVC(6.0.3790.0);
    ['Thu', ',', '31', 'Jul', '2008', '12:10:26', '-0700']
    - date: ['31', 'Jul', '2008']
    - day: Thu
    - time: ['12:10:26', '-0700']
      - tz: -0700
    - tz: -0700
    ['mime-version: ', '1.0']
    - version: 1.0
    ['from: ', ''AVDN Ingestor' <sqatest@ictvsys.pvt>']
    - body: 'AVDN Ingestor' <sqatest@ictvsys.pvt>
    ['to: ', 'sqatest@ictvsys.pvt']
    - body: sqatest@ictvsys.pvt
    ['date: ', '31', 'Jul', '2008', '17:53:40', '+0000']
    - tz: +0000
    ['subject: ', 'Upload Status for test']



From here, if you need more headers than the ones I've picked out, look through the code and the examples on this site, and see if you can extend this parser for yourself.  (I've posted the complete parser code at .)



-- Paul
#### 2008-08-02 11:07:51 - robinsiebler
If I -just- wanted to print the date and subject, what would be the proper way to do that?
#### 2008-08-02 12:26:41 - robinsiebler
In your example, I get ['date: ', '31', 'Jul', '2008', '17:53:40', '+0000']



How would I use a parse action to convert '31', 'Jul', '2008' into '20080731'?
#### 2008-08-02 16:05:48 - ptmcg
To convert '31 Jul 2008' to '20080731', you need a few things.  First, you need a dict to map the month abbreviations to their respective indexes 1-12.  The month abbreviations can be gotten from the calendar module.  Here is a one-line to create this dict:



    month_index = dict(zip(calendar.month_abbr[1:],range(1,12+1)))



Now to set a parse action, we want to take the tokens returned by the date expression, and reformat them.  Here is what we get for the date:



    '31', 'Jul', '2008'



These 3 strings will be passed as a list to the parse action.  Let's call this parameter 't' for 'tokens'.  Here is what this parse action would look like:



    def reformatDate(t):
        day = int(t[0])
        mon = month_index[t[1]]
        yr  = int(t[2])
        return '%04d%02d%02d' % (yr, mon, day)



Then to attach this parse action to the date expression, use:



    date.setParseAction(reformatDate)



If you get used to using lambdas, you could write this in a single line as:



    date.setParseAction(lambda t: '%04d%02d%02d' % (int(t[2]),month_index[t[1]],int(t[0])))





Now that dates are being converted to more easily sorted strings of digits, let's focus on just processing the date and subject lines.  This really is just a Python programming exercise at this point, iterating over the list of headers, and then processing those that match the desired header name:



    for msg_hdr in mail_header.searchString(sample):
        for hdr in msg_hdr:
            if hdr[0] == 'date: ':
                print 'Date =', hdr[1]
            elif hdr[0] == 'subject: ':
                print 'Subject =', hdr.body



You can see how easy it is to pick out the subject body, since we gave the subject value the results name 'body'.  You could do similar results naming for other elements in your grammar.



Now try to take this as a starting point, and experiment with some refinements of your own.



-- Paul

---
## 2008-08-02 06:35:38 - ecik - Comments in parsed code
Hi! First, I'd like to say 'thanks' for all the work you've provided to make pyparsing just as good as it's now, it really simplified my life.

Anyway, I've got a problem (unfortunately). I'm working on some S-Expressions-like parser. It works good but, since some expressions may get complicated, it would be nice if user can provide comments to his/her code.

I tried to do something like that:



    comment = Suppress(QuotedString(quoteChar=';', endQuoteChar='\n'))

but it gives me:



    sexp.py:65: SyntaxWarning: endQuoteChar cannot be the empty string

which is not odd since pyparsing tries to strip the string. So my question is: what do I have to do to be able to write comments in my sexprs parser?

#### 2008-08-02 08:13:07 - ptmcg
Thanks for posting - I'm glad pyparsing is of help to you!



This is untested, but can you try one of these?



    comment = QuotedString(';',endQuoteChar=LineEnd())

(this is an interesting use of QuotedString, by the way, I never thought of using it like this)



or 



    comment = ';' + restOfLine



Don't bother wrapping it in a suppress, just pass the comment expression to the grammar's ignore method once you have built the whole thing:



    sexp_grammar.ignore(comment)



This will ignore comments anywhere they occur - you wont have to sprinkle them about within the grammar.



-- Paul
#### 2008-08-02 10:03:35 - ecik
Thanks for that really quick answer!

The first hint, with LineEnd(), doesn't work because LineEnd() has no .strip() function.

The second one works just as I expected and my life was made easier thanks to .ignore which I missed before ;). Thanks!



Anyway, in the meantime, I insisted on using QuotedString and I managed to make ugly workaround:





    class S(str):
        def strip(self, x=''):
            return self
    
    comment = QuotedString(quoteChar=';', endQuoteChar=S('\n'))


#### 2008-10-23 02:26:08 - steven-d
I'm having some problems with using comments in a line-oriented parser.





    text = r'''this line has no comment
    nor does this
    but this line does # have a comment
    and so does this # also
    # comment
    this line follows a comment'''
    
    from pyparsing import *
    
    ws = ' \t'
    ParserElement.setDefaultWhitespaceChars(ws)
    
    comment = '#' + restOfLine
    line = Word(alphanums + ' \t') + LineEnd().suppress()
    parser = OneOrMore(line) + StringEnd()
    parser.ignore(comment)
    
    try:
        for i, item in enumerate(parser.parseString(text)):
            print 'Line %d: %s' % (i+1, item)
    except ParseException, err:
        print err.line
        print ' '*(err.column-1) + '^'
        print err



the parser fails when it reaches the bare comment:





    # comment
    ^
    Expected end of text (at char 108), (line:5, col:1)



but if I remove that entire line, it works fine.



What have I missed?



Thanks,



Steven.
#### 2008-10-25 22:58:03 - ptmcg
Look at this again:



    line = Word(alphanums + ' \t') + LineEnd().suppress()



Ignoring comments, your line consisting solely of a comment doesn't match line - there is no Word or alphanums or spaces.



Changing to this works:



    line = Optional(Word(alphanums + ' \t')) + LineEnd().suppress()



-- Paul

---
## 2008-08-04 11:13:25 - robinsiebler - Text that spans multiple lines
I'm sorry if I'm trying your patience, but I've looked through the docs and can't figure out if there is any particular way to deal with this.  Each e-mail in my text file of collected e-mail has a mime attachement that looks like this:

    ----boundary_2_09ab8836-ff06-41a6-94d6-59258539bf88
    content-type: text/xml; name=status.xml; charset=utf-8
    content-transfer-encoding: base64
    content-disposition: attachment
    
    PGF2ZG5fcG5tIHZlcnNpb249IjEuMSIgc2VuZGVyPSJhdmRud2VzdGRjMSIgZG9jbnVtVy
    PSJvaHV0bHBxeXptM25xd3lrYm9wd3pwdTUiIHN0YXR1cz0ic3VjY2Vzc2Z1bCI+PGbnR=
    ----boundary_2_09ab8836-ff06-41a6-94d6-59258539bf88--


The encoded part is all I care about.  How do I represent a bunch of lines of what amounts to gibberish?

#### 2008-08-04 11:31:47 - robinsiebler
I tried adding the following, but it didn't work, so obviously, I am not understanding something :/





    aStart = 'content-disposition: attachment'
    aEnd = '----boundary'
    attachment = Group(aStart + SkipTo(aEnd,True))
    
    mail_header = received_hdr + fulldatetime + mime_version_hdr + \
        from_hdr + to_hdr + date_hdr + subject_hdr + attachment


#### 2008-08-04 12:22:19 - ptmcg
Thanks for taking a stab at this on your own.  Unfortunately, pyparsing is not a wildcard matching parser.  Note that in your expression for mail_header, you go directly from subject_hdr to attachment, but I think in your original sample, there were some extra e-mail header lines in there.



But you are starting to at least try combining some of the expressions, and that is great!



Here is the reply I worked up to your specific question about detecting attachments.  See if you can combine this with the previous work.



-- Paul





    sample = '''
    ----boundary_2_09ab8836-ff06-41a6-94d6-59258539bf88
    content-type: text/xml; name=status.xml; charset=utf-8
    content-transfer-encoding: base64
    content-disposition: attachment
    
    PGF2ZG5fcG5tIHZlcnNpb249IjEuMSIgc2VuZGVyPSJhdmRud2VzdGRjMSIgZG9jbnVtVy
    PSJvaHV0bHBxeXptM25xd3lrYm9wd3pwdTUiIHN0YXR1cz0ic3VjY2Vzc2Z1bCI+PGbnR=
    ----boundary_2_09ab8836-ff06-41a6-94d6-59258539bf88--
    
    '''
    
    from pyparsing import *
    import base64
    
    # from the pyparsing wiki: http://pyparsing-public.wikispaces.com/Helpful+Expressions#toc2
    _hexStr = lambda n : Word(hexnums,exact=n)
    uuid = Combine(_hexStr(8) + ('-'+_hexStr(4))*3 + '-' + _hexStr(12))
    
    # general format for a mime header
    mime_hdr = Group(Combine('content-' + Word(alphas+'-')) + 
                                    Suppress(':') + empty + restOfLine)
    mime_hdrs = Dict(OneOrMore(mime_hdr))
    
    # general format for a line of mime attachment gibberish
    mime_line = Word(alphanums+'+/=')
    
    # define beginning and ending attachment markers
    # (use matchPreviousLiteral method from pyparsing to match beginning
    # and ending markers)
    attachment_marker = Combine('----boundary_' +Word(nums) + '_' + uuid)
    attachment_begin = attachment_marker
    attachment_end = matchPreviousLiteral(attachment_begin) + '--'
    
    # put them all together
    attachment = attachment_begin + \
                mime_hdrs('headers') + \
                Group(OneOrMore(mime_line))('body') + \
                attachment_end
    
    # extract attachments - note use of dict-like access to headers
    for att in attachment.searchString(sample):
        print att.headers.dump()
        print att.headers['content-type']
        print att.body
        print base64.decodestring(''.join(att.body))



prints:



    [['content-type', 'text/xml; name=status.xml; charset=utf-8'], 
        ['content-transfer-encoding', 'base64'], ['content-disposition', 'attachment']]
    - content-disposition: attachment
    - content-transfer-encoding: base64
    - content-type: text/xml; name=status.xml; charset=utf-8
    text/xml; name=status.xml; charset=utf-8
    ['PGF2ZG5fcG5tIHZlcnNpb249IjEuMSIgc2VuZGVyPSJhdmRud2VzdGRjMSIgZG9jbnVtVy', 
        'PSJvaHV0bHBxeXptM25xd3lrYm9wd3pwdTUiIHN0YXR1cz0ic3VjY2Vzc2Z1bCI+PGbnR=']
    <avdn_pnm version='1.1' sender='avdnwestdc1' docnum
    W#�&��WF����6�w��&�w�SR'7FGW3�'7V66W76gV�#��nt

(The base64 decoding is probably scrambled because you snipped some of the intervening encoded lines.)
#### 2008-08-04 12:30:22 - robinsiebler
>but I think in your original sample, there were some extra e-mail >header lines in there.



Are you saying that I have to define every line, even if I don't care about it?  If so, I'm confused now.
#### 2008-08-04 12:56:46 - ptmcg
<ul class="quotelist"><li>Are you saying that I have to define every line,</li><li>even if I don't care about it? If so, I'm confused now.</li></ul>

Well, that's pretty much how parsers work.  You can create an expression that is a catchall for other header lines, as I alluded in your earlier thread, matching something like:



    header_label: header_value



with an expression like:



    other_header = Word(alphas,alphas+'-') + ':' + restOfLine



and then add this to your msg_header definition as:



    mail_header = received_hdr + fulldatetime + mime_version_hdr + \
        from_hdr + to_hdr + date_hdr + subject_hdr + \
        ZeroOrMore(other_header)



Does that help?
#### 2008-08-04 13:17:01 - robinsiebler
I guess I am just being dense, because -even- with all your help, I'm still doing something wrong. :/





    from pyparsing import *
    import base64, calendar, datetime, time
    
    def reformatDate(t):
        day = int(t[0])
        mon = month_index[t[1]]
        yr  = int(t[2])
        return '%04d%02d%02d' % (yr, mon, day)
    
    monthAbbr = oneOf( list(calendar.month_abbr) )
    dayAbbr = oneOf( list(calendar.day_abbr) )
    timezone = Combine(oneOf('+ -') + Word(nums,exact=4))
    time = Combine((Word(nums,exact=2) + ':')*2 + Word(nums,exact=2)) + timezone('tz')
    date = Word(nums) + monthAbbr + Word(nums,exact=4)
    date.setParseAction(reformatDate)
    fulldatetime = Group(dayAbbr('day') + ',' + date('date') + time('time'))
    versionNum = Combine(Word(nums)+'.'+Word(nums))
    month_index = dict(zip(calendar.month_abbr[1:],range(1,12+1)))
    
    # from the pyparsing wiki: http://pyparsing-public.wikispaces.com/Helpful+Expressions#toc2
    _hexStr = lambda n : Word(hexnums,exact=n)
    uuid = Combine(_hexStr(8) + ('-'+_hexStr(4))*3 + '-' + _hexStr(12))
    
    # general format for a mime header
    mime_hdr = Group(Combine('content-' + Word(alphas+'-')) +
                                    Suppress(':') + empty + restOfLine)
    mime_hdrs = Dict(OneOrMore(mime_hdr))
    
    # general format for a line of mime attachment gibberish
    mime_line = Word(alphanums+'+/=')
    
    # define beginning and ending attachment markers
    # (use matchPreviousLiteral method from pyparsing to match beginning
    # and ending markers)
    attachment_marker = Combine('----boundary_' +Word(nums) + '_' + uuid)
    attachment_begin = attachment_marker
    attachment_end = matchPreviousLiteral(attachment_begin) + '--'
    
    # put them all together
    attachment = attachment_begin + \
                mime_hdrs('headers') + \
                Group(OneOrMore(mime_line))('body') + \
                attachment_end
    
    ### extract attachments - note use of dict-like access to headers
    ##for att in attachment.searchString(sample):
    ##    print att.headers.dump()
    ##    print att.headers['content-type']
    ##    print att.body
    ##    print base64.decodestring(''.join(att.body))
    
    received_hdr = Group('Received: ' + restOfLine('body'))
    mime_version_hdr = Group('mime-version: ' + versionNum('version'))
    from_hdr = Group('from: ' + restOfLine('body'))
    to_hdr = Group('to: ' + restOfLine('body'))
    date_hdr = Group('date: ' + date + time)
    subject_hdr = Group('subject: ' + restOfLine('body'))
    content_hdr = Group('content-type:' + restOfLine('body'))
    return_hdr = Group('Return-Path:' + restOfLine('body'))
    message_hdr = Group('Message-ID:' + restOfLine('body'))
    x_hdr = Group('X-OriginalArrivalTime:' + restOfLine('body'))
    
    mail_header = received_hdr + fulldatetime + mime_version_hdr + from_hdr + \
                  to_hdr + date_hdr + subject_hdr + content_hdr + return_hdr + \
                  message_hdr + x_hdr
    
    for msg_hdr in mail_header.searchString(sample):
        for hdr in msg_hdr:
            if hdr[0] == 'date: ':
                print 'Date =', hdr[1]
            elif hdr[0] == 'subject: ':
                print 'Subject =', hdr.body



If I add '+ attachment' to the end of mail_header, nothing happens when I try to run the code.  I assume I've done something wrong and an error is occurring, but I don't get any error message in the output window.
#### 2008-08-04 13:37:17 - ptmcg
When I add '+ attachment' to the end of mail_header, this works just fine for me.  But then, I am pasting your attachment sample immediately after the header sample.  So I can only assume that there is some other stuff between the header and the attachment.  Could you post the full text that you are parsing to the pyparsing.pastebin.com?  Then I'll have the complete picture of what you are working with.
#### 2008-08-04 13:42:14 - robinsiebler
I added 1 complete e-mail from the sample file - 
#### 2008-08-04 13:49:35 - ptmcg
Ah!  You see, this chunk of text:



    ----boundary_2_09ab8836-ff06-41a6-94d6-59258539bf88
    content-type: text/plain; charset=us-ascii
    content-transfer-encoding: quoted-printable
    
    For additional information, please see attachment.



is between the message header and the actual attachment.  You'll need to add an expression for this and insert it before '+ attachment'.



-- Paul
#### 2008-08-04 14:08:20 - robinsiebler
Thanks!  I got that working, now I guess I'll try to absorb as much of this as possible so I won't bother you.  :)



Is there any way of displaying an error message to determine where the parsing or whatever is failing?
#### 2008-08-04 14:18:10 - ptmcg
Well, one of the issues that you have is that we are using searchString, which scans through the input for matching patterns.  Since searchString expects to have to skip over text that is not included in the grammar, it pretty much silently steps over parsing exceptions, and moves on through the input.



If you think that we now have a complete enough grammar, you can try using the parseString method, as in 



    msgs = OneOrMore(Group(mail_header)).parseString()

to read all of the messages.  Then iterate over each e-mail in the msgs results structure.



OneOrMore also will not make much of a fuss over any parsing exceptions, once it has parsed the first e-mail.  After all, it just means 'one or more', and once it finds one match, it will be happy.  There is a new optional argument to parseString, though, which forces parsing to read the entire input string: parseAll=True, as in:



    msgs = OneOrMore(Group(mail_header)).parseString(parseAll=True)



If there is a parsing exception before getting to the end of the input, then an exception will be raised.  The pyparsing docs have more info on the ParseException and ParseBaseException classes, to see what you can do with one when you get it.



-- Paul

---
## 2008-08-05 13:51:41 - robinsiebler - Another newbie question
I have a new script I am working on - .



When I type 'content.' after I have defined content, I only get the options I would normally get for a string instead of the Pyparsing options.  Why is that?  Because of that behavior, I couldn't figure out how to have the parser strip the quotes from 'notification'.  Also, I still haven't quite figured out how to use setResultsName.  The only 2 values I care about are 'status' and 'notifications', so I'd like to strip the quotes from result for 'notification' and give it a name.



Thanks!

#### 2008-08-05 14:48:28 - ptmcg
Well, I hate to turn you completely upside down, but it is helpful to recognize a standard data format when you see one.  This is XML data.  An XML tag is something that looks like an opening '<', a tag name, zero or more attributes of the form 'name='value'', and either a closing '/>' (indicating an empty tag with no body) or a closing '>' (a leading tag, to be followed later by a closing tag of the form '</', tag name, '>').  With this in mind, there is no way you should be defining expressions like:



    contact = '><contact phone=' + dblQuotedString

The 'contact' information does *not* start with a '><' string, you've just bled the terminating '>' from the opening 'avdn_pnm' tag over into the next expression.  Each tag's data follows the XML standard definition, and you should structure your parser along the same lines - it will be much more robust over time.  For instance, it is very natural to break tags apart with line breaks or whitespace, and hardcoding a leading '><' will not be able to read such data.



Python provides some standard XML parsing modules, such as ElementTree, which is bundled with standard Python as of version 2.5.  There are also XML-friendly constructions in pyparsing, that completely bypass 95% of the expressions you are creating.



Here is your script rewritten to use XML-aware constructs:



    from pyparsing import *
    # none of these modules are used in this excerpt
    #~ import os, sys, util
    
    #--------- pyparsing variables -------------------------------------------------
    pmstart,pmend = makeXMLTags('avdn_pnm')
    
    # these tags are only used as empty start tags, end tags not needed
    contact       = makeXMLTags('contact')[0]
    assetstate    = makeXMLTags('assetstate')[0]
    
    content = pmstart + contact + OneOrMore(Group(assetstate))('assets') + pmend
    
    #--------- end of pyparsing variables ------------------------------------------
    
    # be sure to prefix this string with an 'r', so that \v is not interpreted 
    # as a VT control char
    sample = r'''<avdn_pnm version='1.01' sender='avdnwestdc1' docnumber='5yze404550z5oo45frtrw1zy' status='successful'>
    <contact phone='408-555-1212' email='foo@yahoo.com' />
    <assetstate providerid='AAAA.COM' assetid='AAAA2FC58EF939E50000' filename='\video\whendidyoulastsee_h320.224_160_ntsc_ac3.jpg' notification='120' />
    <assetstate providerid='AAAA.COM' assetid='AAAA2FC58EF939E50000' filename='\video\whendidyoulastsee_h320.224_160_ntsc_ac3_01.jpg' notification='120' />
    <assetstate providerid='AAAA.COM' assetid='AAAA2FC58EF939E50000' filename='\video\whendidyoulastsee_h320.224_160_ntsc_ac3_02.jpg' notification='120' />
    <assetstate providerid='AAAA.COM' assetid='AAAA2FC58EF939E50000' filename='\video\whendidyoulastsee_h320.224_160_ntsc_ac3_03.jpg' notification='120' />
    <assetstate providerid='AAAA.COM' assetid='AAAA2FC58EF939E50000' filename='\video\whendidyoulastsee_h320.224_160_ntsc_ac3_04.jpg' notification='120' />
    <assetstate providerid='AAAA.COM' assetid='AAAA2FC58EF939E50000' filename='\video\whendidyoulastsee_h320.224_160_ntsc_ac3_05.jpg' notification='120' />
    <assetstate providerid='AAAA.COM' assetid='AAAA2FC58EF939E50000' filename='\video\whendidyoulastsee_h320.224_160_ntsc_ac3_06.jpg' notification='120' />
    <assetstate providerid='AAAA.COM' assetid='AAAA2FC58EF939E50000' filename='\video\whendidyoulastsee_h320.224_160_ntsc_ac3_07.jpg' notification='120' />
    <assetstate providerid='AAAA.COM' assetid='AAAA2FC58EF939E50000' filename='\video\whendidyoulastsee_h320.224_160_ntsc_ac3_08.jpg' notification='120' />
    <assetstate providerid='AAAA.COM' assetid='AAAA2FC58EF939E50000' filename='\video\whendidyoulastsee_h320.224_160_ntsc_ac3_09.jpg' notification='120' />
    <assetstate providerid='AAAA.COM' assetid='AAAA2FC58EF939E50000' filename='\video\whendidyoulastsee_h320.224_160_ntsc_ac3.mpg' notification='120' />
    </avdn_pnm>'''
    
    for tag in content.searchString(sample):
        # dump entire tag contents, including named fields
        print tag.dump()
        # print a specific named field
        print tag.status
        # iterate over assets and print specific fields of each
        for asset in tag.assets:
            print asset.providerid, asset.notification



makeXMLTags returns a pair of expressions for the given tag string: the first is the opening '<tagname>' expression, with support for parsing 0 or more attributes; and the second is the closing '</tagname>' expression.  Since contact and assetstate don't use closing tags in your sample, I just use the opening tag (which is why you see code like `makeXMLTags("contact")[0]`.



makeXMLTags automatically defines results names based on the XML attributes it finds in the input text.  For attribute values that are quoted strings, makeXMLTags automatically strips off the quotation marks.



-- Paul
#### 2008-08-05 14:52:34 - ptmcg
Oops, forgot to post the output from this script (I've truncated the long lines with '...' for posting purposes):



    ['avdn_pnm', ['version', '1.01'], ['sender', 'avdnwestdc1'], ...
    - assets: [['assetstate', ['providerid', 'AAAA.COM'], ['assetid', ...
    - docnumber: 5yze404550z5oo45frtrw1zy
    - email: foo@yahoo.com
    - empty: True
    - endAvdn_Pnm: </avdn_pnm>
    - phone: 408-555-1212
    - sender: avdnwestdc1
    - startAvdn_Pnm: ['avdn_pnm', ['version', '1.01'], ['sender', 'avdnwestdc1'], ...
      - docnumber: 5yze404550z5oo45frtrw1zy
      - empty: False
      - sender: avdnwestdc1
      - status: successful
      - version: 1.01
    - startContact: ['contact', ['phone', '408-555-1212'], ['email', 'foo@yahoo.com'], True]
      - email: foo@yahoo.com
      - empty: True
      - phone: 408-555-1212
    - status: successful
    - version: 1.01
    successful
    AAAA.COM 120
    AAAA.COM 120
    AAAA.COM 120
    AAAA.COM 120
    AAAA.COM 120
    AAAA.COM 120
    AAAA.COM 120
    AAAA.COM 120
    AAAA.COM 120
    AAAA.COM 120
    AAAA.COM 120


#### 2008-08-05 15:02:51 - robinsiebler
I'm still confused on one issue. When I type 'content.' in the editor or during debugging, I only get the options I would normally get for a string instead of the Pyparsing options.  When I type 'asset.', I don't get -anything-, even though if I type 'asset.notification<enter>' I get the value.  Why is that?
#### 2008-08-05 15:30:30 - ptmcg
'content' in your code is a pyparsing Group object, and so at runtime you should get the options that are defined for a Group, and any superclass of Group.



'asset' is a pyparsing ParseResults object, that is returned at runtime after parsing some input string.  ParseResults have additional logic in them that allow them (using `__getitem__` and `__getattr__` methods) to support 'attributes' that are not known at compile time.  Your IDE does not, nay <em>could</em> not, know what these attributes will be at runtime.  If you are lucky, it <em>might</em> know that this will be a ParseResults object, and give you options to enter methods like 'dump', 'keys', 'values', 'pop', etc.  In the debugger, the dynamic attributes would be available, if you type 'asset.keys()' for instance.  In Python3.0, pyparsing update the values returned by 'dir' to include dynamically assigned attributes, so the chance are better that at that time in the future, what you ask <em>could</em> be possible at run/debug time.  But it will <em>never</em> be possible at IDE edit/compile time.  Such is the way with the dynamic typing system in Python.



Read up on Python's `__getattr__` feature for defining attributes on an object that are not statically defined.



It is unfortunate, but you seem to be learning about 9 different programming topics all at once (Python, XML, parsers, pyparsing, and so on).  This really isn't the best way to learn, since any given problem could be an issue in one of half a dozen different areas.  On top of that, pyparsing <em>does</em> take advantage of some fairly sophisticated parts of Python.  Not just dynamic attribute creation, but also operator overloading.  Isn't it weird that



    '[' + 'abc' + ']'

evaluates to the string '[abc]', but



    '[' + Word('abc') + ']'

evaluates to a pyparsing expression that will parse not only '[abc]', but also '[a]', '[ab]', '[bca]', and even '[bcbababacabbaabbccab]'?  This is part of pyparsing's programming style, but it assumes that you have some model of programming to build on in the first place.  



I fear you may have jumped into the deep end of the pool before being completely ready.  At minimum, I'd begin by getting a good grasp of Python under your belt, and then some of Python's object-oriented programming support (there are many free tutorials for this stuff online, or go get one of the books listed at www.python.org).  At least once you have this foundation, you should have a little better idea for yourself where some of these topics might be covered in existing Python docs, in far clearer and more detailed terms than I can whip off the top of my head.



-- Paul
#### 2008-08-05 15:38:25 - ptmcg
Doh, I messed up the previous post. Any words that are underlined are supposed to be pre- and post-fixed by double-underscores, as in __getattr__.



Sorry,

-- Paul

---
## 2008-08-06 11:15:45 - efaherty - Recursive terminals
I have the following grammar:

    IF_STATEMENT = IF + LPARN + BOOLEAN_CONDITION + RPARN + LCURL + STATEMENT + RCURL
    STATEMENT = IF_STATEMENT ^ FOR_LOOP ^ RETURN

This will not work as STATEMENT needs IF_STATEMENT and IF_STATEMENT needs STATEMENT.



How do I get around this in pyparsing?



btw, sorry if this is not the place to post this question.

#### 2008-08-07 10:48:09 - ptmcg
Thanks for posting, I usually only check the Discussion tab on the wiki home page.



This is a common grammar issue, that of recursively defining an expression.  Pyparsing addresses this problem using the Forward() class, used to 'forward declare' an expression so that you can refer to it before it has been completely specified.  In your particular case (I expect you have the same issue with FOR_LOOP as you do with IF_STATEMENT), you would write:



    # 'forward declare' STATEMENT so it can be used in other expressions
    STATEMENT = Forward()
    
    # define expressions that use STATEMENT
    IF_STATEMENT = IF + LPARN + BOOLEAN_CONDITION + RPARN + LCURL + ZeroOrMore(STATEMENT) + RCURL
    FOR_LOOP = FOR + LPARN + \
        Optional(ASSIGNMENT) + SEMI + \
        Optional(BOOLEAN_CONDITION) + SEMI + \
        Optional(STATEMENT) + RPARN + LCURL + ZeroOrMore(STATEMENT) + RCURL
    
    # now 'inject' expression definition into STATEMENT
    STATEMENT << ( IF_STATEMENT | FOR_LOOP | RETURN )
    



Check out the examples page, and look for those examples marked as having recursive grammars.



-- Paul
#### 2008-08-09 06:08:14 - efaherty
Sorry i posted this in the wrong section :)



thanks for the help!

---
## 2008-08-09 06:06:11 - efaherty - nestedExpr question question
Hi,



I have a string which is a code block surrounded in curly braces:





    {
        foo = bar
        if (foo) {
            then bar2();
        }
    }



I want to be able to read this as a single element without ignoring whitespaces.  I am currently using nestedExpr('{','}') and a helper function to reassemble the string from the list of lists.  



The problem with this is that i need to retain the whitespaces from the input.



I have had a look at QuotedString and it seems it cannot handle the nesting.



Do you have any ideas on how i can do what i want?



thanks,



Eamonn

#### 2008-08-09 06:15:45 - efaherty
I have just realised i could use Forward with QuotedString using a recursive element.



If this the prefered way?
#### 2008-08-09 07:27:24 - efaherty
I have found that i cannot do it with QuotedString.  



Any ideas please?
#### 2008-08-09 07:56:52 - ptmcg
The current version of pyparsing has a parse action called keepOriginalText that might do what you want:



    expr = nestedExpr('{','}')
    print expr.parseString(s)
    
    expr.setParseAction(keepOriginalText)
    print expr.parseString(s)



prints:



    [['foo', '=', 'bar', 'if', '(foo)', ['then', 'bar2();']]]
    ['\n{\n    foo = bar\n    if (foo) {\n        then bar2();\n    }\n}']





keepOriginalText has some issues, though.  It uses the inspect module to peek up the call stack.  In the next release of pyparsing (which you can get from the SF SVN repository), there is a helper called originalTextFor that I plan to supercede the use of the keepOriginalText parse action.  Here's what it would look like:



    expr = originalTextFor(nestedExpr('{','}'))
    print expr.parseString(s)



prints:



    ['{\n    foo = bar\n    if (foo) {\n        then bar2();\n    }\n}']



Will one of these work for you?



-- Paul
#### 2008-08-09 18:06:50 - efaherty
Thanks for your response!



originalTextFor looks like it will do the trick perfectly.



I will give it a go and let you know how I get on.  



Are there any issues using the latest version from svn?  Do you have any idea when your next release will be?



Thanks for the great work!



Eamonn
#### 2008-08-09 18:36:52 - ptmcg
No issues I can think of, pyparsing 1.5.1 should be fully compatible with 1.5.0.  (You can look at the CHANGES file to see what sort of things are included.)



I hope to get 1.5.1 out before the end of the month.  There was a Python 2.3 compatibility bug that I just fixed, that is probably the most urgent driver to release soon.



Meanwhile, if you prefer, you can just extract the source for originalTextFor into your own local method, and then remove that method once you upgrade to 1.5.1.



-- Paul
#### 2008-08-11 05:22:12 - efaherty
thanks for your help. 



I plan to distribute my work under the MIT license.  I do not plan on making any money from it.  Would it be okay to distribute your originalTextFor function with my source code until version 1.5.1 is released?
#### 2008-08-11 06:38:52 - ptmcg
I don't see any problem with that.  Maybe mark it with a comment that it was excerpted from pyparsing 1.5.1, and should be removed once you have upgraded, but that would be as much for your own reminder as for any license issue.



Cheers,

-- Paul

---
## 2008-08-12 05:58:08 - chris_laws - regular expression extracting groups
OK, first off I had to re-post this here. I clicked on the 'help on how to format text' link within the 'Post Message' form. That discarded my text and took me to some other page where I re-entered my query and posted it. It's stuck on that page I think. Perhaps the site maintainer can delete that.



Anyway... onto my issue...



I recently posted a query on comp.lang.python and got a very useful response with some example pyparsing code for my regex issue. I've been messing around with that example in an attempt to understand how it's working and get familiar with it so I'm quite new to pyparsing.



In it's briefest form a code snipper follows:





    # interesting during parsing but not wanted in results
    LBRACE,RBRACE,EQ = map(Suppress,'{}=')
    
    # generic function used for all 'key =' fields
    keylabel = lambda s : Literal(s) + EQ
    
    GROUP_SOURCE = 'source'
    GROUP_SRC_VENDOR_ID = 'source_vendor_id'
    GROUP_SRC_DEVICE_ID = 'source_device_id'
    GROUP_SRC_INSTANCE_ID = 'source_instance_id'
    
    grp_source = keylabel('source') + Combine(Word(alphanums,max=8)(GROUP_SRC_VENDOR_ID) + '-' + Word(alphanums,max=8)(GROUP_SRC_DEVICE_ID) + '.' + Word(alphanums,max=16)(GROUP_SRC_INSTANCE_ID))(GROUP_SOURCE)
    



If 'msg' is my ParseResults object returned from a parseString() call I can currently access msg.source. I would have expected that I could also access msg.source_vendor_id, msg.source_device_id and msg.source_instance_id. However, these just return an empty string

msg.source returns me the full 'source=vendor-device.instance' string but I don't seem to be able to access the smaller component groups that make up the 'source' group.



Is assigning names to these internal components merely for debugging/exception purposes or should I be able to access results with these names in the msg object? Does the outer name (GROUP_SOURCE in this instance) override the inner group names?



Thanks,

Chris

#### 2008-08-12 17:32:33 - ptmcg
Try 'print msg.source.source_device_id'



To see all keys and nested structures, print msg.dump().



-- Paul
#### 2008-08-12 17:56:08 - chris_laws
OK that was simple. 

It was late at night - that's my excuse and I'm sticking to it!



Thanks,

Chris

---
## 2008-08-14 16:05:06 - ecir-hana - OT: LPEG
Hello,



I'm just watching a video about Parsing Expression Grammars for LUA, maybe it could be of some interest for you....





#### 2008-08-15 08:38:25 - ptmcg
Thanks for posting this, it is very interesting.  I watched the first half last night, hope to watch the rest this weekend.



-- Paul

---
## 2008-09-02 18:49:22 - luanzhu - Word match and searchString (newbie question)
Hi, 



I just found out pyparsing and really like what I have seen so far. Here I have a question with searchString function:



    >>>from pyparsing import *
    >>>word = Word(nums, printables)
    >>> word.searchString('12 t56')
    ([(['12'], {}), (['56'], {})], {})



It seems like the digits '56' in the word 't56' is also matched. Is there any way to let pyparsing to match '12' only? My intention is to match any word started with a digit only.



Any comments will be appreciated!



Thanks a lot,

LZ

#### 2008-09-02 20:46:00 - ptmcg
searchString is pretty thorough in stepping through the input string looking for matches.  If you only want to match character groups that start with numbers, then prefix your Word definition with a WordStart() expression:



    >>> from pyparsing import *
    >>> word = WordStart() + Word(nums,printables)
    >>> word.searchString('12 t56')
    ([(['12'], {})], {})



Unlike other parsing packages, pyparsing does not just return nested lists of strings, but instead returns ParseResults objects.  However, the repr string for ParseResults includes both the list of strings and the dict of any results names.  If you just want to see the matched strings, then use asList():



    >>> word.searchString('12 t56').asList()
    [['12']]
    >>> word.searchString('12 t56 34z2_slkdf').asList()
    [['12'], ['34z2_slkdf']]



-- Paul
#### 2008-09-03 11:52:04 - luanzhu
Thank you so much!



This is exactly what I need! I am just amazed how easy pyparsing is to use, and how powerful it is at the same time.



Thanks again. Keep up the good work!



LZhu
#### 2008-09-04 05:13:45 - ptmcg
If you've not yet done so, download either the source or docs distributions from SourceForge.  These contain more detailed documentation for pyparsing than what you get by just using easy_install or the Win32 binary executable installer.  Or check out the Documentation page, with links to other online articles and pyparsing resources.



I also just had my second pyparsing article published in Python Magazine!



-- Paul
#### 2008-09-05 08:06:49 - luanzhu
I just downloaded the source as you suggested. Thanks.



Also, congratulations to your recent publication! :) I am looking forward to reading the article.



Thanks.

---
## 2008-09-05 06:48:43 - Leonidas-from-XIV - Handle \r\n like \n?
Hi,



I have the following code (I already got help with blank via c.l.p), but I am using it in a web-context where I get \r\n as line-separator instead of \n. When I replace \r\n by \n prior to parsing, it works, but I'd prefer a solution which would handle \r itself. I tried different things like changing the whitespaceChars or creating a suppressed White('\r) token (and taking \r out of DefaultWhitespaceChars).





    from pyparsing import *
    
    ParserElement.setDefaultWhitespaceChars(' \t\r')
    
    w = Word(alphas)
    eol = LineEnd().suppress()
    blank =
    (LineStart() + LineEnd()).setParseAction(replaceWith('<PAGEBREAK>'))
    parser = OneOrMore(w | blank | eol)
    
    # works
    parser.parseString('A\n\nB')
    # does not work
    parser.parseString('A\r\n\r\nB')

I'd expect that \r get filtered out because I used setDefaultWhitespaceChars but this does not seem to be the case.



regards,

Marek

#### 2008-09-05 08:05:28 - ptmcg
Aha!  You have uncovered a bug in pyparsing!  The initializers for LineStart and LineEnd explicitly set their set of whitespace characters to ' \t', with this statement:



    self.setWhitespaceChars( ' \t' )



with the intention of removing '\n' from the list of whitespace characters.  I've changed this to:



    self.setWhitespaceChars( ParserElement.DEFAULT_WHITE_CHARS.replace('\n','') )



so that any setting of default whitespace will be suitably respected by the created LineStart and LineEnd instances.



The second problem here is that LineStart is especially finicky about detecting the beginning of the line.  If a LineStart advances past any leading whitespace on the line, then it will fail to match.  So this string:



    'A\r\n\r\nB'

will not match LineStart() after reading the first \n, since there is a leading \r on that line.



Here are two suggestions on how to address this:

1. Disable whitespace skipping for the LineStart() at the beginning of the definition of blank with:



    blank = (LineStart().leaveWhitespace() + eol).setParseAction(replaceWith('<PAGEBREAK>'))

or 2: Instead of defining blank as a LineStart and a LineEnd, define it as a LineEnd that is followed by another LineEnd (without consuming the second LineEnd) - do this using the FollowedBy lookahead class:



    blank = (eol + FollowedBy(eol)).setParseAction(replaceWith('<PAGEBREAK>'))



-- Paul
#### 2008-09-05 08:36:43 - Leonidas-from-XIV
Great, now it works just fine (I took the first option), thanks a lot! I tried to understand how LineStart is checking whether it is really at the start of a line. My understanding is that LineStart should theck for \n and still accept any leading whitespace that is in LineStart().whiteChars.



I am trying to understand the first option, but I seem to run into problems, namely leaveWhitespace() works exactly the other way round than I would think the following:



A\r\n\r\nB - That would mean that A is matched (it is), then there's \r which is a whitespace, then there is \n - LineStart() would fit, but there is \r\n in the queue since \r wasn't consumed and LineStart() would see \n only as it ignores \r. Now it would match. Then comes another \n which gets supressed (eol). With leaveWhitespaces() I'd think that LineStart wouldn't filter out the \r out of the \r\n and couldn't match. But obviously, the opposite happens. Could you please tell me where I'm going wrong? I'm trying to understand it better.



regards,

Marek
#### 2008-09-05 09:36:13 - ptmcg
leaveWhitespace() means 'don't skip over any whitespace before trying to match this expression'.  To follow the parsing action as it happens, use setName and setDebug to have pyparsing emit debugging lines as it works through the parser:



    w.setName('w').setDebug()
    eol.setName('eol').setDebug()
    blank.setName('blank').setDebug()



This will return this output:



    Match w at loc 0(1,1)
    Matched w -> ['A']
    Match w at loc 1(1,2)
    Exception raised:Expected w (at char 2), (line:1, col:1)
    Match blank at loc 1(1,2)
    Exception raised:Expected start of line (at char 1), (line:1, col:2)
    Match eol at loc 1(1,2)
    Matched eol -> []
    Match w at loc 3(2,1)
    Exception raised:Expected w (at char 4), (line:2, col:1)
    Match blank at loc 3(2,1)
    Match eol at loc 3(2,1)
    Matched eol -> []
    Matched blank -> ['<PAGEBREAK>']
    Match w at loc 5(3,1)
    Matched w -> ['B']
    Match w at loc 6(3,2)
    Exception raised:Expected w (at char 6), (line:3, col:2)
    Match blank at loc 6(3,2)
    Exception raised:Expected start of line (at char 6), (line:3, col:2)
    Match eol at loc 6(3,2)
    Matched eol -> []
    Match w at loc 7(3,3)
    Exception raised:Expected w (at char 7), (line:3, col:3)
    Match blank at loc 7(3,3)
    Exception raised:Expected start of line (at char 6), (line:3, col:2)
    Match eol at loc 7(3,3)
    Exception raised:Expected end of line (at char 7), (line:3, col:3)
    ['A', '<PAGEBREAK>', 'B']



In plain English, here is the sequence of events to match the string 'A\r\n\r\nB':

1. Try to match a word after skipping over whitespace - matches 'A'

2. Advance to first '\r'

3. Try to match a word after skipping over whitespace - no match

4. Try to match a blank (don't skip whitespace) - not at line start, so fail

5. Try to match an eol after skipping whitespace - match!

6. Advance to second '\r'

7. Try to match a word after skipping over whitespace - no match

8. Try to match a blank (don't skip whitespace) - match!

9. Advance to 'B'

10. Try to match a word after skipping over whitespace - matches 'B'

11. Try to match a word after skipping over whitespace - fail

12. Try to match a blank (don't skip whitespace) - fail

13. Try to match an eol after skipping over whitespace - match!

14. Advance to end of string

15. Try to match a word after skipping over whitespace - fail

16. Try to match a blank (don't skip whitespace) - fail

17. Try to match an eol after skipping over whitespace - fail

18. Parsing ends



-- Paul

---
## 2008-09-18 00:13:57 - chrisdew - adding autocomplete to pyparsing
I'm looking to add an autocomplete feature to pyparsing.  



The idea is, that instead of raising an exception on incomplete (not incorrect) input, it will instead return a list of the possible words (literals) or letters.  



I have got a very naive extension to 'Or' working, so that it aggregates possible labels and returns them up through the parser.



Does any one have a better way to write autocomplete?  My way seems to be very hacky (in a bad way) - as I'm abusing Exceptions to carry and aggregate data back up through the parser.  Also my approach of hacking at a copy of pyparsing.py causes the original non-autocompleting functionality to be slightly less efficient.  



Anyone else interested in autocompletion?



Has similar work been done before?

#### 2008-09-18 00:48:48 - ptmcg
Yes, this came up on the pyparsing mailing list last January.  Here is the thread in the online archive, 'How can I determine if a fragment is valid?' ().  It includes a code sample, without requiring any patch to pyparsing.



-- Paul
#### 2008-09-18 01:42:05 - chrisdew
Thanks, interesting link, though what they're talking about doesn't return the possible extensions to the input.

With a pyparser which allows either two literals, if the input is empty then the error contains only the first literal.  I had to change some comparisons from > to >= in 'Or' to enable the aggregating of both possibilities.

I've just listed some examples below: - I hope the formatting is correct...

Currently:

    [[>>> from pyparsing import *>>> parser = Literal('foobar') ^ Literal('food')>>> parser.parseString('f')    raise maxExceptionpyparsing.ParseException: Expected 'foobar' (at char 0), (line:1, col:1)]]

Hacked 'Or':

    [[>>> from pyparsing2 import *>>> parser = Literal('foobar') ^ Literal('food')>>> parser.parseString('f')Exception: Expected 'food' (at char 0), (line:1, col:1)Expected 'foobar' (at char 0), (line:1, col:1)]]

Wanted:

    [[>>> from pyparsing2 import *>>> parser = Literal('foobar') ^ Literal('food')>>> parser.parseString('f')['oobar', 'ood']               # or perhaps[('foobar', 0), ('food', 0)]   # where 0 is position of possible token]]
#### 2008-09-18 01:46:27 - chrisdew
next try at formatting...





    Currently:
    >>> from pyparsing import *
    >>> parser = Literal('foobar') ^ Literal('food')
    >>> parser.parseString('f') 
    raise maxExceptionpyparsing.ParseException: Expected 'foobar' (at char 0), (line:1, col:1)
    
    Hacked 'Or':
    >>> from pyparsing2 import *
    >>> parser = Literal('foobar') ^ Literal('food')
    >>> parser.parseString('f')
    Exception: Expected 'food' (at char 0), (line:1, col:1)Expected 'foobar' (at char 0), (line:1, col:1)
    
    Wanted:
    >>> from pyparsing2 import *
    >>> parser = Literal('foobar') ^ Literal('food')
    >>> parser.parseString('f')
    ['oobar', 'ood']             # or perhaps
    [('foobar', 0), ('food', 0)] # where 0 is position of possible token



---
## 2008-09-19 02:23:16 - kib2 - problem with the given pgn parser
Hi,



I've got an error in trying to parse the given chess PGN string (wich is valid) :





    # pgn.py rel. 1.1 17-sep-2004
    #
    # Demonstration of the parsing module, implementing a pgn parser.
    #
    # The aim of this parser is not to support database application,
    # but to create automagically a pgn annotated reading the log console file
    # of a lecture of ICC (Internet Chess Club), saved by Blitzin.
    # Of course you can modify the Abstract Syntax Tree to your purpose.
    #
    # Copyright 2004, by Alberto Santini http://www.albertosantini.it/chess/
    #
    from pyparsing import alphanums, nums, quotedString
    from pyparsing import Combine, Forward, Group, Literal, oneOf, OneOrMore, Optional, Suppress, ZeroOrMore, White, Word
    from pyparsing import ParseException
    
    #
    # define pgn grammar
    #
    
    tag = Suppress('[') + Word(alphanums) + Combine(quotedString) + Suppress(']')
    comment = Suppress('{') + Word(alphanums + ' ') + Suppress('}')
    
    dot = Literal('.')
    piece = oneOf('K Q B N R')
    file_coord = oneOf('a b c d e f g h')
    rank_coord = oneOf('1 2 3 4 5 6 7 8')
    capture = oneOf('x :')
    promote = Literal('=')
    castle_queenside = Literal('O-O-O') | Literal('0-0-0') | Literal('o-o-o')
    castle_kingside = Literal('O-O') | Literal('0-0') | Literal('o-o')
    
    move_number = Optional(comment) + Word(nums) + dot
    m1 = file_coord + rank_coord # pawn move e.g. d4
    m2 = file_coord + capture + file_coord + rank_coord # pawn capture move e.g. dxe5
    m3 = file_coord + '8' + promote + piece # pawn promotion e.g. e8=Q
    m4 = piece + file_coord + rank_coord # piece move e.g. Be6
    m5 = piece + file_coord + file_coord + rank_coord # piece move e.g. Nbd2
    m6 = piece + rank_coord + file_coord + rank_coord # piece move e.g. R4a7
    m7 = piece + capture + file_coord + rank_coord # piece capture move e.g. Bxh7
    m8 = castle_queenside | castle_kingside # castling e.g. o-o
    
    check = oneOf('+ ++')
    mate = Literal('#')
    annotation = Word('!?', max=2)
    nag = ' $' + Word(nums)
    decoration = check | mate | annotation | nag
    
    variant = Forward()
    half_move = Combine((m3 | m1 | m2 | m4 | m5 | m6 | m7 | m8) + Optional(decoration)) \
      + Optional(comment) + Optional(variant)
    move = Suppress(move_number) + half_move + Optional(half_move)
    variant << '(' + OneOrMore(move) + ')'
    # grouping the plies (half-moves) for each move: useful to group annotations, variants...
    # suggested by Paul McGuire :)
    move = Group(Suppress(move_number) + half_move + Optional(half_move))
    variant << Group('(' + OneOrMore(move) + ')')
    game_terminator = oneOf('1-0 0-1 1/2-1/2 *')
    
    pgnGrammar = Suppress(ZeroOrMore(tag))  + ZeroOrMore(move) + Suppress(game_terminator)
    
    def parsePGN( pgn, bnf=pgnGrammar, fn=None ):
      try:
        return bnf.parseString( pgn )
      except ParseException, err:
        print err.line
        print ' '*(err.column-1) + '^'
        print err
    
    if __name__ == '__main__`:
      # input string
      pgn = '''[Event 'GMA, Wijk aan Zee NED']
    [Site '?']
    [Date '2003.??.??']
    [Round '1']
    [White 'Anand,V']
    [Black 'Radjabov,T']
    [Result '1/2']
    [WhiteElo '2750']
    [BlackElo '2620']
    [ECO 'C12']
    [PlyCount '55']
    [Annotator 'Hathaway']
    
    1. e4 e6 { I'm not terribly familiar with the style of Radjabov, so I don't know if this is his usual opening. }
    2. d4 d5 3. Nc3 Nf6 (3...Bb4 
    { The Winawer Variation is probably best, though not as easy to play. }) 4. Bg5
    { threatens e4-e5xf6 }
     (4. e5 
    { keeps pieces on the board and avoids ...dxe4 }) 4...Bb4 (4...Be7 
    { is more common and aims to trade dark-square bishops to ease Black's cramp }) (4...dxe4 
    { aims to avoid any cramp by bringing pieces into alignment for trading, though White does get at least one very good piece (Ne4 or Bg5) and an easier time castling queen-side, to stir up king-side threats }
     5. Nxe4 Be7  (
    { or Rubinstein's }
     5...Nbd7) ) 5. e5 h6 6. Bd2 (6. Bh4 g5 7. exf6 gxh4 
    { Black seems to equalize a little easier after this as he can win Pf6 in exchange for Ph4. }) 6...Bxc3 (6...Nfd7 7. Qg4 
    { and White isn't incurring any weaknesses, but is either gaining Bb4 for Nc3 or after ...Bb4-f8 Black is cramped again }
      (7. Nb5 $5 Bxd2+ 8. Qxd2 a6 9. Na3) ) 7. bxc3 Ne4 8. Qg4
    { White immediately takes aim at the backward Pg7 & Rh8 and usually Pf7 & Ke8. For the moment Bd2 serves to defend Pc3 and to prevent ...Qd8-g5 (offering a queen trade to end the pressure) . }
     (
    { While }
     8. h4 
    { is often useful in the French Defense with this pawn structure, I don't know that it's been tried in this opening on this move. }) 8...g6 9. Bd3 (9. h4 
    { could take over for Bd2 in guarding g5 and preparing a later attack by f2-f4, h4-h5 or vice versa. It also would allow Rh1 to develop to build the direct frontal threats to Pf7 & Pg6. }
     9...c5 10. Bd3 Nxd2 11. Kxd2 Qa5 12. dxc5 Qxc5 13. Ne2 Qxf2 $4 14. Raf1 Qc5 15. Bxg6 fxg6 16. Qxg6+)  (9. Qd1 
    { Fritz7; Odd! }) 9...Nxd2 10. Kxd2 c5 11. Nf3
    { This has been considered the main line for many years, but I wonder if White can allow ...c5-c4 and not use more pawns to fight through Black's pawns. }
     (11. dxc5 
    { is probably still wrong because of ...Qg5+ }) (11. h4 
    { still makes sense }) 11...Bd7 (11...c4 $6 
    { The problem with this is that however much it slows White, it also limits Black's queen-side offensive possibilities. }) (
    { Prematurely playing }
     11...cxd4 
    { lets White straighten-out his pawns and Black has made no real progress. }
     12. cxd4)  (11...Qa5 $5 
    { Fritz7: with the idea of ...cxd4 }) 12. dxc5 Qe7 13. Rab1 Bc6 14. Nd4 Nd7
    { These last few moves have been quite unusual for a French Defense, but they make sense; Qe7 defends Pf7 while Bc6 defends Pb7 and Nd7 threatens Pc5 & Pe5. }
    15. Rhe1 (15. Nxc6 bxc6 16. Rb7 Qxc5 17. Qf4 g5 18. Qd4 Qa5 19. Rb2 c5 $11 
    { Fritz7 }) 15...Nxc5 16. Re3
    { another way of getting the rook into position, in front of the king-side pawns, to threaten Black's king-side pawns }
    16...h5 17. Qg3 O-O-O
    { After this it would seem Black's pieces can handle any threats White can generate. However, black might also have ideas of winning. How might he do that? Well, ...Be8, ...Kc8-b8-a8, ...Rd8-c8, ...Nc5-a4 and Pc3 is a target (slow I know) . Another idea is to keep Kd2 from ever escaping to safety by advancing ...h5-h4-h3 to break open the king-side and open the h-file for Black's rooks. }
     (17...h4 $15 
    { Fritz7 }) (17...Nxd3 $15 
    { Fritz7 }) 18. Ke1 Qc7 (18...h4 19. Qg4 Rh5) 19. h4
    { Anand aims to keep the king-side perfectly safe to ensure a draw. }
     (19. Qh4 
    { Fritz7 }) 19...Qa5 20. Kf1 (20. Nxc6 bxc6 21. Kf1 Kd7 20. Qf4 Ke8 $11 
    { Fritz7 }) 20...Rd7 (
    { Premature is }
     20...Qxa2 21. Ree1 Qa5  (21...Ba4 $11 
    { Fritz7 })  22. Ra1 Qxc3 23. Nxc6 bxc6 24. Ba6+ $18) 21. Qf4
    { This general activity is perfect. It threatens Pf7, defends Nd4 and in some cases prepares for Qf4-b4 to attack Kc8. }
     (21. Ree1 
    { Fritz7 }) (21. Nxc6 bxc6 22. Ree1 
    { Fritz7 }) 21...Rhd8
    { Black is probably wondering why he organized his pieces to only defend light squares. Only Qa5 and Nc5 can get to dark squares and that makes White's task of coordinating much easier. }
     (21...Qxa2 
    { still premature }
     22. Nxc6 bxc6 23. Qb4 Nb7 24. Ree1)  (21...Qxc3 $4 22. Nxc6 bxc6 23. Ba6+)  (21...Rc7 $14 
    { Fritz7 }) (21...Na4 $14 
    { Fritz7 }) 22. Kg1 (22. Nxc6 bxc6 23. Qb4 Qxb4 24. cxb4 d4 25. Ree1 Na4 $11 
    { Fritz7 }) 22...Nxd3 23. Rxd3 (23. cxd3 Qxc3 24. Rg3 Rc7 $14 
    { Fritz7 }) 23...Qc5 (23...Qxa2 24. Rdd1 Qc4 $11 
    { Fritz7 }) 24. Rb4 a5 $2 (24...Rc7 
    { Mark and Fritz7 agree! }) 25. Rb1 Rc7 26. Qc1 Be8 27. Nb3 (27. Qb2 
    { If White commits too quickly to the b-file then Black might actually create some play against Ph4 and on the c-file. }
     27...Qe7  (27...a4 $11 
    { Fritz7 })  28. Nf3 Rc4 
    { possibly preparing ...b5 }) 27...Qb6 (27...Qc4 28. Nxa5 Qxh4 $14 
    { Fritz7 }) 28. Nd4
    { Black created the weakness (Pa5) and can't quite defend it, so Anand forces a draw. }
    1/2-1/2'''
    
      # parse input string
      tokens = parsePGN(pgn, pgnGrammar)
      print 'tokens = ', tokens
    



PyParsing is returning :


    1. e4 e6 { I'm not terribly familiar with the style of Radjabov, so I don't know if this is his usual opening. }
              ^
     Expected Re:('1\\-0|0\\-1|1\/2\\-1\/2|\\*') (at char 226), (line:14, col:10)

    tokens =  None
    



Have you got any idea ? Thanks.

#### 2008-09-19 02:44:54 - kib2
Hum...



it seems to me that comments are not handled correctly.

A comment can contain *anything*, in particular '-',''',etc.
#### 2008-09-19 03:01:07 - ptmcg
I'm guessing that they can't contain a '}', though, since that would be considered the end of the comment.



Sounds to me like a good case for using CharsNotIn.  Try this:



    comment = Suppress('{') + CharsNotIn('}') + Suppress('}')



It is also interesting to me that, unlike programming languages, it appears that PGN only permits comments in certain places - at least according to this submitted grammar.  Ordinarily, comments are permitted (and skipped over) anywhere by using the ignore method:



    top_level_grammar_element.ignore(comment)



Would this also be more appropriate for PGN?  If so, then the various places Optional(comment) occur in the grammar could be removed, and a single:



    pgnGrammar.ignore(comment)



would take care of things.



-- Paul
#### 2008-09-19 03:30:10 - kib2
Thanks Paul, 



there's a great post here :  about pgn comments. They can appear even before the game starts !



but now, there is also other problems :

a black move can start with '...', ie '2...Nf6' and now the parser fails with :


    2. d4 d5 3. Nc3 Nf6 (3...Bb4 
                        ^
    Expected Re:('1\\-0|0\\-1|1\/2\\-1\/2|\\*') (at char 351), (line:15, col:21)
    
    tokens =  None

I've changed the move_number so that it's now :

    move_number = Optional(comment) + Word(nums) + OneOrMore(dot)

but the same error comes out.
#### 2008-09-20 00:19:13 - ptmcg
At first glance, I don't think the problem is in move_number, but in half_move.  In variants, a half move can be shown just as a couple of dots.  Try modifying half_move to:



    half_move = Combine((m3 | m1 | m2 | m4 | m5 | m6 | m7 | m8 | Word('.')) + Optional(decoration)) \
      + Optional(comment) + Optional(variant)



-- Paul

---
## 2008-09-20 16:35:57 - pilsn - Dict: why are int converted to string?
In pyparsing 1.5, in class Dict, integer tokens can't be keys of the dictionnary but are converted to strings (with _ustr function).



What is the reason of this behaviour? 

I think this is counter-intuitive as those keys are already parsed items, moreover python dictionaries can have integers as keys without problem...

#### 2008-09-20 17:15:30 - ptmcg
This is because ParseResults support both dict and list semantics, which Python dictionaries don't.  When a ParseResults has `__getitem__` called with an int argument, it has to assume that this is going to be a list index.  If ints might also be results name keys, then it is ambiguous as to which token should be returned.



-- Paul
#### 2008-09-20 17:42:43 - pilsn
Oh,yes sorry, it's getting late here..

---
## 2008-09-22 05:33:29 - nnathan - Adding exception/parse failures in semantics
Hi,



I'm new to pyparsing (about 4 hours in having done some reading), and I'm wondering what is a nice way to add exceptions to parsing. I'm trying to parse IP addresses in dotted-quad ('nnn.nnn.nnn.nnn') or CIDR notation ('nnn.nnn.nnn.nnn / (nn | nnn.nnn.nnn.nnn)').



I'm just starting out, but I want to encode some semantics, such as 'nnn' be in the range 0-255.



I have a very quick parser I'm playing with:





    #!/usr/bin/python
    
    from pyparsing import nums, hexnums, Word, Combine, Optional
    
    octet = Word(nums, min=1, max=3)
    dotquad = Combine(octet + ('.'+ octet)*3)
    
    print dotquad.parseString('255.255.255.255')
    



So far so good...



I want to add a parseAction function to octet so that it does bounds checking. But should I have it raise an ParseException or raise my own?



Thanks in advance for any advice.

#### 2008-09-22 07:06:32 - nnathan
I modified the code and I went with raising an exception in the parse action.





    from pyparsing import nums, hexnums, Word, Combine, Optional, ParseException, Suppress, traceParseAction
    
    def parse_octet(p):
            d = int(p[0])
            if d < 0 or d > 255:
                    raise ParseException(p[0]+' is an invalid octet')
    
            return d
    
    octet = Word(nums, min=1, max=3).setParseAction(parse_octet)
    dotquad = octet + (Suppress('.')+ octet)*3
    
    print dotquad.parseString('4.2.2.1')
    print dotquad.parseString('4.2.300.1')



I get the following exception:

`pyparsing.ParseException: 300 is not a valid octet (at char 0), (line:1, col:1)`



Is there a way to suppress the '(at char 0), (line:1, col:1)' or make widen the context as to where the problem appears in the original string being parsed ('300.2.2.1')?
#### 2008-09-22 08:53:27 - ptmcg
Doing this kind of additional validation is a perfect use for a parse action.



To add the location of the error, you'll have to use the 3-argument form of the ParseException constructor, passing in the original parse string and the current parsing location.  To get these, you'll have to similarly expand the argument list of your parse action:



    def parse_octet(instr, loc, p):
            d = int(p[0])
            if d > 255:
                    raise ParseException(instr,loc,p[0]+' is an invalid octet')
            return d



I also removed the test for 'if d < 0', since there is no way that your definition of octet would match a negative number.



Looks like you are off to a good start!



-- Paul
#### 2008-09-23 06:35:34 - nnathan
Thanks for the help & encouragement.



I have made further progress but am having issues with setFailAction. The following doesn't fire `fail_v4addr` when I parse `'4.2.2.1 / 255.'` or `'4.2.2'`.



I still do get the expecting end of input exception, but I was hoping to make the details more granular.





    from pyparsing import nums, hexnums, Word, Combine, Optional, ParseFatalException, ParseException, replaceWith, Suppress, traceParseAction, ParseResults, StringEnd
    
    
    def parse_v4prefix(instr, loc, p):
            prefix_len = int(p[0])
    
            if prefix_len > 32:
                    raise ParseException(instr, loc, 'Invalid prefix length: %d' % prefix_len)
    
            return prefix_len
    
    def parse_netmask(instr, loc, p):
            prefix_len = 0
            contiguous = True
            e = ParseException(instr, loc, 'Invalid Subnet Mask %d.%d.%d.%d' % tuple(p))
    
            for x in p:
                    if x > 0 and not contiguous:
                            raise e
    
                    for b in reversed(xrange(8)):
                            if x & (1<<b):
                                    if contiguous:
                                            prefix_len += 1
                                    else:
                                            raise e
                            else:
                                    contiguous = False
    
            return ParseResults([prefix_len])
    
    def fail_v4addr(s, loc, expr, err):
            raise ParseFatalException('Invalid IPv4 Address')
    
    def parse_octet(instr, loc, p):
            d = int(p[0])
    
            if d > 255:
                    raise ParseException(instr, loc, p[0]+' is an invalid octet')
    
            return d
    
    v4prefix = Word(nums, min=1, max=2).setParseAction(parse_v4prefix)
    octet = Word(nums, min=1, max=3).setParseAction(parse_octet)
    v4addr = (octet + (Suppress('.')+ octet)*3).setFailAction(fail_v4addr).setParseAction(lambda x: [tuple(x)])
    v4netmask = v4addr.copy().setParseAction(parse_netmask)
    v4cidr =  v4addr + Suppress('/') + v4prefix ^ v4addr + Optional(Suppress('/')) + v4netmask
    cidr = (v4addr.copy().setParseAction(lambda x: [tuple(x), 32]) ^ v4cidr) + StringEnd()



Also, is it possible ot just have a dict return all the necessary information. I'm toying around with an idea, that the parser ends up returning a dict such as:





    {
      'version': 4
      'address': (4, 2, 2, 1),
      'prefix_len': 27
    }



This intermediate form would certainly improve what I'm trying to achieve as my end result, which is creating a class that providers CIDR objects so I can do operations such as use the 'in' operator to see if one CIDR is in another, such as: 4.2.2.0/24 is in 0.0.0.0/0 etc.



I'm having such a blast with pyparsing though. I've always struggled to grasp yacc/bison. Far too much syntax on top of syntax. I feel very much at home with pyparsing.
#### 2008-09-23 06:40:23 - nnathan
Actually toying around with the above fail_v4addr, I notice it does get fired. But I'm having issues terminating the parsing by raising ParseException or ParseFatalException. Going by the docs, if I am to raise ParseFatalException it should stop parsing immediately. Or does it only stop parsing that element and continue?
#### 2008-09-23 06:54:05 - ptmcg
Hmmm, ParseFatalException is supposed to be fatal, as in complete stopping of all parsing.  I'll have to look into that.



I don't have a lot of time this morning for a detailed reply, but let me steer you toward a couple of pyparsing concepts:



1. You can get your dict behavior using results names.  Check out some of the examples on the pyparsing wiki, or some of the documentation available online.  (I think even the CHANGES file has some notes on results names.)



2. A recent addition to pyparsing is the '-' operator.  '-' is a variation on '+', in that it does not do any backtracking.  '-' is good for stopping parsing at a reasonable place.  For instance, if you had a syntax that had a leading label 'IP address:' followed by your v4addr expression, then you would expect that after successfully parsing 'IP address:' in the input string, any error in parsing a following IP address would be an obvious error, no point in continuing parsing.  So instead of writing 



    Literal('IP address:') + v4addr

write



    Literal('IP address:') - v4addr



-- Paul

---
## 2008-09-24 21:55:50 - dminor14 - What is your license
Hi,

I'm working for a hardware manufacturer. I'm considering using pyparsing as part of in in-house development tool. The code would never be shipped with any of our products. Nor would the tool ever be sold. Could you give me your opinion about this use as well as a link to your actual license?

Thanks,

David Minor

Orbotech (www.orbotech.com)

#### 2008-09-25 21:16:40 - ptmcg
Pyparsing is distributed under the MIT license, which has no restrictions on any use, commercial or otherwise.



Still, there is nothing that would prevent you from making a donation to the project anyway. :)  But that is entirely up to you.



The license is included in the pyparsing source code header, and also in a separate LICENSE file that is included with the source distribution.



You may be confused by one of the grammars that is available upon request, the Verilog grammar.  This grammar took me quite a while to write, and so I only distribute it free for non-commercial use - of someone were to incorporate it into a hardware design tool or service, then I would want them to buy a commercial license for that grammar.  But pyparsing itself is really quite liberally licensed.  



Good luck with your project!



-- Paul

---
## 2008-09-29 00:57:59 - chrisdew - completion - questions on extending pyparsing for autocompletion
I have now put some alpha autocompletion code - called 'completion' on the cheeseshop -   The only documentation is the half-dozen doctests in the module documentation - but anyone familiar with using Pyparsing will soon twig.

As completion follows in the spirit of Pyparsing, I would like to get this sort of functionality integrated within pyparsing.  I have a couple of questions:

1. Why does Pyparsing raise exceptions to propagate parse failures  upwards, instead of just returning a ParseResult, with a 'failed' flag (and a list of possible completions).

2. Do people think it would be better to keep autocompletion 'outside' of Pyparsing, but using an identical syntax?  It seems to me that adding autocompletion to Pyparsing would significantly complicate existing code, and lessen the Pyparsing's efficiency, when autocompletion is enabled.

#### 2008-09-29 06:51:32 - ptmcg
Chris -



Answering your second question first, I would like to avoid forking pyparsing if possible, so I would prefer to have autocompletion implemented as something complementary to the existing pyparsing, as opposed to creating a different branch entirely.  This may be possible by writing a wrapper class that can be used around Or's and MatchFirst's, that decorates the returned exception with the list of possible completions.  (But I'm still leery of the content of such a list when the grammar gets to be non-trivial.)



Your first question goes back to Day 0 of pyparsing design.  In earlier days of my career, I wrote a parser similar to what you suggest, having each expression return a success/failure boolean.  This was before exceptions were added to C++, so I essentially mirrored exception handling with my own code.  I've not tried this with pyparsing, but I suspect there would be a serious slowdown of the code.



So as I write this, I may be coming around to your comments in question 2. Autocompletion is not really a mainstream parsing function, and I would not want to burden the normal parsing code with any significant overhead to support it - especially in applications that are not doing anything with autocompletion.  This is why a wrapper class approach is appealing to me.  You could use it for your application, but those not interested in this feature would just simply not add the wrapper class.  Again, I'll take a look at your code and your examples, and see if I get some further ideas.



-- Paul
#### 2008-09-30 02:49:47 - chrisdew
Thanks for your reply.



Would divorcing completion from parsing be a good approach?



(For some reason I thought the only/best way to implement autocompletion was as a optional side-effect of parsing.  Writing 'completion' made me see that it wasn't so difficult to write from scratch.)



I could add a completeString(text) method to each of the classes descended from Parser within Pyparsing.



At some future point, duplicate code between completeString and parseString could be refactored on a class-by-class basis, if some classes have a lot of similarity between their parsing and their autocompletion.



The use case for completeString is different from parseString - completeString will typically only be called with a line's worth of text, to be autocompleted against a 'statement' subset of a complete language grammar.



Could you elaborate on your ideas for a wrapper class?  I currently think that a separate completeString method may be more elegant than wrapper classes for each descendant of Parser.  



For example, autocompletion has to look at all of the failed 'sub-parsers' in each parser, to determine whether the current input could be made valid by the addition of a few characters.  Whereas parseString often just needs to re-raise the last thrown ParseException.  Keeping parseString and completeString separate would avoid any performance/complexity impact on parsing.



Thoughts on the approach of implementing a separate completeString(text) method for each subclass of Parser?



Regards,



Chris.

---
## 2008-09-30 03:29:06 - hgaudecker - nestedExpr with multiple characters in opener/closer
Hi, 



I am trying to write a parser for a language which employs sort-of C-style comments, sort of because /* ... */ nest. 



What seems the obvious thing to me, i.e. writing



    Comment = nestedExpr('/*','*/',None,None)
    Comment.setParseAction(replaceWith(''))
    print Comment.transformString('/* see * what / happens here */ ')



leads to this output:



    /* see * what / happens here */ 



The issue seems to be the CharsNotIn(opener+closer+...) statement when 'content' is assigned a value within the definition of NestedExpr. Might the following be preferable?





    content = (Combine(OneOrMore(~ignoreExpr + 
                                 NotAny(opener) + NotAny(closer) +
                                 CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS,exact=1))
                       ).setParseAction(lambda t:t[0].strip()))



This would work for me, but I might be missing something... 



Many thanks,

Hans-Martin

#### 2008-09-30 17:00:17 - ptmcg
Great post!  I've taken your patch and folded it into the current SVN version (hope to have a 1.5.1 release in the next few days).



Welcome to pyparsing!



-- Paul
#### 2008-10-01 01:56:43 - hgaudecker
Thanks, I'm starting to feel very comfortable here! 



In your updated version there seems to be a minor issue when no ignoreExpr is specified:





    Comment = nestedExpr('/*','*/',None,None)
    Comment.setParseAction(replaceWith(''))
    print Comment.transformString('This /*should*/ go.')



leads to:





    This /*should*/ go.



I must admit I do not fully understand everything what is happening within nestedExpr, but is there any reason for the asymmetry in the assign statements for content depending on whether ignoreExpr is None or not? It seems to work fine if ignoreExpr is not None, so the following just makes the two cases equal except for the ~ignorExpr





    if ignoreExpr is not None:
        content = Combine(OneOrMore(~ignoreExpr + ~Literal(opener) + ~Literal(closer) +
                                    CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS,exact=1))
                         ).setParseAction(lambda t:t[0].strip())
    else:
        content = Combine(OneOrMore(~Literal(opener) + ~Literal(closer) + 
                                    CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS,exact=1))
                         ).setParseAction(lambda t:t[0].strip())



or a bit shorter:





    if ignoreExpr is None:
        ignoreExpr = NoMatch()
    content = Combine(OneOrMore(~ignoreExpr + ~Literal(opener) + ~Literal(closer) +
                                    CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS,exact=1))
                         ).setParseAction(lambda t:t[0].strip())



Again, this works for me but as I am not sure of the inner workings of nestedExpr this may not be a helpful solution...



HM
#### 2008-10-01 22:20:14 - ptmcg
One other comment I meant to make was that you can simplify your code by using expr.suppress() instead of expr.setParseAction(replaceWith('')) - the two are equivalent.



I'll look at nestedExpr again later tonight or tomorrow.



-- Paul
#### 2008-10-01 22:49:54 - ptmcg
Ok, the issues I've been having with nestedExpr stem from my initial concept that nestedExpr was going to be used for nested ()'s, {}'s, []'s and <>'s, not multichar openers and closers.  I seem to remember getting a question about this sometime in the past, and at that time I steered the questioner to use Literals to wrap the expressions, and that he would have to write his own content expression.



Your suggestion to remove the asymmetry between the with and without ignoreExpr cases is the solution, but my concern now is that, in order to generically support any opening and closing nested expression delimiters, we have imposed a performance penalty on the 95% use cases of single-character openers and closers, ()'s and the like.  So I have kept both, adding the optimally performing code for single characters, to be used when single character delimters are given, and then the more generic version we have come up with here, to be used if multichar delimiters are given.  And all this happens in the user's code during parser creation time, so I'm not that worried about performance.  The benefit will come at parse time, since the expression created by nestedExpr will be tuned for single-char or multi-char delims.



I've updated my unit tests, and am updating SVN in the next few minutes.  Thanks again for the help,



-- Paul

---
## 2008-09-30 05:35:02 - onto - converting a yapps 1.1 program to pyparsing
Hi,



I'm new to pyparsing, and I've recently been playing around with the program 'novelwriting':





It uses yapps 1.1, but I would like to write a similar type of program using pyparsing. (Does one already exist?)



Do you know if it's relatively easy to translate a yapps grammar to pyparsing?



Is it possible to translate the 'parametric rules', or the  '@-calls' from the novelwriting grammar to pyparsing?



Thanks for any examples/tips/guidelines!

Regards,

Rich.

#### 2008-10-01 22:54:47 - ptmcg
Rich,



I am not familiar with either yapps or novelwriting, but after looking briefly at the novelwriting web page, there is nothing in the grammar that looks like it would be too difficult to implement in pyparsing.



Parametric rules are going to involve some kind of parseAction that looks up in a running dict of previously defined values what the substitution value should be.  You might look at the Examples page - there is a macro preprocessor example there.



Keep us posted on your progress, and welcome to pyparsing!

-- Paul
#### 2008-10-06 12:44:54 - onto
Thanks for the info - hopefully I'll get some time to work on this soon!

---
## 2008-09-30 06:45:46 - hgaudecker - Newbie Question on LineStart + Whitespace
posted this under 'Getting Help' yesterday, forgive me for putting it in the wrong place... 



I just got started on parsing a language syntax with pyparsing. First step is to get rid of comments -- say we have both the dblSlashComment-type and one where an entire line can be commented out if its first nonwhitespace-character is an asterisk.



My shot at the second time seems to work in isolation alright, but not if combined with the dblSlashComment... The following code:





    teststring = ''' skip one line
    * no whitespace in front, line disappears all right
    
     * some whitespace char: line does not necessarily disappear
    
    '''
    
    astComment = LineStart() + Literal('*') + restOfLine
    astComment.setParseAction(replaceWith(''))
    print 'output 1:' + astComment.transformString(teststring)
    
    fullComment = dblSlashComment | astComment
    fullComment.setParseAction(replaceWith(''))
    print 'output 2:' + fullComment.transformString(teststring)



leads to this output:





    output 1: skip one line
    
    output 2: skip one line
     * some whitespace char: line does not necessarily disappear
    code



Can anybody explain to me what is happening here and how to fix it? (a simple workaround is to strip leading whitespace in advance, but in the medium run I would like to specify the comments via the 'ignore' option and leave the text as is). I searched a bit and played around with leaveWhitespace() and friends but did not come up with an explanation, let alone a solution.



Many thanks,

Hans-Martin


---
## 2008-10-01 14:25:48 - pilsn - copying a parser
Often I want to copy a parser to attach a new action,

currently I do:





    newparser = And([oldparser])
    newparser.setParseAction(doSomeNewTreatment)



Is there a simpler way to do this? 

I saw the method ParserElement.copy but it doesn't seem to work as I expect.



To see an exemple of what kind of thing I wanted to do, see:





I put it back here:





    from pyparsing import Word, nums, And
    
    # parsing an int and 'tagging' to see better that this is not used 
    intp = Word(nums) 
    intp.setParseAction(lambda t: int(t[0])+1000000) 
    
    # parsing 2 intp and returning them : this leads a correct result 
    test1 = intp('i') + intp('j') 
    test1.setParseAction(lambda t: (t.i, t.j)) 
    
    # now, here is the unexpected part: 
    # parsing only 1 intp 
    test2 = intp('i')  # I have to put And([intp])('i') here
    # intp.copy()('i') doesn't seem to work
    test2.setParseAction(lambda t: t.i) 
    
    print test1.parseString(' 1 2 ') 
    print test2.parseString(' 1 ') 
    
    # this returns: 
    # 
    # [(1000001, 1000002)] 
    # ['1'] 
    # 
    # instead of: 
    # 
    # [(1000001, 1000002)] 
    # [1000001] 



#### 2008-10-01 22:09:18 - ptmcg
copy() is not your problem.  Here is the bug:



    test2 = intp('i')
    test2.setParseAction(lambda t: t.i) 



By calling setParseAction, you are replacing the integer conversion lambda with the one that returns t.i.  Try your parser using this call instead:



    test2.addParseAction(lambda t: t.i) 



This *adds* the t.i lambda to the chain of parse actions, instead of replacing the one defined on intp.



You should be able to trace through your code now and see how constructing an And and then attaching parse actions to the enclosing And expression leaves the intp conversion parse action alone, in both test1 and in your undesirable form 'And([intp])('i')'.



-- Paul

---
## 2008-10-01 19:25:24 - eleybourn - SQL Parser
Hey all, 



I'm well on the way to completing a complete SQL parser, and I am very happy with pyparsing. However i'd like some help in possibly improving the speed of the parsing. 



I've included a snipped of the INSERT statement syntax. What can I do to improve the efficeincy. It's fairly good already. It runs in 0.04s per query atm, but I'd like to try and bring it down to 0.01 if possible. 



TTFN

Evan





    '''
    Temporal Proxy
    (c) Looking Glass Solutions 2007
    Licensed under GPL v2
    '''
    
    ## SQL COMMANDS: http://www.postgresql.org/docs/8.3/interactive/sql-commands.html
    ## USE PSYCO
    
    # Importing the required modules
    import os, sys, getopt, string
    from pyparsing.pyparsing import Literal, CaselessLiteral, Word, Upcase, delimitedList, Optional, \
        Combine, Group, alphas, nums, alphanums, ParseException, Forward, oneOf, quotedString, \
        ZeroOrMore, restOfLine, Keyword, commaSeparatedList, CharsNotIn, CaselessKeyword, QuotedString, alphas8bit, \
        NotAny, ParserElement
    import time
    
    try: 
        import psyco 
        psyco.full() 
    except: 
        pass
    
    # Variables #
    LIMIT,GROUP,ORDER,BY,DISTINCT,ALL,RESTRICT,CASCADE,USING,INDEX,TABLESPACE,CREATE,DROP,TABLE,SELECT,INSERT,UPDATE,DELETE,WHERE,AS,SET,FROM,ON,INTO,VALUES,ONLY = map(CaselessKeyword, 'limit group order by distinct all restrict cascade using index tablespace create drop table select insert update delete where as set from on into values only'.split())
    DEFAULT,NULL,TRUE,FALSE = map(CaselessKeyword, 'default null true false'.split())
    NOTNULL = CaselessKeyword('not null')
    E = CaselessLiteral('E')
    
    arithSign = Word('+-',exact=1)
    
    major_keywords = CREATE | DROP | SELECT | INSERT | UPDATE | DELETE | WHERE | AS | SET | FROM | ON | GROUP | ORDER
    realNum = Combine( Optional(arithSign) + ( Word( nums ) + '.' + Optional( Word(nums) ) |
                ( '.' + Word(nums) ) ) + Optional( E + Optional(arithSign) + Word(nums) ) )
    intNum = Combine( Optional(arithSign) + Word( nums ) +
                Optional( E + Optional('+') + Word(nums) ) )
    keywords = DEFAULT | NULL | TRUE | FALSE
    
    comment = '--' + restOfLine
    
    name = ~major_keywords + Word(alphanums + alphas8bit + '_')
    value = realNum | intNum | quotedString | name | keywords # need to add support for alg expressions
    
    
    #INSERT Statement
    '''
        INSERT INTO table [ ( column [, ...] ) ]
        { DEFAULT VALUES | VALUES ( { expression | DEFAULT } [, ...] ) [, ...] | query }
        [ RETURNING * | output_expression [ AS output_name ] [, ...] ]
        '''
    
    ins_columns = Group(delimitedList( name ))
    ins_values = Group(delimitedList( value ))
    # define the grammar
    insert_stmt = INSERT + INTO + name.setResultsName( 'table' ) \
                + Optional( '(' + ins_columns.setResultsName( 'columns' ) + ')') \
                + VALUES + '(' + ins_values.setResultsName( 'vals' ) + ')' + ';'
    insert_stmt.ignore( comment )
    
    def insert(query):
        global insert_stmt
    
        try:
            start = time.time()
            ParserElement.enablePackrat()
            tokens = insert_stmt.parseString( query )
            end = time.time()
            #query = translation.insert(tokens, information_schema, server)
            print '\tPARSE\t', round((end - start), 2), '\t', round((time.time() - end), 2)
        except ParseException:
            return False
        return tokens
    
    tokens = insert('INSERT INTO organisation (organisation_id, name, organisation_type, parent_organisation_id) VALUES ('123','Company','123 Corp','-1');')
    print tokens.dump()



#### 2008-10-01 19:28:14 - eleybourn
On a side note; This parser is being released (open source) as part of a temporal proxy. (). I'd like to release the pyparser component directly to this project/website. How would you like me to provide it. :-)



TTFN

Evan
#### 2008-10-01 22:00:06 - ptmcg
Interestingly enough, it seems that the packrat parsing is actually slowing things down!  I tried commenting out the call to enablePackrat, and my test time went from 0.03 to 0.02.  I tried a few other things, one common 'speedup' that I often try is to use a Regex for integer and real terminals.  Terminals get tried *many* times during parsing, and a single level Regex will run faster than a composite expression.  Here are the Regex's I used, following the pattern of the expressions you started with:



    realNum = Regex(r'[+-]?(\d+\.\d*|\.\d+)([Ee][+-]?\d+)?')
    intNum = Regex(r'[+-]?\d+([Ee][+-]?\d+)?')



Otherwise, no performance issue jump out at me.  (I did notice that you plan to add support for arithmetic expressions in the values list - that will probably slow things down quite a bit, and you'll probably end up re-enabling packratting, since arith expressions have a lot of recursion and retrying of expressions).



The lightest weight way to package pyparsing with your project is to just drop the single pyparsing.py file right in with your source code.



Good luck with your project, and thanks for using pyparsing!



-- Paul
#### 2008-10-02 08:27:05 - ptmcg
I see that you are presenting a paper on this parser at the OSDC - let me know if you want any technical editing help on it.  In any event, please be sure to send me a link once you have given your talk.



Best of luck!



-- Paul

---
## 2008-10-07 11:45:13 - TheGrudge - C++ #include parser
Hi,

I'm trying to re-format nearly 600 sourcecode files for a C++ project.
Right now they have a structure like this:

/*  <h6 id="toc0">==========================</h6>
 * Copyright....
 *  <h6 id="toc1">==========================</h6>
 */

#include <QWidget>
#include <QPushButton>

#include <kcombobox.h>
#include <kapplication.h>

#include 'myself.h'

namespace...
class....

Since there are a lot of those includes in a file and since they are not sorted, you have to look quite long to see if every include is already defined in there. Most of the time there are duplicate entries due to not sorted include statements.

What I want to achieve is something like this:


    /* ==========================
     * Copyright....
     * ==========================
     */
    
    
    // own includes.
    
    #include 'myself.h'
    
    // Qt includes.
    
    #include <QPushButton>
    #include <QWidget>
    
    // KDE includes.
    
    #include <kapplication.h>
    #include <kcombobox.h>
    
    // Local includes.
    
    #include 'mylib1.h'
    #include 'mylib2.h'
    
    namespace...
    class....


I tried to do it with bash / sed / awk and it somehow worked, but not good enough. Since sed is only working on one line at a time, I have no good control over what should be parsed / sorted and what should not be touched.



For example sometimes you have something like this:

    #ifdef HAVE_MARBLEWIDGET
    #include <marble/MarbleWidget.h>
    using namespace Marble;
    #endif // HAVE_MARBLEWIDGET

Now I don't want to extract this include and sort it into one of those include groups.

Now my question: Can I build a parser with pyparsing that 'knows' how to handle such blocks?

In general I need a header block (the first copyright comment), a include block with all the sorted statements (but not those between #ifdef macros) and the sourcecode block to build the new sourcecode file.

Is this possible? The include blocks should be sortable, so mostly the parser should generate an order like this:

    own
    Qt
    KDE
    other libs
    local


Is it easy to define such blocks in pyparsing?



Andi

#### 2008-10-07 11:46:22 - TheGrudge
There are some characters missing in the code (they have been thrown away?) but you should (hopefully) understand what I mean.
#### 2008-10-07 12:55:24 - ptmcg
I would approach this problem in pieces.  Recognizing any one of these bits can certainly be done, but the reformatting and reordering includes within each source file will get complicated.  For one thing, the order of includes in C/C++ can be significant, such as when one include file defines macros used in a later include file.  I assume you have already verified this is either not to be an issue in your source code base, or that you have a plan to address it when it happens.



To your example, yes it is easy to define a pyparsing expression for the #ifdef block you posted:



    #ifdef HAVE_MARBLEWIDGET
    #include <marble/MarbleWidget.h>
    using namespace Marble;
    #endif // HAVE_MARBLEWIDGET



Here is a first pass at such an expression:



    from pyparsing import *
    
    ident = Word(alphas+'_', alphanums+'_')
    include = '#include' + (ident | quotedString | QuotedString('<',endQuoteChar='>'))('target')
    # try some test cases
    assert '#include 'stdio.h'' == include
    assert '#include <xyzzy>' == include
    
    ifdef = '#ifdef' + ident
    endif = Literal('#endif')
    ifdefBlock = ifdef + ZeroOrMore(~endif + restOfLine + lineEnd) + endif
    
    ifdefBlock.ignore(cppStyleComment)
    
    test = '''\
    #ifdef HAVE_MARBLEWIDGET
    #include <marble/MarbleWidget.h>
    using namespace Marble;
    #endif'''
    
    # see if we match the sample text
    assert test == ifdefBlock



This would <em>not</em> handle nested ifdefs though.



Note that I've already included a definition of a general purpose #include statement for you, including the defining of a results name 'target' for the target of the include statement.  You could just try scanning for include statements with code like this:



    test = '''
    #include <xyzzy>
    
    #include 'stdio.h'
    
    #ifdef HAVE_MARBLEWIDGET
    #include <marble/MarbleWidget.h>
    using namespace Marble;
    #endif
    
    #include <QWidget>
    '''
    
    for match in (ifdefBlock|include).searchString(test):
        if match.target:
            print match.target



By searching for ifdefBlock before include, pyparsing's search will match the full block containing any embedded #include statements, so these wont get matched as bare includes.



I hope that's enough to get you going for a while.



Cheers,

-- Paul
#### 2008-10-08 01:23:02 - TheGrudge
> For one thing, the order of includes in C/C++ can be significant,
> such as when one include file defines macros used in a later
> include file. I assume you have already verified this is either
> not to be an issue in your source code base, or that you have a
> plan to address it when it happens.

Hi Paul,

sure I know that the order is important sometimes, if such cases appear, I will fix the order manually. The plan is to sort most of the includes alphabetically if possible, if not reorder the block so that it compiles again.

I will not run the script on the whole sourcecode folder but on each subfolder seperately, so I can do a 'git diff' and see my changes.



Thanks for your examples, I will have a look at them now :-) 

Yesterday I tried to create a big token that just works on the whole file (since sourcecode isn't that big I thought you can just read it into one string), but after that my sourcefile was too messed up :-)



But I'm sure your examples will help me a lot!!



Andi
#### 2008-10-08 07:55:19 - ptmcg
One other thing - since you will be picking apart and reordering lines of code, you may need to keep track of where the #includes are found in the original text.  If you use scanString instead of searchString, you will get a generator that will give you the start and end location of every match of the expression.  For instance, here is the above example using scanString:



    test = '''\
    #include <xyzzy>
    
    #include 'stdio.h'
    
    #ifdef HAVE_MARBLEWIDGET
    #include <marble/MarbleWidget.h>
    using namespace Marble;
    #endif
    
    #include <QWidget>
    '''
    
    for match,start,end in (ifdefBlock|include).scanString(test):
        if match.target:
            # lineno is a function defined in pyparsing
            sourcelinenum = lineno(start, test)
            print match.target, 'found on line', sourcelinenum



prints:



    xyzzy found on line 1
    'stdio.h' found on line 3
    QWidget found on line 10


#### 2008-10-10 11:16:07 - TheGrudge
I'm busy with other stuff right now in this project so the converter script will have to wait for some time now. But thank you again for your help, I'm sure it will give me a great start!



Andi

---
## 2008-10-09 02:50:49 - rhattersley - Recording position of match
I have a complex grammar and associated parseActions which I'm using to create a tree of objects representing the source string. This works beautifully, for which pyparsing (1.5.0) can take all the credit.



For a lot of my matches I need to record the start & end locations within the corresponding object. I've tried using getTokensEndLoc within the parseAction callbacks but that adds an enormous performance overhead - the time to process a simple test file jumps from 0.08s to 13s!



Is there an alternative to using getTokensEndLoc? Is there another change I can make which will improve its speed?



FYI: Running it through the profiler identifies the call



    fstack = inspect.stack()       (pyparsing.py : line 3280)

as responsible for the overwhelming majority of the extra load.

#### 2008-10-09 04:57:43 - rhattersley
Deleting the pyparsing.pyc file that came in the 1.5.0 distribution and allowing it to be re-created (we're running Python 2.5.2) gives a speed up of 5 times.



Adding the tweak to pyparsing.py mentioned in  gives another substantial improvement ... although obviously this is a 'bad thing to do'.
#### 2008-10-09 06:26:17 - ptmcg
Try this:



    from pyparsing import Empty, Combine, delimitedList, Word, nums
    # define an expression that always matches, and simply reports its location
    endLocnMarker = Empty().setParseAction(lambda s,loc,t: loc)
    
    # create a helper method that appends the end marker to an expression, 
    # and gives it a results name to make it easy to get
    def exprWithEndLocn(expr):
        def cleanupTokens(s,l,t):
            del t[-1:]
            t['startOfTokens'] = l
        return ((expr+endLocnMarker('endOfTokens'))
                    .setParseAction(cleanupTokens))
    
    # working example of exprWithEndLocn
    ipAddress = delimitedList(Word(nums),'.',combine=True)('ip_addr')
    
    test = '''This is an IP address 1.1.1.1 and this isn't A.B.C.D
        Here is another 192.168.0.1'''
    
    for match in exprWithEndLocn(ipAddress).scanString(test):
        print match[0].dump()
        print '%(ip_addr)s starting at %(startOfTokens)d and ending at %(endOfTokens)d' % match[0]
    



Prints:



    ['1.1.1.1']
    - endOfTokens: 29
    - ip_addr: 1.1.1.1
    - startOfTokens: 22
    1.1.1.1 starting at 22 and ending at 29
    ['192.168.0.1']
    - endOfTokens: 84
    - ip_addr: 192.168.0.1
    - startOfTokens: 73
    192.168.0.1 starting at 73 and ending at 84



-- Paul
#### 2008-10-13 01:36:39 - rhattersley
Excellent - thank you for the prompt and thorough reply - much appreciated!



It's an elegant solution which I can easily apply to my grammar ... and like many good solutions it seems obvious with hindsight!

---
## 2008-10-12 03:03:10 - steven-d - Ignoring trailing spaces
First time poster, newbie at pyparsing.



I have a grammar which parses text delimited by braces { }. The 

text inside the braces can include whitespace, but leading and 

trailing whitespace is not important. Here's very simplified 

version:





    from pyparsing import *
    
    lbrace = Literal('{').suppress()
    rbrace = Literal('}').suppress()
    astring = Word(alphas + ' \t')
    parser = lbrace + astring + rbrace
    
    s = '    {  alpha    beta gamma    }'
    print s, '=>', parser.parseString(s)



When I run this code, the result is almost what I want.

I get:



    {  alpha    beta gamma    } => ['alpha    beta gamma    ']



but I don't want the spaces following gamma. I can easily call

rstrip() on the result, but is there a better way?





Steven.

#### 2008-10-12 08:16:39 - ptmcg
Steven -



Welcome to pyparsing!  I went through a similar thought process when I wrote an expression for comma-separated lists.  Here is a step-by-step progression through your question.



As a first cut, and as you hinted at, the simplest grammar would be something like this:



    bracedText = lbrace +
        CharsNotIn('}').setParseAction(lambda t: t[0].strip()) + 
        rbrace



But let's try to build up this expression in a different way.



First step, define some tokens for the braces, and words inside the braces.



    lbrace = Literal('{').suppress()
    rbrace = Literal('}').suppress()
    nonrbrace = printables.replace('}','')



First simple parser is to just get the individual words inside the braces:



    bracedText = lbrace + OneOrMore(Word(nonrbrace)) + rbrace
    print bracedText.parseString('    {  alpha    beta gamma    }')



As you can expect, here are the tokens you get:



    ['alpha', 'beta', 'gamma']



Now to get the whitespace between the words.  Pyparsing includes the White class for when you need to explicitly parse whitespace.  So we can expand the contents between the braces to accept the whitespace:



    bracedText = lbrace + 
        OneOrMore(Word(nonrbrace) + Optional(White())) + 
        rbrace



Now we get these tokens:



    ['alpha', '    ', 'beta', ' ', 'gamma', '    ']



But this has two problems: the whitespace and words are not joined together, and we have that trailing bit of whitespace that you don't want.



We could stop here and add a parse action that cleans this up, like this:



    bracedText.setParseAction( lambda t:''.join(t[:-1]) )



And that would be sufficient.  But let's keep going on the path to building up an expression without any post-processing.



To combine all of the tokens into a single string, wrap the OneOrMore expression with a pyparsing Combine:



    bracedText = lbrace + 
        Combine(OneOrMore(Word(nonrbrace) + Optional(White()))) + 
        rbrace



Now to take care of the last whitespace, we can be more specific about the whitespace we want to match.  Specifically, we only want whitespace that is <em>not</em> followed by a '}'.



    bracedText = lbrace + 
        Combine(OneOrMore(Word(nonrbrace) + 
                          Optional(White() + ~FollowedBy('}')))) + 
        rbrace



This does the trick.  We now get this as our resulting tokens:



    ['alpha    beta gamma']



To answer your second post, about how to add an escaped '}' character, you have to do 2 things: remove '\' from the list of printables; and define an escaped character expression to include in the OneOrMore expression:



    nonrbrace = printables.replace('}','').replace('\\','')
    escapedchar = '\\' + Word(printables,exact=1)
    escapedchar.setParseAction(lambda t:t[1])
    
    bracedText = lbrace + 
        Combine(OneOrMore((escapedchar | Word(nonrbrace)) + 
                            Optional(White() + ~FollowedBy('}')))) + 
        rbrace
    
    print bracedText.parseString('    {  alpha    beta \} gam\}ma    }')



Which prints:



    ['alpha    beta } gam}ma']



Cheers,

-- Paul
#### 2008-10-12 15:11:01 - steven-d
Thanks Paul, that's fantastic. That gives me something to work on.



-- 

Steven.
#### 2008-11-04 12:21:51 - mchaput
I think it's common enough that the built-in Word class should at least include a keyword argument to add automatic recognition of escaping (e.g. escapechar='\\'). It's like whitespace processing -- it's much more convenient to not have to do it yourself ;)
#### 2008-11-04 18:50:25 - oyster


    i.setResultsName('something')

will not give the result a name 'something', to do that, we have to use 



    i=i.setResultsName('something')



but



    i.setParseAction(fn)

will set the Action of i to fn. if you don't want that, you must use



    j=i.copy().setParseAction(fn)

then use j to parse
#### 2008-11-04 18:52:20 - oyster
sorry, I mean to post a new topic :(

---
## 2008-10-12 03:29:10 - steven-d - Escaping characters
Here's a very simple toy grammar:





    lbrace = Literal('{').suppress()
    rbrace = Literal('}').suppress()
    aword = Word(alphas)
    astring = Word(alphas)
    parser = lbrace + aword + astring + rbrace



I'd like the astring token to include braces, if I escape 

them first with backslashes. I don't want braces to nest. 

Inside the astring token, braces should be ordinary characters.



To avoid any doubt:

    parsing r'{abc def\{}' should return ['abc', 'def}']
    parsing r'{abc def\g}' should return ['abc', 'defg']
    but parsing '{abc def{}' should fail
    and so should r'{abc\{ def\{}'.

If need be, I can unescape the astring token myself, although

I'd prefer not to.



How do I change the grammar to do that?



Thank you, 





Steven

#### 2008-10-12 15:12:19 - steven-d
For the record:



Paul has given a good answer to this question in the next thread, 'Ignoring trailing spaces'.

---
## 2008-10-16 09:00:00 - dozpav - Recursive Element with parseActions
Hi everybody..

I'm actually in trouble with nested elements and ParseActions.

Using this example string:



/books/book[para ftcontains 'sandokan']/title[. ftcontains 'Adventures']



My wish is to obtain this xml:





    <query>
        <context>/books/book</context>
        <predicate>
           <context>para</context>
           <contains>sandokan</contains>
        </predicate>
        <context>title</context>
        <predicate>
          <context>.</context>
          <contains>Adventures</contains> 
        </predicate>
    </query>
          
Using this semplified form of grammar taken from W3 Full-text xquery


    RelativePathExpr = Group(StepExpr + (ZeroOrMore(oneOf('/ //') + StepExpr))).setParseAction(RelativePathExpr_f).setResultsName('RelativePathExpr')
    StepExpr brings to AxisStep
    AxisStep        = Group((ReverseStep ^ ForwardStep) + Predicate).setResultsName('AxisStep') 
    Predicate       = Group('['+ Expr + FTContainsExpr']').setResultsName('Predicate')

Where Expr contains nested RelativePathExpr + FTContainsExpr

I can't understand how to distinguish the behaviour of my parseAction for the general call 

    '/books/book[para ftcontains 'sandokan']/title[. ftcontains 'Adventures']'

and for the predicate call

    [para ftcontains 'sandokan']



i would avoid to use multiple parseactions for the same rule

Any ideas?



Greetings

#### 2008-10-16 18:51:20 - ptmcg
I'm not sure I can see everything that is going on here, but maybe you could get some additional info using the decorator traceParseAction that is included in pyparsing.  Add it to your code like this:



    @traceParseAction
    def RelativePathExpr_f(tokens):
        ...



-- Paul
#### 2008-10-20 09:16:11 - dozpav
Thanks for you reply, i solved changing my (bad) grammar structure



Greetings

---
## 2008-10-16 17:10:33 - teenwag - pyparsing.py:378: undefined name 'j'
pyparsing-1.5.0]$ pyflakes pyparsing.py 

pyparsing.py:378: undefined name 'j'





    def insert( self, index, insStr ):
            self.__toklist.insert(index, insStr)
            # fixup indices in token dictionary
            for name in self.__tokdict:
                occurrences = self.__tokdict[name]
                for k, (value, position) in enumerate(occurrences):
                    occurrences[k] = _ParseResultsWithOffset(value, position + (position > j))



#### 2008-10-16 18:35:13 - ptmcg
Thanks for the note.  This bug is fixed in the version that is currently in SVN.  I guess it's time to release 1.5.1...



-- Paul
#### 2008-10-18 07:16:00 - ptmcg
I released version 1.5.1 early this morning.  If you upgrade to this version, you should see this problem resolved.



Thanks for using pyparsing - write back if you have comments or questions, or have a cool parser application that you want to brag on. :)



-- Paul

---
## 2008-10-18 19:33:20 - oyster - need help with my BASIC-like language
Hello, everyone. I want to write a translator and interpreter for a language I am using.

I am totally new to write such a thing.

First of all, is pyparsing suitable for me? Because 'The pyparsing module is ... to creating and executing simple grammars', and I think a translator and interpreter is too complex( at least for me)! If not, is there other module I can turn to?



I write the code but of cause there is many bugs. The first 2 blocks me are:

1.



    print rValue.parseString( '1+2',parseAll=False)

gives only ['1']





    print rValue.parseString( '1+2',parseAll=True)
    Traceback (most recent call last):
      File 't1016.py', line 134, in ?
        print rValue.parseString( '1+2',parseAll=True)
      File 'h:\pure_pylib\yacc\pyparsing\pyparsing.py', line 1051, in parseString
        StringEnd()._parse( instring, loc )
      File 'h:\pure_pylib\yacc\pyparsing\pyparsing.py', line 929, in _parseNoCache
        loc,tokens = self.parseImpl( instring, preloc, doActions )
      File 'h:\pure_pylib\yacc\pyparsing\pyparsing.py', line 2116, in parseImpl
        raise exc
    pyparsing.ParseException: Expected end of text (at char 1), (line:1, col:2)





    print rValue.parseString( '\n1+2',parseAll=True)

causes an infinite loop





2.the whole program yields []



Can somebody help me? Thanks in advance.



This is a Basic-like language, so comma is used to seperate the function parameter, and parameter can by omitted but not the comma

The program always starts a function whose name is 'main'.





    Prg->        empty| Funtion{0,}
    
    Function->   funcHead+funcBody
    
    funcHead->   'function' ID ',' Type ',' optional Comment ',' optional Comment
                 ('var' ID ',' Type ',' optional IsByRef ',' optional IsOptional ',' optional IsMatrix, optional Comment){0,}
    
    funcBody->   localVar{0,}
                 Stmt
    
    Stmt->       empty|funCall|rValue|compdStmt|assignStmt|
                 break |continue |end |
                 print Optional(rValue)|
                 returnStmt
    
    funCall->   ID '(' delimitedList ')'
    
    compdStmt-> ifStmt|forStmt|whileStmt|classStmt
    
    ifStmt->    if rValue
                Stmt
                (else if rvalue
                   Stmt){0,}
                (else
                   Stmt){0,1}
                end if
    
    #the 1st rValue is the start value;2nd->to value;3rd->step or 1
    forStmt->   for rValue, rValue,rValue|empty, ID|matrixAccess|empty
                Stmt
                next
    
    whileStmt-> while rValue
                  Stmt
                wend
    
    classStmt-> class ID ( delimitedList  )
                Function{0,}
                end class
    
    assignStmt->ID = rvalue
                |matrixAccess = rvalue
    
    compStmt->  rValue oneOf('= > < >= <= <> !=') rValue
    
    matrixAccess->ID ([ rValue ]){1,}
    
    rValue->    Num |Str |ID |OpUni|matrixAccess
                |funCall
                |( rValue ( oneOf('+ - * /') rValue){0,}  )
                | rValue (oneOf('+ - * /') rValue)(0,}
                |compStmt
    
    OpUni->     oneOf('+ -') rValue
    
    Num, Str, ID, and Type of var are defined by re pattern





    import pprint
    
    from pyparsing import *
    ParserElement.setDefaultWhitespaceChars(' \t')
    
    test='''
    function fib, int
        var x, int
        local a, int
        if x<=2
            return 1
        else
            return fib(x-1)+fib(x-2)
        end if
    
    function main, int
        local i, int
        for 1,5,,i
            print fib(i)
        next
        for 1,5,,
            print '*'
        next
    '''
    
    
    ID=Regex('(?i)a-z[a-z0-9]*')
    
    #string
    AtomStr=dblQuotedString
    
    #number
    AtomDecIntNum=Regex('[+-]{0,1}\d{1,}')
    AtomDecHexNum=Regex('(?i)[+-]{0,1}0x[\da-f]{1,}')
    AtomDecFloatNum=Regex('[+-]{0,1}\d{1,}\.') ^ Regex('\d{0,}\.\d{1,}')
    #AtomDecSciNum=(AtomDecIntNum ^ AtomDecFloatNum) + '(?i)e' + AtomDecIntNum      ####????
    AtomDecSciNum=Regex('(?i)[+-]{0,1}\d{1,}\.?e\d{1,}') ^  Regex('(?i)[+-]{0,1}\d{1,}\.\d{1,}e\d{1,}')
    AtomNum=AtomDecIntNum ^ AtomDecHexNum ^ AtomDecFloatNum ^ AtomDecSciNum
    
    rValue=Forward()
    #uniry operator
    OpUni=oneOf('+ -') + rValue
    
    FunCall=Forward()
    MatrixAccess=Forward()
    OpCmp=Forward()
    #right value
    rValue << (AtomNum |AtomStr |ID|OpUni|MatrixAccess |   FunCall | '(' + rValue + ZeroOrMore(oneOf('+ - * /')+rValue) + ')' | rValue + ZeroOrMore(oneOf('+ - * /')+rValue)|  OpCmp )
    
    #access matrix element
    MatrixAccess << (ID + OneOrMore('[' + rValue +']' ))
    
    #compare
    OpCmp<<(rValue +oneOf('= < > != <>') +rValue|ID|AtomNum|AtomStr)
    
    #assignment
    AssignStmt=ID+'='+rValue|MatrixAccess+'='+rValue
    
    Stmt=Forward()
    #while
    WhileStmt=CaselessKeyword('while') + rValue +'\n'+Stmt+'\n'+CaselessKeyword('wend')
    
    #for
    ForStmt=CaselessKeyword('for') +rValue+','+rValue+','+(rValue|empty)+','+(ID|MatrixAccess|empty)+'\n'\
               + Stmt+'\n'\
               +CaselessKeyword('next')
    
    #if
    IfStmt=CaselessKeyword('if') +rValue+'\n'\
                +Stmt\
            +Optional('\n' +ZeroOrMore(CaselessKeyword('else if')+'\n'+Stmt+'\n'))\
            +Optional(CaselessKeyword('else')+'\n'+ Stmt+'\n') \
            +CaselessKeyword('end if')
    
    #function parameter or var decalre statement
    ParList=empty^delimitedList(rValue)
    
    Fun=Forward()
    #define class
    ClassStmt=CaselessKeyword('class') + ID +'(' +ParList +')'+\
                    OneOrMore(Fun+'\n')+\
                    CaselessKeyword('end class')
    
    #compound stmt
    CompdStmt=WhileStmt|ForStmt|IfStmt|ClassStmt
    
    #function call
    FunCall<<(ID+'('+ParList+')')
    
    #Return
    ReturnStmt=CaselessKeyword('return')+Optional(rValue)
    
    #Statement
    Stmt<<(FunCall|rValue|empty|CompdStmt|AssignStmt|
                CaselessKeyword('pass') |
                CaselessKeyword('break') |CaselessKeyword('continue') |CaselessKeyword('end') |
                CaselessKeyword('print') +Optional(rValue)|
                ReturnStmt|
                Group(Stmt)
                )
    
    #type name
    TypeName=CaselessKeyword('int')|CaselessKeyword('float')|CaselessKeyword('str')|ID
    
    #decalre local var
    LocalVar=ZeroOrMore(CaselessKeyword('local')+ID+','+TypeName+Optional(ParList))
    
    #function body
    FunBody=LocalVar+Stmt
    
    #function head
    FunHead=CaselessKeyword('function')+ID+',' +TypeName+Optional(ParList)
    FunHead+=ZeroOrMore('var'+ID+','+TypeName+Optional(ParList))
    
    #function
    Fun<<(FunHead+FunBody)
    
    #program
    Prg=empty|ZeroOrMore(Fun)
    
    singleLineComment = ''' + restOfLine|CaselessKeyword('rem')+ restOfLine
    Prg.ignore( singleLineComment )
    
    
    tokens = Prg.parseString( test,parseAll=False)
    print 'tokens = '
    pprint.pprint( tokens.asList() )
    
    
    #print rValue.parseString( '1+2',parseAll=False)
    #print rValue.parseString( '1+2',parseAll=True)



#### 2008-10-19 00:02:16 - ptmcg
This is really quite an ambitious project.  There are a lot of things going on here - some are pyparsing warts and gotchas, but some are basic language design and language parser issues.



Just hacking at your program without much overall redesign, I've gotten your Prg expression to get this much from your test fibonacci program:



    ['function',
     'fib',
     ',',
     'int',
     'var',
     'x',
     ',',
     'int',
     'local',
     'a',
     ',',
     'int',
     'if']



Here are the changes I made to get this far:

1. Your regex definition for ID was wrong.  Instead of:



    ID=Regex('(?i)a-z[a-z0-9]*')

I changed it to:



    ID=Regex('(?i)[a-z][a-z0-9]*').setName('identifier')



Why did I add the setName call?  Because when you get a parser exception using the default strings created by pyparsing, you get things like:



    pyparsing.ParseSyntaxException: Expected Re:('(?i)a-z[a-z0-9]*') (at char 10), (line:2, col:10)

It is much nicer-looking to get messages like:



    pyparsing.ParseSyntaxException: Expected identifier (at char 9), (line:1, col:10)



2. The recursive definition of rValue is very complex.  I carved this problem into some smaller pieces, first defining an operand to an expression:



    operand = AtomNum | AtomStr | MatrixAccess | FunCall | ID

The order of these items is *critical*.  ID *must* be the last item on this list - why?  Because FunCall and MatrixAccess both start with an ID, followed by some other stuff.  Let's see what would happen with a different order, and parse the matrix access string 'x[0][1]'.





    operand = ID | AtomNum | AtomStr | MatrixAccess | FunCall
    operand.parseString('x[0][1]')



This code only gives us the single token 'x', followed by the unrecognized syntax '[0][1]'.  Since ID is listed first in the list, it will match the leading 'x', declare victory, and then move on to the next bit of code, not realizing that the 'x' was really part of a larger thing, a MatrixAccess.  You *could* fix this by changing '|' to '^' operators, as in:



    operand = ID ^ AtomNum ^ AtomStr ^ MatrixAccess ^ FunCall

and then the order wouldn't matter.  But this forces the parser to try to evaluate *all* of the different options, and for such a low-level expression in your grammar, this would be a huge parse-time speed penalty.  The solution is to put ID after MatrixAccess and FunCall.  Now, there is still a problem with this expression for operand, but it will not affect the parser - can you see what it is? (Think about what a FunCall looks like...):



    operand = AtomNum | AtomStr | MatrixAccess | FunCall | ID



Now that operand is defined, I used the operatorPrecedence helper method to create arithmetic and comparison expressions.



    rValue << operatorPrecedence( operand,
        [
        (oneOf('+ -'),1,opAssoc.RIGHT),
        (oneOf('* /'),2,opAssoc.LEFT),
        (oneOf('+ -'),2,opAssoc.LEFT),
        (oneOf('= < > != <>'),2,opAssoc.LEFT),
        ]
        )



This allows you to rip out a lot of the OpCmp and 'rValue + ZeroOrMore(oneOf('+ - * /') + rValue' type code.



In fact, with this definition of rValue, you can now parse your '1+2' expression successfully - a first step!



3. I think you misunderstand what the empty expression is for.  Here is your definition of a program (Prg):



    Prg=empty|ZeroOrMore(Fun)



empty *always* matches, and does not advance the parse location.  This can by why you get infinite loops in parsing sometimes.  Try this:



    word = empty | Word(alphas)
    OneOrMore(word).parseString('earwax')

This loops forever.  The grammar successfully matches empty, over and over again.  People sometimes trip over empty when converting grammars from traditional BNFs.  Something like this:



    # functionCall ::= identifier ( arglist )
    # arglist ::= empty | arg | arg , arglist
    functionCall = identifier + '(' + 
        (empty | rValue | rValue + OneOrMore(','+rValue)) + 
        ')'

BNF is not Python, nor is it pyparsing.  Idioms you learn in BNF need to be adjusted when porting to a pyparsing expression.  BNF does not support things like Optional or ZeroOrMore, so alternation with empty is the idiom to handle these cases.  Usually BNF expressions like 'something | empty' should really translate to 'Optional(something)' in pyparsing.  Instead of 'somethingA | somethingA + somethingB', write 'somethingA + Optional(somethingB)'.  Instead of 'somethingA | somethingA + OneOrMore(somethingB)', write 'somethingA + ZeroOrMore(somethingB)'.  And 'somethingA + ZeroOrMore(','+somethingA)' should just be written as 'delimitedList(somethingA)'.



I would remove empty from everywhere in your grammar for now, and use Optional and ZeroOrMore to handle those cases where you want emptiness to be accepted in place of an optional expression.  Now Prg becomes just:



    Prg=ZeroOrMore(Fun)



4. Significant whitespace makes for tricky parsers.  This is why C uses ;'s to end statements (which can go on for many lines), and uses {}'s to mark compound statements.  You are using end of lines as implicit statement terminators.  That's okay, BUT for this convenience you will have to do more work in the parser.  I see that you set the default whitespace to remove '\n's (which I will call NL, for 'newline', since I find backslashes ugly and non-ergonomic, after all these years even).  This will suppress pyparsing's default behavior of skipping over NLs - so now you have to explicitly include NLs wherever they occur in your grammar.  I see that you started doing this in a few places, but I suggest defining a constant NL = Literal('\n') so that it is easier to add, and more visible in the grammar.



5. How does the parser know when a function definition ends?  I put in a hack to give the parser a hint, but I'm not sure it will be sufficient.  Try this for a start:



    Stmt<<~Literal('function') + \
            (
            CompdStmt|
            AssignStmt|
            CaselessKeyword('pass') |
            CaselessKeyword('break') |
            CaselessKeyword('continue') |
            CaselessKeyword('end') |
            CaselessKeyword('print') +Optional(rValue) |
            ReturnStmt |
            rValue#|
            # Group(Stmt)
            )

(You should now know why I put rValue at the end of this list.  I also commented out Group(Stmt) - what were you trying to do here?)



I *strongly* encourage you to start small and work your way up.  A good place to start is rValue, which I see you have done so.  Next try parsing a MatrixAccess, or a ReturnStmt, etc.  One technique I have started using is to assert equality of an expression with a test string, right after defining the expression.  Like this:



    ReturnStmt=CaselessKeyword('return')+Optional(rValue)
    assert 'return X' == ReturnStmt

(You have to not use the -O switch when running Python for asserts to be processed.)  This technique will help you work the kinks out of sub-expressions, before building them up into larger ones.  For instance:



    ID=Regex('(?i)a-z[a-z0-9]*')
    assert 'fib'==ID

would have helped you identify your low-level bug in ID.



Be patient, a project like this can take many weeks.  Once you move beyond parsing into actually running code, please read my article in the May issue of Python Magazine - it goes into much more detail on how to make a parser into an interpreter/compiler.



-- Paul
#### 2008-10-19 06:14:22 - oyster
Hi, Paul. Thank you very much for your reply with elaborate explanation. I learned much from it. ^-^

And I will follow your 'assert' suggestion in my project. I have never used it in python, but it seems more helpful and effective then my 'print sth' debug method.

Regards.

---
## 2008-10-21 15:52:10 - steven-d - Parsing line by line
I have a data file where each line matches one of a number 
of tokens, call them 'Spam' or 'Eggs'. I have a parser 
for Spam and a parser for Eggs, both of which use 
setParseAction to build a Spam or Eggs instance. I 
process the file like this simplified snippet:


    parser = spam | eggs
    for line in data.split('\n'):
        result = parser.parseString(line)[0]
        if isinstance(result, Spam):
            print 'Spam spam spam'
        elif isinstance(result, Eggs):
            print 'Eggs'



Is this a reasonable approach, or is there a more idiomatic
way to do it?


Thanks,  
Steven

#### 2008-10-21 20:11:32 - ptmcg
Looks okay to me.

You *could* add __str__ methods to Spam and Eggs so that you don't have to do the isinstance test, just do this:

    class Spam(object):
        ....
        def __str__(self):
            return 'Lovely Spam, wonderful spam...'
    
    class Eggs(object):
        ....
        def __str__(self):
            return 'Stop talking about my eggs!'
    
    parser = spam | eggs
    
    # Set up source var, to easily interchange with a file,
    # generator, or other iterable input source.
    # I like splitlines better than, split('\n'), don't know why
    source = data.splitlines()
    
    for line in source: 
        resultobj = parser.parseString(line)[0]
        print str(resultobj)



Then you don't even have to break this up line by line.  Assuming that Spam and Eggs expressions aren't ambiguous and run into each other, you might actually be able to do something like:



    resultobjs = OneOrMore(spam|eggs).parseString(data)
    for obj in resultobjs:
        print str(obj)



-- Paul
#### 2008-10-21 20:14:21 - ptmcg
Just to be on the safe side, to make sure whitespace-skipping doesn't accidentally take you across newlines, here's a more complete solution:



    ParserElement.setDefaultWhitespaceChars(' \t')
    EOL = LineEnd()
    
    allObjects = OneOrMore( (spam|eggs) + EOL )
    resultobjs = allObjects.parseString(data)
    ...
    



---
## 2008-10-22 02:02:31 - oyster - nested for and more
below is the code I want to parse a 'FOR-NEXT' statement. But there are 2 problem

1.I cannot use Stmt as the body of 'FOR-NEXT'( 2nd test)

2.I cannot use the nested 'FOR-NEXT'(3rd, 4th test)



if seems that ZeroOrMore is not greedy. How to fix this problem? thanx





    from pyparsing import *
    ParserElement.setDefaultWhitespaceChars(' \t') 
    
    @traceParseAction
    def aa(tokens):
        print tokens
    
    
    NL='\n'
    ID=Regex('(?i)[a-z][a-z0-9]*').setName('identifier')
    
    Stmt=Forward()
    ForStmt=Group(CaselessKeyword('for') +ID+','+ID+Optional(',' + Optional(ID)+Optional(','+(ID))))+Suppress(NL)\
               +ZeroOrMore(ZeroOrMore(delimitedList(Stmt, NL))+NL).setParseAction(aa)\
               +CaselessKeyword('next') 
    
    CompdStmt=ForStmt
    
    #Statement
    Stmt<<(
            CompdStmt|
            ID
            )
    
    test='''for a,b,c,d
    next'''
    print ForStmt.parseString(test)
    print
    
    test='''for a,b,c,d
    for a,b,c,d
    next
    next'''
    print ForStmt.parseString(test)
    
    test='''for a,b,c,d
    
        cos
    next'''
    print ForStmt.parseString(test)
    
    
    test='''for a,b,c,d
        for e,f,g,h
            sin
        next
        cos
    next    
    '''
    print ForStmt.parseString(test)
    



#### 2008-10-22 02:34:58 - oyster
I forget to mention, if Group is used like this:



    ForStmt=Group(CaselessKeyword('for') +ID+','+ID+Optional(',' + Optional(ID)+Optional(','+(ID))))+Suppress(NL)\
               +Group(ZeroOrMore(ZeroOrMore(delimitedList(Stmt, NL))+NL).setParseAction(aa))\
               +CaselessKeyword('next') 

I hope I get this nested result for the last example(no ' and NL is shown for a clear looking)



    [
    [for a,b,c,d],
        [
         [[for e,f,g,h], [sin], next],
         cos,
        ] 
    next   
    ]


#### 2008-10-24 23:52:30 - oyster
the code with same structure works and does not works for nested 'IF-ELSE IF-ELSE-END IF' BASIC code

1.I say it works, because this code does not exit with pyparsing.ParseException

2.I say it does not work, because the result of 2,4 and 5 test is not the expected nested list 



    from pyparsing import *
    ParserElement.setDefaultWhitespaceChars(' \t') 
    
    
    NL='\n'
    ID=Regex('(?i)[a-z][a-z0-9]*').setName('identifier')
    
    Stmt=Forward()
    IfStmt=Group(CaselessKeyword('if') +ID+NL)\
                   +Group(ZeroOrMore(Stmt+NL)+ZeroOrMore(NL))\
               +Group(ZeroOrMore(CaselessKeyword('else if')+ID+ NL)\
                   +Group(ZeroOrMore(Stmt+NL)+ZeroOrMore(NL)))\
               +Group(ZeroOrMore(CaselessKeyword('else')+NL)\
                   +Group(ZeroOrMore(Stmt+NL)+ZeroOrMore(NL)))\
               +CaselessKeyword('end if') 
    
    
    CompdStmt=IfStmt
    
    #Statement
    Stmt<<(
            CompdStmt|
            ID
            )
    
    test='''if a
    end if'''
    print IfStmt.parseString(test)
    print
    
    test='''if a
        if b
        end if
    
        if c
        else
            sth
        end if
    end if'''
    print IfStmt.parseString(test)
    print
    
    test='''if a
        sth
    end if'''
    print IfStmt.parseString(test)
    print
    
    test='''if a
    else if b
    else
    end if'''
    print IfStmt.parseString(test)
    
    test='''if a
        sth1
    else if b
        sth2
    else
        sth3
    end if'''
    print IfStmt.parseString(test)


#### 2008-10-28 04:27:18 - oyster
anybody at home?

and any hints to solve my problem while parsing BASIC-style nested FOR-NEXT, IF-ELSE IF-ELSE_END IF?
#### 2008-10-28 05:49:00 - ptmcg
The terminating 'next' is being parsed as an ID, so the nested ForStmts aren't working.  Change ID to:



    ID=~Literal('next') + Regex('(?i)[a-z][a-z0-9]*').setName('identifier')



And things will work better.  In general, you will need to make sure that ID does not match any keyword such as 'if', 'else', 'for', etc.  Easiest way to do that is define an expression of all keywords, and then define ID as:



    ID=~keyword + Regex('(?i)[a-z][a-z0-9]*').setName('identifier')



-- Paul
#### 2008-10-28 05:56:36 - ptmcg
Also, for your tests, you can either use the parseAll=True argument of parseString to make sure that partial parsing is not misinterpreted as success, or you can assert parse matching using this form (good for testing grammar fragments while building up a larger parser):



    assert testString == ForStmt



---
## 2008-10-23 15:31:15 - ptmcg - I've been busy with work...
Sorry for the delay in getting back to all your questions - this has been a busy week at work, and the weekend is not looking much better.



Thanks for your patience everyone, I'll get to these questions as soon as I can.  (Other pyparsing-istas are welcome to chime in, too.)



-- Paul

#### 2008-10-24 04:20:11 - steven-d
No problem Paul. I'm sure I speak for everyone when I say we appreciate all the help you give, and we also appreciate you taking the time to let us know that you're busy.



Have a good weekend, whatever you're doing.
#### 2008-10-24 18:53:14 - oyster
Happy weekend, Paul!

---
## 2008-10-30 00:11:07 - oyster - be carefulwith enablePackrat
bug or not?

as we all know, enablePackrat can gain a high speed( though it is not notable in the following code, which is only a small part of my program).

but we should use it carefully



    
                                         | ParserElement.enablePackrat()| #ParserElement.enablePackrat() 
    -------------------------------------+------------------------------+--------------------------------                                  
    operand = MatrixAccess|FunCall|ID    |          result a            |          result b
    -------------------------------------+------------------------------+--------------------------------                                      
    operand = MatrixAccess^FunCall^ID    |          result b            |          result b
    -------------------------------------+------------------------------+--------------------------------





    result a
    ['FUNCTION', 'main', '\n', ['WHILE', 'a', 'a', '\n', 'pass', 'pass', '\n', 'WEND'], '\n', 'ENDFUNCTION']
    
    result b
    ['FUNCTION', 'main', '\n', ['WHILE', 'a', '\n', 'pass', '\n', 'WEND'], '\n', 'ENDFUNCTION']



obviously, in result a, 'a' and 'pass' are repeated twice, that is a wrong result



so I changed all '|' in my program to '^'. I don't know whether there will be problem, but I have not met one till now.





    from pyparsing import *
    ParserElement.setDefaultWhitespaceChars(' \t')
    ParserElement.enablePackrat()           #<------note 1
    
    NL='\n'
    MultiNL=OneOrMore(NL)
    
    #######################################################
    ######################define keywords##################
    #######################################################
    WHILE_=(CaselessKeyword('while')).setParseAction(lambda e:'WHILE')
    WEND_=(CaselessKeyword('wend')).setParseAction(lambda e:'WEND')
    FUNCTION_=(CaselessKeyword('function')).setParseAction(lambda e:'FUNCTION')
    ENDFUNCTION_=(CaselessKeyword('end')+CaselessKeyword('function')).setParseAction(lambda e:'ENDFUNCTION')
    KEYWORD_=WHILE_^WEND_^FUNCTION_^ENDFUNCTION_
    
    ID=~KEYWORD_ + Regex('(?i)[a-z][a-z0-9]*').setName('identifier')
    
    rValue=Forward()
    FunCall=Forward()
    MatrixAccess=Forward()
    
    operand = MatrixAccess ^ FunCall ^ ID                #<------note 2
    operand = MatrixAccess |FunCall | ID
    
    #right value
    rValue << operand
    
    #access matrix element
    MatrixAccess << (ID + OneOrMore('[' + rValue +']' ))
    
    Stmt=Forward()
    #while
    WhileStmt=WHILE_ + rValue +NL+ZeroOrMore(Stmt+NL)+WEND_
    
    #function parameter or var decalre statement
    ParList=delimitedList(rValue).setName('ParList')
    
    Fun=Forward()
    
    #function call
    FunCall<<(ID+'('+Optional(ParList)+')')
    
    #Statement
    Stmt<<~FUNCTION_ + \
            (
            WhileStmt^
            rValue
            )
    
    #######################################################
    ######################define a function################
    #######################################################
    Fun<<(
    FUNCTION_+ID+MultiNL
    +ZeroOrMore(Group(Stmt)+MultiNL)
    +ENDFUNCTION_
    )
    
    test='''function main
        while a
            pass
        wend
        end function    '''
    print 'Fun.parseString(test)=', Fun.parseString(test)



#### 2008-10-30 05:27:09 - ptmcg
Well, I call this a bug - enablePackrat should not alter the results you get from the parser.  



Thanks for sending a concise example, should make it easier to nail down the problem.



-- Paul

---
## 2008-10-30 19:30:29 - oyster - invisible visible var
code says



    >>> from pyparsing import *
    >>> ID=Regex('[a-z]{1,}').setResultsName('id')
    >>> parsedPrg=ID.scanString('a')
    >>> print parsedPrg
    <generator object at 0x00AAE058>        #<--you see, it is something
    >>> for i in parsedPrg:
        print i
    
    
    ((['a'], {'id': [('a', 0)]}), 0, 1)     #<--you see, it is something
    >>> def emitID(item):
        print 'item=', item
        for i in item:
            print i
    
    
    >>> emitID(parsedPrg)
    item= <generator object at 0x00AAE058>  #the contents of var is only visible for the 1st line!
                                            #why is it invisible for the FOR loop?
    



#### 2008-10-30 20:52:08 - ptmcg
Your question revolves around Python generators, not pyparsing.



scanString returns a generator, which yields each a tuple for each found match: (tokens, start_location, end_location).  A generator yields values until it reaches the end of its loop, if it has one.  After that, any more calls to the generator's next() method (which is what the for loop does) raises StopIteration.



Look at this simple example, a generator that just returns the value 1 and then ends:



    >>> def simpleGenerator(): yield 1
    ...
    >>> z = simpleGenerator()
    >>> z
    <generator object at 0x01CA92D8>
    >>> print z
    <generator object at 0x01CA92D8>
    >>> print z.next()
    1
    >>> z
    <generator object at 0x01CA92D8>
    >>> print z.next()
    Traceback (most recent call last):
      File '<stdin>', line 1, in <module>
    StopIteration



Watch what happens when I loop over the same generator twice:



    >>> z = simpleGenerator()
    >>> for i in z: print i
    ...
    1
    >>> for i in z: print i
    ...
    >>>



Only the first loop extracts the contents, then the second loop has nothing to iterate over.



In your example, you kept using the same generator variable parsedPrg even after you looped through it to the end.  Try this with your code:



    parsedPrg=ID.scanString('a')
    emitID(parsedPrg)
    ... should print out parsed tokens ...
    emitID(parsedPrg)
    ... should print out nothing, because the generator has been fully iterated over...



-- Paul

---
## 2008-10-31 14:03:04 - DanFelts - parsing HTML Input 
I must be totally confused.



Anyways.. when I make a simple makeHTMLTags for input fields in a HTML page..



Example:



    istart,iend = makeHTMLTags('input')



It doesn't parse any of the input tags.  I usually run the html code through beutifulsoup first to make sure it cures bad html.  But it still doesnt parse the inputs.



Any help will be appreciated, and I am sure you hear this a lot, but great work on pyparsing.



Dan

#### 2008-10-31 16:01:10 - ptmcg
Dan -



Thanks for the compliments!



Can you post some HTML source or a URL where the input tags are not being found?



-- Paul
#### 2008-10-31 16:06:58 - ptmcg
This code extracts all of the <input> tags from the Google page:



    from pyparsing import *
    import urllib
    
    # get some html from the world's most famous INPUT html page
    html = urllib.urlopen('http://www.google.com').read()
    
    istart,iend = makeHTMLTags('input')
    
    for i,s,e in istart.scanString(html):
        print i.dump()
        print

prints:



    ['input', ['name', 'hl'], ['type', 'hidden'], ['value', 'en'], False]
    - empty: False
    - name: hl
    - startInput: ['input', ['name', 'hl'], ['type', 'hidden'], ['value', 'en'], False]
      - empty: False
      - name: hl
      - type: hidden
      - value: en
    - type: hidden
    - value: en
    
    ['input', ['type', 'hidden'], ['name', 'ie'], ['value', 'ISO-8859-1'], False]
    - empty: False
    - name: ie
    - startInput: ['input', ['type', 'hidden'], ['name', 'ie'], ['value', 'ISO-8859-1'], False]
      - empty: False
      - name: ie
      - type: hidden
      - value: ISO-8859-1
    - type: hidden
    - value: ISO-8859-1
    
    ['input', ['autocomplete', 'off'], ['maxlength', '2048'], ['name', 'q'], ['size', '55'], ['title', 'Google Search'], ['value', ''], False]
    - autocomplete: off
    - empty: False
    - maxlength: 2048
    - name: q
    - size: 55
    - startInput: ['input', ['autocomplete', 'off'], ['maxlength', '2048'], ['name', 'q'], ['size', '55'], ['title', 'Google Search'], ['value', ''], False]
      - autocomplete: off
      - empty: False
      - maxlength: 2048
      - name: q
      - size: 55
      - title: Google Search
      - value: 
    - title: Google Search
    - value: 
    
    ['input', ['name', 'btnG'], ['type', 'submit'], ['value', 'Google Search'], False]
    - empty: False
    - name: btnG
    - startInput: ['input', ['name', 'btnG'], ['type', 'submit'], ['value', 'Google Search'], False]
      - empty: False
      - name: btnG
      - type: submit
      - value: Google Search
    - type: submit
    - value: Google Search
    
    ['input', ['name', 'btnI'], ['type', 'submit'], ['value', 'I'm Feeling Lucky'], False]
    - empty: False
    - name: btnI
    - startInput: ['input', ['name', 'btnI'], ['type', 'submit'], ['value', 'I'm Feeling Lucky'], False]
      - empty: False
      - name: btnI
      - type: submit
      - value: I'm Feeling Lucky
    - type: submit
    - value: I'm Feeling Lucky
    



-- Paul
#### 2008-11-05 08:59:06 - DanFelts
Ah.. ok.. now I get it.. Duh.. thanks for the help.



Dan

---
## 2008-11-01 07:20:13 - oyster - how to get a real name for this case?
is it possible to get 'emitIfStmt' in my code? thanx


    from pyparsing import *
    ParserElement.setDefaultWhitespaceChars(' \t')
    ParserElement.enablePackrat()
    
    NL='\n'
    MultiNL=OneOrMore(NL)
    
    FOR_=(CaselessKeyword('for')).setParseAction(lambda e:'FOR')
    NEXT_=(CaselessKeyword('next')).setParseAction(lambda e:'NEXT')
    IF_=(CaselessKeyword('if')).setParseAction(lambda e:'IF')
    ENDIF_=(CaselessKeyword('end')+CaselessKeyword('if')).setParseAction(lambda e:'ENDIF')
    FUNCTION_=(CaselessKeyword('function')).setParseAction(lambda e:'FUNCTION')
    KEYWORD_=(FOR_^NEXT_^ IF_^ENDIF_^ FUNCTION_).setName('KEYWORD').setResultsName('KEYWORD')
    
    ID_=(~KEYWORD_ + Regex('(?i)[a-z][a-z0-9]*')).setName('ID').setResultsName('emitID')
    
    Stmt=Forward()
    
    ForStmt=Group(FOR_ +ID_+','+ID_+MultiNL)\
                +Optional(ZeroOrMore(Stmt + MultiNL))\
               +NEXT_
    ForStmt=ForStmt.setName('ForStmt').setResultsName('emitForStmt')
    
    #if
    IfStmt=Group(IF_ +ID_+MultiNL)\
               +ENDIF_
    IfStmt=IfStmt.setName('IfStmt').setResultsName('emitIfStmt')
    
    
    CompdStmt=ForStmt^IfStmt
    
    #Statement
    Stmt<<(~FUNCTION_ + \
            (
            CompdStmt^
            ID_
            ))
    
    testPrg='if a\nend if'
    print CompdStmt.parseString(testPrg).getName()      #<--emitIfStmt, which is what I need
    print Stmt.parseString(testPrg).getName()           #<--None. why not be 'emitIfStmt'
#### 2008-11-01 07:29:55 - oyster
I mean for 'Stmt.parseString(testPrg).getName()'
#### 2008-11-01 08:40:29 - ptmcg
Some simplification tips first:



    FOR_=(CaselessKeyword('for')).setParseAction(lambda e:'FOR')
    NEXT_=(CaselessKeyword('next')).setParseAction(lambda e:'NEXT')
    IF_=(CaselessKeyword('if')).setParseAction(lambda e:'IF')
    ENDIF_=(CaselessKeyword('end')+CaselessKeyword('if')).setParseAction(lambda e:'ENDIF')
    FUNCTION_=(CaselessKeyword('function')).setParseAction(lambda e:'FUNCTION')



can be more simply written as:



    FOR_ = CaselessKeyword('FOR')
    NEXT_ = CaselessKeyword('NEXT')
    IF_ = CaselessKeyword('IF')
    ENDIF_ = CaselessKeyword('END') + CaselessKeyword('IF')
    FUNCTION_ = CaselessKeyword('FUNCTION')



CaselessKeyword was designed to return the original constructor argument, regardless of the case of the parsed text:



    >>> print CaselessKeyword('XYZZY').searchString('xyzzy XYzzY Xyzzy')
    [['XYZZY'], ['XYZZY'], ['XYZZY']]





There is no ambiguity among the following alternatives:



    (FOR_^NEXT_^ IF_^ENDIF_^ FUNCTION_)



Using '^' operators creates an Or expression, which will evaluate all the alternatives and choose the longest.  Sometimes there is no choice but to do this, but in this particular case, MatchFirst will be sufficient.  Plus, you can order the elements in some order of expected frequency:



    (FOR_ | NEXT_ | IF_ | ENDIF_ | FUNCTION_)





As for the results names, I think I know where you are going with this, but let me just ask - are you going to use results names to help you walk through the parsed results to then execute the statements?  If so, I'd like to encourage that you take a different tack, by using parse actions to return executable objects which you can then pass to a VM.  This is how the BF interpreter is implemented in the May issue of Python magazine, or the Adventure Game interpreter included in pyparsing's examples directory.



-- Paul
#### 2008-11-01 21:21:17 - oyster
but for consistency, I do think Stmt.parseString(testPrg).getName() should return a name, not 'None'
#### 2008-11-01 21:43:50 - oyster
I am writing a code translator, not a interpreter

I will have a look  at the Adventure Game interpreter to find if its tack is suitable

thanx

---
## 2008-11-03 01:06:41 - oyster - did I misunderstand Optional?
for my code, only the first test case print 'True', the other 2 'False'

a quiz to me :(



    from pyparsing import *
    ParserElement.setDefaultWhitespaceChars(' \t')
    
    rValue=Regex('[a-z]{1,}')
    MultiNL=OneOrMore('\n')
    IF_=CaselessKeyword('IF')
    ELSE_=CaselessKeyword('ELSE')
    ENDIF_=(CaselessKeyword('end')+CaselessKeyword('if')).setParseAction(lambda e:'End If')
    
    OptionalMultiStmtNL=Optional(delimitedList(Group(rValue), Group(MultiNL))+Group(MultiNL))
    
    IfStmt=(
        Group(
                Group(IF_ + rValue +Group(MultiNL))
                + OptionalMultiStmtNL
                )
        +Optional(
                        Group(ELSE_+Group(MultiNL)+ OptionalMultiStmtNL)
                    )
        +ENDIF_)
    a='''if a
    else
    end if'''
    print a==IfStmt
    
    a='''if a
    b
    else
    end if'''
    print a==IfStmt
    
    
    a='''if a
    else
    b
    end if'''
    print a==IfStmt



#### 2008-11-03 02:03:12 - oyster
please change the rValue to 'rValue=Regex('[a-z]')'
#### 2008-11-04 20:05:33 - ptmcg
No you didn't misunderstand Optional.  You are still tripping over rValues accidentally parsing keywords.  



To try to diagnose the behavior, I changed rValue to:



    rValue=Regex('[a-z]').setDebug()



This prints out these log messages:



    Match Re:('[a-z]') at loc 2(1,3)
    Matched Re:('[a-z]') -> ['a']
    Match Re:('[a-z]') at loc 5(2,1)
    Matched Re:('[a-z]') -> ['e']
    Match Re:('[a-z]') at loc 10(3,1)
    Matched Re:('[a-z]') -> ['e']
    True
    Match Re:('[a-z]') at loc 2(1,3)
    Matched Re:('[a-z]') -> ['a']
    Match Re:('[a-z]') at loc 5(2,1)
    Matched Re:('[a-z]') -> ['b']
    Match Re:('[a-z]') at loc 7(3,1)
    Matched Re:('[a-z]') -> ['e']
    False
    Match Re:('[a-z]') at loc 2(1,3)
    Matched Re:('[a-z]') -> ['a']
    Match Re:('[a-z]') at loc 5(2,1)
    Matched Re:('[a-z]') -> ['b']
    Match Re:('[a-z]') at loc 7(3,1)
    Matched Re:('[a-z]') -> ['e']
    False
    Match Re:('[a-z]') at loc 2(1,3)
    Matched Re:('[a-z]') -> ['a']
    Match Re:('[a-z]') at loc 5(2,1)
    Matched Re:('[a-z]') -> ['e']
    Match Re:('[a-z]') at loc 10(3,1)
    Matched Re:('[a-z]') -> ['b']
    Match Re:('[a-z]') at loc 12(4,1)
    Matched Re:('[a-z]') -> ['e']
    False



I know that there are no 'e' rValues, so I suspect that they are the leading characters of the 'else' keywords.  So I went back and looked at how statements are parsed, and I see that there is ambiguity when parsing keywords - to clear this up, I added a negative lookahead to the rValue expression:



    rValue=~(IF_ | ELSE_ | ENDIF_ ) + Regex('[a-z]')



With this change, your IfStmt's all parse successfully.



-- Paul
#### 2008-11-05 05:11:57 - oyster
thank, but I am still puzzled



first, I write 



    ParserElement.setDefaultWhitespaceChars(' \t')

so I think 'else' will be recoginzed as one word, and its length is 4 which is not a match of Regx('[a-z]')
but from your explanation, pyparsing read only one letter by ignoring the delimited SPACE

that is not a greedy try-to-match IFStmt


second

    >>> print CaselessKeyword('if') ==  Regex('[a-z]')</li></ul></ul></ul>False



so I think to move 'if' out from Regex('[a-z]') is <strong>not an imaginable</strong> action


    rValue=~(IF_ | ELSE_ | ENDIF_ ) + Regex('[a-z]')


sorry I don't know how to express my chaos brain in detail



maybe the only solve to me is to try to remember it
#### 2008-11-05 07:21:14 - ptmcg
No, pyparsing does not use logic like ''else' will be recoginzed as one word, and its length is 4 which is not a match of Regx('[a-z]')'.  Pyparsing works expression by expression through the grammar, it does not pull words from the input stream and then try to match them to the grammar (as tokenizer/parser pairs like lex and yacc do).  So when the parse position is at the leading 'e' of 'else', and the next expression to evaluate is Regex('[a-z]'), it *will* match - 'e' is certainly a match for that regex.  You could try extending the regex in rValue to force a single character, by following [a-z] with a negative lookahead or word boundary, something like Regex(r'[a-z]\b').  Then the leading 'e' of 'else' would fail to match such a regex.

---
## 2008-11-03 06:36:35 - oyster - reconstruct expressions from operatorPrecedence results
I want to write a traslator from one language to another. The basic idea is to scan the parseString result, according to each getName, go to its function to transform code. Now, every is ok, except:



1. How to get the expression agian from operatorPrecedence.parseString results?

I have thought, but have no idea since I can't assign a ressultName to each operator group



2. My previous 'IF-ELSE IF-ELSE-END IF' related issue



As for using setParseAction, my mind is still in a mess and can not coin a right code. But I think extra () maybe appear if I use setParseAction :(



thanks for your help

#### 2008-11-04 20:11:50 - ptmcg
I think this general idea will work, but my suggestion is that, instead of post-processing the parsed tokens to lookup functions to do the transformation, use a parse action to wrap the parsed tokens inside a callable object *at parse time* and the callable object does the proper transformation.



I'll work up a short example and post it in the next few days.  The O'Reilly e-book also includes a detailed example of a conversion of search strings into a data structure of callable filter objects.



-- Paul
#### 2008-11-06 05:36:34 - oyster
Paul, thanx



till now I can only finish a simple langague translator

via 'setParseAction' function.



I still can not get a resonable logic for complex one.

for example, a tiny langauge like this



    NL=NewLine()
    program=ZeroOrMore(stmt+NL)
    stmt=Number| ArithmeticalExpression
    ArithmeticalExpression=operatorPrecedence( Number,
        [
        (oneOf('+ -'),1,opAssoc.RIGHT),
        (oneOf('* /'),2,opAssoc.LEFT),
        (oneOf('= < > != <> <= >='),2,opAssoc.LEFT),
        (oneOf('+ -'),2,opAssoc.LEFT),
        ])
    Number=Regex('[+-]{0,1}'[1-9][0-9]{0,}')



where shall we assign action? If we do



    def actNumber(item):
        return item[0]
    Number=Number.setParseAction(actNumber)

then how to translator



    1+2*(4)/(5+6)



that is still a very simple demo.

I am exhausted now by my BASIC-like language translator, which is

more complex. I think it is better to take a rest for me

until your exmaple give me a new drive.



I am anxious to read your exmaple, but take your time :)



See you.
#### 2008-11-06 07:12:48 - ptmcg
Here is your example fleshed out with parse actions.  If you want more description, please consider buying the O'Reilly Short Cut 'Getting Started with Pyparsing', in which the interpreting of a search string follows the same pattern, or get the May issue of Python Magazine, which uses the pattern to create a Brainf*ck interpreter.



Note that I am doing some re-casing of your names.  By capitalizing only class names, it is a little easier to keep straight whether you are referring to a class or a variable/expression.  'number' is a variable, 'NumericConstant' is a class.



Before the next release, I'll update the simpleArith.py example to include these parse actions.



-- Paul







    from __future__ import division
    from pyparsing import *
    import operator
    
    NL=OneOrMore(LineEnd()).suppress()
    
    def evalOperand(opd):
        if isinstance(opd,ParseResults):
            return evalOperand(opd[0])
        else:
            return opd()
    
    class InterpreterObject(object):
        def __init__(self,t):
            self.tokens = t[0]
    
    class NumericConstant(InterpreterObject):
        def __call__(self):
            return int(self.tokens[0])
    
    class UnarySign(InterpreterObject):
        def __call__(self):
            mult = 1
            for sign in self.tokens[:-1]:
                mult *= {'+':1, '-':-1}[sign]
            return mult*evalOperand(self.tokens[-1])
    
    class DivMultOp(InterpreterObject):
        def __call__(self):
            t = self.tokens
            ret = evalOperand(t[0])
            for opn,operand in zip(t[1::2],t[2::2]):
                func = {'*':operator.mul, '/':operator.truediv}[opn]
                ret = func(ret,evalOperand(operand))
            return ret
    
    class AddSubOp(InterpreterObject):
        def __call__(self):
            t = self.tokens
            ret = evalOperand(t[0])
            for opn,operand in zip(t[1::2],t[2::2]):
                func = {'+':operator.add, '-':operator.sub}[opn]
                ret = func(ret,evalOperand(operand))
            return ret
    
    #~ Number=Regex('[+-]{0,1}[1-9][0-9]{0,}')
    number=Regex('[1-9][0-9]*')
    number.setParseAction(NumericConstant)
    
    # UnarySign, DivMultOp, and AddSubOp are all parse actions.
    # Since these 'parse actions' are actually classes, then the action that will
    # be performed is that an object of that type will be constructed, and its
    # __init__ method called with the arguments that would have been passed
    # to an actual function
    arithExpr=operatorPrecedence( number,
        [
        (oneOf('+ -'),1,opAssoc.RIGHT, UnarySign),
        (oneOf('* /'),2,opAssoc.LEFT, DivMultOp),
        (oneOf('+ -'),2,opAssoc.LEFT, AddSubOp),
        ])
    
    # left as an exercise to the reader
    comparisonExpr = operatorPrecedence( arithExpr,
        [
        (oneOf('= < > != <> <= >='), 2, opAssoc.LEFT),
        ('not', 1, opAssoc.RIGHT),
        ('or', 2, opAssoc.LEFT),
        ('and',2, opAssoc.LEFT),
        ])
    
    # not necessary - a lone number *is* an arithExpr
    #~ stmt=number | arithExpr
    stmt = arithExpr
    comment = '#' + restOfLine
    stmt.ignore(comment)
    
    tests = '''\
    1
    2+3
    1+2*(4)/(5+6)  # your original test case
    4+5/6-7*8
    4+5/6-7*(-8)
    -4+5/6-7*(-8)
    +--+-+--4+5/6-7*(-8)  # pile up those unary signs
    -4+-5/6-7*(-8)
    -4+--5/6-7*(-8)  # can we keep add, subtract and unary plus/minus straight?
    '''.splitlines()
    
    #~ program=ZeroOrMore(stmt+NL)
    
    for test in tests:
        if not test: continue
        res = arithExpr.parseString(test,parseAll=True)
        print test
        print eval(test)
        print evalOperand(res)
        print



Prints:



    1
    1
    1
    
    2+3
    5
    5
    
    1+2*(4)/(5+6)  # your original test case
    1.72727272727
    1.72727272727
    
    4+5/6-7*8
    -51.1666666667
    -51.1666666667
    
    4+5/6-7*(-8)
    60.8333333333
    60.8333333333
    
    -4+5/6-7*(-8)
    52.8333333333
    52.8333333333
    
    +--+-+--4+5/6-7*(-8)  # pile up those unary signs
    52.8333333333
    52.8333333333
    
    -4+-5/6-7*(-8)
    51.1666666667
    51.1666666667
    
    -4+--5/6-7*(-8)  # can we keep add, subtract and unary plus/minus straight?
    52.8333333333
    52.8333333333



---
## 2008-11-03 08:38:39 - cpennington - Line based parsing and empty lines: possible bug?
I've been trying to parse a line based grammar, and want to ignore empty lines. However, I'm running into issues when trying to parse them. A minimal example is below:




```
from pyparsing import LineStart, LineEnd, ZeroOrMore, replaceWith

ParserElement.setDefaultWhitespaceChars(" \t\r")
EOL = LineEnd().suppress()
empty_line = (LineStart() + EOL).setParseAction(replaceWith("&lt;EMPTYLINE&gt;"))
assert ZeroOrMore(empty_line).parseString("", True).asList() == ['&lt;EMPTYLINE&gt;']
assert ZeroOrMore(empty_line).parseString("\n", True).asList() == ['&lt;EMPTYLINE&gt;', '&lt;EMPTYLINE&gt;']
assert ZeroOrMore(empty_line).parseString("    \n", True).asList() == ['&lt;EMPTYLINE&gt;','&lt;EMPTYLINE&gt;']
assert ZeroOrMore(empty_line).parseString("\n   \n", True).asList() == ['&lt;EMPTYLINE&gt;', '&lt;EMPTYLINE&gt;', '&lt;EMPTYLINE&gt;']


```




I think the problem is in LineStart. It checks several conditions without taking into consideration that there might be ignored whitespace between it and the start of the line.

#### 2008-11-03 17:24:44 - oyster
don't know whether this code is helpful

Do you need to know where is the <EMPTYLINE>?



    from pyparsing import *
    
    ParserElement.setDefaultWhitespaceChars(' \t\r')
    EOL = LineEnd().suppress()
    
    validLine=something
    
    wholePrg=ZeroOrMore(Optional(validLine)+ZeroOrMore(EOL))
    


#### 2008-11-04 08:35:36 - cpennington
In my case, I don't (and I was able to work around the bug by leaving the whitespace in and then explicitly capturing it with a White). I wanted to make sure the bug was noted, though (I suppose maybe I should throw it on the sourceforge tracker as well)

---
## 2008-11-03 11:28:36 - cpennington - New failOn parameter for SkipTo doesn't work
Summary:

Basically, when the failOn parameter doesn't parse, it raises an exception that causes SkipTo not to parse, which is directly opposite to the expected behavior






```

from pyparsing import SkipTo

#Passes
assert SkipTo("a").parseString("ccca").asList() == ["ccc"]

#Should pass, but doesn't
assert SkipTo("a", failOn="b").parseString("ccca").asList() == ["ccc"]

#Should fail, and does (but perhaps not w/ the right exception)
assert SkipTo("a", failOn="b").parseString("cbca").asList() == ["ccc"]

```


#### 2008-11-04 19:34:55 - ptmcg
This is a bug in SkipTo/failOn.  I'll try to have an updated version posted to the SourceForge SVN later tonight or tomorrow.



-- Paul
#### 2008-11-06 09:26:53 - ptmcg
Still working on the SVN update - network is *DOG* slow!
#### 2008-11-06 10:37:25 - ptmcg
Just got SF SVN repository updated, try the latest version.

---
## 2008-11-04 18:54:00 - oyster - be careful with setResultsName and setParseAction


    i.setResultsName('something')

will not give the result a name 'something', to do that, we have to use 



    i=i.setResultsName('something')



but



    i.setParseAction(fn)

will set the Action of i to fn. if you don't want that, you must use



    j=i.copy().setParseAction(fn)

then use j to parse

#### 2008-11-04 19:14:10 - ptmcg
Yes, setResultsName is one of the weak spots of the pyparsing API.  As useful as it is to define results names, 'setResultsName' sounds like it is a simple attribute setter.  In fact, it really should be named 'copyAndSetResultsName'.  The way this came about was that I wanted you to be able to define a basic parsing expression, and then reuse it in multiple places with different results names.  Look at this example (using the new syntax for setResultsName):



    integer = Word(nums)
    time = integer('hours') + ':' + integer('minutes') + ':' + integer('seconds')
    date = integer('year') + '/' + integer('month') + '/' + integer('day')



Unfortunately, for this to work, each different results name has to work with its own private copy of the original integer expression.



So in general, I would advise that people use the style as I show above: define basic expression prototypes, and then assign results names when those basic expressions are used to build up a more complex expression.



-- Paul
#### 2009-05-02 02:12:51 - jestar_jokin
I was not aware that setResultsName returned a copy, so I had some problems with parse actions not getting triggered when I expected.



I was using a string element that required some custom parsing for escape characters. I assigned a parse action. I then used that element in some compound statements, which gave a results name for the element. No problems there.



Then I moved the parse action to later in the code, after the compound statements, and BAM!



Anyway, I found you can work around it by grouping the element, assigning the <em>group</em> a results name, and assigning a simple lambda parse action that returns the first token.





    from pyparsing import *
        gstring = quotedString
        gvarname1 = Suppress(Literal('$')) + gstring('resName')
        gvarname2 = Suppress(Literal('$')) + gstring
        gvarname3 = Suppress(Literal('$')) + Group(gstring)('resName').setParseAction(lambda s, loc, toks: toks[0])
    
        gstring.setParseAction(lambda s, loc, toks: ['actioned'])
    
        # Returns [''hello'']
        print gvarname1.parseString('$'hello'')
        # Returns ['actioned']
        print gvarname2.parseString('$'hello'')
        # Returns ['actioned']
        print gvarname3.parseString('$'hello'')



---
## 2008-11-04 19:04:00 - oyster - operatorPrecedenceWithName
As I have said in previous posts, I want to write a language translator. One of the problem is 'how to reconstuct the expressions by using operatorPrecedence results', I extended the original operatorPrecedence function so that we can associate a resultsname with expressions matching. This modification solved my problem. Because in fact this modification does not break any code which use 'operatorPrecedence', and I think it is useful (at least for me), so can it appear in the next version pyparsing and replace the old 'operatorPrecedence'?



btw, I marked my change in bold





    def operatorPrecedenceWithName( baseExpr, opList**, useSuppress=True** ):
        '''Helper method for constructing grammars of expressions made up of
           operators working in a precedence hierarchy.  Operators may be unary or
           binary, left- or right-associative.  Parse actions can also be attached
           to operator expressions.
    
           Parameters:
            - baseExpr - expression representing the most basic element for the nested
            - opList - list of tuples, one for each operator precedence level in the
              expression grammar; each tuple is of the form
              (opExpr, numTerms, rightLeftAssoc, parseAction, resultsName), where:
               - opExpr is the pyparsing expression for the operator;
                  may also be a string, which will be converted to a Literal;
                  if numTerms is 3, opExpr is a tuple of two expressions, for the
                  two operators separating the 3 terms
               - numTerms is the number of terms for this operator (must
                  be 1, 2, or 3)
               - rightLeftAssoc is the indicator whether the operator is
                  right or left associative, using the pyparsing-defined
                  constants opAssoc.RIGHT and opAssoc.LEFT.
               - parseAction is the parse action to be associated with
                  expressions matching this operator expression (the
                  parse action tuple member may be omitted)
               **- parseName is the name to be associated with
                  expressions matching this operator expression   **
        '''
        ret = Forward()
        **if useSuppress:
            lastExpr = baseExpr | ( Suppress('(') + ret + Suppress(')') )
        else:
            lastExpr = baseExpr | ( '(' + ret + ')' )**
        for i,operDef in enumerate(opList):
            **opExpr,arity,rightLeftAssoc,pa,pn= (operDef + (None,None))[:5]**
            if arity == 3:
                if opExpr is None or len(opExpr) != 2:
                    raise ValueError('if numterms=3, opExpr must be a tuple or list of two expressions')
                opExpr1, opExpr2 = opExpr
            thisExpr = Forward()#.setName('expr%d' % i)
            if rightLeftAssoc == opAssoc.LEFT:
                if arity == 1:
                    matchExpr = FollowedBy(lastExpr + opExpr) + Group( lastExpr + OneOrMore( opExpr ) )
                elif arity == 2:
                    if opExpr is not None:
                        matchExpr = FollowedBy(lastExpr + opExpr + lastExpr) + Group( lastExpr + OneOrMore( opExpr + lastExpr ) )
                    else:
                        matchExpr = FollowedBy(lastExpr+lastExpr) + Group( lastExpr + OneOrMore(lastExpr) )
                elif arity == 3:
                    matchExpr = FollowedBy(lastExpr + opExpr1 + lastExpr + opExpr2 + lastExpr) + \
                                Group( lastExpr + opExpr1 + lastExpr + opExpr2 + lastExpr )
                else:
                    raise ValueError('operator must be unary (1), binary (2), or ternary (3)')
            elif rightLeftAssoc == opAssoc.RIGHT:
                if arity == 1:
                    # try to avoid LR with this extra test
                    if not isinstance(opExpr, Optional):
                        opExpr = Optional(opExpr)
                    matchExpr = FollowedBy(opExpr.expr + thisExpr) + Group( opExpr + thisExpr )
                elif arity == 2:
                    if opExpr is not None:
                        matchExpr = FollowedBy(lastExpr + opExpr + thisExpr) + Group( lastExpr + OneOrMore( opExpr + thisExpr ) )
                    else:
                        matchExpr = FollowedBy(lastExpr + thisExpr) + Group( lastExpr + OneOrMore( thisExpr ) )
                elif arity == 3:
                    matchExpr = FollowedBy(lastExpr + opExpr1 + thisExpr + opExpr2 + thisExpr) + \
                                Group( lastExpr + opExpr1 + thisExpr + opExpr2 + thisExpr )
                else:
                    raise ValueError('operator must be unary (1), binary (2), or ternary (3)')
            else:
                raise ValueError('operator must indicate right or left associativity')
            if pa:
                matchExpr.setParseAction( pa )
            **if pn:
                matchExpr=matchExpr.setResultsName( pn )**
            thisExpr << ( matchExpr | lastExpr )
            lastExpr = thisExpr
        ret << lastExpr
        return ret
    




---
## 2008-11-09 03:00:45 - oyster - infinite loops
this code is for parsing 'a+b>2' ,(a>b)+(c<d), where + is arithmetic operators, and >< are comparsion operators

I know if I define operatorPrecedence only once, everything is ok, but the operatorPrecedence code blokc gets big, so I take Paul's style to seperate them in order to maintain easily.

But my code sees bugs( for exapmle, it can't bear a pair of ())
how to fix the bad code? thanx


    from pyparsing import *
    
    ParserElement.setDefaultWhitespaceChars(' \t')
    ParserElement.enablePackrat()
    
    ID_=(Regex('[a-z][a-z0-9]*'))
    
    operand = Group( ID_  )
    
    comparisonExpr=Forward()
    arithExpr=operatorPrecedence( operand|comparisonExpr,
        [
        (oneOf('+ -'),1,opAssoc.RIGHT),
        (oneOf('* /'), 2, opAssoc.LEFT),
        (oneOf('+ -'), 2, opAssoc.LEFT),
        ])
    
    comparisonExpr << (
    ~(arithExpr|'('+arithExpr+')')
    +operatorPrecedence( arithExpr,
        [
        (oneOf('= < >'), 2, opAssoc.LEFT),
        ])
        )
    
    rValue=(arithExpr^comparisonExpr)
    
    tests='a'
    print rValue.parseString(tests, parseAll=True)       #that is ok
    tests='a>a'
    print rValue.parseString(tests, parseAll=True)       #error
    tests='(a)'
    print rValue.parseString(tests, parseAll=True)       #infinite loop
    print
    tests='a=a'
    print rValue.parseString(tests, parseAll=True)       #error
    print
    tests='(a)*a'
    print rValue.parseString(tests, parseAll=True)       #infinite loop
    print
    tests='(a)'
    print rValue.parseString(tests, parseAll=True)       #infinite loop

#### 2008-11-13 18:07:28 - ptmcg
I'm a little confused about your grammar.  Let's start with your use of comparisionExpr's as operands in an arithmetic expression:



    arithExpr=operatorPrecedence( operand|comparisonExpr,
    [
    (oneOf('+ -'),1,opAssoc.RIGHT),
    (oneOf('* /'), 2, opAssoc.LEFT),
    (oneOf('+ -'), 2, opAssoc.LEFT),
    ])



Where comparisonExpr gets later defined as (stripping out some of the lookahead stuff):



    operatorPrecedence( arithExpr,
    [
    (oneOf('= < >'), 2, opAssoc.LEFT),
    ])
    )



I don't get it.  Are you saying that this is a valid arithmetic expression?



    -(5 > 2) + (12-3<7) * 2/4=3



How are you planning on converting the comparison operations to numeric values?



I think I may have led you astray on an earlier post, since this code looks familiar:



    comparisonExpr = operatorPrecedence( arithExpr,
    [
    (oneOf('= < >'), 2, opAssoc.LEFT),
    ])
    )



There is little point in using operatorPrecedence if there is only one level of precedence.  And in fact, you are probably better off just explicitly defining comparisonExpr as:



    comparisonOp = oneOf('= < > >= <= !=')
    comparisonExpr = arithExpr + comparisonOp + arithExpr



or possibly even:



    comparisonOp = oneOf('= < > >= <= !=')
    comparisonExpr = arithExpr + comparisonOp + arithExpr + 
            Optional(comparisonOp + arithExpr)



if you want to support comparison expressions like Python's '1 < x < 10' shorthand for '1 < x and x < 10'.



Also, you need to look at the operands you pass to operatorPrecedence as a trivial or degenerate form of the compound expression.  What I mean is, in this simple case:



    operand = integer | varname
    arithExpr=operatorPrecedence( operand,
        [
        (oneOf('+ -'),1,opAssoc.RIGHT),
        (oneOf('* /'), 2, opAssoc.LEFT),
        (oneOf('+ -'), 2, opAssoc.LEFT),
        ])



'4' *is* an arithmetic expression, just a trivial one with no operations.



Now look at the code you posted.  Is arithExpr really a comparisonExpr?  I don't think so.



Let's add another operatorPrecedence expression, a booleanExpr:



    booleanExpr = (
        operatorPrecedence( comparisonExpr,
        [
        (Keyword('not'), 1, opAssoc.RIGHT),
        (Keyword('and'), 2, opAssoc.LEFT),
        (Keyword('or'), 2, opAssoc.LEFT),
        ])
        )



Here you can see that 'x > y', even though it is just a single comparison expression, *is* a boolean expression, just a simple one without any and's or or's.



So later on, there is no need to define rValue as:



    rValue = comparisonExpr | booleanExpr | ...



If something matches as a comparisonExpr, it will match as a booleanExpr, too.



If you are planning on using the Python interpretation of giving boolean expressions a numeric 1 or 0 value for True or False, then you can do this:

- define booleanExpr as a Forward

- use booleanExpr | integer | varname as the operand to arithExpr

- change the assignment above to use '<<' instead of '='



Here is your code again, and no infinite loops:



    from pyparsing import *
    
    ParserElement.setDefaultWhitespaceChars(' \t')
    ParserElement.enablePackrat()
    
    ID_= Regex('[a-z][a-z0-9]*')
    
    #~ operand = Group( ID_ ) # please don't Group if you don't have to
    booleanExpr = Forward()
    operand = ID_ | Word(nums) | '(' + booleanExpr + ')'
    
    arithExpr=operatorPrecedence( operand,
        [
        (oneOf('+ -'),1,opAssoc.RIGHT),
        (oneOf('* /'), 2, opAssoc.LEFT),
        (oneOf('+ -'), 2, opAssoc.LEFT),
        ])
    
    comparisonOp = oneOf('= < > >= <= !=')
    comparisonExpr = arithExpr + comparisonOp + arithExpr
    assert 'a > a' == comparisonExpr
    
    booleanExpr << (
        operatorPrecedence( comparisonExpr,
        [
        (Keyword('not'), 1, opAssoc.RIGHT),
        (Keyword('and'), 2, opAssoc.LEFT),
        (Keyword('or'), 2, opAssoc.LEFT),
        ])
        )
    
    #~ rValue=(booleanExpr | comparisonExpr | arithExpr)
    rValue=(booleanExpr | arithExpr)
    
    #~ tests='a'
    #~ print rValue.parseString(tests, parseAll=True) #that is ok
    #~ tests='a>a'
    #~ print rValue.parseString(tests, parseAll=True) #error
    #~ ...
    # I find this form easier to extend
    
    tests = '''\
        a
        (a > a)
        a > a
        (a)
        a=a
        (a)*a
        (a+b-x)*a
        '''.splitlines()
    for t in tests:
        t = t.strip()
        if not t: continue
        print t
        try:
            rValue.parseString(t,parseAll=True)
        except ParseException:
            print 'Error parsing '%s'' % t
    
    # or use this form, and you will see the nesting levels created by 
    # the operatorPrecedence expression
    from pprint import pprint
    for t in tests:
        t = t.strip()
        if not t: continue
        pprint( rValue.parseString(t).asList() )
    



-- Paul



(BTW, in working on this code, there seems to be a bug in the '==' comparison operation for ParserElement - this discussion thread was helpful to me in finding this bug, thanks!)
#### 2008-11-14 20:47:02 - oyster
thanx, paul :)

---
## 2008-11-14 04:14:26 - steven-d - Line-oriented parsing, with goodies
I'm publishing this for comments, and hopefully it will be useful to others as well. Please feel free to reuse this code without restrictions if it is useful to you.



I have a parser for line-oriented data, with the following conditions:



<ul><li>whitespace at the beginning and end is unimportant, but whitespace within the line is significant.</li></ul>

<ul><li>you can join physical lines into a single logical line by ending the first line with a backslash.</li></ul>

<ul><li>Python-style # comments are ignored.</li></ul>

<ul><li>you can escape backslashes and hashes by preceding them with a backslash.</li></ul>

This code does contain a small buglet: if you follow a backslash with a space, the space is eaten.



After parsing my file with this parser to get logical lines, I then pass each line to another parser. I find that easier than trying to build a single giant parser to do everything.



Comments on how to improve this will be appreciated.







    from pyparsing import *
    
    # Exclude newlines from the default whitespace characters.
    # We need to deal with them manually.
    ws = ' \t'
    ParserElement.setDefaultWhitespaceChars(ws)
    
    
    # Define punctuation and legal characters.
    backslash = '\\'
    hashmark = '#'
    standard_chars = printables.replace(backslash, '').replace(hashmark, '')
    
    # Escape codes.
    escaped_hash = Literal(backslash + hashmark)
    escaped_backslash = Literal(backslash + backslash)
    escape = (backslash + Word(printables, exact=1)) | escaped_hash | escaped_backslash
    
    # Free-form text includes internal whitespace, but not leading or trailing.
    text = OneOrMore(White(ws) | Word(standard_chars) | escape)
    text.setParseAction(lambda tokens: ''.join(tokens))
    
    comment = (hashmark + restOfLine).suppress()
    
    # Define line-related parts.
    EOL = LineEnd().suppress()
    SOL = LineStart().leaveWhitespace()
    continuation = (Literal(backslash).leaveWhitespace() + EOL).suppress()
    blankline = SOL + LineEnd()
    
    
    # Define parts of the document and the parser.
    physical_line = text + EOL
    physical_line.setParseAction(lambda tokens: tokens[0].rstrip())
    logical_line = OneOrMore(text + continuation) + physical_line
    logical_line.setParseAction(lambda tokens: ''.join([x.lstrip() for x in tokens]))
    
    line = physical_line | logical_line | EOL
    body = OneOrMore(line)
    parser = body + StringEnd()
    parser.ignore(blankline)
    parser.ignore(comment)
    
    data = r'''# this is a comment
      # comment with leading whitespace
    line 1
    line 2
    
    line 3 # has a comment
    line 4 \# does not have a comment
    line 5 \
      continues over three \
      physical lines
    
    line 6 \
    
      continues over two physical lines, with a blank in between
    
    line 7 doesn't continue \\
    line \8 in\cludes \escapes (and also a\ bug).
        line 9 also has a comment # which gets rid of this backslash \
    and lastly line 10.
    
    '''
    
    
    try:
        for item in parser.parseString(data):
            print item
    except ParseException, err:
        print err.line
        print ' '*(err.column-1) + '^'
        print err



#### 2008-11-14 12:51:26 - ptmcg
Nice front-end to line-oriented parsing.  It addresses many common source code parsing issues in a first source pass.



I see that you have a \-escaping bug on line 8, here is a modified definition of escape that will fix that (also requires moving the definition of EOL up above escape):



    nonWhiteChar = Word(printables, exact=1).setName('non-whitespace char') 
    escape = escaped_hash | escaped_backslash |  
                    Combine(backslash + ~EOL - nonWhiteChar)



The Combine forces the '\' and its escapee to be adjacent, with no whitespace.  Your current version is parsing 'a\ bug' with '\b' followed by 'ug'.  Combine also concatenates the tokens, so that all of the tokens parsed by escape are now returned as nice 2-character strings, instead of '\#' and '\\' being returned as strings (because they are defined as Literals) and '\z' returned as the list ['\','z'].



Note the use of '-' inside the Combine; '-' is the same as '+' in terms of defining a sequence of required expressions, but they behave differently at parse time.  Using '-' instead of '+' prevents backtracking, so that if you get this far in the grammar, any subsequent failures are flagged as a syntax error.  Look at the Combine expression closely:



    Combine(backslash + ~EOL - nonWhiteChar)



First of all, Combine forces all tokens to be adjacent, with no intervening whitespace, so 'a\ bug' will not parse the '\ b' as an escaped character.  Then the negative lookahead '~' ensures that we are not positioned on a line-continuing '\'.  At this point, if we find anything that is not whitespace, then this is a syntax error, so we use '-' instead of '+' to add the final expression in the sequence, nonWhiteChar.



With these changes, we now get a nice error message for your bug:



    line \8 in\cludes \escapes (and also a\ bug).
                                           ^
    Expected non-whitespace char (at char 322), (line:17, col:40)



Instead of this:



    line \8 in\cludes \escapes (and also a\ bug).
    ^
    Expected end of text (at char 283), (line:17, col:1)



Lastly, you can post-process the '\x' escaped characters with the simplest of parse actions:



    escape.setParseAction(lambda t : t[0][1])

since what you really wanted was the 'x' character anyway, not the backslash.



You might also need to do some special processing to make sure you don't accidentally process the contents of quoted strings.  This change may be sufficient, but I haven't tested it:



    text = OneOrMore(White(ws) | quotedString | Word(standard_chars) | escape)



-- Paul
#### 2008-11-14 16:56:46 - steven-d
That's great Paul, thank you for your help.



For my application, it probably makes sense for backslash-space to be an error, so I like the bugfix you suggest. I can't process the escapes at this stage, but the second pass will allow character escapes (e.g. \t for tab) and number escapes. I'll need a more complicated parse action, but yours will be a good start.



Thanks again, not only for a great product but for taking the time to help so often.





-- 

Steven
#### 2008-11-14 17:56:02 - steven-d
Hmmm... I don't get the expected error message on line 8. Instead I get this:





    line \8 in\cludes \escapes (and also a\ bug).
    ^
    Expected end of text (at char 279), (line:17, col:1)



If I use setDebug() on the escape parser, I get this output (ignoring the first 46 lines):





    Matched {'\#' | '\\' | Combine:({{'\' ~{Suppress:(LineEnd)}} Empty non-whitespace char})} -> ['\\e']
    Match {'\#' | '\\' | Combine:({{'\' ~{Suppress:(LineEnd)}} Empty non-whitespace char})} at loc 317(17,39)
    Exception raised:Expected non-whitespace char (at char 318), (line:17, col:40)
    Match {'\#' | '\\' | Combine:({{'\' ~{Suppress:(LineEnd)}} Empty non-whitespace char})} at loc 284(17,6)
    Matched {'\#' | '\\' | Combine:({{'\' ~{Suppress:(LineEnd)}} Empty non-whitespace char})} -> ['\\8']
    Match {'\#' | '\\' | Combine:({{'\' ~{Suppress:(LineEnd)}} Empty non-whitespace char})} at loc 289(17,11)
    Matched {'\#' | '\\' | Combine:({{'\' ~{Suppress:(LineEnd)}} Empty non-whitespace char})} -> ['\\c']
    Match {'\#' | '\\' | Combine:({{'\' ~{Suppress:(LineEnd)}} Empty non-whitespace char})} at loc 297(17,19)
    Matched {'\#' | '\\' | Combine:({{'\' ~{Suppress:(LineEnd)}} Empty non-whitespace char})} -> ['\\e']
    Match {'\#' | '\\' | Combine:({{'\' ~{Suppress:(LineEnd)}} Empty non-whitespace char})} at loc 317(17,39)
    Exception raised:Expected non-whitespace char (at char 318), (line:17, col:40)
    line \8 in\cludes \escapes (and also a\ bug).
    ^
    Expected end of text (at char 279), (line:17, col:1)



So I can see the 'Expected non-whitespace char' raised, but then it gets eaten and replaced by 'Expected end of text'. Is that normal behaviour? Is the right way to fix this to add a setFailAction() handler to escape to raise ParseFatalException?



I've tried that, but I get unexpected results. With a setFailAction() handler on escape, there is no change in behaviour unless I *also* use setDebug(), in which case parsing ends at char 0, (line:1, col:1).







-- 

Steven
#### 2008-11-14 20:52:01 - ptmcg
??? Hmmm, maybe I made some other changes - here is the full parser code that I used:



    from pyparsing import *
    
    # Exclude newlines from the default whitespace characters.
    # We need to deal with them manually.
    ws = ' \t'
    ParserElement.setDefaultWhitespaceChars(ws)
    
    
    # Define punctuation and legal characters.
    backslash = '\\'
    hashmark = '#'
    standard_chars = printables.replace(backslash, '').replace(hashmark, '')
    
    # Define line-related parts.
    EOL = LineEnd().suppress()
    SOL = LineStart().leaveWhitespace()
    continuation = (Literal(backslash).leaveWhitespace() + EOL).suppress()
    blankline = SOL + LineEnd()
    
    # Escape codes.
    escaped_hash = Literal(backslash + hashmark)
    escaped_backslash = Literal(backslash + backslash)
    #~ escape = (backslash + Word(printables, exact=1)) | escaped_hash | escaped_backslash
    nonWhiteChar = Word(printables, exact=1).setName('non-whitespace char') 
    escape = escaped_hash | escaped_backslash | Combine(backslash + ~EOL - nonWhiteChar)
    escape.setParseAction(lambda t : t[0][1])
    
    # Free-form text includes internal whitespace, but not leading or trailing.
    text = OneOrMore(White(ws) | quotedString | Word(standard_chars) | escape)
    text.setParseAction(lambda tokens: ''.join(tokens))
    
    comment = (hashmark + restOfLine).suppress()
    
    
    # Define parts of the document and the parser.
    physical_line = text + EOL
    physical_line.setParseAction(lambda tokens: tokens[0].rstrip())
    logical_line = OneOrMore(text + continuation) + physical_line
    logical_line.setParseAction(lambda tokens: ''.join([x.lstrip() for x in tokens]))
    
    line = physical_line | logical_line | EOL
    body = OneOrMore(line)
    parser = body + StringEnd()
    parser.ignore(blankline)
    parser.ignore(comment)



-- Paul
#### 2008-11-14 23:06:05 - steven-d
Paul, I've run the code you posted, and I still get the same exception:





    Expected end of text (at char 279), (line:17, col:1)


#### 2008-11-14 23:50:38 - ptmcg
Version 1.5.1 of pyparsing fixed a bug when '-' is used inside a Combine expression.  Make sure you are using pyparsing 1.5.1 ('easy_install --upgrade pyparsing' will do it).



-- Paul
#### 2009-01-08 15:57:59 - ptmcg
Jenni -



biterscripting is really an alternative language to Python altogether.  Personally, biterscripting looks to me quite Perl-ish, with variable references using leading '$', and unintuitive commands such as lex, wex, and strex ('lex' meaning 'line extract' is especially misleading).  If you are looking for a scripting language designed especially for parsing and extracting data, you might take a look at awk, or possibly ruby.



Pyparsing tries to take a proven and mature scripting environment, Python, and supplement it with easy-to-use parsing constructs.



Here is your stock data parser using pretty vanilla Python:



    # ABC,5.0,5.32,5.67,4.78
    # DEF,11.67,10.74,11.67,10.32
    
    data = open(csvfilename)
    
    for line in data:
        fields = line.split(',')
        ticker = fields[0]
        open,close,high,low = (float(f) for f in fields[1:])
    
        if close > open and high > open:
            print ticker



-- Paul
#### 2009-01-09 10:28:19 - ptmcg
Python code to download a file from the internet:



    import urllib
    
    pagebody = urllib.urlopen('http://www.yahoo.com').read()



I would strongly encourage you to investigate Python as your scripting language of choice, especially if you are, as you say, not a programmer by trade - biterscripting seems pretty weak to me, an ugly hybrid of Perl and bash.

---
## 2008-11-14 21:00:58 - oyster - setDefaultWhitespaceChars bug
there are 2 problems

1. does ParserElement.setDefaultWhitespaceChars works for all ParserElement which does not have its owen setDefaultWhitespaceChars

2. why matched return False





    from pyparsing import *
    
    ParserElement.setDefaultWhitespaceChars(' \t')
    
    INT_=CaselessKeyword('Int')
    DOUBLE_=CaselessKeyword('Double')
    
    BYREF_=CaselessKeyword('Byref')
    OPTIONAL_=CaselessKeyword('Optional')
    
    VAR_=CaselessKeyword('VAR')
    
    ID_=Regex('(?i)[_a-z][_a-z0-9]*').setResultsName('ID')
    
    Atom=(
          Regex('[1-9][0-9]{0,2}')
          + ZeroOrMore(Suppress(Optional('_')) + Regex('[0-9]{3}'))
         )
    ##this effects other ParserElement!
    ##I use about 1 hour to figure out that strange behavior :(
    Atom.setDefaultWhitespaceChars('')
    
    #ParserElement.setDefaultWhitespaceChars(' \t')     ##please note here
    
    TypeName=INT_|DOUBLE_
    
    FunVarAtt=Optional(BYREF_)+Optional(OPTIONAL_)
    FunVarOneLine=(
                    VAR_+ID_+
                    Optional(
                        Suppress(',') + Optional(TypeName, default='Int') +
                        Optional(
                                Suppress(',')+Optional(Group(FunVarAtt), default='')
                                + Optional(Suppress(',')+restOfLine)
                                ,default='')
                        ,default='Int')
                  )
    FunVarOneLine.setDebug()
    print 'var c,  , byref' == FunVarOneLine
    print
    print 'var c,, byref' == FunVarOneLine



if the line 21 is commented, I get



    Match {blahblah} at loc 0(1,1)
    Matched {blahblah} -> ['VAR', 'c', 'Int', '']         <--matched?
    False                                                 <--Falsed
    
    Match {blahblah} at loc 0(1,1)
    Matched {blahblah} -> ['VAR', 'c', 'Int', ['Byref']]
    True





if I delete the # at the beginning of line 21, I get





    Match {blahblah} at loc 0(1,1)
    Matched {blahblah} -> ['VAR', 'c', 'Int', ['Byref']]
    True
    
    Match {blahblah} at loc 0(1,1)
    Matched {blahblah} -> ['VAR', 'c', 'Int', ['Byref']]
    True



#### 2008-11-15 07:07:26 - ptmcg
When you call Atom.setDefaultWhitespaceChars, you are really invoking the class method ParserElement.setDefaultWhitespaceChars.  This is a feature of Python itself, not a peculiarity of pyparsing.  There is no setDefaultWhitespaceChars method on any class other than the base ParserElement class.



If you just want to set the whitespace for Atom, use setWhitespaceChars.  The purpose of setDefaultWhitespaceChars is so that you *don't* have to call setWhitespaceChars on each and every created ParserElement (many of which are created implicitly when using '+' and '|' operators).



And if I may make so bold as to make some other suggestions:



In this code:



    Atom=(
          Regex('[1-9][0-9]{0,2}')
          + ZeroOrMore(Suppress(Optional('_')) + Regex('[0-9]{3}'))
         )

aren't you really defining an integer?  You are doing a good job of suppressing the '_', but you are allowing whitespace between...



Ah! Now I see why you are calling setDefaultWhitespaceChars!  You don't want '10 _ 123' to be parsed the same as '10_123'.  The solution you are looking for is the Combine class:



    Integer=Combine(
          Regex('[1-9][0-9]{0,2}')
          + ZeroOrMore(Suppress(Optional('_')) + Regex('[0-9]{3}'))
         ).setName('integer').setParseAction(lambda t:int(t[0]))
    assert '10' == Integer
    assert '10_000_000' == Integer
    assert '10_000_00' != Integer
    assert Integer.parseString('10_000_000')[0] == 10000000



I've changed the expression name from Atom to Integer, because Atom usually has a larger meaning within an overall grammar, and you might need to use Integer's in some cases when you don't want Atom's.  Later on, I would expect to see some code like:



    Atom = Integer | ID # for example

(Ideally, you should reserve capitalized names for classes only, by the way - this is a pretty well-accepted Python convention.)





Be careful not to confuse setName with setResultsName.  In this code:



    ID_=Regex('(?i)[_a-z][_a-z0-9]*').setResultsName('ID')

every use of ID_ will get the results name 'ID'.  This will get to be a problem when a given expression uses ID_ in multiple ways, such as:



    ImportStatement = 'import' + ID_ + Optional('as' + ID_)



Use setResultsName when describing how an expression is used within a larger expression.  Use setName when you want to generically name the syntax of the expression.  Use setResultsName (or even better, just use the 'expr(resultsName)' form) to show what role the expression plays within the larger expression:



    ImportStatement = 'import' + ID_('module') + Optional('as' + ID_('alias'))



The distinction between setName and setResultsName might be clearer when we look at how the names are used when an exception is raised.  In our mythical ImportStatement expression, look at the exception we get when parsing 'import 100':



    pyparsing.ParseException: Expected Re:('(?i)[_a-z][_a-z0-9]*') (at char 7), (line:1, col:8)



Ick!  Now if we used setName to say that ID_ represents and identifier:



    ID_.setName('identifier')

we now get the nicer:



    pyparsing.ParseException: Expected identifier (at char 7), (line:1, col:8)





The reason for your unexpected 'Matched' vs. '==' -> False behavior is that with the whitespace-skipping bugged up, the final 'byref' was not parsed.  setDebug() will report matches of an expression while doing the overall parsing, and a valid FuncVarOneLine *was* found - it just didn't include the trailing 'byref' because of the whitespace problem.  '==' on the other hand will only report True if the entire string parses successfully.



-- Paul

---
## 2008-11-17 14:45:24 - steven-d - Should ParseSyntaxException be a subclass of ParseException?
Should ParseSyntaxException be a subclass of ParseException? I can't imagine that the relationship should be the other way around.


    >>> issubclass(pyparsing.ParseSyntaxException, pyparsing.ParseException)
    False

I've looked at the pyparsing class diagram, but ParseSyntaxException isn't even listed. (Disclaimer: I'm running version 1.5.1, but still looking at the docs from 1.5.0.) What is the recommended way to capture parsing exceptions? I've been doing this:


try:
    for item in parser.parseString(data):
        print item
except ParseException, err:
    print err.line
    print ' '*(err.column-1) + '^'
    print err

but that doesn't capture ParserSyntaxException. Should I just catch anything that inherits from BaseParseException or is that too much?


-- 

Steven

#### 2008-11-17 16:58:51 - ptmcg
The docs from each exception classes show the inheritance tree for each exception, and from these you can see that the overall inheritance tree of pyparsing exceptions is:



    exception.Exception
    |
    +- ParseBaseException
       |
       +- ParseException
       |
       +- ParseFatalException
          |
          +- ParseSyntaxException



So to catch all exceptions that can occur during parsing, you should catch ParseBaseException.  ParseException is a routine exception, indicating failure to match the current expression.  During the course of the parsing of a complex grammar, many 1000's of such exceptions can occur and be raised.  However, if ParseFatalException or any subclass of it (there is currently only 1, ParseSyntaxException), then parsing ends immediately.



Sorry about the published class diagrams, I forgot there had been so many changes since the last update - I'll get them updated and into SVN as soon as I can.



-- Paul

---
## 2008-12-09 12:00:04 - ajone825 - how to input a file (Newb)
How do I make pyparser grab the file to input via command line?

#### 2008-12-10 10:55:50 - ptmcg
Welcome to pyparsing, and to Python for that matter!  This is really just a basic Python question, how to access the command line args passed into the Python interpreter from the command line?



The answer lies in the sys module (part of standard Python) - the command line args are stored in the list variable 'argv'.  Here is a simple script:



    import sys
    print sys.argv



If I run this script with the command line 'python z.py first second third', I get this result:



    ['z.py', 'first', 'second', 'third']
    



If you pass in a file name as the first argument, then you could read the contents and parse it using something like this:



    import sys
    from pyparsing import Word, alphas, alphanums
    
    # define a little parser that recognizes word groups that
    # start with a letter
    wrd = Word(alphas,alphanums)
    
    # create a parse action to capitalize a parsed word
    capitalize = lambda t: t[0].title()
    wrd.setParseAction(capitalize)
    
    
    if len(sys.argv) > 1:
        filecontent = file(sys.argv[1]).read()
    
        # now the entire file has been read into the string variable
        # filecontent, which you can then pass to the parseString,
        # searchString, scanString, or transformString 
        # method of your pyparsing grammar variable
    
        # print a copy of the input file, with every word capitalized
        print wrd.transformString(filecontent)
    
    else:
        print 'missing filename argument'



There are a number of good getting-started books for Python.  If you are already familiar with programming in general, you might jump straight to reading the Python Cookbook - it will give you a bunch of quick shortcuts, plus expose you to many of the standard Python idioms.  This will help you to take advantage of Python's best features, rather than just repeat your learnings and methods from your previous language (which can be very frustrating!).



There is also a Python tutor mailing list, and they are very open to all sorts of getting-started questions.



Cheers!

-- Paul

---
## 2008-12-13 13:05:38 - orestis82 - Error tolerance (needed for syntax highlighting?)
Hello,



as many others before me, and no doubt many others after me, I'm writing a text editor.



I'd like to add syntax highlighting, and PyParsing seems to be suited for the job. However, the problem with text editors is that:



a) The input is sometimes incomplete (when a user is typing)

b) The input may be wrong (typos, syntax errors etc)

c) The grammar itself may not be complete.



I'm not very familiar with error tolerance in parsers (I can't find much information about it) - does pyparsing support this?



Ideally I'd like to have something like TextMate's scopes - each token is assigned one or more hierarchical scopes, so eg. a parameter in a function definition in python will have the following scopes:



source.python meta.function.definition.parameter



and so on. My instinct is that to implement this nicely you'd need to have states in your parser - some rules are used in one state, some rules in other states, and detecting some tokens performs a state transition. 



It seems that textmate defines language syntax as regular expressions with captures, lookbehinds, references and all that. It also has a begin...end concept (the state bit I mentioned before). I wonder if pyparsing can support this elegantly or if it's not targeted at this kind of application.



Thanks very much!

#### 2008-12-17 08:11:52 - ptmcg
Pyparsing is not really targetted to text highlighting like this, and I think googling will turn up some existing Python text highlighting modules that might get you going quicker than re-engineering them with pyparsing.



Pyparsing implements states in its expressions implicitly, they are not really exposed to the user.  Here is an example grammar:

- A followed by a hexadecimal digit

- B optionally followed by a lower case vowel

- lower case vowels may optionally be followed by an integer in parentheses

- C alone



You can visualize this as having a super state that looks for A, B, or C, and then branching to other states for the optional parts before branching back to the main super state.



In pyparsing, you implement this by defining expressions, and linking them with repetition, alternation, and optional processing:



    from pyparsing import Word, hexnums, Word, nums, oneOf, Optional, OneOrMore, Group
    
    Aexpr = 'A' + Word(hexnums,exact=1)
    integer = Word(nums)
    lcVowel = oneOf('a e i o u') + Optional('(' + integer + ')')
    Bexpr = 'B' + Optional(lcVowel)
    Cexpr = 'C'
    
    grammar = OneOrMore(Group(Aexpr | Bexpr | Cexpr))
    
    print grammar.parseString('A0BeBaA1Be(14)CBA12')



prints



    [['A', '0'], ['B', 'e'], ['B', 'a'], ['A', '1'], 
     ['B', 'e', '(', '14', ')'], ['C'], ['B'], ['A', '1']]



-- Paul

---
## 2008-12-16 09:02:02 - mUogoro - Callback execution inside grammar rules
I'm using the pyparsing module for parsing fields of simple C structures. Fields of basic types are parsed with a simple rule like this





    basicType = Keyword( 'void' ) |\
    Keyword( 'char' ) |\
    Keyword( 'int' ) |\
    Keyword( 'float' ) |\
    Keyword( 'double' )



The problem is when I've to parse 'redefined' types like





    typedef int int32;



The parser is built inside another program and uses a 'typedef callback' to get the original type every time a redefined type is found ('int' in the example above). The idea is to call the typedef callback when the basicType raises a ParseError and parse again the the field after its type is substituted with the correct type (I'm suppose the substitution has to be done in the parsed string...am I wrong?). Now the question: how can I 'encapsulate' the typedef callback execution in a rule executed after the basicType rule? Do I need to define a new ParseElement which executes the callback in its parse method?



Thanks for your help (and excuse me for my very bad english :) ),

Daniele

#### 2008-12-17 07:59:01 - ptmcg
Daniele -



There are 4 expressions you will need to handle this situation:

- a typedef expression to recognize and process 'typedef int int32;'

- a basic type expression, which you already have

- a user-defined type expression, to recognize types defined using typedef (and to which you will attach your callback to replace the original value with the defined value)

- a type expression that tests for basic types or user-defined types



To the typedef expression, attach a parse action that captures the user-defined name and the corresponding basic type.



So the code should look something like this (not tested):



    from pyparsing import *
    
    identifier = Word(alphas+'_', alphanums+'_')
    basicType = Keyword( 'void' ) |\
                Keyword( 'char' ) |\
                Keyword( 'int' ) |\
                Keyword( 'float' ) |\
                Keyword( 'double' )
    typedefExpr = Keyword('typedef') + basicType('baseType') + identifier('udType')
    userDefinedType = identifier.copy()
    
    typeExpr = basicType | userDefinedType
    
    typedefs = {}
    def captureTypeDefs(tokens):
        typedefs[tokens.udType] = tokens.baseType
    typedefExpr.setParseAction(captureTypeDefs)
    
    def validateUdType(tokens):
        if tokens[0] in typedefs:
            return typedefs[tokens[0]]
        else:
            raise ParseException(''%s' is not a valid type' % tokens[0])
    userDefinedType.setParseAction(validateUdType)



You should be sure to reinitialize the typedefs variable to an empty dict before starting to parse a new file.



HTH, and welcome to pyparsing!

-- Paul
#### 2008-12-19 06:44:08 - ajaksu
Hi Daniele, Paul :)



Here's how I'm doing something similar, to treat typedef-ed types as types (simplified and adapted to your usecase, so might be buggy):





    from pyparsing import Word, Keyword, MatchFirst, ParseException, alphanums, alphas
    
    types = set('int void long char double short float'.split())
    typekeywords = [Keyword(t) for t in types]
    basicType = MatchFirst(typekeywords)
    
    identifier = Word(alphas + '_', alphanums + '_')
    udType = identifier('udType')
    
    typedef = Keyword('typedef') +  basicType  + udType + ';'
    
    typedefs = {}
    def captureTypeDefs(tokens):
        global basicType
        newtype = tokens.udType
        basetype = tokens[1]
    
        if basetype in typedefs: # Recursive typedef
            basal = typedefs[basetype]
            print '(Unrolling '%s' to '%s')' % (basetype, basal),
            basetype = basal
        if newtype in typedefs:
            print '(Redefining '%s' from '%s' )' % (newtype, typedefs[newtype]),
        print 'Adding Type: '%s' (maps to '%s')' % (newtype, basetype)
        typedefs[newtype] = basetype
        basicType |=   Keyword(newtype)
    
    typedef.setParseAction(captureTypeDefs)
    
    test_input = '''
    typedef int int32;
    typedef int int32;
    typedef int32 integer;
    typedef integer integer32;
    '''
    
    typedef.searchString(test_input)
    

    

The idea is that a typedef-ed type will behave like a basetype, to handle recursive typedefs.



IIUC, you can use Paul's validateUdType as typeExpr's parseAction (as udTypes will be merged into basicType). That's why 'types' is a set, to allow fast checking of typeExpr against true primitive types.



Cheers,

Daniel

---
## 2008-12-18 05:40:54 - mayama - Nested elements
Hello,

So far I'm very pleased with pyparsing.

But I am writing a translator for a mini-language and I'm trying to figure out if it can be done with it.



The language represents 'if' like this:





    [ a_ok ? action1 action2 : action3 ]



And this should be translated to C code like this:





    if(a_ok())
    {
      action1;
      action2;
    }
    else
    {
      action3;
    }



This is very easy to do with pyparsing just setting a parseAction.

But what if we can have nested 'if's like this:





    [ a_ok ? action1 [ b_ok ? action2 : action3 ] : action4 ]



Would it be easy to translate the above to this:





    if(a_ok())
    {
      action1;
      if(b_ok())
        action2;
      else
        action3;
    }  
    else
    {
      action4;
    }



Of course it can be done, but I'd like to know if there is a simple and clean way to do it (something like having the parseAction being able to trigger the parseActions of the internal elements so that they would render themselves recursively).



regards,

mayama

#### 2008-12-18 06:30:42 - mayama
Sorry. Just learned about parseString and was trying to use it for everything. Now I can see what I described can be done simply using transformString. Pyparsing is cool!
#### 2008-12-18 09:36:27 - ptmcg
This sample might spark some ideas.



-- Paul





    from pyparsing import *
    
    test = '''[ a_ok ? action1 [ b_ok ? action2 : action3 ] : action4 ]'''
    
    LBRACK,RBRACK,COLON,QMARK = map(Suppress,'[]:?')
    stmt = Forward()
    word = Word(alphas, alphanums+'_')
    
    basicAction = word
    
    ifStmt = Group( LBRACK + 
                word('condition') + QMARK + 
                Group( OneOrMore(stmt) )('thenPath') + 
                Optional( COLON + Group( OneOrMore(stmt) )('elsePath')) +
                RBRACK )
    
    stmt << ( ifStmt | basicAction )
    
    print stmt.parseString(test)[0].dump()
    
    # add some parse actions to generate the desired code...
    basicAction.setParseAction(
        lambda toks: toks[0] + ';'
        )
    
    indent = lambda s,ind : ind + ('\n'+ind).join(s.split('\n'))
    ifStmt.setParseAction(
        lambda toks: '\n'.join(
            [ 'if(' + toks[0].condition + ')', ] +
            [ indent(s,'  ') for s in toks[0].thenPath ] +
            ( ([ 'else', ] +
              [ indent(s,'  ') for s in toks[0].elsePath ]) if toks[0].elsePath else [] )
            )
        )
    
    print stmt.transformString(test)

prints:



    ['a_ok', ['action1', ['b_ok', ['action2'], ['action3']]], ['action4']]
    - condition: a_ok
    - elsePath: ['action4']
    - thenPath: ['action1', ['b_ok', ['action2'], ['action3']]]
    if(a_ok)
      action1;
      if(b_ok)
        action2;
      else
        action3;
    else
      action4;


#### 2008-12-20 01:39:52 - mayama
Paul, thanks a lot. I just need to tweak the above code a little and my translator will be ready. Pyparsing is cool!

---
## 2008-12-19 03:03:21 - chrisi_g - nestedExpr: unexpected behavior
Hello,



please excuse my bad english.

I have some difficulties with nestedExpr.



The following code demonstrates my problem:





    from pyparsing import *
    
    data = '''Python is [[a awesome|a fantastic] scripting language|[a awesome|a fantastic] programming language]'''
    
    def alternative(string, begin, token):
        if len(token) > 1:
            print 'token:', token
    
    text = CharsNotIn('[|]')
    
    alt = Forward()
    alt_group = Forward()
    alt << OneOrMore(text | alt_group).setParseAction(alternative)
    alt_group << nestedExpr('[', ']', delimitedList(alt , delim='|'))
    
    parser = StringStart() + OneOrMore(text | alt_group) + StringEnd()
    parser.parseString(data).asList()



The output is this:

    token: [['a awesome', 'a fantastic'], ' programming language']


But imho it should be:

    token: [['a awesome', 'a fantastic'], ' scripting language']
    token: [['a awesome', 'a fantastic'], ' programming language']

I can't see, what I'm doing wrong. Can you please help me?



Best regards,

chrisi

#### 2008-12-19 07:36:10 - ptmcg
Your English is fine, not to worry!



I added some pyparsing debugging statements to get a little more insight into what is happening in your grammar.  (One of these days, I should write an article on pyparsing debugging...)



Note the addition of setName and setDebug calls:



    from pyparsing import *
    
    data = '''Python is [[a awesome|a fantastic] scripting language|[a awesome|a fantastic] programming language]'''
    
    def alternative(string, begin, token):
        if len(token) > 1:
            print 'token:', token
    
    text = CharsNotIn('[|]')
    
    alt = Forward()
    alt_group = Forward()
    alt << OneOrMore(text | alt_group).setParseAction(alternative)
    alt_group << nestedExpr('[', ']', 
                   delimitedList(alt , delim='|').setName('altlist').setDebug())
    
    alt.setName('alt').setDebug()
    alt_group.setName('alt_group').setDebug()
    
    parser = StringStart() + OneOrMore(text | alt_group) + StringEnd()
    
    print (' '*9).join(map(str,range(10)))
    print (''.join(map(str,range(10))))*10
    print data
    print
    print parser.parseString(data).asList()
    



prints:





    0         1         2         3         4         5         6         7         8         9
    0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789
    Python is [[a awesome|a fantastic] scripting language|[a awesome|a fantastic] programming language]
    
    Match alt_group at loc 10(1,11)
    Match altlist at loc 12(1,13)
    Match alt at loc 12(1,13)
    Match alt_group at loc 21(1,22)
    Exception raised:Expected '[' (at char 21), (line:1, col:22)
    Matched alt -> ['a awesome']
    Match alt at loc 22(1,23)
    Match alt_group at loc 33(1,34)
    Exception raised:Expected '[' (at char 33), (line:1, col:34)
    Matched alt -> ['a fantastic']
    Matched altlist -> ['a awesome', 'a fantastic']
    Match altlist at loc 33(1,34)
    Match alt at loc 33(1,34)
    Match alt_group at loc 33(1,34)
    Exception raised:Expected '[' (at char 33), (line:1, col:34)
    Exception raised:Expected !W:([|]) (at char 33), (line:1, col:34)
    Exception raised:Expected !W:([|]) (at char 33), (line:1, col:34)
    Match altlist at loc 34(1,35)
    Match alt at loc 35(1,36)
    Match alt_group at loc 53(1,54)
    Exception raised:Expected '[' (at char 53), (line:1, col:54)
    Matched alt -> ['scripting language']
    Match alt at loc 54(1,55)
    Match alt_group at loc 54(1,55)
    Match altlist at loc 55(1,56)
    Match alt at loc 55(1,56)
    Match alt_group at loc 64(1,65)
    Exception raised:Expected '[' (at char 64), (line:1, col:65)
    Matched alt -> ['a awesome']
    Match alt at loc 65(1,66)
    Match alt_group at loc 76(1,77)
    Exception raised:Expected '[' (at char 76), (line:1, col:77)
    Matched alt -> ['a fantastic']
    Matched altlist -> ['a awesome', 'a fantastic']
    Match altlist at loc 76(1,77)
    Match alt at loc 76(1,77)
    Match alt_group at loc 76(1,77)
    Exception raised:Expected '[' (at char 76), (line:1, col:77)
    Exception raised:Expected !W:([|]) (at char 76), (line:1, col:77)
    Exception raised:Expected !W:([|]) (at char 76), (line:1, col:77)
    Matched alt_group -> [['a awesome', 'a fantastic']]
    Match alt_group at loc 98(1,99)
    Exception raised:Expected '[' (at char 98), (line:1, col:99)
    token: [['a awesome', 'a fantastic'], ' programming language']
    Matched alt -> [['a awesome', 'a fantastic'], ' programming language']
    Matched altlist -> ['scripting language', ['a awesome', 'a fantastic'], ' programming language']
    Match altlist at loc 98(1,99)
    Match alt at loc 98(1,99)
    Match alt_group at loc 98(1,99)
    Exception raised:Expected '[' (at char 98), (line:1, col:99)
    Exception raised:Expected !W:([|]) (at char 98), (line:1, col:99)
    Exception raised:Expected !W:([|]) (at char 98), (line:1, col:99)
    Matched alt_group -> [[['a awesome', 'a fantastic'], 'scripting language', ['a awesome', 'a fantastic'], ' programming language']]
    Match alt_group at loc 99(1,100)
    Exception raised:Expected '[' (at char 99), (line:1, col:100)
    ['Python is ', [['a awesome', 'a fantastic'], 'scripting language', ['a awesome', 'a fantastic'], ' programming language']]



Notice what gets matched as an altlist at location 76:



    Matched altlist -> ['scripting language', ['a awesome', 'a fantastic'], ' programming language']



I don't have more time at the moment to give you more guidance on how to resolve this, but I hope you can puzzle this out with this added info.



Welcome to pyparsing!

-- Paul

---
## 2008-12-27 23:28:33 - pgurumur - Having some trouble parsing.
I want to parse the following with name and location key value pairs being required

    vlan 145 {
     name FOO
     location foobar
    }
    

I am able to create a simple parser using pyparsing. The parser is able to print errors, if the first section misses either the name or location, but if the first section is perfect and rest of them have mistakes, I does not print any exception and only parses upto that point.





    #!/usr/bin/env python2.5
    
    import os
    
    # import everything
    from pyparsing import *
    
    class ParserError(Exception):
       pass
    
    class Lexer:
       def __init__(self, Filename):
          self._error = ParserError()
          self._file = None
          self._parse = None
          self._tokensMin = 3
    
          if Filename:
             self._file = Filename
          else:
             self._err('no filename provided to parse')
    
          if os.path.exists(self._file):
             try:
                self._fh = open(self._file, 'r')
             except IOError, message:
                self._err(message)
             else:
                self._fh.close()
          else:
             self._err('%s: does not exist' %self._file)
    
          self._tokensInit()
    
       def _err(self, Message):
          if Message:
             raise ParserError, Message
    
       def _tokensInit(self):
          quote = Optional(Suppress('\''))
          hex = 'abcdefABCDEF0123456789'
          leftbrace = Suppress('{')
          rightbrace = Suppress('}')
    
          vlan = Keyword('vlan', caseless = True).suppress() + Word(nums)
          name = Keyword('name', caseless = True).suppress() + quote + Word(
                alphanums + '-' + '_') + quote
          locn = Keyword('location', caseless = True).suppress() + quote + Word(
                alphanums + '-' + '_') + quote
    
          self._parse = OneOrMore(vlan + leftbrace + name + locn + rightbrace)
          self._parse.ignore(Literal('\\') + LineEnd())
          self._parse.ignore(Literal('#') + restOfLine)
    
       def parse(self):
          result = None
          resLen = 0
          try:
             result = self._parse.parseFile(self._file)
          except ParseException, ex:
             message = 'parsing failed, file %s, exception: %s' %(self._file, ex)
             self._err(message)
          else:
             resLen = result
    
          assert resLen != self._tokensMin, \
                'parsing failed, expected atleast %d tokens, got %d' %(
                   resLen, self._tokensMin)
    
          return result



Main python code, which calls the lexer





    #!/usr/bin/env python2.5
    
    import lexer, optparse, os, string, sys
    
    argc = len(sys.argv)
    prog = os.path.split(sys.argv[0])[1]
    version = '1.0'
    
    def error(Message):
       if Message:
          global prog
          print '%s: %s' %(prog, Message)
    
       sys.exit(1)
    
    def ParseOptions(Options):
       if Options:
          if Options.file:
             result = None
             try:
                parse = lexer.Lexer(Options.file)
             except lexer.ParserError, message:
                error(message)
    
             try:
                result = parse.parse()
             except lexer.ParserError, message:
                error(message)
    
             print result
    
    def GetOpt():
       usageStr = '%s [options]' %prog
       verStr = '%s %s' %(prog, version)
       parser = optparse.OptionParser(usage=usageStr, version=verStr)
       parser.add_option('-f', '--filename', help='Parse file', dest='file')
    
       (options, args) = parser.parse_args()
       ParseOptions(options)
    
    if __name__ == '__main__`:
       if (argc <= 1):
          error('need more arguments, use -h or --help to view more options')
       else:
          GetOpt()



Example configuration file:

    vlan 147 { \
       name BAR    \
       location 6023 }
    
    vlan {
       name GOO }
    
    vlan 14 {
       name BOO

#### 2008-12-28 01:54:06 - ptmcg
Prabhu -



Welcome to pyparsing!



Your question is a common one, and is covered under the first FAQ.  If that explanation is not clear, please write back.



-- Paul
#### 2008-12-28 11:26:43 - pgurumur
Thanks Paul - that did the trick.



