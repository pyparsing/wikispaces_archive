## Pyparsing Wikispaces Discussion - 2009

[Note: these entries are fairly old, and predate many new features of pyparsing,
and are predominantly coded using Python 2.
They are captured here for historical benefit, but may not contain
the most current practices or features. We will try to add editor
notes to entries to indicate when discussions have been 
overtaken by development events.]

[2009-01-06 03:14:24 - kerny404 - Simple logic expression parser](all_wiki_discussion_toc_2009.md#2009-01-06-031424---kerny404---simple-logic-expression-parser)  
[2009-01-08 11:24:01 - bobdevine - small bug + parsing pairs](all_wiki_discussion_toc_2009.md#2009-01-08-112401---bobdevine---small-bug--parsing-pairs)  
[2009-01-26 03:56:34 - HWJ - newbee needs some initial help](all_wiki_discussion_toc_2009.md#2009-01-26-035634---hwj---newbee-needs-some-initial-help)  
[2009-01-27 01:14:54 - HWJ - Most needed : a quick reference](all_wiki_discussion_toc_2009.md#2009-01-27-011454---hwj---most-needed--a-quick-reference)  
[2009-01-27 06:33:11 - HWJ - withoutAttribute - howto](all_wiki_discussion_toc_2009.md#2009-01-27-063311---hwj---withoutattribute---howto)  
[2009-01-29 04:05:09 - HWJ - SkipTo(...,include=True)  BUG ?](all_wiki_discussion_toc_2009.md#2009-01-29-040509---hwj---skiptoincludetrue--bug-)  
[2009-01-29 23:39:18 - HWJ - ignore + SkipTo (how does it work?)](all_wiki_discussion_toc_2009.md#2009-01-29-233918---hwj---ignore--skipto-how-does-it-work)  
[2009-01-30 11:31:57 - HWJ - nestedExpr -> infinite loop (WHY)](all_wiki_discussion_toc_2009.md#2009-01-30-113157---hwj---nestedexpr---infinite-loop-why)  
[2009-02-02 06:09:51 - debug - Parsing a simple LOGO like language](all_wiki_discussion_toc_2009.md#2009-02-02-060951---debug---parsing-a-simple-logo-like-language)  
[2009-02-06 05:29:46 - mvanderw - If I match _this_, then parse it like _that_](all_wiki_discussion_toc_2009.md#2009-02-06-052946---mvanderw---if-i-match-_this_-then-parse-it-like-_that_)  
[2009-02-07 16:02:33 - stakhanov2 - memory issues with Py3k](all_wiki_discussion_toc_2009.md#2009-02-07-160233---stakhanov2---memory-issues-with-py3k)  
[2009-02-17 13:11:34 - djlawler - Ironpython 2.01](all_wiki_discussion_toc_2009.md#2009-02-17-131134---djlawler---ironpython-201)  
[2009-02-18 06:41:05 - mUogoro - C function pointer parser](all_wiki_discussion_toc_2009.md#2009-02-18-064105---muogoro---c-function-pointer-parser)  
[2009-02-18 16:07:52 - ahusak - Issue with lineno() and the loc variable.](all_wiki_discussion_toc_2009.md#2009-02-18-160752---ahusak---issue-with-lineno-and-the-loc-variable)  
[2009-02-26 01:23:22 - asb_india - Help for the newbie](all_wiki_discussion_toc_2009.md#2009-02-26-012322---asb_india---help-for-the-newbie)  
[2009-03-06 10:42:55 - djayc - Match a word in the "right most" position.](all_wiki_discussion_toc_2009.md#2009-03-06-104255---djayc---match-a-word-in-the-"right-most"-position)  
[2009-03-12 01:01:15 - NewbeCaroline - Need to filter out lines based on expr and then calculate time difference](all_wiki_discussion_toc_2009.md#2009-03-12-010115---newbecaroline---need-to-filter-out-lines-based-on-expr-and-then-calculate-time-difference)  
[2009-03-12 17:08:32 - stanchan - Issue with parsing multiline statement](all_wiki_discussion_toc_2009.md#2009-03-12-170832---stanchan---issue-with-parsing-multiline-statement)  
[2009-03-18 09:24:55 - gmonkey5 - Getting more specific error messages from the MatchFirst object](all_wiki_discussion_toc_2009.md#2009-03-18-092455---gmonkey5---getting-more-specific-error-messages-from-the-matchfirst-object)  
[2009-03-21 07:50:56 - dbv - citation parser](all_wiki_discussion_toc_2009.md#2009-03-21-075056---dbv---citation-parser)  
[2009-03-23 07:53:32 - dbv - Special Characters](all_wiki_discussion_toc_2009.md#2009-03-23-075332---dbv---special-characters)  
[2009-03-25 10:03:23 - reeeh2000 - Recursive Grammer](all_wiki_discussion_toc_2009.md#2009-03-25-100323---reeeh2000---recursive-grammer)  
[2009-03-27 12:43:38 - pratdam - pyparsing  for  Windows INF  Files ](all_wiki_discussion_toc_2009.md#2009-03-27-124338---pratdam---pyparsing--for--windows-inf--files-)  
[2009-03-28 19:47:27 - reeeh2000 - Pyparsing for Python3](all_wiki_discussion_toc_2009.md#2009-03-28-194727---reeeh2000---pyparsing-for-python3)  
[2009-03-31 07:22:48 - john-l - Example and lessons learned: parsing SPARQL in RDFLib](all_wiki_discussion_toc_2009.md#2009-03-31-072248---john-l---example-and-lessons-learned-parsing-sparql-in-rdflib)  
[2009-03-31 16:25:30 - reeeh2000 - Tokenizing](all_wiki_discussion_toc_2009.md#2009-03-31-162530---reeeh2000---tokenizing)  
[2009-04-02 13:41:59 - dbv - 64-bit Windows](all_wiki_discussion_toc_2009.md#2009-04-02-134159---dbv---64-bit-windows)  
[2009-04-04 12:03:54 - ellisonbg - Subtle infinite loop with Optional](all_wiki_discussion_toc_2009.md#2009-04-04-120354---ellisonbg---subtle-infinite-loop-with-optional)  
[2009-04-10 07:10:57 - JaimeWyant - 1.5.2 pypi release?](all_wiki_discussion_toc_2009.md#2009-04-10-071057---jaimewyant---152-pypi-release)  
[2009-04-15 08:18:50 - Llanilek - Simple Question](all_wiki_discussion_toc_2009.md#2009-04-15-081850---llanilek---simple-question)  
[2009-04-18 12:49:17 - pynguin - problems with setName()](all_wiki_discussion_toc_2009.md#2009-04-18-124917---pynguin---problems-with-setname)  
[2009-04-20 06:37:44 - stakhanov2 - memory issues with Py3k (reminder)](all_wiki_discussion_toc_2009.md#2009-04-20-063744---stakhanov2---memory-issues-with-py3k-reminder)  
[2009-04-22 13:31:35 - stanchan - Top level parsing](all_wiki_discussion_toc_2009.md#2009-04-22-133135---stanchan---top-level-parsing)  
[2009-04-28 06:47:49 - reynaudd - maximum recursion depth exceeded](all_wiki_discussion_toc_2009.md#2009-04-28-064749---reynaudd---maximum-recursion-depth-exceeded)  
[2009-04-28 12:40:02 - JesterEE - Optional() TypeError Bug?](all_wiki_discussion_toc_2009.md#2009-04-28-124002---jesteree---optional-typeerror-bug)  
[2009-04-29 13:15:02 - catherinedevlin - strange propagation of ignoreExprs into elements](all_wiki_discussion_toc_2009.md#2009-04-29-131502---catherinedevlin---strange-propagation-of-ignoreexprs-into-elements)  
[2009-05-13 15:08:50 - oafilipoai - scanString problem](all_wiki_discussion_toc_2009.md#2009-05-13-150850---oafilipoai---scanstring-problem)  
[2009-05-13 16:20:23 - oafilipoai - White spaces not ignored](all_wiki_discussion_toc_2009.md#2009-05-13-162023---oafilipoai---white-spaces-not-ignored)  
[2009-05-14 10:40:06 - oafilipoai - Excluding keywords in comments or strings](all_wiki_discussion_toc_2009.md#2009-05-14-104006---oafilipoai---excluding-keywords-in-comments-or-strings)  
[2009-05-18 05:03:29 - asb_india - Parsing selected](all_wiki_discussion_toc_2009.md#2009-05-18-050329---asb_india---parsing-selected)  
[2009-05-29 16:18:10 - dbv - Windows amd64](all_wiki_discussion_toc_2009.md#2009-05-29-161810---dbv---windows-amd64)  
[2009-06-02 05:36:06 - ivdkleyn - Problems with recursive parsing](all_wiki_discussion_toc_2009.md#2009-06-02-053606---ivdkleyn---problems-with-recursive-parsing)  
[2009-06-03 01:32:24 - BlGene - C++ parser](all_wiki_discussion_toc_2009.md#2009-06-03-013224---blgene---c-parser)  
[2009-06-05 13:06:10 - ssscripting - need some help](all_wiki_discussion_toc_2009.md#2009-06-05-130610---ssscripting---need-some-help)  
[2009-06-09 03:10:58 - nederhrj - [Newbie] Converting ad-hoc text into XML](all_wiki_discussion_toc_2009.md#2009-06-09-031058---nederhrj---[newbie]-converting-ad-hoc-text-into-xml)  
[2009-06-11 02:24:51 - WernerBruhin - PayPal email - again](all_wiki_discussion_toc_2009.md#2009-06-11-022451---wernerbruhin---paypal-email---again)  
[2009-06-22 02:10:24 - asb_india - Parsing Selected values](all_wiki_discussion_toc_2009.md#2009-06-22-021024---asb_india---parsing-selected-values)  
[2009-06-25 17:51:45 - GrahamDennis - Inconsistent results](all_wiki_discussion_toc_2009.md#2009-06-25-175145---grahamdennis---inconsistent-results)  
[2009-06-27 05:38:30 - GrahamDennis - C statement parser design / max recursion depth exceeded](all_wiki_discussion_toc_2009.md#2009-06-27-053830---grahamdennis---c-statement-parser-design--max-recursion-depth-exceeded)  
[2009-06-30 05:30:38 - ptmcg - DSL article feedback (Mark Fink)](all_wiki_discussion_toc_2009.md#2009-06-30-053038---ptmcg---dsl-article-feedback-mark-fink)  
[2009-06-30 05:41:50 - ptmcg - OnLAMP article feedback (bearophile)](all_wiki_discussion_toc_2009.md#2009-06-30-054150---ptmcg---onlamp-article-feedback-bearophile)  
[2009-06-30 05:45:53 - ptmcg - OnLAMP article feedback (Alex Shinn)](all_wiki_discussion_toc_2009.md#2009-06-30-054553---ptmcg---onlamp-article-feedback-alex-shinn)  
[2009-06-30 05:51:58 - ptmcg - OnLAMP article feedback (Dave Feustel)](all_wiki_discussion_toc_2009.md#2009-06-30-055158---ptmcg---onlamp-article-feedback-dave-feustel)  
[2009-06-30 07:14:09 - ptmcg - GSWP errata (Bruce van der Kooij)](all_wiki_discussion_toc_2009.md#2009-06-30-071409---ptmcg---gswp-errata-bruce-van-der-kooij)  
[2009-07-01 05:56:43 - asb_india - Parsing mutiple line](all_wiki_discussion_toc_2009.md#2009-07-01-055643---asb_india---parsing-mutiple-line)  
[2009-07-23 01:59:25 - asb_india - Parsing non latin and latin character](all_wiki_discussion_toc_2009.md#2009-07-23-015925---asb_india---parsing-non-latin-and-latin-character)  
[2009-07-26 01:52:23 - mbutow - not ignoring some "special" comments?](all_wiki_discussion_toc_2009.md#2009-07-26-015223---mbutow---not-ignoring-some-"special"-comments)  
[2009-07-28 07:17:06 - zaviichen - partial matching](all_wiki_discussion_toc_2009.md#2009-07-28-071706---zaviichen---partial-matching)  
[2009-08-02 17:22:46 - gregglind - initchars, bodychars](all_wiki_discussion_toc_2009.md#2009-08-02-172246---gregglind---initchars-bodychars)  
[2009-08-13 13:16:52 - john-l - indentedBlock performance problem with proposed solution](all_wiki_discussion_toc_2009.md#2009-08-13-131652---john-l---indentedblock-performance-problem-with-proposed-solution)  
[2009-08-19 14:10:33 - gregglind - list of the overloaded operators?](all_wiki_discussion_toc_2009.md#2009-08-19-141033---gregglind---list-of-the-overloaded-operators)  
[2009-08-21 11:01:21 - gregglind - ParseResults item values "stringlike" sometimes, "listlike" other times](all_wiki_discussion_toc_2009.md#2009-08-21-110121---gregglind---parseresults-item-values-"stringlike"-sometimes-"listlike"-other-times)  
[2009-08-24 17:09:58 - oafilipoai - parsing to object hierarchy ](all_wiki_discussion_toc_2009.md#2009-08-24-170958---oafilipoai---parsing-to-object-hierarchy-)  
[2009-08-27 07:12:31 - akonsu - (newbie) setResultsName question](all_wiki_discussion_toc_2009.md#2009-08-27-071231---akonsu---newbie-setresultsname-question)  
[2009-08-31 23:05:00 - marjoj - (newbie) What is needed to run pyparsing?](all_wiki_discussion_toc_2009.md#2009-08-31-230500---marjoj---newbie-what-is-needed-to-run-pyparsing)  
[2009-09-01 16:26:53 - nemith - Trying to use dictOf on a group of objects](all_wiki_discussion_toc_2009.md#2009-09-01-162653---nemith---trying-to-use-dictof-on-a-group-of-objects)  
[2009-09-02 14:30:15 - nemith - Using dictOf with many lines](all_wiki_discussion_toc_2009.md#2009-09-02-143015---nemith---using-dictof-with-many-lines)  
[2009-09-03 06:58:33 - reyman64 - recursive parsing on data](all_wiki_discussion_toc_2009.md#2009-09-03-065833---reyman64---recursive-parsing-on-data)  
[2009-09-10 04:49:28 - asb_india - Thanks for all the help](all_wiki_discussion_toc_2009.md#2009-09-10-044928---asb_india---thanks-for-all-the-help)  
[2009-09-15 21:15:40 - crmccreary - parsing fortran type text files](all_wiki_discussion_toc_2009.md#2009-09-15-211540---crmccreary---parsing-fortran-type-text-files)  
[2009-09-17 01:03:14 - codeape - Missing from subversion repository + praise](all_wiki_discussion_toc_2009.md#2009-09-17-010314---codeape---missing-from-subversion-repository--praise)  
[2009-09-17 02:05:53 - bytecolor - AST](all_wiki_discussion_toc_2009.md#2009-09-17-020553---bytecolor---ast)  
[2009-09-23 15:29:24 - maxstylus - parsing an ini file store in parent-child relationship (lists)](all_wiki_discussion_toc_2009.md#2009-09-23-152924---maxstylus---parsing-an-ini-file-store-in-parent-child-relationship-lists)  
[2009-10-01 06:35:15 - Lllama - iptables parsing](all_wiki_discussion_toc_2009.md#2009-10-01-063515---lllama---iptables-parsing)  
[2009-10-02 04:34:58 - nielAtpyparsing - Problem with recursive/nested parsing](all_wiki_discussion_toc_2009.md#2009-10-02-043458---nielatpyparsing---problem-with-recursivenested-parsing)  
[2009-10-08 03:58:35 - nielAtpyparsing - Installing pyparsing on HP-UX with python 2.5](all_wiki_discussion_toc_2009.md#2009-10-08-035835---nielatpyparsing---installing-pyparsing-on-hp-ux-with-python-25)  
[2009-10-12 14:33:39 - gregglind - isinstance considered harmful :)](all_wiki_discussion_toc_2009.md#2009-10-12-143339---gregglind---isinstance-considered-harmful-)  
[2009-10-16 14:17:23 - nathanielpeterson - extending fourFn.py ](all_wiki_discussion_toc_2009.md#2009-10-16-141723---nathanielpeterson---extending-fourfnpy-)  
[2009-10-19 12:23:49 - gregglind - Treating notAny as words?](all_wiki_discussion_toc_2009.md#2009-10-19-122349---gregglind---treating-notany-as-words)  
[2009-10-29 04:19:17 - zike2000 - AttributeError: 'module' object has no attribute 'copy'](all_wiki_discussion_toc_2009.md#2009-10-29-041917---zike2000---attributeerror-module-object-has-no-attribute-copy)  
[2009-10-29 05:44:22 - marjoj - setResultsName() problem](all_wiki_discussion_toc_2009.md#2009-10-29-054422---marjoj---setresultsname-problem)  
[2009-11-02 13:32:58 - derenrich - Reserved Words](all_wiki_discussion_toc_2009.md#2009-11-02-133258---derenrich---reserved-words)  
[2009-11-14 07:41:13 - tomekpe - nestedExpr and whitespaces](all_wiki_discussion_toc_2009.md#2009-11-14-074113---tomekpe---nestedexpr-and-whitespaces)  
[2009-11-15 06:18:49 - gabriele.lanaro - Ideas on parsing a particular format](all_wiki_discussion_toc_2009.md#2009-11-15-061849---gabrielelanaro---ideas-on-parsing-a-particular-format)  
[2009-11-25 05:00:17 - nfirvine - Parsing a unixy command line language](all_wiki_discussion_toc_2009.md#2009-11-25-050017---nfirvine---parsing-a-unixy-command-line-language)  
[2009-12-03 02:27:07 - drake1337 - nested #ifdef ... #endif](all_wiki_discussion_toc_2009.md#2009-12-03-022707---drake1337---nested-ifdef--endif)  
[2009-12-07 03:34:41 - nielAtpyparsing - Ignoring embedded keywords/punctuation in quoted string characters](all_wiki_discussion_toc_2009.md#2009-12-07-033441---nielatpyparsing---ignoring-embedded-keywordspunctuation-in-quoted-string-characters)  
[2009-12-09 22:23:47 - drake1337 - streamlined issue?](all_wiki_discussion_toc_2009.md#2009-12-09-222347---drake1337---streamlined-issue)  
[2009-12-13 03:32:17 - zzarko - Parse action executed, but it shouldn't](all_wiki_discussion_toc_2009.md#2009-12-13-033217---zzarko---parse-action-executed-but-it-shouldnt)  
[2009-12-18 13:38:34 - tk2600 - Delimiter is multiple <space>](all_wiki_discussion_toc_2009.md#2009-12-18-133834---tk2600---delimiter-is-multiple-space)  
[2009-12-18 14:19:46 - tk2600 - Matching possibly over 100 grammars](all_wiki_discussion_toc_2009.md#2009-12-18-141946---tk2600---matching-possibly-over-100-grammars)  
[2009-12-22 13:09:29 - tk2600 - setParseAction() to take external param?](all_wiki_discussion_toc_2009.md#2009-12-22-130929---tk2600---setparseaction-to-take-external-param)  
[2009-12-22 13:15:06 - tk2600 - Reference across grammars?](all_wiki_discussion_toc_2009.md#2009-12-22-131506---tk2600---reference-across-grammars)  
[2009-12-23 02:59:11 - ptmcg - New DSL code, compatible with Python 2.7 and onward](all_wiki_discussion_toc_2009.md#2009-12-23-025911---ptmcg---new-dsl-code-compatible-with-python-27-and-onward)  
[2009-12-24 10:21:41 - drake1337 - pyparsing core](all_wiki_discussion_toc_2009.md#2009-12-24-102141---drake1337---pyparsing-core)  
[2009-12-26 12:02:48 - cgkanchi - Thanks for the feedback!](all_wiki_discussion_toc_2009.md#2009-12-26-120248---cgkanchi---thanks-for-the-feedback)  
[2009-12-28 00:41:34 - marjoj - LineStart and whitespace](all_wiki_discussion_toc_2009.md#2009-12-28-004134---marjoj---linestart-and-whitespace)  
[2009-12-29 18:39:03 - sighup - Greedy delimited list?](all_wiki_discussion_toc_2009.md#2009-12-29-183903---sighup---greedy-delimited-list)  


---
## 2009-01-06 03:14:24 - kerny404 - Simple logic expression parser
I would like to parse simple expressions like

(P -\> Q) / S == T



And generate the truth table. This must be done incrementally,

generating before the truth values for the variables and then for every subformula.



Now I would like to understand if I can do everything from pyparsing, setting

the correct actions..

This is the actual parser



VARS = set()

STACK = dict()

def get_parsed_vars(var):

    '''docstring for get_parsed_vars'''

    if DEBUG: print 'getting ', var

    global VARS, STACK

    VARS.add(var[0])

    STACK = gen_vars(list(VARS))



def gen_stack(expr):

    '''creates the final expression'''

    if DEBUG: print 'working on ', expr





def parse(st):

    '''parse a string and gives it to the solver'''

    # variable has only one char

    var = oneOf(list(alphas))

    var.setParseAction(get_parsed_vars)

    op = oneOf(r'/ /\ -\> ==') 

    # forward grammar to build recursive grammars

    unary = binary = expr = Forward() 

    neg = Literal('!')

    unary \<\< (neg + expr).setParseAction(gen_stack)

    binary \<\< (expr + op + expr).setParseAction(gen_stack)

    expr \<\< binary | var | unary



    expr = operatorPrecedence(var, 

        [ 

        (r'!', 1, opAssoc.RIGHT), 

        (r'/', 2, opAssoc.LEFT), 

        (r'/\\', 2, opAssoc.LEFT), 

        (r'-\>', 2, opAssoc.LEFT), 

        (r'==', 2, opAssoc.LEFT), 

        ]) 



    # expr.setParseAction(gen_stack)

    return expr.parseString(st).asList()



It parses correctly the variables and the unary operations but not

the binary operations..



Does this grammar makes sense anyway?

This is the result for example:

In [993]: tables.parse('!P / Q -\> S')

Out[993]: [[[['!', 'P'], '\/', 'Q'], '-\>', 'S']]



Any hint is welcome, thanks

#### 2009-01-07 07:16:18 - ptmcg
Welcome to pyparsing!



I can see in your code that you have taken several runs at this parser, because there are some leftover bits that don't really contribute any more.  Stripping out the unnecessaries, your parse routine should boil down to just:



    def parse(st):
        '''parse a string and gives it to the solver'''
        # variable has only one char
        var = oneOf(list(alphas))
    
        expr = operatorPrecedence(var, 
            [ 
            (r'!', 1, opAssoc.RIGHT), 
            (r'/', 2, opAssoc.LEFT), 
            (r'/\\', 2, opAssoc.LEFT),
            (r'-\>', 2, opAssoc.LEFT), 
            (r'==', 2, opAssoc.LEFT), 
            ]) 
    
        # expr.setParseAction(gen_stack)
        return expr.parseString(st)



To take this parser to the next level (of being able to evaluate the expression), please look at the techniques in this example: 



What this example does is, instead of creating just an AST of tokens, which you then have to re-traverse to evaluate the incremental True/False values of each sub-expression, the example shows how to construct incremental evaluators for each sub-expression, which have their own evaluation method.  Then to evaluate, you just evaluate the outermost expression, and its evaluation method recursive works through the nested data structure of evaluators.



My point is, if you parse '!P', you already know at parse time that you are parsing a NOT operator and a variable - why store them as ordinary strings?  Then you have to come back later to detect that a '!' operator was used, and so a Not operation must be done.  Instead, <em>at parse time</em> convert the string '!P' to an instance (using a parse action) that already knows how to evaluate itself.  That is what simpleBool.py does.



Some other comments ---



Back before the operatorPrecedence helper was written, you had to use Forward's to define a recursive grammar.  But operatorPrecedence does all of that for you.



In Python, this is never going to work:



    unary = binary = expr = Forward()



All this does is attach 3 different variable names to the same Forward instance.  You can see the effect of this in the interactive interpreter:



    \>\>\> from pyparsing import *
    \>\>\> unary = binary = expr = Forward()
    \>\>\> unary \<\< Word(alphas)
    \>\>\> unary
    Forward: W:(abcd...)
    \>\>\> binary
    Forward: W:(abcd...)
    \>\>\> expr
    Forward: W:(abcd...)



And when posting code in this wiki, please surround your code with [[code]] tags, each on its own line.  This will set off your code in code blocks, and will preserve leading whitespace.



Please write back as you plug away at this parser, it looks very interesting.



-- Paul

---
## 2009-01-08 11:24:01 - bobdevine - small bug + parsing pairs
A small bug: You have an undefined variable in the 'else' clause where it gives an error message and then continues on.



File '/usr/lib/python2.5/site-packages/pyparsing.py', line 3190, in oneOf while i \< len(symbols)-1: UnboundLocalError: local variable 'symbols' referenced before assignment 



I jit it while I was trying to parse a line composed of a delimitedList that has either one word or a pair of words separtated by commas. My problem was that it was parsing it but not saving the result in the dict. I was fooling around with OneOf when I stumbled over the bug. 



As an example of what I want: a, bb cc, dd ee, f, gg hh to be 5 tokens, where 'bb cc' is considered one with the whitespace. 



I've tried Optional but I couldn't get it to capture the second word with setResultsName. Was it suppressed? What's a good way to parse?

#### 2009-01-08 13:02:34 - ptmcg
Bob -



Pyparsing includes a helper called commaSeparatedList, I think it will do what you want.



Welcome to pyparsing!



-- Paul



(Thanks for the bug note - I think I'll change this to raise an exception, since oneOf really needs to have a string or a sequence of some kind, and should not continue otherwise.)

---
## 2009-01-26 03:56:34 - HWJ - newbee needs some initial help
Hi,



my first tiny trial on using pyparsing failed (i.e. no output at all).

I'd like to extract all text (including white space)

between '\btt' and '\ett'.

Many thanks for a hint,

Helmut.



Here is my failing first attempt



    import pyparsing as PP
    from pprint import pprint
    
    Left=PP.Suppress('\\btt')
    Right=PP.Suppress('\\ett')
    
    def printit(OrigString,TokenStart,P_Result) :
      print '*** Im HERE'
      pprint(OrigString)
      pprint(TokenStart)
      pprint(P_Result)
    
    VerbText= PP.SkipTo(Right).setParseAction(printit)
    # Verbatim=PP.SkipTo(Left)+VerbText
    Verbatim=Left+VerbText+Right
    
    Test='''
      header
      \btt ABC
           DEF
       123\ett
       trailer
    '''
    
    Verbatim.scanString(Test)



#### 2009-01-26 08:06:07 - ptmcg
You are really quite close!  Your pyparsing logic is right on the money, your problems are actually Python problems:



1. Your input string contains '\b' without escaping the leading '\' character.  This actually represents a backspace character, just as '\t' is a tab character and '\n' is a newline.  Using backslashes as part of your delimiters just complicates matters, if you are designing this format, you might try something else that has no 'escaping' issues.  Look at this excerpt from an interactive Python session:



    \>\>\> print 'A\bBC\bD'
    BD



But if you are stuck with these delimiters, then you should use 'raw string literals'.  Raw literals do not process the backslashes as escapes, but leave them alone as just plain '\' characters, so there is no need to write them as '\\'.  More importantly in your case, there is also no substitution of \x formatting characters, like \t (tab), \n (newline), or \b (backspace) with their actual whitespace.  To write a raw literal:

    - put an 'r' just before the leading quote

    - do *not* double any backslashes (exception, if the string ends with a backslash, it still has to be doubled)



    Left=PP.Suppress(r'\btt')
    Right=PP.Suppress(r'\ett')
    
    Test=r'''
      header
      \btt ABC
           DEF
       123\ett
       trailer
    '''



2. scanString vs. searchString - scanString returns a generator expression, which requires that you iterate over it using a for loop or list construction, something like:



    for results,startloc,endloc in Verbatim.scanString(test):
        print results



Just calling scanString alone doesn't do anything.



You *can* call searchString though, which is really just a thin wrapper around scanString.  searchString returns a list of all of the matched results, leaving out the match start/end locations.  As a Python list expression, it is little more than:



    [ results for results,startloc,endloc in Verbatim.scanString(test) ]



To use searchString, just change your last line to:



    Verbatim.searchString(Test)





That's it!  At this point, you should have a working test program.  However, I'd like to suggest a few other style points.  Python has a generally accepted format for variable names, codified in PEP-8 ().  Capitalized words are pretty much reserved just for class names.  To other Pythoners, your use of capitalized variable names is jarring to the eye. Established Python developers would expect to see names like this in your program:



    left
    right
    verbatim
    test



Perhaps it is a little thing - your code will certainly run as it is - but when you post code for others to review or use in some way, it creates an additional hurdle in a situation where you are already trying to minimize any barriers to communication.  (Pyparsing generally follows PEP-8 in using capitalized words for class names and lower case words for variable names, although it deviates when variable and method names are made from multiple words - 'likeThis' instead of the PEP-8 recommended 'like_this'.  But if I were to write it again today, I'd use the PEP-8 form.)



Good luck!

-- Paul
#### 2009-01-26 09:29:42 - HWJ
Many thanks for your help.



Just one more question.

Using 

Verbatim=PP.SkipTo(Left)+VerbText  



instead of



Verbatim=Left+VerbText+Right



does not suppress the initial mark r'\btt'

though Left was defined as



Left=PP.Suppress('\\btt')



Is the combination of 'Suppress' + 'SkipTo' unreasonable?



Thanks,

Helmut.
#### 2009-01-26 10:02:46 - ptmcg
No there is nothing unreasonable about using Suppress in a SkipTo.  But look at what your parser is doing:



    Verbatim=PP.SkipTo(Left)+VerbText



The first thing this does is to skip over everything up to - but NOT including - the Left expression.  At this point, the parser is positioned at the '\' of '\btt'.  Now VerbText is defined as everything up until the next Right expression - well, since the parser is currently positioned at the '\btt', that will get parsed as part of VerbText.



Once you get to the Left expression, you'll need to explicitly parse past it so that it will get suppressed.  You can do this:



    Verbatim=PP.SkipTo(Left)+Left+VerbText



Or you can set the include flag of SkipTo to True: 



    Verbatim=PP.SkipTo(Left, include=True)+VerbText

meaning that you not only want to SkipTo the next Left, but also include the Left expression's parsed text too.
#### 2009-01-26 10:06:20 - ptmcg
If you used the Windows installer executable, or easy_install, this will just install the basic pyparsing package.  If you go to the download page on SourceForge and get the source package, this will include a number of example scripts, a class diagram, and a directory of HTML documentation on all of pyparsing's classes and definitions.  The documentation page on the wiki will also point you to a number of online resources.



Cheers!

-- Paul

---
## 2009-01-27 01:14:54 - HWJ - Most needed : a quick reference
Hi,



what I am missing most is a quick reference (e.g. like the

well known Python quick reference) for PyParsing.



IHMO, it should list all exported classes, show their base clase

and most important methods (extending the base class).



Furthermore the exported list of predefined variables/functions

should be listed with some infoon its usage.



Perhaps the work could be split by using a special wiki for that,

once someone (most likely Paul) has setup the initial format

for such a quick reference. Perhaps one could use the REST format

which is used by many Python projects and Python itself.

It's simple and it can produce output in different common formats.



Personally I have purchased the 'Getting Started with Pyparsing'

from O'Reilly. This is a really good introduction but not more.

So, IHMO, it doesn't make a quick reference obsolete.

#### 2009-01-27 08:07:53 - ptmcg
Have you downloaded the source distribution from SourceForge?  It includes an epydoc-generated htmldoc directory, and a (slightly out-of-date) class diagram.



But you are correct, it is not really the same as a handy quick reference card.



-- Paul

(and thanks for buying the O'Reilly e-book!)

---
## 2009-01-27 06:33:11 - HWJ - withoutAttribute - howto
Hi,



I want to match all ankers (\<a) in an html document which do NOT

have a class (e.g.) attribute.



So I'd need something like





    a_start,a_stop= makeHTMLTags('a')
    a_start.setParseAction(withoutAttribute(('class',withAttribute.ANY_VALUE))



Is there anything better than copying the code of

function withAttribute from pyparsing.py and negating it?



Thanks for a hint,

Helmut.

#### 2009-01-27 08:25:50 - ptmcg
I'm trying to keep pyparsing's API from getting too many application-specific helpers and classes.  Like Python, pyparsing's basic concepts are sufficient to get people off the ground, without their having to learn the whole pyparsing API.  The richer I make the API (more helpers, more classes), the more complex the learning of it.



I've added a page to the publicly-editable Pyparsing wiki, , and seeded it with my first cut at a withoutAttribute method.  Feel free to fine-tune it as you like, or to post your own submissions.  (Maybe I need to start something like a Pyparsing Cookbook, and have the most popular submissions get included in the examples, or published in an article or something.)



-- Paul
#### 2009-01-27 08:59:54 - HWJ
Thanks, I've put a slightly more general version there.

You can specify a given <em>unwanted</em> value (or withoutAttribute.ANY_VALUE) for a given attribute.

Please correct my English,



Helmut.

---
## 2009-01-29 04:05:09 - HWJ - SkipTo(...,include=True)  BUG ?
Unless I'm missing something,

SkipTo(...,include=True) doesn't play nice with

transformString



The following example demonstrates the problem



    import pyparsing as PP
    print 'Using PyParsing version %s' % (PP.__version__)
    # this prints Using PyParsing version 1.5.2
    
    Key1= PP.Keyword('START')
    Key2= PP.Keyword('END')
    
    S='''Line 1
      START body
      END
      trailer
    '''
    
    MyText= PP.SkipTo(Key1,include=True).suppress() + PP.SkipTo(Key2,include=True) + \
            PP.SkipTo(PP.StringEnd()).suppress()
    
    Got= MyText.transformString(S)
    
    import pprint
    pprint.pprint(Got)
    
    # this prints '['body\\n  ', 'END']'
    # i.e. stringified list of 2 elements



Many thanks for a hint,

Helmut.

#### 2009-01-29 12:28:28 - ptmcg
Maybe more of a surprise than a bug, but maybe that depends on your point of view.



SkipTo(expr,include=True) does not return a string, but a 2-element list, the first element contains the skipped text, the second element the target expr.



Here are two resolutions to this situation:



    MyText= PP.SkipTo(Key1,include=True).suppress() + PP.Combine(PP.SkipTo(Key2,include=True)) + \
            PP.SkipTo(PP.StringEnd()).suppress()
    
    Got= MyText.transformString(S)
    print Got

or:



    MyText= PP.SkipTo(Key1,include=True).suppress() + PP.SkipTo(Key2) + Key2 +\
            PP.SkipTo(PP.StringEnd()).suppress()
    
    Got= MyText.transformString(S)
    print Got



At the very least, I should improve the documentation for SkipTo to remove the element of surprise.



Thanks for the post,

-- Paul

---
## 2009-01-29 23:39:18 - HWJ - ignore + SkipTo (how does it work?)
Ignoring some pattern encountered during an active SkipTo

doesn't do what I expect - what am I missing.

Many thanks for a hint,

Helmut.



Please have a look at



    import pyparsing as PP
    print 'Using PyParsing version %s' % (PP.__version__)
    # this prints Using PyParsing version 1.5.2
    
    Key1= PP.Keyword('START')
    Key2= PP.Keyword('END')
    
    S='''Line 1
      START body RemoveMe
      END
      trailer
    '''
    
    # MyText= PP.SkipTo(Key1,include=True).suppress() + PP.Combine(PP.SkipTo(Key2,include=True)) + \
    #         PP.SkipTo(PP.StringEnd()).suppress()
    MyText= PP.SkipTo(Key1,include=True).suppress() + PP.SkipTo(Key2)+ Key2 + \
            PP.SkipTo(PP.StringEnd()).suppress()
    MyText.ignore(PP.Literal('RemoveMe'))
    
    Got= MyText.transformString(S)
    
    import pprint
    pprint.pprint(Got)
    
    # this (both versions) prints 'body RemoveMe\n  END'
    



#### 2009-01-30 01:39:11 - HWJ
Sorry, I just found out that SkipTo has an ignore option.

BUT, that doesn't work either:



    import pyparsing as PP
    print 'Using PyParsing version %s' % (PP.__version__)
    # this prints Using PyParsing version 1.5.2
    
    Key1= PP.Keyword('START')
    Key2= PP.Keyword('END')
    
    S='''Line 1
      START body RemoveMe
      END
      trailer
    '''
    
    MyText= PP.SkipTo(Key1,include=True).suppress() + PP.Combine(PP.SkipTo(Key2,include=True,ignore=PP.Literal('RemoveMe'))) + \
            PP.SkipTo(PP.StringEnd()).suppress()
    Got= MyText.transformString(S)
    
    import pprint
    pprint.pprint(Got)
    
    # this still prints 'body RemoveMe\n  END'



In addition I get the message

found ignoreExpr, advance to 28

but still, it's present in the result 'Got'



<ul><ul><ul><li>I am confused ***</li></ul></ul></ul>Helmut.

---
## 2009-01-30 11:31:57 - HWJ - nestedExpr -> infinite loop (WHY)
Hi,



I want to extract an html \<table\> which might contain

nested tables.

I have some problems with nestedExpr.

First, for non-simple opener/closer it needs a 'content'.

Since I want anything between the outermost \<table\> and \</table\>

I have used  content=Regex(r'.*') which probably

leeds to many backtracks?



Unfortunately, my versions seems to lead to an infinite loop.

Why and how to debug that oneself?

Many thanks for a hint,

Helmut.





    import pyparsing as PP
    
    table_s,table_e = PP.makeHTMLTags('table')
    
    itable_s,itable_e = PP.makeHTMLTags('table')
    itable_s.setParseAction(PP.withAttribute(cellpadding=''0''))
    
    
    Entry= PP.nestedExpr(opener=table_s,closer=table_e,content=PP.Regex(r'.*')) # .setParseAction(lambda X : ' '.join(X))
    TabStart= PP.SkipTo(itable_s).suppress()
    MyText= TabStart+PP.OneOrMore(Entry+TabStart)+PP.SkipTo(PP.StringEnd()).suppress()
    MyText.ignore(PP.htmlComment)
    
    htmlText= '''
    \<table width='450' border=0\>Regex(r'.*')
                \<table width='100%' border='1' cellpadding='1' cellspacing='0'\>
                            This Line
        \</table\>
    TO_BE_SKIPPED
            \</td\>
        \</tr\>
    \</table\>
    \<table width='100%' border='0' cellpadding='0' cellspacing='0'\>
                \<table width='100%' border='0'\>
                \</table\>
    TO_BE_SKIPPED
            \</td\>
        \</tr\>
    \</table\>
    Rest
    '''
    
    Result= MyText.transformString(htmlText)
    
    print Result



#### 2009-02-01 20:57:16 - ptmcg
In general, once you start trying to parse more complex HTML such as nested tables, I recommend you consider using BeautifulSoup instead of pyparsing.  Still, there are some things to learn about pyparsing from this problem.



First of all, pyparsing does not do 'backtracking' in the sense that regular expressions do.  Pyparsing is much more of an automaton engine.  Something like this is just never going to work with pyparsing:



    A = Literal('A')
    Z = Literal('Z')
    letters = oneOf(list('ABCDEFGHIJKLMNOPQRSTUVWXYZ'))
    word_starting_with_A_and_ending_with_Z = A + OneOrMore(letters) + Z



because any terminating 'Z' gets parsed as part of the repetition in OneOrMore.



To use a nestedExpr, you correctly found that with the opening and closing delimiters being the complex opening and closing TABLE tags, you have to specify some expression for content.  So look at the HTML you posted.  After parsing the opening \<TABLE ...\> tag, you pretty much want to read everything up to the matching \</TABLE\> tag, or the opening of another table.  So I tried defining table_content as:



    table_content = PP.SkipTo(table_s | table_e)



Then using nestedExpr, I defined a nested table as:



    nested_table = PP.nestedExpr(table_s, table_e, table_content)



I then used searchString (why are you using transformString?) to try to find the nested tables in your sample HTML.  It turns out this loops forever, but with little feedback to help diagnose the problem.  So I enabled debugging on the table_s, table_e and table_content expressions, using setDebug() (good to also give expressions names with setName(), otherwise debug output is pretty cryptic):



    table_s.setDebug()
    table_e.setDebug()
    table_content = PP.SkipTo(table_s | table_e).setName('content').setDebug()



And I found that the closing content was looping forever with a SkipTo positioned right at the closing tag, and returning empty strings.  So to fix this, I constrain table_content to *not* accept an empty string by using a parse action:



    def content_must_not_be_empty_string(tokens):
        if tokens[0]=='':
            raise PP.ParseException('content cannot be empty')
    table_content.setParseAction(content_must_not_be_empty_string)



And now after commenting out the setDebug calls, I get this after searching your sample text:



    [['\nNON TABLE STUFF', ['\n                        This Line'],
        '\nTO_BE_SKIPPED\n        \</td\>\n    \</tr\>']]
    [[[], '\nTO_BE_SKIPPED\n        \</td\>\n    \</tr\>']]



The only way to approximate something like backtracking is to use explicit lookahead using pyparsing's FollowedBy expression.  Otherwise, you need to put yourself in the place of the pyparsing parser, and execute the given grammar expressions as they are defined.



-- Paul
#### 2009-02-02 02:36:59 - HWJ
Thanks Paul!



My real test was to download all of Sourceforge's pyparsing mailing list archive and extract only relevant parts like the question and the

answers. I used 'transformString' to delete all the unwanted parts and

store the 'pure information' to disk.



From a user's point of view (i.e. my point of view) it's a pity

that pyparsing cannot parse an empty nested expression.

Something like

\<abc\> info \<abc\>\</abc\>\</abc\>

should be parsed since within XML,e.g., it's not unlikely to occur.



Thanks,

Helmut.

---
## 2009-02-02 06:09:51 - debug - Parsing a simple LOGO like language
Hi-



I've recently started porting a version of Turtle to the 3d app Maya, most of it is working after writing my own simple parser however I became stumped when looking to parse loops or more accurately nested loops such as this one:



REPEAT 10

  REPEAT 4

    DRAW 50

    RIGHT 90

  NEXT

  RIGHT 36

NEXT



Would pyparsing be a suitable solution for this kind of problem?



Kind regards



Dom

#### 2009-02-02 07:45:02 - ptmcg
This is absolutely a suitable place to use pyparsing!  I had an article in Python magazine last spring on parsing the Brainf*ck language, and the language you are sketching out is not much more complicated.  But overall, there are still a number of complex issues that you'll need to hash out, such as dealing with arithmetic expressions (to support commands like 'RIGHT X' or 'DRAW Y*10' - not all of your command arguments will be nice literal integers), and conditional expressions (to support IF and WHILE control flow statements).



Here is my guess at your language statements:



    LEFT n
    RIGHT n
    DRAW n
    COLOR n,n,n
    COLOR colorname
    REPEAT n ... NEXT
    IF condition ... ENDIF
    IF condition ... ELSE ... ENDIF
    WHILE condition ... ENDWHILE



where 'n' could be an arithmetic expression, colorname could be a single alpha word (like RED or BLUE), and condition could be a conditional or boolean expression.  Start off your grammar using arithExpr and boolExpr as expressions for these arguments, but for simplicity just define them as:



    integer = Word(nums)
    TRUE = Literal('TRUE')
    FALSE = Literal('FALSE')
    arithExpr = integer
    boolExpr = TRUE | FALSE



Then you can define your next level statements like:



    leftStmt = 'LEFT' + arithExpr
    rightStmt = 'RIGHT' + arithExpr
    drawStmt = 'DRAW' + arithExpr



and so on.  Then later, when you expand arithExpr to actually support full arithmetic expressions like '12*100/7+X', you just redefine arithExpr, and all the rest of the commands just get updated definition.



Now the trick is that you have REPEAT n ... NEXT, which is a statement which can be composed of statements, which can in turn be composed of statements, etc.  This recursive requirement is met in pyparsing using the Forward class.  Since some statements can be composed of other statements, we'll create a placeholder, and just call it 'statement'.



    statement = Forward()



Now we can finish our definition of REPEAT:



    repeatStmt = 'REPEAT' + arithExpr +
            Group(OneOrMore(statement)) + 'NEXT'



Seems like we just cheated, doesn't it?  How do we actually define what a statement *is*?  The answer is that we modify our placeholder now to say that a statement could be a LEFT, RIGHT, DRAW, or REPEAT statement, using the '\<\<' operator:



    statement \<\< Group( leftStmt | rightStmt | drawStmt | repeatStmt )



This 'inserts' the definition of what can go into a statement, even an expression that recursively includes statement definitions.  (Need to take some care here in the general case, but for this little grammar, there is no ambiguity, so no chance of going down a recursive infinite loop.)



Now you can parse your little program using:



    program = OneOrMore(statement)
    
    for stmt in program.parseString(testProg):
        print stmt.asList()



Here is the whole thing (using pprint to pretty print the nested output):



    from pyparsing import *
    
    integer = Word(nums) # expand to include leading sign 
                         # or floating point if you like
    TRUE = Literal('TRUE')
    FALSE = Literal('FALSE')
    arithExpr = integer  # to be expanded later
    boolExpr = TRUE | FALSE  # likewise
    
    # placeholder for all statements
    statement = Forward()
    
    # some simple statements
    leftStmt = 'LEFT' + arithExpr
    rightStmt = 'RIGHT' + arithExpr
    drawStmt = 'DRAW' + arithExpr
    
    # a statement composed of other statements
    repeatStmt = 'REPEAT' + arithExpr + \
            Group(OneOrMore(statement)) + 'NEXT'
    
    # insert definition of statements into statement placeholder
    statement \<\< Group( leftStmt | rightStmt | drawStmt | repeatStmt )
    
    program = OneOrMore(statement)
    
    testProg = '''\
    REPEAT 10
    REPEAT 4
    DRAW 50
    RIGHT 90
    NEXT
    RIGHT 36
    NEXT
    '''
    
    from pprint import pprint
    pprint(program.parseString(testProg).asList())



And the output is:



    [['REPEAT',
      '10',
      [['REPEAT', '4', [['DRAW', '50'], ['RIGHT', '90']], 'NEXT'],
       ['RIGHT', '36']],
      'NEXT']]



I used Group to cluster together all the bits of each statement separate from the next, and grouped the body of the repeat to simplify getting at those statements too.  The result is nested parse tree that retains the statement nesting of your original program.



Now don't think I've written the whole program for you, I've just given you a good starting point.  You still need to add:

- IF and WHILE control flow statements, and something to change the pen color (I guessed at a couple forms of a COLOR statement)

- variables and assignment statements (so you can write 'THETA = 30' 'RIGHT THETA')

- arithmetic expressions (look up pyparsing's operatorPrecedence helper to simplify this, there are some good examples on the Examples page)

- boolean condition expressions

- using parse actions to compile the parsed tokens into executable/callable objects (please! pick up a copy of the Brainf*ck article, it steps through this process pretty thoroughly)



This last step is *far* preferable to parsing the code into a nested tree and then traversing it with code like 'if element[0]=='DRAW': ...' - instead, use a parse action to create a DrawCommand object at parse time, and then execute that object.



Write back and let us know how it is going!



-- Paul
#### 2009-02-02 13:53:39 - debug
Wow I don't know how much to thank you, I've bought a copy of pymag from their site and I'll  read it properly later, I've managed to piece together this so far based upon your code:



[code]

<ol><li>todo: get commenting working</li></ol>

from pyparsing import *

from pprint import pprint



integer = Word(nums)  # expand to include leading sign 

comment = Literal(';').suppress()

functionName = Word(alphas)

userString = Word(alphas, exact=1)

arithExpr = integer

variableName = Word(alphas)

compOperators = oneOf('== = != \<\> \< \> \<= \>=')





TRUE = Literal('TRUE')

FALSE = Literal('FALSE')



<ol><li>to be expanded later</li></ol>boolExpr = TRUE | FALSE # likewise



<ol><li>placeholder for all statements</li></ol>statement = Forward()





<ol><li>some simple statements</li></ol>leftStmt = 'LEFT' + arithExpr

rightStmt = 'RIGHT' + arithExpr

drawStmt = 'DRAW' + arithExpr



<ol><li>special(?) statements composed of other statements</li></ol>repeatStmt = 'REPEAT' + arithExpr + Group(OneOrMore(statement)) + 'NEXT'



functionStmt = '#' + functionName + Group(OneOrMore(statement)) + 'RETURN'



ifStatement = 'IF' + userString + compOperators + arithExpr + Group(OneOrMore(statement)) + Optional('ELSE' + Group(OneOrMore(statement))) + 'ENDIF'



assignmentStatement = variableName + '=' + Word(nums)



commentMarker = comment + Optional(Word(alphas)).suppress()



<ol><li>insert definition of statements into statement placeholder</li></ol>statement \<\< Group( leftStmt | rightStmt | drawStmt | repeatStmt | functionStmt | ifStatement | commentMarker | assignmentStatement)



program = OneOrMore(statement)



testProg = '''\

#MOO

LEFT 10

RETURN

REPEAT 10

REPEAT 4

DRAW 50

RIGHT 90

NEXT

RIGHT 36

NEXT

IF A == 5

LEFT 10

ELSE

RIGHT 100

ENDIF

THETA = 30

'''

pprint(program.parseString(testProg).asList())

[/code]



Hopefully I have got most (or at least some... *crosses fingers*) right, I'm really new to Python (about 2 months into learning programming) but pyparsing seems to make things a breeze.

The language I'm basing this version of Logo/Turtle on is Bill Kendricks web-turtle which appears to be a pretty minimalist version of Turtle Graphics. Some commands like colour are going to have to be omitted as this isn't accepted within maya.



I've got a couple of other questions though if thats cool:



1) What does pyparser return? is it a list or some kind of custom datatype?



2) Assuming all the code is parsed correctly and I'm ready to get onto the actual interpreter I've been reading that using a Stack(?) system is the only way to run all of this because of loops. Do you know of any basic primers of stack theory or anything that'll point me in the right direction?



Thanks again!



Dom
#### 2009-02-06 10:26:18 - ptmcg
Pyparsing returns a ParseResults object.  A ParseResults is sort of a cross between a dict and a list (and an object too, for that matter!), in that you can access the pieces of it by index (like a list), by results name (like a dict), or by results name as an attribute (like an object).  Here is an example, where we need to parse an employee record like this:



    0010, Paul McGuire, 32, 999-99-9999, Le Gran Fromage, #12345, #78710, #314159



That is, an employee id, a name, age, social security number, title, and a series of patents the employee has written.  We'll store this data in a string variable called emprec.



If we do just basic tokenizing, we might write this parser:



    emp_id = Word(nums)
    name = originalTextFor(OneOrMore(Word(alphas)))
    integer = Word(nums)
    ssnum = Combine(Word(nums,exact=3) + '-' + \
        Word(nums,exact=2) + '-' + Word(nums,exact=4))
    patentnum = Combine('#' + Word(nums))
    COMMA = Suppress(',')
    emp_data = emp_id + COMMA + name + COMMA + integer + COMMA + \
        ssnum + COMMA + name + COMMA + delimitedList(patentnum)
    
    print emp_data.parseString(emprec).asList()



And we get back a ParseResults containing these strings:



    ['0010', 'Paul McGuire', '32', '999-99-9999', 'Le Gran Fromage', 
     '#12345', '#78710', '#314159']



At this point, we have done little more than split the input on the separating commas.  In fact, we threw away quite a bit of information:

- '32' is not just a string, but represents the number 32

- the 3 patent numbers should be grouped as their own sublist, since some employees might have 3 patents or 5 or none



So one of the things to change is to do some parse-time conversion.  When we parse this integer expression 'Word(nums)', we know immediately at parse time that this string will successfully parse as an integer.  Why return it as a string, just so we can walk through the strings later, testing for save conversions, etc.?  Let's just do the conversion right while we are parsing.  To do this, we'll use a parse action:



    integer.setParseAction(lambda tokens: int(tokens[0]))



Parse actions are passed a ParseResults object containing the tokens that match just this bit of the grammar (you can also build parse actions that also get the current parse location and the original input string, but this particular parse action doesn't need that information).  If the parse action doesn't return anything, or returns None, then the original parsed tokens are used as they were parsed.  BUT, if the parse action returns another value - in this case, the parse action is a lambda that returns an int - then that value takes the place of the parsed tokens in the input list.  With just this one change, let's see how the output differs:



    ['0010', 'Paul McGuire', 32, '999-99-9999', 'Le Gran Fromage', 
     '#12345', '#78710', '#314159']



Ah!  Note now that 32 is an actual int, not just the characters '3' and '2' in a string.



To group together the patent numbers, pyparsing offers the Group class.  Here is a modified emp_data that groups these values:



    emp_data = emp_id + COMMA + name + COMMA + integer + COMMA + \
        ssnum + COMMA + name + COMMA + Group(delimitedList(patentnum))



(all we did was enclose the trailing delimitedList inside a Group).



Now we get this output:



    ['0010', 'Paul McGuire', 32, '999-99-9999', 'Le Gran Fromage', 
     ['#12345', '#78710', '#314159']]



Still, we have room for improvement, as we really would like to refer to these fields by name instead of by index number.  For one thing, if the order changes in the future, or adds an embedded optional field, we not only have to change the parser, but also any code that uses raw indexes to access the fields - that is uses results[2] to get the employee age.



So pyparsing also offers results names, which you can define by simply following the element in the grammar with the results name in quotes (essentially, you are *calling* the object and passing the results name).  Here is our emp_data parser, enhanced using results names:



    emp_data = emp_id('eid') + COMMA + name('ename') + COMMA + \
        integer('age') + COMMA + ssnum('socsec') + COMMA + \
        name('etitle') + COMMA + Group(delimitedList(patentnum))('patents')



Printing out the results as a list using asList gives us exactly the same result as before:



    ['0010', 'Paul McGuire', 32, '999-99-9999', 'Le Gran Fromage', 
     ['#12345', '#78710', '#314159']]



But if we use dump() instead of asList(), we get a full catalog of all the results names:



    ['0010', 'Paul McGuire', 32, '999-99-9999', 'Le Gran Fromage', 
     ['#12345', '#78710', '#314159']]
    - age: 32
    - eid: 0010
    - ename: Paul McGuire
    - etitle: Le Gran Fromage
    - patents: ['#12345', '#78710', '#314159']
    - socsec: 999-99-9999



And we can access them like keys for a dict:



    print results.keys()
    print results['age']



We can also treat results like an employee object:



    print results.age
    print results.title
    print len(results.patents)



Now if the order of the items changes in the future, we'll have to change our parser accordingly, <em>but the post-parsing code is immune to the changes!</em>



So that is the little tutorial on ParseResults.



As for your question about stack processing, this topic usually goes hand-in-hand with traditional parsers which have separate tokenize and parsing passes through the data.  When using pyparsing, <strong>if the data is grouped properly</strong>, the stack will have already been built for you, and the results will be returned as a nested list.  For a paper to read, I recommend the very one you just downloaded, the BF article from PyMag.  Just as we used a parse action to convert the string '32' to the integer 32, that article's parser/interpreter converts the BF commands into executable objects, to be run inside a simple VM.  The stacking/nesting of statements is already handled by the pyparsing grammar at parse time, which makes your executor code almost simple-minded.  (Here is the code for the VM from the PyMag article:



    class VirtualMachine(object):
        def __init__(self):
            self.ptr = 0
            self.array = collections.defaultdict(int)
    
        def run(self, instructions):
            for instr in instructions:
                instr.execute(self)



Sorry not to reply sooner, hope this was worth the wait.



-- Paul
#### 2009-02-07 09:31:27 - debug
Hi-



Thanks for the tutorial, I've been trying to work through the magazine article for a couple of days but a couple of things are well beyond me (I trust class inheritance will eventually make sense to me with enough time.. hopefully). I've tried putting together the VirtualMachine class a couple of times but whenever I run it all I always get the same error, which is not being able to call a string object:





    
    class MurtleInstruction(object):
        def __init__(self,toks):
            pass
    
    class VirtualMachine(object):
        def __init__(self):
            self.ptr = 0
            self.array = collections.defaultdict(int)
    
        def run(self, instructions):
            for instr in instructions:
                instr.execute(instr)
    
    class DrawCommand(MurtleInstruction):
        def execute(self, vm):
            #draw commands go here
            pass
    
    drawStmt.setParseAction(DrawCommand)
    
    vm = VirtualMachine()
    vm.run(commands)
    



Thanks again! 



Dom
#### 2009-02-07 11:15:45 - ptmcg
In this code, commands must be the results from parsing some draw commands:



    commands = OneOrMore(drawStmt).parseString(draw_command_input_string)



Also, you did not copy the VM's run() method quite right.



    instr.execute(instr)

should be:



    instr.execute(self)



Don't worry about inheritance too much right now, just inheriting from a do-nothing MurtleInstruction is perfectly fine.  In fact, in this example, and when implementing in Python, inheritance per se is optional, as long as all the classes like DrawCommand, etc. implement execute().
#### 2009-02-07 13:59:24 - debug
When you said 'Some are born to write parsers. Some become parser writers. And some have parser writing thrust upon them', you may have omitted another group of people that are useless are trying to write parsers.. which is me.



This is the full code so far, I know the problem is in this line:





    compiled = OneOrMore(statement).parseString(testProg)





    from pyparsing import *
    import collections
    
    integer = Word(nums)  # expand to include leading sign 
    comment = Literal(';').suppress()
    functionName = Word(alphas)
    userString = Word(alphas, exact=1)
    arithExpr = integer
    variableName = Word(alphas)
    compOperators = oneOf('== = != \<\> \< \> \<= \>=')
    
    
    TRUE = Literal('TRUE')
    FALSE = Literal('FALSE')
    
    # to be expanded later
    boolExpr = TRUE | FALSE
    
    # placeholder for all statements
    statement = Forward()
    
    
    # some simple statements
    leftStmt = 'LEFT' + arithExpr
    rightStmt = 'RIGHT' + arithExpr
    drawStmt = 'DRAW' + arithExpr
    moveStmt = 'MOVE' + arithExpr
    
    # special(?) statements composed of other statements
    repeatStmt = 'REPEAT' + arithExpr + Group(OneOrMore(statement)) + 'NEXT'
    
    functionStmt = '#' + functionName + Group(OneOrMore(statement)) + 'RETURN'
    
    ifStatement = 'IF' + userString + compOperators + arithExpr + Group(OneOrMore(statement)) + Optional('ELSE' + Group(OneOrMore(statement))) + 'ENDIF'
    
    assignmentStatement = variableName + '=' + Word(nums)
    
    commentMarker = comment + Optional(Word(alphas)).suppress()
    
    # insert definition of statements into statement placeholder
    statement \<\< Group( leftStmt | rightStmt | drawStmt | repeatStmt | functionStmt | ifStatement | commentMarker | assignmentStatement)
    
    
    testProg = '''\
    LEFT 10
    RETURN
    REPEAT 10
    REPEAT 4
    DRAW 50
    RIGHT 90
    NEXT
    RIGHT 36
    NEXT
    IF A == 5
    LEFT 10
    ELSE
    RIGHT 100
    ENDIF
    THETA = 30
    '''
    
    # VM
    
    compiled = OneOrMore(statement).parseString(testProg)
    
    print (compiled)
    
    class MurtleInstruction(object):
        def __init__(self,toks):
            pass
    
    class VirtualMachine(object):
        def __init__(self):
            self.ptr = 0
            self.array = collections.defaultdict(int)
    
        def run(self, instructions):
            for instr in instructions:
                instr.execute(self)
    
    
    class DrawMurtle(MurtleInstruction):
        def execute(self, vm):
            #code to come later
            pass
    
    class LeftMurtle(MurtleInstruction):
        def execute(self, vm):
            #code to come later
            pass
    
    class RightMurtle(MurtleInstruction):
        def execute(self, vm):
            #code to come later
            pass
    
    class RepeatMurtle(MurtleInstruction):
        def execute(self, vm):
            #code to come later
            pass
    
    class FunctionMurtle(MurtleInstruction):
        def execute(self, vm):
            #code to come later
            pass
    
    class IfMurtle(MurtleInstruction):
        def execute(self, vm):
            #code to come later
            pass
    
    class AssignmentMurtle(MurtleInstruction):
        def execute(self, vm):
            #code to come later
            pass
    
    
    drawStmt.setParseAction(DrawMurtle)
    
    leftStmt.setParseAction(LeftMurtle)
    
    rightStmt.setParseAction(RightMurtle)
    
    repeatStmt.setParseAction(RepeatMurtle)
    
    functionStmt.setParseAction(FunctionMurtle)
    
    ifStatement.setParseAction(IfMurtle)
    
    assignmentStatement.setParseAction(AssignmentMurtle)
    
    
    vm = VirtualMachine()
    vm.run(compiled)



Thanks again, also despite my not being able to understand it completely I still think this is one hell of an awesome Python module, will it ever been included in the standard Python library? Because in my opinion... limited as it maybe, it beats the hell out of most of the other parsing systems that are shipped with Python and its simple.



Dom
#### 2009-02-07 14:44:43 - ptmcg
Dom -



Well, flattery is <em>always</em> welcome, thanks! :) A number of people have suggested that I submit pyparsing for stdlib inclusion, at the moment pyparsing isn't fully PEP8 compliant, and it is a bit imprecise in the way it handles (or fails to handle) ambiguous grammars.  But thank you for the sentiment.



You are really quite close in the code you posted.  Just a few tweaks will get you going.



You create compiled by using the parser <em>before</em> you define any parse actions, so your VM only gets a list of parsed strings, not MurtleInstruction objects.  So just before calling vm.run(), I inserted these two statements:



    compiled = OneOrMore(statement).parseString(testProg)
    print (compiled)



They are the same ones you had above, but now all of the expressions have useful parse actions attached to them.  Now when I print out compiled, instead of



    [['LEFT', '10']]



I get:



    [[\<__main__.LeftMurtle object at 0x00AFC390\>]]



Notice that this is a 2-level nested list.  This is because when I first wrote the parser, we did not yet have the parse actions defined, so I used Group around all statements:



    statement \<\< Group( leftStmt | rightStmt | drawStmt | repeatStmt | functionStmt | ifStatement | commentMarker | assignmentStatement)



so that all of the tokens for all of the statements would not blend together.  Now that you are constructing separate objects for each statement, we can remove the Group command, and change statement to:



    statement \<\< ( leftStmt | rightStmt | drawStmt | repeatStmt | functionStmt | ifStatement | commentMarker | assignmentStatement)



With this change, compiled now looks like:



    [\<__main__.LeftMurtle object at 0x00AFB370\>]

which can be passed to the VM's run method as a nice list of commands.



Now you are almost ready to have your VM do something.  Let me give you another hint: in your __init__ method for MurtleInstruction, you just 'pass'.  Instead, insert this code:



    self.toks = toks



to save off the tokens that were passed in at parse time.  Now inside the execute() method of LeftMurtle, you can do something like this:



    def execute(self, vm):
            print 'Turn LEFT by %d murtlenoids' % int(self.toks[1])



If you modify leftStmt to be:



    leftStmt = 'LEFT' + arithExpr('angle')



you can write execute as:



    def execute(self, vm):
            print 'Turn LEFT by %d murtlenoids' % int(self.toks.angle)



It's a subtle difference, but once again, I strongly encourage you to embrace the concept of results names.  They *really* neaten up the code, and you don't have to chase down all those [1] indexes when you find you want a slightly different format, or add an optional parameter.



Add this parse action to integer:



    integer.setParseAction(lambda toks: int(toks[0]))



and you wont even need to do the int conversion in the execute method - the angle will already <em>be</em> an int by the time it gets there.



One other tidbit, I recommend you redefine your ifStatement as:



    ifStatement = 'IF' + booleanExpr + Group(OneOrMore(statement)) + Optional('ELSE' + Group(OneOrMore(statement))) + 'ENDIF'



instead of 



    ifStatement = 'IF' + userString + compOperators + arithExpr + Group(OneOrMore(statement)) + Optional('ELSE' + Group(OneOrMore(statement))) + 'ENDIF'



Just as you have defined arithExpr as the placeholder for the eventual more complex arithmetic expressions instead of just numeric literals.  When you get around to implementing arithExpr and booleanExpr, please look at the simpleBool and simpleArith examples on the Wiki, to make use of operatorPrecedence (note - booleanExpr will probably include a reference to arithExpr).



Don't be too hard on yourself in 'getting it' with parsing.  It is definitely the domain of a twisted mind, and takes a little getting used to.  To me, it is sort of my own style of sudoku puzzle, and I'm hoping to stave off the inevitable Alzheimer's by keeping my frontal cortex active with this kind of stuff.
#### 2009-03-01 12:03:40 - debug
Hello again-



I've been plodding away at this now for a couple of weeks and nearly all the features are complete (which is amazing so thanks again), there is one problem I have though which is loops (that eternal problem.. well for me and possibly other people stuck in bad scifi):

if I put this into it now:



    repeat 10
    left 10
    draw 3
    next



I get this returned:





    ['REPEAT', 10, \<murtleParser.LeftMurtle object at 0x2336f130\>, \<murtleParser.DrawMurtle object at 0x2336f190\>, 'NEXT']



Which I assume is a good thing as it seems to be picking up the left and draw objects and unlike the last problem I had it doesn't seem to be based around the idea of a two tier nested list.



My repeat function looks like this:





    class RepeatMurtle(MurtleInstruction):
        def __init__(self,toks):
            self.numInstrs = toks[1]
            self.toks = toks
        def execute(self,vm):
            for instr in range(self.numInstrs):
                instr.execute(vm)



Any ideas how to fix this? Ideally I'd just prefer a hint (or a nudge in the right direction) as it'll teach me to be more self sufficient :)



Thanks.
#### 2009-03-01 19:09:32 - ptmcg
You have some misconceptions in your code that you need to clean out.  The names are telling.  toks[1] does not contain the number of instructions, but the number of iterations for the loop to run.  So when the execute method loops 'for instr in range(self.numInstrs):', the index instr is not significant of anything - you might as well just call it '_'.



When you parse a REPEAT, there are two things you want to get out of it:

- the number of times to loop

- the statements that make up the loop body (this will be a list of some sort)



Then your execute function will do a nested for, something that looks like:



    for loopCount in range(number of times to loop):
        for statement in loopBodyStatements:
            statement.execute(self)



The last time you posted your code, your expression for a REPEAT statement looked like: 



    repeatStmt = 'REPEAT' + arithExpr + Group(OneOrMore(statement)) + 'NEXT'



Ideally, I would ask that you use results names for this expression:



    repeatStmt = 'REPEAT' + arithExpr('repeatCount') + 
        Group(OneOrMore(statement))('repeatBody') + 'NEXT'



Then RepeatMurtle can look like:



    class RepeatMurtle(MurtleInstruction):
        def __init__(self,toks):
            self.toks = toks
        def execute(self,vm):
            for loopCount in range(int(self.toks.repeatCount)):
                for instr in self.toks.repeatBody:
                    instr.execute(self)



Using results names instead of list indexes makes the code a lot easier to follow.  If you like, you can do this kind of thing in <u>init</u>:



    def __init__(self,toks):
            self.loopCount = int(toks.repeatCount)
            self.loopInstrs = self.toks.repeatBody



And then write execute() purely in terms of the instance variables self.loopCount and self.loopInstrs.



I hope this wasn't too much of a spoiler - I couldn't really think of any hints that didn't pretty much spell things out anyway.  (I think you bought the Python Mag article from last May - see the discussion about the handling of the Loop class.  In that case, there was no explicit loop counter, Loop works like a while statement.  But there is still that loop-within-loop.)



-- Paul
#### 2009-03-01 19:17:33 - ptmcg
One other thing, and this is more Python style than pyparsing per se.  When you need to iterate over a list of items, <em>please</em> learn the technique of iterating, and <em>unlearn</em> the technique of counting up by index.  That is, to work with every statement in the loop body, do this:



    for stmt in loopBody:
        # do something with stmt



and <em>not</em> this:



    for i in range(len(loopBody)):
        # do something with loopBody[i]



If you <em>absolutely</em> need the index, then iterate over the enumerate method:



    for i,stmt in enumerate(loopBody):
        # do something with stmt and i



The reason this comes up often in pyparsing applications is that after parsing, so many parsed tokens are lists of an unpredictable length.



This 'range(len(list_of_values))' business is a leftover from C or BASIC - use the data structures like they are real data structures, not just like flat arrays.  Your code will clean up considerably.



-- Paul

---
## 2009-02-06 05:29:46 - mvanderw - If I match _this_, then parse it like _that_
Hi! First off, I'm new to Python and pyparsing, as I've not done any scripting in the past decade, so please bear with me. :-) I've run into a problem that should be rather trivial, but I'm having trouble deciding how best to tackle it.



I want to parse a user-generated comma-separated file containing product codes and a variable length list of items associated with that code. I'd paste a sample, but it's secret, so here's a made-up line that follows the spec:





    03-22-2009,DRESS,'Cathy08',PMS 103,PMS 104,PMS 105,XS,S,M,L
    03-22-2009 14:22:11,TEE,'Dennis12',PMS 327,PMS 328,PMS 329,PMS 330,PMS 3242,PMS 3252,PMS 3262,PMS 3272,XS,S,M,L,XL



Now I know if it's a DRESS that there will be 7 arguments following the product name (Cathy and Dennis in my example). If it's a TEE there will be 13.



Obviously I can parse this just fine using DelimitedList(), but I'd like more rigorous/reliable parsing, partly to catch user errors and partly to make it easier in post-parsing (ie I don't want to insert a color code in a size table!). However, I'm catching myself going about it in ways that seem far too complex (like using an if statement to check what product I'm in) for what seems to me to be an easy task. If possible I'd like a simple pointer or even just a hint to the 'correct' approach I should be using. :-)



Many thanks!

#### 2009-02-06 10:56:47 - ptmcg
Welcome to pyparsing and to Python as well!  I am happy to see so many pyparsing users that are new to Python, and are still able to make headway in developing text parsers (not always a pleasant process).



Since your actual input text is secret/proprietary, I'll structure my answers around the text you posted - I hope my comments are relevant to your original problem text.



First off, let me say I'm a little skeptical about some of your assumptions.  Can you really be sure that 'TEE' will always have 13 arguments?  Some of these arguments seem likely to vary, such as what if a particular 'TEE' product comes in XXL and XXXL, or leaves out XS?  In fact, to my domain-ignorant eye, I see both of these lines as having the <em>same</em> format:



- a date or date-time stamp

- a product code

- a product name

- a list of qualifying numbers of the form 'PMS ###'

- a list of product sizes, such as XS, S, M, L, XL, etc.



This lets us use a generic parser like this:



    date = Combine((Word(nums,exact=2)+'-')*2+Word(nums,exact=4))
    time = Combine((Word(nums,exact=2)+':')*2+Word(nums,exact=2))
    prodcode = Word(alphas.upper())
    prodname = quotedString
    colorcode = Combine('PMS ' + Word(nums))
    size = Combine(ZeroOrMore('X') + oneOf('S L') )| 'M'
    
    COMMA = Suppress(',')
    
    prodEntry = (date + Optional(time)) + COMMA + prodcode + COMMA + \
        prodname + COMMA + Group(delimitedList(colorcode)) + COMMA + \
        Group(delimitedList(size))



Still, let's go with your assumption that TEE and DRESS are sufficient specification for the rest of the data line.  I would define specific expressions for a TEE line and a DRESS line, like:



    listOf = lambda n,expr,sep=COMMA : (expr + sep)*(n-1) + expr
    
    dressEntry = (date + Optional(time)) + COMMA + \
        'DRESS' + COMMA + prodname + COMMA + \
        Group(listOf(3,colorcode)) + COMMA + Group(listOf(4,size))
    
    teeEntry = (date + Optional(time)) + COMMA + \
        'TEE' + COMMA + prodname + COMMA + \
        Group(listOf(8,colorcode)) + COMMA + Group(listOf(5,size))



(I created a little helper called listOf that defines a little comma-separated list of a specific length.)



Now if you try to parse a dressEntry that has the name 'DRESS' but the wrong number of arguments, then the parser will pick that up immediately.



You could also do this with the generic parser, by adding a validating parse action to prodEntry - based on the prodCode value, you could verify that the colorcode and size lists each have the right number of entries, and raise a ParseSyntaxException if they don't.



Lastly, please read the other discussion thread about the LOGO language parser - my last post goes on at tedious length about using results names to help keep the post-parsing code manageable.



cheers, and Welcome!

-- Paul
#### 2009-02-07 02:21:04 - mvanderw
Ah! Much cleaner indeed. Excellent.



Yes, it's actually quite likely the specs change over time, which is why I want to pick that up. Common scenario here is 'someone' decides to add a colorline/size without discussing it first, which then becomes a problem down the road.



FYI, what I figured I could was parse the line following the entry depending on the entry contents (if entry = 'X' then it has y num arguments). That might just be me hardwired in thinking of control flows though.



Really nice. Thanks a bunch!
#### 2009-02-07 11:42:33 - ptmcg
Ok, so if the specs are likely to change over time, then let me show you how to use the generic parser, and then have a parse action do the prodcode-specific argument validation.



First off, I'll redo the prodEntry expression, but add results names.  This will make it easier for us to access the specific pieces of the parsed data:



    prodEntry = (date + Optional(time)) + COMMA + \
        prodcode('prodcode') + COMMA + \
        prodname('prodname') + COMMA + \
        Group(delimitedList(colorcode))('colorcodes') + COMMA + \
        Group(delimitedList(size))('sizes')



Planning ahead for our validation method, I'll define a simple Python dict that will capture for us the valid numbers of colorcodes and sizes for each prodcode (I made up a couple more so that this doesn't seem quite so trivial, and to illustrate how you would add more prodcodes):



    prodcode_validation_table = {
        'DRESS' : (3,4),
        'TEE' : (8,5),
        'PANT' : (2,5),
        'SHORT' : (4,8),
        }



Now I'll define a parse action method, a method that will accept an integer (representing the location of the match within the input string) and a ParseResults object (containing the parsed tokens for the current incarnation of prodEntry).  The purpose of this parse action will be to:

1. verify the prodcode is valid

2. lookup the valid number of colors and sizes form the validation table

3. get the numbers of colorcodes and sizes from the parsed data

4. check if the parsed lists are the correct length, and if not, raise a pyparsing ParseFatalException, which will stop parsing immediately





    def validate_entry(locn, parse_data):
    
        if parse_data.prodcode in prodcode_validation_table:
    
            valid_color_count,valid_size_count = \
                prodcode_validation_table[parse_data.prodcode]
    
            num_colorcodes = len(parse_data.colorcodes)
            num_sizes = len(parse_data.sizes)
    
            if num_colorcodes != valid_color_count:
                raise ParseFatalException('',locn,
                    'Expected %d colorcodes, found %d' % 
                        (valid_color_count, num_colorcodes))
    
            if num_sizes != valid_size_count:
                raise ParseFatalException('',locn,
                    'Expected %d sizes, found %d' % 
                        (valid_size_count, num_sizes))
    
        else:
            raise ParseFatalException('',locn,
                'Found unrecognized prodcode '%s'' % parse_data.prodcode)



And then lastly, we attach the parse action to the prodEntry expression:



    prodEntry.setParseAction(validate_entry)        



Here is a little test script with test data and parser access code to show how this works:



    data = '''\
    03-22-2009,DRESS,'Cathy08',PMS 103,PMS 104,PMS 105,XS,S,M,L
    03-22-2009 14:22:11,TEE,'Dennis12',PMS 327,PMS 328,PMS 329,PMS 330,PMS 3242,PMS 3252,PMS 3262,PMS 3272,XS,S,M,L,XL
    03-22-2009,DRESS,'Cathy08',PMS 103,PMS 104,PMS 105,XS,S,M,L,XL
    03-22-2009,DRESS,'Cathy08',PMS 103,PMS 104,PMS 105,PMS 999,XS,S,M,L
    03-22-2009,CAPRI,'Cathy08',PMS 103,PMS 104,PMS 105,PMS 999,XS,S,M,L
    '''
    
    for d in data.splitlines():
        try:
            print prodEntry.parseString(d).dump()
        except ParseFatalException, pe:
            print d
            print pe.msg
        print



Prints:



    ['03-22-2009', 'DRESS', ''Cathy08'', ['PMS 103', 'PMS 104', 'PMS 105'], 
     ['XS', 'S', 'M', 'L']]
    - colorcodes: ['PMS 103', 'PMS 104', 'PMS 105']
    - prodcode: DRESS
    - prodname: 'Cathy08'
    - sizes: ['XS', 'S', 'M', 'L']
    
    ['03-22-2009', '14:22:11', 'TEE', ''Dennis12'', 
     ['PMS 327', 'PMS 328', 'PMS 329', 'PMS 330', 'PMS 3242', 'PMS 3252', 'PMS 3262', 'PMS 3272'],
     ['XS', 'S', 'M', 'L', 'XL']]
    - colorcodes: ['PMS 327', 'PMS 328', 'PMS 329', 'PMS 330', 'PMS 3242', 'PMS 3252', 'PMS 3262', 
     'PMS 3272']
    - prodcode: TEE
    - prodname: 'Dennis12'
    - sizes: ['XS', 'S', 'M', 'L', 'XL']
    
    03-22-2009,DRESS,'Cathy08',PMS 103,PMS 104,PMS 105,XS,S,M,L,XL
    Expected 4 sizes, found 5
    
    03-22-2009,DRESS,'Cathy08',PMS 103,PMS 104,PMS 105,PMS 999,XS,S,M,L
    Expected 3 colorcodes, found 4
    
    03-22-2009,CAPRI,'Cathy08',PMS 103,PMS 104,PMS 105,PMS 999,XS,S,M,L
    Found unrecognized prodcode 'CAPRI'
    



Now as your spec changes, you wont have to chase down a bunch of prodcode-specific expressions and constants, you can just modify the validation table and let the parser do all the hard work!



-- Paul

---
## 2009-02-07 16:02:33 - stakhanov2 - memory issues with Py3k
First of all: Thanks a lot for your work on pyparsing!

I'm reasonably experienced with python, and have used

ply before for a Python project that I'm now reimplementing

in Python 3.0.  For the new implementation I've chosen

to go with pyparsing instead which, so far I have not

regretted.



But, now I'm having an issue with memory usage.



For testing purposes I've instrumented my code in such

a way that no references to anything coming out of the

parser are kept.  The first couple of iterations of the

test go fine.  Then as I keep reiterating the same

test, it goes slower and slower.  I can see that the

python interpreter's image keeps growing in memory usage,

and that the garbage collector takes longer and longer

to run.  Eventually the programme somehow grinds to a

near-halt.



As I run the parser, the garbage collector keeps finding

lots of collectable stuff, but nothing uncollectable,

so it doesn't seem to be an issue about unresolvable

reference cycles.



To me it looks like references to certain objects

are actually kept somewhere, and I just can't find them.



Have you had this symptom in pyparsing before?

Any ideas where I could start looking?

#### 2009-02-07 18:46:40 - ptmcg
1. Are you using packrat parsing?

2. When you rerun the test, do you rebuild the parser every time, or just reuse the same one over and over?

3. Just for kicks, try commenting out the <u>eq</u> method of ParserElement and see if you get the same behavior.



The only overt cacheing that is done in pyparsing is the creation and reuse of Exception objects (unless you have packrat parsing on).  There used to be cyclic references kept in the ParseResults class, but I long ago changed the parent back pointers to wkref's, so this should no longer be an issue.



If Py3k is doing something internally with stack traces w.r.t. their related exceptions, it is possible that the reuse of exceptions is not allowing some internal Python exception/traceback cache to ever get flushed.  It should not be difficult to go through pyparsing and reinstate the code that creates new exceptions instead of reusing the cached exceptions, and see if the memory issue persists.



HTH,

-- Paul

(...and thanks for the Py3K testing - are you using the pyparsing_py3.py file from the Sourceforge SVN repository, or did you run 2to3 on pyparsing.py?)
#### 2009-02-08 01:49:42 - stakhanov2
1. not using packrat

2. hmm... how do I know this? I just call parseString

   on the same ParserElement repeatedly.

3. tried that, didn't make a difference.



Originally, I was using the pyparsing_py3.py from the

release package, but now that you mention it, I've

used the sf SVN version.  ...still, no difference.



In both instances, I've had to fix some minor syntax

errors, to get it running.



For testing purposes I've now stripped down my stuff

to the bare grammar, and can still reproduce the error.



You can have a look at it at



  



Further testing revealed, that this is only the case

with Python 3.0 not with Python 2.4.
#### 2009-02-08 02:04:25 - stakhanov2
I've now also tried doing this with 2to3

on the original pyparsing.py. ...same symptom.
#### 2009-02-17 04:08:54 - stakhanov2
I've ported this to PLY now.



You can check out the new version of the testing

programme at:



  



basically the very same programme works perfectly

fine with both PLY and pyparsing on Python 2.6.

It also works with PLY on Python 3.0.1.



Doesn't work with pyparsing on Python 3.0.1, though.



cheers,



Richard
#### 2009-02-18 09:41:57 - ptmcg
Richard -



Would you mind rerunning this test, after commenting out the __slots__ declaration for the ParseBaseException class in pyparsing?  This affected the behavior in IronPython, and may be related to the memory issue you identified.



Thanks,

-- Paul
#### 2009-02-18 13:19:28 - stakhanov2
...same symptom, sorry.

---
## 2009-02-17 13:11:34 - djlawler - Ironpython 2.01
pyparsing (from svn) works for simple examples like greeting.py!  That is the good news.  The bad news is that anything complicated dies. Usually with something like this:



\>ipy romanNumerals.py

Traceback (most recent call last):

  File 'romanNumerals.py', line 57, in romanNumerals.py

  File 'c:\program files\ironpython2\Lib\pyparsing.py', line 1106, in scanString



SystemError: BaseException.Microsoft.Scripting.Actions.IDynamicObject.GetMetaObj

ect(Microsoft.Linq.Expressions.Expression)



Unfortunately I have no clue what this is and no really good way to debug this.



Tantalizing that it gets this close....



Regards,



David

#### 2009-02-17 15:55:14 - ptmcg
I haven't looked at IP for a while.  The last time I looked, the biggest problem was in the implementation of <u>radd</u> not successfully resolving 'string + ParserExpression' expressions (jython had similar problems).



I'll download the latest IronPython and see if I can make better progress.



-- Paul
#### 2009-02-17 17:07:25 - ptmcg
Some good news, I made a lot of progress by commenting out the <u>slots</u> declaration in the ParseBaseException class.  Now I can successfully run chemicalFormulas, romanNumerals, simpleBool and simpleArith.  But some other examples that work with CPython raise StackOverflow exceptions.



A small step forward, but at least it is forward...



-- Paul
#### 2009-02-24 11:33:45 - djlawler
Thanks I will poke around with it a little bit too...now that I know there is some interest  :)



Regards,



David
#### 2009-02-24 12:41:43 - djlawler
Wow - with that one little change, it seems that MOST of the demos run fine.  Which ones did you run into issues with?
#### 2009-02-24 19:55:11 - ptmcg
idlparse for one, fails for me with a StackOverflow exception.
#### 2009-02-24 20:18:00 - djlawler
I think it works for me!?:



\>ipy idlParse.py



        /*

         * a block comment *

         */

        typedef string[10] tenStrings;

        typedef sequence\<string\> stringSeq;

        typedef sequence\< sequence\<string\> \> stringSeqSeq;



        interface QoSAdmin {

            stringSeq method1( in string arg1, inout long arg2 );

            stringSeqSeq method2( in string arg1, inout long arg2, inout long ar

g3);

            string method3();

          };



tokens =

[['typedef', 'string', '[', '10', ']', 'tenStrings', ';'], ['typedef', ['sequence', '\<', 'string', '\>'], 'stringSeq', ';'], ['typedef',  ['sequence', '\<', ['sequence', '\<', 'string', '\>'], '\>'],  'stringSeqSeq',  ';'], ['interface',  'QoSAdmin',  '{',  'stringSeq',  'method1',  '(',  ['in', 'string', 'arg1'],  ['inout', 'long', 'arg2'],  ')',  ';',  'stringSeqSeq',  'method2',  '(',  ['in', 'string', 'arg1'],  ['inout', 'long', 'arg2'],  ['inout', 'long', 'arg3'],  ')',  ';',  'string',  'method3',  '(',  ')',  ';',  '}',  ';']]





        /*

         * a block comment *

         */

        typedef string[10] tenStrings;

        typedef

            /  <strong>* </strong><strong> *

             * a block comment *

             */

            sequence\<string\> /*comment inside an And */ stringSeq;

        /* */  /</strong>/ /<strong>*/ /</strong>**/

        typedef sequence\< sequence\<string\> \> stringSeqSeq;



        interface QoSAdmin {

            stringSeq method1( in string arg1, inout long arg2 );

            stringSeqSeq method2( in string arg1, inout long arg2, inout long ar

g3);

            string method3();

          };



tokens =

[['typedef', 'string', '[', '10', ']', 'tenStrings', ';'], ['typedef', ['sequence', '\<', 'string', '\>'], 'stringSeq', ';'], ['typedef',  ['sequence', '\<', ['sequence', '\<', 'string', '\>'], '\>'],  'stringSeqSeq',  ';'], ['interface',  'QoSAdmin',  '{',  'stringSeq',  'method1',  '(',  ['in', 'string', 'arg1'],  ['inout', 'long', 'arg2'],  ')',  ';',  'stringSeqSeq',  'method2',  '(',  ['in', 'string', 'arg1'],  ['inout', 'long', 'arg2'],  ['inout', 'long', 'arg3'],  ')',  ';',  'string',  'method3',  '(',  ')',  ';',  '}',  ';']]





          const string test='Test String\n';

          const long  a = 0;

          const long  b = -100;

          const float c = 3.14159;

          const long  d = 0x007f7f7f;

          exception TestException

            {

            string msg;

            sequence\<string\> dataStrings;

            };



          interface TestInterface

            {

            void method1( in string arg1, inout long arg2 );

            };



tokens =

[['const', 'string', 'test', '=', ''Test String\\n'', ';'], ['const', 'long', 'a', '=', '0', ';'], ['const', 'long', 'b', '=', '-100', ';'], ['const', 'float', 'c', '=', '3.14159', ';'], ['const', 'long', 'd', '=', '0x007f7f7f', ';'], 'exception', 'TestException', '{', ['string', 'msg', ';'], [['sequence', '\<', 'string', '\>'], 'dataStrings', ';'], '}', ';', ['interface',  'TestInterface',  '{',  'void',  'method1',  '(',  ['in', 'string', 'arg1'],  ['inout', 'long', 'arg2'],  ')',  ';',  '}',  ';']]





        module Test1

          {

          exception TestException

            {

            string msg;

            ];



          interface TestInterface

            {

            void method1( in string arg1, inout long arg2 )

              raises ( TestException );

            };

          };



          exception TestException

          ^

Expected '}' (at char 45), (line:4, col:11)





        module Test1

          {

          exception TestException

            {

            string msg;

            };



          };



tokens =

['module',

 'Test1',

 '{',

 'exception',

 'TestException',

 '{',

 ['string', 'msg', ';'],

 '}',

 ';',

 '}',

 ';']





Running this on XP SP3, Ironpython 2.01.  I have .net 3.5 SP1 on my machine.  I do have something like 3.3GB of memory on ths machine though running he program with task manager open does not show a lot of memory usage.  Odd.



Regards,



David
#### 2009-02-24 21:57:57 - ptmcg
I'm not so lucky:



    cmd\> 'c:\Program Files\IronPython 2.0.1\ipy.exe' idlparse.py
    
            /*
             * a block comment *
             */
            typedef string[10] tenStrings;
            typedef sequence\<string\> stringSeq;
            typedef sequence\< sequence\<string\> \> stringSeqSeq;
    
            interface QoSAdmin {
                stringSeq method1( in string arg1, inout long arg2 );
                stringSeqSeq method2( in string arg1, inout long arg2, inout long arg3);
                string method3();
              };
    
    
    Process is terminated due to StackOverflowException.
    
    cmd\> 'c:\Program Files\IronPython 2.0.1\ipy.exe' jsonParser.py
    
    Process is terminated due to StackOverflowException.



And why is the startup so godawful slow! ???



These work:

nested.py

macroExpander.py

wordsToNum.py

invRegex.py



These don't:

getNTPServers.py (Unicode error)

dhcp_leases_parser.py (Attribute error - possibly a pyparsing incompatibility)



-- Paul
#### 2009-03-03 05:41:30 - djlawler
The slow startup is due to IronPython 'compiling' stuff that gets imported by default.  This makes it slow to startup but makes everything after importing faster.  I believe the developers are aware of the issue:







Since June 2008 things have gotten a bit quicker.....but still not anywhere close to CPython.  The -X:Interpret switch mentioned in the thread above only speeds up my application by a couple of seconds.



Regards,



David

---
## 2009-02-18 06:41:05 - mUogoro - C function pointer parser
I'm trying to write my custom parser used for analyze C function pointers declaration. For simple function declaration I've no problems. I simply split my parser in three rules: retType, funcName and arguments. retType parses a C type (plus every type specifier/qualifier and stars characters related to pointers). funcName parses the function name (if any). Finally, arguments parses the arguments list (like retType, adding the eventually argument name parsing). Applyed to this declaration





    int (*) (int, long, double)



the parser returns this results





    retType: int
    funcName:
    arguments: int, long, double 



If the parser works well with simple declaration, it totally fails to parse complex declaration like this:





    int(*)(long, char(*)(const char))  (*myFunc)  (int, char*, void(*)(void*))



where:



    retType: int(*)(long, char(*)(const char))
    funcName: myFunc
    arguments: int, char*, void(*)(void*)



Could you give me some useful hints to parse this nested function pointers declaration, please? Every help is more appreciated.



Daniele



P.S.: excuse for my poor english... correct it too, please :)

#### 2009-02-18 07:28:03 - mUogoro
I've partially solved the problem in this way: I define (something similar to) those grammar rules:





    type = .... #Parse c types
    funcPointer = OneOrMore( retType + funcName + arguments )
    retType = type
    argument = funcPointer | type + argName
    arguments = argument + ',' + arguments | argument 



For a complex declaration like this:





    char(*)(long) (*)( volatile double, void(*foo)(unsigned**) )



the parsing process outputs this parseResult:





    ['char', '*', 'long', '*', 'volatile', 'double', 'void', '*', 'foo', 'unsigned', '*', '*']



Now the problem is how to separate information related to returned type and arguments function pointers from the main function pointer. 

What I would to obtain, supposing the example above, is:





    retType = ['char', '*', 'long']
    funcName = 
    arguments = 'volatile double', [ 'void', '*', 'foo', 'unsigned', '*', '*']



Any advice? :)



Daniele
#### 2009-02-18 08:16:30 - mUogoro
Regarding my proposed solution, the retType rule is built using the Optional construct: this allows to chain consecutive function pointers (as in the last example shown in previous post). The last modification that I've done consists in grouping the funcPointer rule results, so function pointers are wrapped inside nested list in the obtained parseResult.

An example: this declaration





    foo(*)(long)(*)(void*)  (*myFunc)( volatile double, void(*foo)(unsigned**) )



once parsed, produces this result





    [['foo', '*', 'long'], ['*', 'void', '*'], ['*', 'myFunc', 'volatile', 'double', ['void', '*', 'foo', 'unsigned', '*', '*']]]



The last 'outer' list refers to the main function pointer plus arguments | function pointer arguments, while the previous lists refer to returned-type-related function pointers. I think that with my solution I'm able to parse every function pointer declaration (I've to test it :) ), but I also think that the grammar rules that I've defined don't represent the best or more elegant solution. Every advice about alternative implementations are welcome. :)



Daniele
#### 2009-02-18 09:38:35 - ptmcg
Daniele -



My congratulations and sympathies to you!  Tackling C function pointer declarations with all the variations you are handling is quite a tricky job, and you seem to be making a good go of it.



Here are some suggestions (it is not easy without seeing the actual parser code, but I can guess a few things based on the ParseResults you are posting):

1. Use Group.  Pyparsing's default behavior is to list all matched tokens together into one big list.  Group will override that and create sublists.  Look at this example.  Say I have a pattern to match, consisting of a word followed by one or more integers:



    integer = Word(nums)
    pattern = Word(alphas) + OneOrMore(integer)
    
    parser = OneOrMore(pattern)
    
    print parser.parseString('ABC 100 DEF 200 GHI 101 202 303 JKL 503 22')



This gives me:



    ['ABC', '100', 'DEF', '200', 'GHI', '101', '202', '303', 'JKL', '503', '22']



Yes, I have done some parsing, but the results are not structured at all, and I have to do a lot more work to walk this list, and see where one word starts and the next one stops.  If you have post-parsing code that does this, then you probably can help yourself a bit more with a refined parser.



Now I make this one change in the parser:



    pattern = Group(Word(alphas) + OneOrMore(integer))



And now I get these results:



    [['ABC', '100'], ['DEF', '200'], ['GHI', '101', '202', '303'], ['JKL', '503', '22']]



See how each group has been captured into its own list?  Now I can write this code:



    results = parser.parseString('ABC 100 DEF 200 GHI 101 202 303 JKL 503 22')
    
    for patt in results:
        print patt[0], ','.join(patt[1:])



and get:



    ABC 100
    DEF 200
    GHI 101,202,303
    JKL 503,22



I suggest you do the same with your argument definition.



    argument = funcPointer | Group(type + argName)

or 



    argument = Group(funcPointer | type + argName)



2. Use delimitedList.  I know the canonical EBNF form for a delimited list is:



    item := ... something ...
    items := item ',' items | item



And this *does* translate directly to pyparsing using the Forward class.  But it is a lot of work, slows down the parser, and creates extra recursion and nesting.  I *strongly* suggest you instead implement this as:



    arguments = delimitedList(Group(argument))



',' is the default delimiter, but other delimiters are possible (you can even use an expression for a delimited).  delimiters are also stripped out for you.  Also notice that I am grouping the separate arguments, so that after parsing, I can just walk the list and see all the tokens for each arguement.





3. Use results names.  This will also help simplify your post-parsing work.  (There should be some examples of this in the online docs, or in other threads on this discussion page.)





But excellent job on getting this far on your own!  Post back and let us know how things are going (or send a link to your application, and I'll add you to the Who's Using Pyparsing page).



-- Paul

---
## 2009-02-18 16:07:52 - ahusak - Issue with lineno() and the loc variable.
I'm having some trouble getting the line number correctly while using pyparsing, and I think there may be a problem with how loc is calculated.



The initial idea was to just print out the tokens that were parsed, and the line they were parsed from, but I kept getting line numbers back like '1 1 2 3 4 5 6 7 7 8 9'



here's my code, which I've stripped down a ton to keep things simple, and should hopefully demonstrate what's going on:



    #!/usr/local/bin/python
    from pyparsing import *
    
    class GenericObject(object):
        def __init__( self, s, l, t ):
            print 'l:(', l,')', ' s:(',s[l:l+10],')', ' t:(', t, ')'
            self.lineNumber = lineno(l, s)
            try:
                self.attributeList = t[0].asList()
            except AttributeError:
                self.attributeList = t
        def __str__ (self):
            return str(self.attributeList)
    
    class VersionObject(GenericObject):
        keyAttributes = [ 0 ]
    
    class CosFromGwObject(GenericObject):
        keyAttributes = [ 0 ]
    
    ### basic variables
    integer = Word(nums)
    string = Word(alphanums+'_./')
    
    ### version
    versionHead = Keyword('Current configuration - running on version').suppress()
    colon = Literal(':').suppress()
    
    ### Definitions
    CharNumbers = Group( integer ).setParseAction(GenericObject)
    Version = Group( versionHead + string + colon ).setParseAction(VersionObject)
    CosFromGw = Group( Keyword('ndd -set skyxmod skyx_cos_from_gw').suppress() + integer ).setParseAction(CosFromGwObject)
    
    matches = OneOrMore(CharNumbers ^ Version ^ CosFromGw) + StringEnd()
    
    data = '''00000000011111111112222222222333333333344444444444
    01234567890123456789012345678901234567890123456789
    Current configuration - running on version 7.0.4 :
    ndd -set skyxmod skyx_cos_from_gw 1 '''
    
    parseTree = matches.parseString(data)
    
    for obj in parseTree:
        print obj.lineNumber, obj.__class__.__name__, obj



The quick summary is to take a config, parse out the different types of lines and their variables, and store those variables in a respective object, along with the line number. I've added in two lines at the beginning of just numbers as part of the debug process of counting which character was doing what.



what seems to be going on is that the loc given back doesn't take the \n into account, and so it's thrown off by one, and because of that the line number is thrown off every so often. 



for example, if you run the above code, it should print out:



    l:( 0 )  s:( 0000000001 )  t:( [['00000000011111111112222222222333333333344444444444']] )
    l:( 50 )  s:(
    012345678 )  t:( [['01234567890123456789012345678901234567890123456789']] )

That second string chunk includes the '\n' from the previous line. The calculation for lineno() on the second line counts from char 0-50. If I counted right, 50 is that last '4' of the previous line but does not include the '\n', and so the math for lineno() is 0+1, still line 1. This problem pops up seemingly at random (it wasn't just on the first two lines), so my first thought was the definitions I wrote were wrong, but that doesn't seem to be the case. I also thought the issue was due to some of the whitespace issues described in the documentation, but from what I can make out (it was a bit confusing), that wasn't an issue either.



This is using pyparsing 1.5.1. Am I doing something wrong, or is there actually a bug hidden somewhere in pyparsing?



I'd appreciate any input you can give in helping me resolve this.

#### 2009-02-19 01:05:03 - ptmcg
Well, this could certainly be construed as a bug, but I'm afraid that if I fix it, a number of other things will break.



It turns out that this really *is* a whitespace-skipping issue.  What is happening is that the parse action is being called with the parse location of any preceding whitespace, not necessarily the location of the actual matched token.  I'll admit that, on the face of it, that sounds like wrong behavior.



As a workaround, you can insert an Empty() as part of your parser just before any expressions that have parse actions attached that depend on the correct location value.



    matches = OneOrMore(Empty() + (CharNumbers ^ Version ^ CosFromGw)) + StringEnd()



Empty always matches, and returns an empty string.  It *also* advances the parser location past any whitespace or ignored expression.  By adding Empty() where I did, it will cause pyparsing to advance to the next non-whitespace character.  Now your location values will start where the actual tokens start.



I will look through my tests to see what the implications are for changing the location value passed to a parse action.  If I am lucky, I can make the location value accurately reflect the parse location of the actual matched tokens, and other parts of pyparsing (such as transformString) wont break as a result.



Nice catch, thanks!

-- Paul
#### 2009-02-19 14:17:53 - ahusak
Just gave the Empty() workaround a try on a larger set of data & grammar, and it worked out just fine. 1-120 with no repeats. =D



Thank you for your help!
#### 2009-02-27 13:40:42 - ahusak
Thanks again for the help, I have a follow-up question for you.



I'm running into a little bit of trouble trying to add a 'catch-all' to the grammar. 



Again, this is meant to read in a config file, understand the values that are present, and associate them with their respective type of line. I've got all the grammar I need, but I'd also like to be able to handle garbled text on a line, a typo, etc. I'd like to mark that line as failed, and then just move on as if nothing happened and finish the rest of the text.



My thought was to create a FailedLine, which would just look for one or more words of any sort, and create a similar FailedObject with that so i can deal with it later. 



here's my code, still pretty similar to what's above:



    #!/usr/local/bin/python
    import pyparsing as p
    
    class GenericObject(object):
        def __init__( self, s, l=None, t= None):
          if not isinstance(s, str):
                self.LineNumber = 0
                self.attributeList=s
          else:
                try:
                    self.attributeList = t[0].asList()
                except AttributeError:
                    self.attributeList=t       
                self.lineNumber = p.lineno(l,s)
    
        def generate (self):
            newwords=[]
            for w in self.attributeList:
                if isinstance(w, list):
                    w = ' '.join(w)
                newwords.append(w)
            t = tuple(newwords)
            return self.linetemplate % t
    
    class VersionObject(GenericObject):
        linetemplate = 'Current configuration - running on version %s :'
    
    class CosFromGwObject(GenericObject):
        linetemplate = 'ndd -set skyxmod skyx_cos_from_gw %s'
    
    class FailedObject(GenericObject):
        linetemplate = 'bad text'
        def __init__( self, s, l=None, t= None):
            super( FailedObject, self ).__init__(s, l, t)
            self.linetemplate = p.line(l,s)
        def generate (self):
           return self.linetemplate
    
    ### basic variables
    cvtLong = lambda toks: long(toks[0])
    integer = p.Word(p.nums).setParseAction(cvtLong)
    string = p.Word(p.alphanums+'_./'-:')
    
    ### version
    versionHead = p.Keyword('Current configuration - running on version').suppress()
    colon = p.Literal(':').suppress()
    
    ### Definitions
    CosFromGw =  p.Group( p.Keyword('ndd -set skyxmod skyx_cos_from_gw').suppress() + integer ).setParseAction(CosFromGwObject)
    Version =    p.Group(versionHead + string+ colon ).setParseAction(VersionObject)
    FailedLine = p.OneOrMore(string).setParseAction(FailedObject)
    
    ### full match set.
    matches = p.OneOrMore( p.Empty() + ( CosFromGw | Version | FailedLine )  )   + p.StringEnd()
    
    data = '''ndd -set skyxmod skyx_cos_from_gw 1 
    Current configuration - running on version 7.0.4 : 
    ndd -set skyxmod skyx_cos_from_gw 1 
    blah blah
    Current configuration -BADLINE running on version 7.0.4 : 
    ndd -set skyxmod skyx_cos_from_gw 1 
    Current configuration - running on version 7.0.4 : 
    blah blah
    '''
    
    parseTree = matches.parseString(data)
    
    for obj in parseTree:
        print obj.lineNumber,'\t|', obj.__class__.__name__, '\t|', obj.generate()



In the text, i've got some 'blah blah' lines just meant to represent a completely wrong line. In the case where you see 'typo' BADLINE, I want to treat it the same way (i'm not worried about trying to force it into the correct object, simply treating the whole thing as failed is fine). 



My matches line seems to be what I think I want: try all the normal lines, then use the FailedLine as a last resort. 



If you try it out, you'll see it does this, but it gives up prematurely, despite the StringEnd() fix that should force it to continue onward. 



I've fooled around with the various iterations of ^'s,|'s,parenthesis, and other boolean options to try and get this right, but can't make it work, so I think I'm going about this the wrong way. Letting an exception be raised seems to stop the whole parsing attempt, which isn't what I want. setFailAction also seemed like it might be what I'm after, but I couldn't get that to act right either. 



Do you have a better way to handle this type of situation?
#### 2009-02-28 08:49:23 - ptmcg
I like the idea of FailedLine, as a sort of catchall expression to skip to the next valid bit.  With this in mind, I modified FailedLine to be:



    FailedLine = p.restOfLine.setParseAction(FailedObject) + p.LineEnd().suppress()



This way, you wont accidentally read off the end of the current line, and keep reading strings that might be part of a next valid line.



I also got a little insight into your problem by adding this code:



    for vname in 'CosFromGw Version FailedLine'.split():
        vars()[vname].setName(vname).setDebug()



This lists out each expression when it is about to be matched, whether it matches or fails, and if it matches, what gets returned.



The problem with changing FailedLine to be restOfLine is that I could end up in an infinite parsing loop at the very end.  So I had to add a ~StringEnd() at the beginning of OneOrMore.  This changes matches to:



    matches = p.OneOrMore( ~p.StringEnd() + p.Empty() + 
                    ( CosFromGw | Version | FailedLine ) ) + p.StringEnd()



This gives this output:



    1     | CosFromGwObject     | ndd -set skyxmod skyx_cos_from_gw 1
    2     | VersionObject     | Current configuration - running on version 7.0.4 :
    3     | CosFromGwObject     | ndd -set skyxmod skyx_cos_from_gw 1
    4     | FailedObject     | blah blah
    5     | FailedObject     | Current configuration -BADLINE running on version 7.0.4 : 
    6     | CosFromGwObject     | ndd -set skyxmod skyx_cos_from_gw 1
    7     | VersionObject     | Current configuration - running on version 7.0.4 :
    8     | FailedObject     | blah blah



-- Paul

---
## 2009-02-26 01:23:22 - asb_india - Help for the newbie
I want to create the parser for the 



'cross(cross(cross(a,b),c),d)'



This can go extending to n level 



How can i make using the pyparsing

#### 2009-04-21 22:43:39 - asb_india
Hello, 



M happy to find the solution using the pyparsing.



And things are going ok for the parser. But i have one small problem 



cross(cross({a,b,c,d,e},{c1}),{c2}) -- Ok



t = u'\xe8re'

cross(cross({a,b,c,d,t},{c1}),{c2}) -- Gives error as now the t is unicode.



Wht shld be done in this case?



Thanks
#### 2009-04-22 05:37:35 - ptmcg
Is your error one in pyparsing, or in outputting the results?  What is the error, exactly?  And what are you trying to represent by:



    t = u'\xe8re'
    cross(cross({a,b,c,d,t},{c1}),{c2}) -- Gives error as now the t is unicode.



Does this mean you were trying to parse this?



    u'cross(cross({a,b,c,d,\xe8re},{c1}),{c2})'



In general, pyparsing is tolerant of unicode, but your parse expressions may need to be defined using unicode strings.



But I suspect your error may have been more to do with encoding or printing out unicode strings.  It is hard to know without seeing the error.



As a general note, when posting requests for help in resolving an error, it is helpful to post the EXACT error that was given.



-- Paul
#### 2009-04-22 21:14:25 - asb_india
Yes i agree Here is the sample code 







    
    
    import pyparsing 
    from pyparsing import *
    
    wd = Word( alphanums )
    LPAR, RPAR, LCPAR, RCPAR = map( Suppress, '(){}' )
    cross = Keyword( 'cross', caseless = True )
    comma = Suppress( ',' )
    
    
    main = LCPAR +  delimitedList(wd)  + RCPAR
    print ' This is the main ',main.parseString('{abc,def}')
    cross = LCPAR + wd + RCPAR
    print ' This is the cross ',cross.parseString('{cross}')
    
    item = Forward()
    l = item | main
    item \<\< ( cross + LPAR + l + comma + cross + RPAR )
    
    
    print '\nSimple Parse String\n ', main.parseString('{abc, def}')
    
    t = 'Rosire'
    t1 = 'asb'
    
    prs = '{' + ','.join(t,t1) + '}'
    print '\n Unicode Parsing \n ',main.parseString(prs)
    





When is do unicode parsing i get error 



SyntaxError: Non-ASCII character '\xc3' in file ex.py on line 22, but no encoding declared; see  for details



Thanks
#### 2009-04-22 21:37:10 - asb_india


I modified and added coding utf-8 but still error 





    # -*- coding: utf-8 -*-
    import pyparsing 
    from pyparsing import *
    
    wd = Word( alphanums )
    LPAR, RPAR, LCPAR, RCPAR = map( Suppress, '(){}' )
    cross = Keyword( 'cross', caseless = True )
    comma = Suppress( ',' )
    
    
    main = LCPAR +  delimitedList(wd)  + RCPAR
    print ' This is the main ',main.parseString('{abc,def}')
    cross = LCPAR + wd + RCPAR
    print ' This is the cross ',cross.parseString('{cross}')
    
    item = Forward()
    l = item | main
    item \<\< ( cross + LPAR + l + comma + cross + RPAR )
    
    
    print '\nSimple Parse String\n ', main.parseString('{abc, def}')
    
    t = 'Rosire'
    t1 = 'asb'
    
    prs = '{' + ','.join([t,t1]) + '}'
    print '\n Unicode Parsing \n ',main.parseString(prs)
    



now the error is different i am pasting the whole o/p



 This is the main  ['abc', 'def']

 This is the cross  ['cross']



Simple Parse String

  ['abc', 'def']



 Unicode Parsing 



Traceback (most recent call last):

  File 'ex.py', line 27, in \<module\>

    print '\n Unicode Parsing \n ',main.parseString(prs)

  File '/var/lib/python-support/python2.6/pyparsing.py', line 1049, in parseString

    loc, tokens = self._parse( instring, 0 )

  File '/var/lib/python-support/python2.6/pyparsing.py', line 925, in _parseNoCache

    loc,tokens = self.parseImpl( instring, preloc, doActions )

  File '/var/lib/python-support/python2.6/pyparsing.py', line 2308, in parseImpl

    loc, exprtokens = e._parse( instring, loc, doActions )

  File '/var/lib/python-support/python2.6/pyparsing.py', line 929, in _parseNoCache

    loc,tokens = self.parseImpl( instring, preloc, doActions )

  File '/var/lib/python-support/python2.6/pyparsing.py', line 2560, in parseImpl

    return self.expr._parse( instring, loc, doActions, callPreParse=False )

  File '/var/lib/python-support/python2.6/pyparsing.py', line 929, in _parseNoCache

    loc,tokens = self.parseImpl( instring, preloc, doActions )

  File '/var/lib/python-support/python2.6/pyparsing.py', line 1473, in parseImpl

    raise exc

pyparsing.ParseException: Expected '}' (at char 5), (line:1, col:6)
#### 2009-04-23 08:17:30 - ptmcg
Ok, good that you learned about adding the encoding at the top of your Python script - that is a Python issue, not pyparsing.



Look closely at the content of t: 'Rosire'

Now look at the definition of wd: wd = Word( alphanums )

Now look at the input string you created: '{Rosire,asb}'

Now look at the exception message from pyparsing:

pyparsing.ParseException: Expected '}' (at char 5), (line:1, col:6)



Starting with '{' in column 1, what character is in column 6?  Is this character in the set of allowed characters for wd?  No, it isn't.  You'd get the same result if you tried to parse '{Rosi_re,asb}'.



In addition to alphanums, pyparsing also includes a string for 8-bit characters that include common European characters with accents, umlauts, etc. - it is alphas8bit.  Try changing the definition of wd to 'Word( alphanums + alphas8bit )'.



Are you using Python3?  If not, then this string '{Rosire,asb}' is NOT a unicode string.  Prior to Python3, a unicode string is indicated with a leading 'u' character, as in u'{Rosire,asb}'.  Pyparsing can parse unicode strings, but you must construct the expressions with unicode as well: wd = Word(unicode(alphanums+alphas8bit)).  In Python3, all strings are unicode, and the leading 'u' is no longer necessary.



-- Paul
#### 2009-04-27 02:03:22 - asb_india
I tried in the way u said. But  the error still continues to be the same. :(
#### 2009-04-27 04:43:29 - ptmcg
I just ran your code using 'wd = Word( alphanums+alphas8bit )' and it worked for me, no errors.
#### 2009-04-27 21:35:32 - asb_india
hmm, 



I am putting my code here again. I get the same error.







    
     # -*- coding:iso-8859-1-*-
    import pyparsing 
    from pyparsing import *
    
    wd = Word(unicode( alphanums + alphas8bit))
    LPAR, RPAR, LCPAR, RCPAR = map( Suppress, '(){}' )
    
    main = LCPAR + delimitedList(wd) + RCPAR
    
    print '\n Unicode Parsing \n ',main.parseString(u'{Rosire,asb}')
    



Still the  error remains the same 



pyparsing.ParseException: Expected '}' (at char 6), (line:1, col:7)
#### 2009-04-27 22:00:10 - ptmcg
I just ran your exact sample code, and got this output:



    
     Unicode Parsing 
      [u'Rosi\xe8re', u'asb']



What version of Python and pyparsing are you using?



-- Paul
#### 2009-04-27 23:03:59 - asb_india


I am using pyparsing '1.5.0' and python 2.5 / 2.6
#### 2009-04-28 00:02:01 - ptmcg
I am running Python 2.5.4, and have tested your posted code with pyparsing 1.5.0, 1.5.1, and 1.5.2, and selected versions going back to 1.4.8, all with no problems.
#### 2009-04-28 03:01:00 - asb_india


Ok i am done with that. It was my mistake i specified wrong encoding at top. 



Thanks for the help. (bow). Thanks once again
#### 2009-02-26 21:59:57 - asb_india
Thanks a lot 



It helped me a lot. It works fine. I tried doing ib bit different assuming the cross is keyword and should match the cross. So i bit modified it like 





    
    wd = Word(alphas)
    LPAR,RPAR = map(Suppress,'()')
    cross = Keyword('cross', caseless=True)
    wd = Word(alphanums+'_'+' ')
    
    item \<\< ( Group(cross + LPAR + delimitedList(item) + RPAR) | wd )
    



I think the syntax is correct.



Now what if i have something like this 



'cross(cross(cross(object1,object2),object2),object2)'



So i start making like this 





    
    wd_obj1 = Keyword('object1', caseless=True)
    wd_obj2 = Keyword('object2', caseless=True)
    
    LPAR,RPAR = map(Suppress,'()')
    cross = Keyword('cross', caseless=True)
    comma = Suppress(',')
    
    item=Forward()
    l = Group(item) | wd_obj1 + comma + wd_obj2
    item \<\< ((cross + LPAR + delimitedList(l) + RPAR) | wd_obj2 )
    



I hope this is the correct way to do do this.



Thanks in the advance.



Regards 



ASB
#### 2009-02-28 12:00:47 - ptmcg
You are on the right track, but you need to let the parser do more of the work.  delimitedList already reads in items separated by commas, so you don't need to explicitly test for 'object1, object2' as another parseable expression.



I changed this code:



    l = Group(item) | wd_obj1 + comma + wd_obj2
    item \<\< ((cross + LPAR + delimitedList(l) + RPAR) | wd_obj2 )



To this:



    l = wd_obj1 | wd_obj2
    item \<\< (Group(cross + Group(LPAR + delimitedList(l | item) + RPAR)) | l )



And this will nest the arguments to each 'cross' function call.  If you need to define more types of elements that could be in the list of arguments, add them to 'l'.  For instance, to add support for integer arguments, expand 'l' as:



    integer = Word(nums)
    l = wd_obj1 | wd_obj2 | integer



and leave everything else alone.



-- Paul
#### 2009-03-01 21:19:19 - asb_india
Thanks once again. This is a great help 



Now i have made slight changes in the input form 



cross(cross(cross({abc,def,ghi},{xyz}),{pqr}),{lmn})



In this 

abc def ghi  are of type object1



xyz pqr lmn are of type object2



So now i change the code like 





    
    class object1():
     def __init__(self,o):
       self.name = o
    
     def __repr__(self):
       return 'TYPE OBJECT1' 
    
    
    class object2():
     def __init__(self,o):
       self.name = o
    
     def __repr__(self):
       return 'TYPE OBJECT2'
    
    LCPAR,RCPAR = map(Suppress,'{}')
    wd = Word(alphanums)
    LPAR,RPAR = map(Suppress,'()')
    cross = Keyword('cross', caseless=True)
    comma = Suppress(',')
    
    
    obj1 = delimitedList(wd)
    obj1.setParseAction(lambda s,a,toks:object1(toks))
    wd_obj1 = LCPAR + obj1 + RCPAR
    
    obj2 = wd
    obj2.setParseAction(lambda s,a,toks:object2(toks))
    wd_obj2 = LCPAR + obj2 + RCPAR
    
    
    
    item=Forward()
    l = Group(item) | wd_obj1 + comma + wd_obj2
    item \<\< ((cross + LPAR + delimitedList(l) + RPAR) | wd_obj2 )
    



I hope this is the correct syntax.



I tried in this way 





    
    l = wd_obj1 | wd_obj2
    item \<\< (Group(cross + Group(LPAR + delimitedList(l | item) + RPAR)) | l )



But the output is not correct.



Can u pls guid again to me.
#### 2009-03-01 21:55:46 - ptmcg
Ok, at this point, you need to just *stop*.



What exactly are you trying to represent?  I thought you were trying to parse some existing format, but now it seems that you are designing this input form for yourself.  But this design is just growing and growing, and I see no end in sight.



Why would you differentiate '{abc}' from '{abc,def}' with two different parsing expressions?  Are these really different objects, or just lists that happen to have 1 element vs. lists with 2 or more elements.  Both are a delimited list of words within braces, it just happens that one of them only has one element in the list.  You may not be familiar with this aspect of delimitedList, but it is in fact how it works.



The key to this whole list idea is to capture what the different kinds of things are that can go into the list - and by different, I mean <em>different</em>, not just a list of one vs. a list of 2 or more.  Different, like an integer, vs. a word, vs. a cross function call.



I think you have experimented and prototyped to get an idea of what is going on, but the next step is to <em>set everything you have done so far aside.</em>  I know this may be difficult, but chalk it up to some dabbling, some testing, some experimenting, but it is not development, and it certainly is not design.



Now.  What is the application you are designing?  What are the elements to be parsed?  What is 'cross' and what is getting crossed?  Will there be other verbs, too, like 'transpose' or 'invert' or 'dot'?  What will the arguments to these commands look like?  Are they vectors?  At this point, object_1 and object_2 just aren't helpful.  Write out the syntax - use BNF, EBNF, simplified BNF, whatever, but write out a plan for what you are going to parse.  Then test out this BNF with some sample text.  You will see if you can represent all of your application concepts appropriately, or if they need to be expanded to support lists of items enclosed in {}'s.



I will be happy to help you as much as I can, but randomly stabbing at your problem is frustrating and inefficient.



-- Paul
#### 2009-03-01 22:15:54 - asb_india
Yes may be you are correct. Actually Cross is the key word. 



We can think it like in terms of set theory 



The general syntax is 



cross({x},{y}) 



This means cross of x and y



now x and y can be list of elements 

x = [1,2,3] y =[4,5]



but it can be extened like 



cross(cross({x},{y}),{z}) to n times





The other possiblitiy is 



cross({a,b},{y})



a = [1,2,3]

b = [4,5,6]

y = [4,5]



Now in this case we will merge a and b to remove duplication 

or any other operator may a + b or like this for which we will get input separately.



so finally making the list in to one (a and b merged say qq) 

again cross of qq and y

Lets say it is zz cross of qq and y 



So this is the basic of what m trying to do.



Now if we want a cross of cross i.e the zz need to be crossed again with tt we will have the synatax like this. 





cross(cross({a,b},{y}),{tt})



This go on. 



I hope this will help wht i am up to. And why i am trying to put in the form of object So that i can use it in my way.



Thanks for your time :)
#### 2009-03-01 23:33:56 - ptmcg
Ah!  Much more helpful.  So the cross function always takes 2 arguments, and the arguments may be:

- a word in braces (representing a set)

- multiple words in braces separated by commas

- a nested call to cross



As a BNF, this becomes (this is not rigorous BNF syntax, but it should get the gist of the meaning):



    cross_call :== 'cross' '(' arg ',' arg ')'
    arg := set_reference  | cross_call
    set_reference := '{' word [',' set_reference]... '}'



To implement this in pyparsing, we need to work from the bottom up:



    LPAR,RPAR,LBRACE,RBRACE,COMMA = map(Suppress,'(){},')
    set_ref = Forward()
    set_ref \<\< (LBRACE + Group(delimitedList(Word(alphas) | set_ref)) + RBRACE)
    arg = Forward()
    cross_call = Keyword('cross') + Group(LPAR + arg + COMMA + arg + RPAR)
    arg \<\< (cross_call | set_ref)
    
    print cross_call.parseString('cross(cross({a,b},{y}),{tt})').asList()



Now if you later want to add support for set literals '[1,2,3]', we would first spell out the changes to the BNF:

integer := digit...

set_literal := '[' [ integer [ ',' integer ]... ] ']'



then add these expressions:



    LBRACK,RBRACK = map(Suppress,'[]')
    integer = Word(nums)
    set_literal = Group(LBRACK + Optional(delimitedList(integer)) + RBRACK)



and change arg to:



    arg \<\< (cross_call | set_ref | set_literal)



If you need to extend this further, <em>first</em> see how you would update the BNF, before just making changes to the Python code.



-- Paul
#### 2009-03-01 23:38:11 - ptmcg
Now if you *really* want to get crazy, change 'cross' to a real binary operator, say 'X', and your set union operator to something like 'U', and now instead of writing 



    cross(cross({a,b},{y}),{tt})



You could write



    ((a U b) X y) X tt



Pyparsing has special support for defining infix expressions with precedence of operations, using the operatorPrecedence method:



    set_ref = Word(alphas)
    expr = operatorPrecedence(set_ref | set_literal,
        [
        ('U', 2, opAssoc.LEFT, union_parse_action),
        ('X', 2, opAssoc.LEFT, cross_parse_action),
        ])
    
    print expr.parseString('((a U b) X y) X tt')



You could even use the unicode characters &#215; and &#8746; in place of 'X' and 'U' if you like.



-- Paul



(Sorry if this is too radical a change to your program, and/or too confusing - but I wanted to give you a little more breadth of concept on what kinds of things you could do with pyparsing.)
#### 2009-03-03 02:59:23 - asb_india


I dont know how to say and what to say. But in simple thanks a lot. I am higly obliged. It help me like anything. I started with the very dummy example on this forum but seeing the reply (prompt) i feel very happy and i am still getting my self acquainted with the pyparsing.
#### 2009-03-09 04:35:30 - asb_india
Hello,



The BNF u suggested is ok. It is 



    cross_call :== 'cross' '(' arg ',' arg ')'
    arg := set_reference  | cross_call
    set_reference := '{' word [',' set_reference]... '}'

But what if i want to separate the items and cross in different object. 

So i make the BNF like





    cross_call :== 'cross' '(' arg ',' cross ')'
    arg := set_reference  | cross_call
    set_reference := '{' word [',' set_reference]... '}'



Will it be ok.
#### 2009-03-09 04:36:17 - asb_india
Sorry the BNF will be 





    cross_call :== 'cross' '(' arg ',' cross_element ')'
    arg := set_reference  | cross_call
    set_reference := '{' word [',' set_reference]... '}'


#### 2009-03-09 23:03:19 - ptmcg
What is a cross_element?  And why would you restrict what the second argument to the cross_call could be?  It seems to me that any of these would be valid:



    cross({e,f}, cross({a},{b,c,d}))
    cross({a,b},[10,11,12])
    cross([7,8,9], cross({a},{b,c,d}))
    cross([1,2,3],[4,5,6])

No?



-- Paul
#### 2009-03-10 02:26:51 - asb_india
Hello 



Its not the case. 



For my syntax the its the cross and the elements.



So in general the syntax can be





    cross({element list},{cross element})
    
    cross({a,b,c,d,e},{c1})

The c1 means only one element. It cannot be a list. This is what i required.



What i want to do this to make the objects Say for ex



Object_element will have a,b,c,d,e



Object_cross will have c1



Object_main will have object_element , object_cross



Like this it will go 



If their is more cross so the syntax will be 







    cross(cross({element list},{cross element}),{cross element})
    
    cross(cross({a,b,c,d,e},{c1}),{c2})



Thanks
#### 2009-02-26 09:05:25 - ptmcg
This is a recursive expression, and so you will end up needing to define one element as a Forward(), then define the component bits using references to that element, and then insert the compound definition back into element, which provides the recursion.



Clear?  I didn't think so!



Start by defining a Forward item that will be the 'atom' of your embedded lists:



item = Forward()



Now the comma-separated lists within parens can be either a single variable, or a variable followed by a list within parens:



wd = Word(alphas)

LPAR,RPAR = map(Suppress,'()')

item \<\< ( (wd + LPAR + delimitedList(item) + RPAR) | wd )



If s is your test string, then I can do this:



print item.parseString(s).asList()



and get:



['cross', 'cross', 'cross', 'a', 'b', 'c', 'd']



So now we've parsed your list, but we've lost all structure that you had in the input string.  We fix this by using Group:



item \<\< ( Group(wd + LPAR + delimitedList(item) + RPAR) | wd )



Now parsing your input string we get:



[['cross', ['cross', ['cross', 'a', 'b'], 'c'], 'd']]



How is that for a start?



-- Paul

---
## 2009-03-06 10:42:55 - djayc - Match a word in the "right most" position.
I'm trying to match something like this:



oneOrMore(Word(alphas)) + oneOf(markers)



Where markers is something like 'END QUIT FIN STOP'



The problem is, I want the words that are prior to the marker to be matched.



So, this works fine:



'THIS IS A SENTENCE STOP'



But this doesn't:



'DON'T STOP UNTIL I SAY SO END'



or slightly different:



'DON'T STOP UNTIL I SAY STOP'



In both of those cases I get 'DON'T' matched and it uses the first STOP as the second part of the match.



How can I get it to look for an ending keyword (it might not be at the end of the line either) while still allowing for the 'stop words' to be in the text prior?



I hope this is clear.. thanks!

#### 2009-03-06 22:32:04 - ptmcg
<ul class="quotelist"><li>So, this works fine:</li><li>'THIS IS A SENTENCE STOP'</li></ul>Really?  I get this error:



Expected Re:('END|QUIT|FIN|STOP') (at char 24), (line:2, col:1)



And 'DON'T STOP UNTIL I SAY STOP' fails because 'DON'T' only matches 'DON' as a Word(alphas), at which point ''' doesn't match anything.



Pyparsing does not do any kind of implicit lookahead or backtracking, the way regular expressions do.  Since 'STOP' matches a Word(alphas), then it gets consumed by the OneOrMore(Word(alphas)).  Then once the parser gets to the end of the input string, having run out of Word(alphas) to process, it then advances in the parser definition to parse one of the end markers - which are already processed in the OneOrMore, so the parser fails.



To get this to work, you have to make the markers not succeed as part of the OneOrMore.  The way to do this is to use the NotAny lookahead, which can be abbreviated using the '~' operator:





    markers = 'END QUIT FIN STOP'
    endMarker = oneOf(markers)
    parser = OneOrMore(~endMarker + Word(alphas)) + endMarker



Now we can successfully user parser to parse 'THIS IS A SENTENCE STOP' into ['THIS', 'IS', 'A', 'SENTENCE', 'STOP'].  Now if you want to parse 'DONT STOP UNTIL I SAY STOP', the easiest is to just wrap the OneOrMore inside another OneOrMore.





    parser = OneOrMore(OneOrMore(~endMarker + Word(alphas)) + endMarker)



which gives ['DONT', 'STOP', 'UNTIL', 'I', 'SAY', 'STOP'].  Even if the input text is 'DONT STOP UNTIL I SAY STOP BUDDY', we still get ['DONT', 'STOP', 'UNTIL', 'I', 'SAY', 'STOP'].



-- Paul

---
## 2009-03-12 01:01:15 - NewbeCaroline - Need to filter out lines based on expr and then calculate time difference
I have a file containing many lines.

I need to find the lines that contains 'ISP'

and then calculate the timedifference.



I think I am ok with time difference, but how can I use pyparser in an intelligent way to find the times that I need?



ex:

...

Mar 5 15:27:53: %EPSC-SESSION-6-INFO: 'EVNT_EPSC_SESSION_INFO' ScLogger: [UeDriver.cpp:106/handleEvent] ISP:synch_start:

...



Mar 5 15:31:38: %EPSC-SESSION-6-INFO: 'EVNT_EPSC_SESSION_INFO' ScLogger: [UeDriver.cpp:111/handleEvent] ISP:synch_finished:



I am under time pressure so please help.

#### 2009-03-13 09:42:39 - NewbeCaroline
Paul,

Thanks again for your support. 

It is sad that our old? installation of python did not support your very elegant first solution.

Now, here is Friday night in Sweden so I am drinking wine and watch an old film made the same year I was born. I will probably not be able try your new solution until Monday since I did not bring my computer.



Hope you will have a nice weekend too.

--C
#### 2009-03-16 01:53:34 - NewbeCaroline
Hi again Paul,

using your second suggestion above everything works fine.

But when adding another plausibel line in the message log

---

log_messages='''\

Mar 5 15:27:45: %PM-6-PROCDIE: epsc-sc is dying

Mar 5 15:27:53: %blah: 'blah blah' ScLogger: [blah/blah] ISP:synch_start:removing 0 UEs

...



Mar 5 15:31:38: %blah: 'blah blah' ScLogger: [blah/blah] ISP:synch_finished:

'''

---

I get this:

Traceback (most recent call last):

  File 'ThirdHelper.py', line 57, in ?

    for logentry in logEntryExpr.searchString(log_messages):

  File '/vobs/cgw_test/systemtest/autott/scripts/ROBUSTNESS/parsingExperiment/pyparsing-1.5.1/pyparsing.py', line 1133, in searchString

    return ParseResults([ t for t,s,e in self.scanString( instring, maxMatches ) ])

  File '/vobs/cgw_test/systemtest/autott/scripts/ROBUSTNESS/parsingExperiment/pyparsing-1.5.1/pyparsing.py', line 1095, in scanString

    nextLoc,tokens = parseFn( instring, preloc, callPreParse=False )

  File '/vobs/cgw_test/systemtest/autott/scripts/ROBUSTNESS/parsingExperiment/pyparsing-1.5.1/pyparsing.py', line 941, in _parseNoCache

    loc,tokens = self.parseImpl( instring, preloc, doActions )

  File '/vobs/cgw_test/systemtest/autott/scripts/ROBUSTNESS/parsingExperiment/pyparsing-1.5.1/pyparsing.py', line 2309, in parseImpl

    loc, resultlist = self.exprs[0]._parse( instring, loc, doActions, callPreParse=False )

  File '/vobs/cgw_test/systemtest/autott/scripts/ROBUSTNESS/parsingExperiment/pyparsing-1.5.1/pyparsing.py', line 967, in _parseNoCache

    tokens = fn( instring, tokensStart, retTokens )

  File '/vobs/cgw_test/systemtest/autott/scripts/ROBUSTNESS/parsingExperiment/pyparsing-1.5.1/pyparsing.py', line 815, in tmp

    return f(t)

  File 'ThirdHelper.py', line 42, in makeDatetime

    return datetime(2009,mon,day,hr,min,sec)

ValueError: month must be in 1..12

---

Thanks for your continous support --C
#### 2009-03-16 08:38:11 - ptmcg
Caroline -



Pyparsing supplies a decorator called traceParseAction that shows what arguments are being passed to a parse action.  This will help diagnose why makeDatetime is getting an invalid month number.  To use the decorator, insert this line just ahead of the definition of makeDatetime in your code:



@traceParseAction



-- Paul
#### 2009-03-16 08:47:47 - NewbeCaroline
Traceback (most recent call last):

  File 'ThirdHelper.py', line 40, in ?

    @traceParseAction

NameError: name 'traceParseAction' is not defined



--C
#### 2009-03-16 08:53:26 - ptmcg
Does ThirdHelper.py import pyparsing?  If you use 'from pyparsing import Word, nums, etc.', then be sure to add 'traceParseAction' to the list of imported names.  If you use 'from pyparsing import *' then I am mystified.



-- Paul
#### 2009-03-16 08:58:00 - NewbeCaroline
sorry

\>\>entering makeDatetime(line: 'Mar 5 15:27:45: %PM-6-PROCDIE: epsc-sc is dying', 0, ['Mar', '5', '15:27:45'])

\<\<leaving makeDatetime (ret: 2009-03-05 15:27:45)

\>\>entering makeDatetime(line: 'Mar 5 15:27:45: %PM-6-PROCDIE: epsc-sc is dying', 4, ['', '5', '15:27:45'])

\<\<leaving makeDatetime (exception: month must be in 1..12)

Traceback (most recent call last):

  File 'ThirdHelper.py', line 59, in ?

    for logentry in logEntryExpr.searchString(log_messages):

  File '/vobs/cgw_test/systemtest/autott/scripts/ROBUSTNESS/parsingExperiment/pyparsing-1.5.1/pyparsing.py', line 1133, in searchString

    return ParseResults([ t for t,s,e in self.scanString( instring, maxMatches ) ])

  File '/vobs/cgw_test/systemtest/autott/scripts/ROBUSTNESS/parsingExperiment/pyparsing-1.5.1/pyparsing.py', line 1095, in scanString

    nextLoc,tokens = parseFn( instring, preloc, callPreParse=False )

  File '/vobs/cgw_test/systemtest/autott/scripts/ROBUSTNESS/parsingExperiment/pyparsing-1.5.1/pyparsing.py', line 941, in _parseNoCache

    loc,tokens = self.parseImpl( instring, preloc, doActions )

  File '/vobs/cgw_test/systemtest/autott/scripts/ROBUSTNESS/parsingExperiment/pyparsing-1.5.1/pyparsing.py', line 2309, in parseImpl

    loc, resultlist = self.exprs[0]._parse( instring, loc, doActions, callPreParse=False )

  File '/vobs/cgw_test/systemtest/autott/scripts/ROBUSTNESS/parsingExperiment/pyparsing-1.5.1/pyparsing.py', line 967, in _parseNoCache

    tokens = fn( instring, tokensStart, retTokens )

  File '/vobs/cgw_test/systemtest/autott/scripts/ROBUSTNESS/parsingExperiment/pyparsing-1.5.1/pyparsing.py', line 3052, in z

    ret = f(*paArgs)

  File '/vobs/cgw_test/systemtest/autott/scripts/ROBUSTNESS/parsingExperiment/pyparsing-1.5.1/pyparsing.py', line 815, in tmp

    return f(t)

  File 'ThirdHelper.py', line 44, in makeDatetime

    return datetime(2009,mon,day,hr,min,sec)

ValueError: month must be in 1..12
#### 2009-03-16 09:15:46 - ptmcg
I am late for work, so I must break off for now.  But see if you can figure out why makeDatetime is being called with these arguments:



\>\>entering makeDatetime(line: 'Mar 5 15:27:45: %PM-6-PROCDIE: epsc-sc is dying', 4, ['', '5', '15:27:45'])



It looks like the leading 'Mar' is getting eaten while parsing the previous line.



-- Paul
#### 2009-03-17 04:17:49 - NewbeCaroline
It seems as if a line does not comply with the last row in logEntryExpr 



-\>    'ISP:' + oneOf('synch_start synch_finished')



It will be entered twice in the makeDatetime. The the second time 'Mar' will not be read.



Why I can not figure out.

--c
#### 2009-03-17 07:33:57 - ptmcg
Caroline -



I suspect the culprit is in the line just before the one you are staring at.  If you could post a copy of this log file to the pastebin at , it might be clearer what is going on.



(I understand that your team has already come up with an alternative solution to getting this data, but I would be interested to see what the problem was, to ensure it was not a bug in pyparsing.)



-- Paul
#### 2009-03-17 23:35:44 - NewbeCaroline
Paul,

I have posted a full log at the paste bin.

--Caroline
#### 2009-03-18 00:22:28 - ptmcg
Well, my first mistake comes from assuming that calendar.month_abbr returns a list of month abbreviations.  It *does* do this, but the first abbreviation is '', so that enumerating over this list nicely maps 'Jan' to 1, 'Feb' to 2, etc.  Unfortunately, I didn't take this into account, so I erroneously accept '' as a valid month abbreviation.



Here are the changes needed to get this to work:



    monthAbbr = oneOf( list(calendar.month_abbr)[1:] )
    
    mname2mon = dict((m,i) for i,m in enumerate(calendar.month_abbr) if m)



With these changes, this successfully parses the log that you posted.



-- Paul

(I'll also update my 'Helpful Expressions' page.)
#### 2009-03-19 18:57:00 - ptmcg
Interested readers/lurkers can find the complete solution at .



The problem was caused by calendar.month_abbr including a 0'th element '' (so that 'Jan' maps to 1 instead of 0).  The code mods address this (and I also cleaned up the example on the 'Helpful Expressions' page).



-- Paul
#### 2009-03-12 08:03:15 - ptmcg
This should get you going.  Look over the comments for any internal explanations, but I hope this is pretty self-explanatory.  Write back if you have more questions.



-- Paul





    #log_messages = file('logfile.log').read()
    log_messages='''\
    Mar 5 15:27:53: %blah: 'blah blah' ScLogger: [blah/blah] ISP:synch_start:
    ...
    
    Mar 5 15:31:38: %blah: 'blah blah' ScLogger: [blah/blah] ISP:synch_finished:
    '''
    
    import calendar
    from datetime import datetime
    from pyparsing import Suppress, Word, nums, oneOf, Combine, removeQuotes, \
        alphanums, QuotedString, quotedString
    
    COLON = Suppress(':')
    integer = Word(nums)
    
    # from the Pyparsing wiki 'Helpful Expressions' page
    monthAbbr = oneOf( list(calendar.month_abbr) )
    
    # create timestamp expression that will parse to a datetime (for 
    # easy deltatime calculation)
    dateExpr = monthAbbr + integer
    timeExpr = Combine(integer + ':' + integer + ':' + integer)
    datetimeExpr = dateExpr + timeExpr
    datetimeExpr.setParseAction(
        lambda tokens: datetime.strptime(' '.join(tokens),'%b %d %H:%M:%S'))
    
    # attach results names to any of the other fields that are of interest
    # then you can add them to the synch results printed out
    logEntryExpr = datetimeExpr('timestamp') + COLON + \
        '%' + Word(alphanums+'-') + COLON + \
        quotedString.setParseAction(removeQuotes) + 'ScLogger' + COLON + \
        QuotedString('[', endQuoteChar=']') + \
        'ISP:' + oneOf('synch_start synch_finished')('synch')
    
    # use logEntryExpr to extract all interesting lines, then match up
    # synch_start and synch_finished entries
    synchStartTime = None
    for logentry in logEntryExpr.searchString(log_messages):
        if logentry.synch == 'synch_start':
            synchStartTime = logentry.timestamp
        else: 
            # must be a synch_finished
            if synchStartTime is not None:
                elapsed = logentry.timestamp-synchStartTime
                print elapsed
                synchStartTime = None
            else:
                print 'found synch_finished with no synch_start'



prints:



    0:03:45


#### 2009-03-12 09:18:47 - NewbeCaroline
Thanks a million Paul!

this seems very promising. 

I will check how it works tomorrow morning since I did not bring my computer home.

I am truly  grateful for your help. Now I can go to sleep without bad dreams :-)
#### 2009-03-13 00:29:43 - NewbeCaroline
Is there something wrong with my installation of the module?

I was trying to run your example here called FirstHelper.py but no luck. Have you any further advice please? --Caroline



[caroline@etevnc01 withAlittelHelp]$ python FirstHelper.py

Traceback (most recent call last):

  File 'FirstHelper.py', line 39, in ?

    for logentry in logEntryExpr.searchString(log_messages):

  File '/vobs/cgw_test/systemtest/autott/scripts/ROBUSTNESS/parsingExperiment/pyparsing-1.5.1/pyparsing.py', line 1133, in searchString

    return ParseResults([ t for t,s,e in self.scanString( instring, maxMatches ) ])

  File '/vobs/cgw_test/systemtest/autott/scripts/ROBUSTNESS/parsingExperiment/pyparsing-1.5.1/pyparsing.py', line 1095, in scanString

    nextLoc,tokens = parseFn( instring, preloc, callPreParse=False )

  File '/vobs/cgw_test/systemtest/autott/scripts/ROBUSTNESS/parsingExperiment/pyparsing-1.5.1/pyparsing.py', line 941, in _parseNoCache

    loc,tokens = self.parseImpl( instring, preloc, doActions )

  File '/vobs/cgw_test/systemtest/autott/scripts/ROBUSTNESS/parsingExperiment/pyparsing-1.5.1/pyparsing.py', line 2309, in parseImpl

    loc, resultlist = self.exprs[0]._parse( instring, loc, doActions, callPreParse=False )

  File '/vobs/cgw_test/systemtest/autott/scripts/ROBUSTNESS/parsingExperiment/pyparsing-1.5.1/pyparsing.py', line 967, in _parseNoCache

    tokens = fn( instring, tokensStart, retTokens )

  File '/vobs/cgw_test/systemtest/autott/scripts/ROBUSTNESS/parsingExperiment/pyparsing-1.5.1/pyparsing.py', line 815, in tmp

    return f(t)

  File 'FirstHelper.py', line 26, in \<lambda\>

    lambda tokens: datetime.strptime(' '.join(tokens),'%b %d %H:%M:%S'))

AttributeError: type object 'datetime.datetime' has no attribute 'strptime'
#### 2009-03-13 01:34:21 - ptmcg
datetime.datetime most assuredly <em>does</em> have an attribute strptime, at least if you are running any recent version of Python.



- What version of Python are you using?

- Do you have a file named datetime.py in the local directory, or somewhere on your PYTHONPATH?  If so, rename this file, as you are importing <em>it</em> instead of the file from the standard lib.  (You can use the inspect module to help see if there is an overriding datetime.py:



    \>\>\> import datetime
    \>\>\> import inspect
    \>\>\> print inspect.getsourcefile(datetime)
    Traceback (most recent call last):
      File '\<stdin\>', line 1, in \<module\>
      File 'C:\Python25\lib\inspect.py', line 383, in getsourcefile
        filename = getfile(object)
      File 'C:\Python25\lib\inspect.py', line 347, in getfile
        raise TypeError('arg is a built-in module')
    TypeError: arg is a built-in module

If you have a local datetime.py, you wont get the exception shown, but instead you will see the name of the file - just rename this so that it does not conflict with the built-in module.



-- Paul
#### 2009-03-13 05:46:14 - NewbeCaroline
Paul,

thanks for this quick answer. I thought you where sleeping during night :-)

I use Python 2.4.4 (#1, Sep  9 2008, 18:13:42)



<ul class="quotelist"><ul class="quotelist"><ul class="quotelist"><li>import datetime</li><li>import inspect</li><li>print inspect.getsourcefile(datetime)</li></ul></ul></ul>None



unfortunately there does not seem to be any file named datetime.py!?

--Caroline
#### 2009-03-13 06:54:31 - ptmcg
datetime.strptime was added in Python 2.5.



Replace the setParseAction line with this:



    def makeDatetime(tokens):
        mname,day = tokens[:2]
        hr,min,sec = tokens[2].split(':')
        mon = dict(zip(calendar.month_abbr,range(12+1)))[mname]
        day,hr,min,sec = map(int,(day,hr,min,sec))
        return datetime(2009,mon,day,hr,min,sec)
    datetimeExpr.setParseAction(makeDatetime)



I'm never fond of doing this kind of list slicing, I much prefer to use results names.  Here is how things would change:



    dateExpr = monthAbbr('mname') + integer('day')
    timeExpr = Combine(integer('hr') + ':' + integer('min') + ':' + integer('sec'))
    datetimeExpr = dateExpr + timeExpr
    def makeDatetime(tokens):
        mon = dict(zip(calendar.month_abbr,range(12+1)))[tokens.mname]
        day,hr,min,sec = map(int,(tokens.day, tokens.hr, tokens.min, tokens.sec))
        return datetime(2009,mon,day,hr,min,sec)
    datetimeExpr.setParseAction(makeDatetime)



Of course, this could be further simplified by attaching parse actions to integer and monthAbbr:



    integer.setParseAction(lambda toks: int(toks[0]))
    
    mname2mon = dict(zip(calendar.month_abbr,range(12+1)))
    monthAbbr.setParseAction(lambda toks: mname2mon[toks[0]])
    
    dateExpr = monthAbbr('mname') + integer('day')
    timeExpr = Combine(integer('hr') + ':' + integer('min') + ':' + integer('sec'))
    datetimeExpr = dateExpr + timeExpr
    def makeDatetime(tokens):
        return datetime(2009, tokens.mname, tokens.day,tokens.hr,tokens.min,tokens.sec)
    datetimeExpr.setParseAction(makeDatetime)



-- Paul



(We consult for companies in Germany and Japan, so I am often up in the middle of the night, sort of a permanent jet lag.)

---
## 2009-03-12 17:08:32 - stanchan - Issue with parsing multiline statement
Can someone let me know if this is a bug or if I am creating the parser incorrectly.  I'm a newbie with pyparsing, so I think it's just me.



Here is the data:



    server servertest1 {
       destination 172.16.0.10:80
       tier protocol tcp
       profile app_server web_server database
       database testdb
       environment test_env
       puppet test_rule
    }



Here is the code:



    LCURLB,RCURLB,COLON,QUOTE = map(Suppress,'{}:'')
    ipaddress = Combine(Word(nums) + ('.' + Word(nums))*3)
    server_name = Word(alphanums+'_-').setResultsName('servername')
    server_ip = ipaddress.setResultsName('serverip')
    server_port = COLON + Word(alphanums+'_-').setResultsName('serverport')
    server_dest = Group(Literal('destination') + server_ip + server_port).setResultsName('serverdest')
    server_proto = Group(Literal('ip protocol') + Word(alphas).setResultsName('protoname')).setResultsName('serverproto')
    server_profilename = restOfLine.setResultsName('profilename')
    server_profile = Group(Literal('profile') + server_profilename).setResultsName('serverprofile')
    server_database = Group(Literal('database') + Word(alphanums+'_-').setResultsName('databasename')).setResultsName('serverdatabase')
    server_environment = Group(Literal('environment') + Word(alphanums+'_-').setResultsName('environmentname')).setResultsName('serverenvironment')
    server_puppetname = OneOrMore(Word(alphanums+'_-').setResultsName('puppetname'))
    server_puppet = Group(Literal('puppet') + server_puppetname).setResultsName('puppet')
    server_options = server_proto | server_profile | server_persist | server_pool | server_puppet
    server_content = server_dest + OneOrMore(server_options)
    server_stmt = Group(Literal('server') + server_name + LCURLB + server_content + RCURLB)



This works, but when I try to change this line:





    server_profilename = restOfLine.setResultsName('profilename')



To this:



    server_profilename = Group(OneOrMore(Word(alphanums+'_-').setResultsName('profilename')))



It consumes everything after profile.  Any ideas?

#### 2009-03-12 18:28:21 - ptmcg
Your source data and parser don't work for me.  The parser refers to server_persist and server_pool, but they aren't defined.  If I remove them, I then get a parsing exception, because there is no server option that matches 'tier protocol tcp'.  I tried a few more changes to the parser and the input data, but I finally gave up.



But in general, the issue you are grappling with is that you see the line breaks, but the parser does not.  When you think 'OneOrMore(Word(alphanums+'_-'))', you assume this will stop at the end of the line - it doesn't.  Just as pyparsing skips over whitespace so that you don't need to write something like



    server_dest = 'destination' + whitespace + server_ip + server_port



it also treats those newlines as whitespace.  So once you match a keyword and then specify the remaining part of the option as 'OneOrMore(Word(alphanums+'_-'))', it is likely to keep reading the rest of the input data, because all you have are Word(alphanums+'_-').  When you use restOfLine, this <em>stops</em> at the line break, so that the next option keyword can be matched.



You can change this behavior by calling setDefaultWhitespaceChars at the top of your program, right after importing pyparsing:



    from pyparsing import *
    ParserElement.setDefaultWhitespaceChars(' \t')
    NL = LineEnd()



Now newlines will <em>not</em> be skipped over automatically.  Of course, now you will have to be explicit about where they are allowed - which is why I also defined the NL expression for matching newlines.  So your parser will start to look like this:



    server_content = server_dest + NL + OneOrMore(server_options + NL)



Some other notes about your parser:

- setResultsName is not my favorite method, and I think your parser would be a little more readable if instead of 'expr.setResultsName('name')' you use the alternate form of just 'expr('name')':



    server_database = Group(Literal('database') + Word(alphanums+'_-').setResultsName('databasename')).setResultsName('serverdatabase')

changes to:



    server_database = Group(Literal('database') + Word(alphanums+'_-')('dbname'))('serverdatabase')

(plus I shortened 'databasename' to 'dbname' - you could even just call it 'name', since it will be scoped within the 'serverdatabase' results name.)



- This is more a matter of personal taste, but if a string is 'added' to an expression using '+', it will automatically convert to a Literal, so the previous line could further simplify to:



    server_database = Group('database' + Word(alphanums+'_-')('dbname'))('serverdatabase')



But I have had other pyparsing users say they dislike this short cut, so I don't typically push it as hard as the results name form.  Use whichever you are more comfortable with.



-- Paul
#### 2009-03-13 09:25:24 - stanchan
Thanks Paul,



It was the default behavior of pyparsing for newlines that was causing the problem.  After setting setDefaultWhitespaceChars(' \t') and explicitly stating each NL location, it starting working correctly.  Sorry about the original code not working correctly.  Should have double check it in pyparsing helper before posting.





    ParserElement.setDefaultWhitespaceChars(' \t')
    NL = LineEnd().suppress()
    LCURLB,RCURLB,COLON,QUOTE = map(Suppress,'{}:'')
    ipaddress = Combine(Word(nums) + ('.' + Word(nums))*3)
    server_name = Word(alphanums+'_-')('servername')
    server_ip = ipaddress('serverip')
    server_port = COLON + Word(alphanums+'_-')('serverport')
    server_dest = Group(Literal('destination') + server_ip + server_port)('serverdest')
    server_proto = Group(Literal('tier protocol') + Word(alphas).setResultsName('name'))('serverproto')
    server_profilename = Group(OneOrMore(Word(alphanums+'_-'))).setResultsName('name')
    server_profile = Group(Literal('profile') + server_profilename).setResultsName('serverprofile')
    server_database = Group(Literal('database') + Word(alphanums+'_-')('name'))('serverdatabase')
    server_environment = Group(Literal('environment') + Word(alphanums+'_-').setResultsName('name')).setResultsName('serverenvironment')
    server_puppetname = OneOrMore(Word(alphanums+'_-')('name'))
    server_puppet = Group(Literal('puppet') + server_puppetname)('puppet')
    server_options = server_proto | server_profile | server_database | server_environment | server_puppet
    server_content = server_dest + NL + OneOrMore(server_options + NL)
    server_stmt = Group(Literal('server') + server_name + LCURLB + NL + server_content + RCURLB)



---
## 2009-03-18 09:24:55 - gmonkey5 - Getting more specific error messages from the MatchFirst object
I am working on a grammar that acts like a simple scripting language.  It supports both conditionals and actions and numeric or string values.  I want to restrict the use of numeric only operators (ie \<, \>=) to numeric types within the grammar definition itself.  The code for the expression looks like:





    expression = (strField + strCmp + strValue | 
                  numField + numCmp + numValue)



The problem I am having is when a file is given that uses an invalid field name in an expression.  Since it matches equally far for both items in the MatchFirst object, the error it returns is 'Expected string field'.  What I would really like it to say is 'Expected string or numeric field'.  Is there an easy way to do this?

#### 2009-03-18 12:47:08 - ptmcg
No, there is really no easy way to do this. Here is a crude hack I worked up that extends pyparsing's FollowedBy class:



    class ValidateOpening(FollowedBy):
        def __init__(self, expr):
            super(ValidateOpening,self).__init__(expr)
        def parseImpl(*args):
            self = args[0]
            try:
                return super(ValidateOpening,self).parseImpl(*(args[1:]))
            except ParseBaseException,pbe:
                pbe.msg = 'Expected ' + str(self)
                raise
    
    upfront_filter = ValidateOpening(strField | numField)
    upfront_filer.setName('string or numeric field')
    expression = upfront_filter + (strField + strCmp + strValue | 
                  numField + numCmp + numValue)

This is necessary because most of the compositional classes in pyparsing do not raise their own exceptions, they just re-raise the ones from their component expressions.
#### 2009-03-18 14:30:09 - gmonkey5
Thanks for the response :)



Playing around a little more, I found another solution.  By creating a NoMatch token and putting it the beginning of the MatchFirst object, I can have a little more control over the error that is returned.  For example:





    expressionFailToken = NoMatch()
    expressionFailToken.setName('string or numeric field')
    expression = (expressionFailToken |
                  strField + strCmp + strValue | 
                  numField + numCmp + numValue)



If the the first token of all the options fail, it will default to the fail token.  If any other tokens match, it will take the best match and give a more specific error.  It still seems like a hack, but it does not involve customizing internal classes.
#### 2009-03-18 14:37:14 - ptmcg
Nice and neat - I completely forgot about NoMatch!



-- Paul

---
## 2009-03-21 07:50:56 - dbv - citation parser
Hi! I'm new to pyparsing but not Python.  I want to build a citation parser for legal documents.  Here is a sample text:



'''Indemnified Capital Investments, S.A. v. R.J. O'Brien & Assoc., Inc., 12 F.3d 1406, 1409 (7th Cir.1993). The New Jersey Superior Court's Appellate Division affirmed the dismissal of Dale's common-law claim, but otherwise reversed and remanded for further proceedings, 308 N. J. Super. 516, 70 A. 2d 270 (1998). See also Warth v. Seldin, 422 U.S. 490, 499 n. 10, 95 S.Ct. 2197, 2205 n. 10, 45 L.Ed.2d 343 (1975).

NFMA, NEPA, or MUSYA. Sierra Club v. Marita, 843 F.Supp. 1526 (E.D.Wis.1994) ('Nicolet ').'''



I want to pick up citations in the form:

\<volume\> \<abbreviation\> \<pages\>



The above example text would result in these citations:



12 F.3d 1406, 1409

\<volume\> = 12

\<abbreviation\> = F.3d

\<pages\> = 1406, 1409



308 N. J. Super. 516

\<volume\> = 308

\<abbreviation\> = N. J. Super.

\<pages\> = 516



70 A. 2d 270

\<volume\> = 70

\<abbreviation\> = A. 2d

\<pages\> = 270



422 U.S. 490, 499 n. 10

\<volume\> = 422

\<abbreviation\> = U.S.

\<pages\> = 490, 499 n. 10



95 S.Ct. 2197, 2205 n. 10

\<volume\> = 95

\<abbreviation\> = S.Ct.

\<pages\> = 2197, 2205 n. 10



45 L.Ed.2d 343

\<volume\> = 45

\<abbreviation\> = L.Ed.2d

\<pages\> = 343



843 F.Supp. 1526

\<volume\> = 843

\<abbreviation\> = F.Supp.

\<pages\> = 1526



Any ideas how I can make a start on this with pyparsing?  Thanks.



Dinesh

#### 2009-03-21 09:17:31 - ptmcg
Dinesh -



I saw your thread on this same question (don't remember whether it was comp.lang.python or the Python tutor list).  I think Kent Johnson took a valiant stab at it.



This is really a tricky problem, since it requires a number of lookaheads and negative lookaheads.  See the comments in the parser example below (which finds all of your stated test cases).



-- Paul





    from pyparsing import *
    
    raw = '''Indemnified Capital Investments, S.A. v. R.J. O'Brien & Assoc., Inc., 
    12 F.3d 1406, 1409 (7th Cir.1993). The New Jersey Superior Court's Appellate 
    Division affirmed the dismissal of Dale's common-law claim, but otherwise 
    reversed and remanded for further proceedings, 308 N. J. Super. 516, blah 70 A. 2d 270 (1998). 
    See also Warth v. Seldin, 422 U.S. 490, 499 n. 10, 95 S.Ct. 2197, 2205 n. 10, 45 L.Ed.2d 343 (1975).
    NFMA, NEPA, or MUSYA. Sierra Club v. Marita, 843 F.Supp. 1526 (E.D.Wis.1994) ('Nicolet ').'''
    
    # some basic things to match for
    integer = Word(nums)
    ordinal = Combine(integer + oneOf('d nd st rd th'))
    ndot = Literal('n.')
    
    # need to forward declare citation, since we will also refer to it
    # within its own definition (as negative lookahead when parsing
    # page numbers)
    citation = Forward()
    
    # source_abbrev is very tricky, have to incorporate negative lookaheads
    # to avoid reading non page numbers as page numbers
    source_abbrev = originalTextFor(
        OneOrMore(ordinal | 
                  (~integer + Word(alphanums+'.')))
        )
    
    # use the first expression for pages if you want to iterate over each 
    # separate page in the list
    pages = Group(delimitedList((Group(integer + ndot + integer) | ~citation + integer)))
    # use the second expression for pages if you just want the list of pages
    # as a string
    pages = originalTextFor(delimitedList((Group(integer + ndot + integer) | ~citation + integer)))
    
    citation \<\< integer('volume') + source_abbrev('abbrev') + pages('pages')
    
    for cit in citation.searchString(raw):
        print cit.dump()
        # can also reference individual fields as cit.volume, cit.pages, etc.
        print
    



Prints:



    ['12', 'F.3d', '1406, 1409']
    - abbrev: F.3d
    - pages: 1406, 1409
    - volume: 12
    
    ['308', 'N. J. Super.', '516']
    - abbrev: N. J. Super.
    - pages: 516
    - volume: 308
    
    ['70', 'A. 2d', '270 ']
    - abbrev: A. 2d
    - pages: 270 
    - volume: 70
    
    ['422', 'U.S.', '490, 499 n. 10']
    - abbrev: U.S.
    - pages: 490, 499 n. 10
    - volume: 422
    
    ['95', 'S.Ct.', '2197, 2205 n. 10']
    - abbrev: S.Ct.
    - pages: 2197, 2205 n. 10
    - volume: 95
    
    ['45', 'L.Ed.2d', '343 ']
    - abbrev: L.Ed.2d
    - pages: 343 
    - volume: 45
    
    ['843', 'F.Supp.', '1526 ']
    - abbrev: F.Supp.
    - pages: 1526 
    - volume: 843


#### 2009-03-21 11:45:37 - dbv
Hi Paul



Yes, Kent made a great attempt. It is a tricky problem and way out of my regex experience.



I ran your program through a much larger legal document with lots of different citation formats and the results are terrific.  There are just a few wrinkles remaining:



i) \<pages\> can be of the format '107, 123-124' or '609, 622, 626-627'



eg.  'The Court gives deference to the Boy Scouts' assertions regarding the nature of its expression, see, Democratic Party of United States v. Wisconsin ex rel. La Follette, 450 U.S. 107, 123-124. The Court then inquires whether Dale's presence as an assistant scoutmaster would significantly burden the expression of those viewpoints.  But that law does not 'impos[e] any serious burdens' on BSA's 'collective effort on behalf of [its] shared goals,' Roberts v. United States Jaycees, 468 U.S. 609, 622, 626-627 (1984), nor does it force BSA to communicate any message that it does not wish to endorse.'



ii) 'at'. Many citations are of the form:



160 N. J., at 608-609

734 A. 2d., at 1227

468 U.S., at 572



eg. 'Roberts, 468 U.S., at 623. See Hurley, 515 U.S., at 576-577. To warrant constitutional protection under the freedom of intimate association. A warrant 160 N. J., at 608-609, 734 A. 2d, at 1221 (quoting Duarte, supra, at 546).'



How can these cites be picked up?



iii) Period '.'  

The \<volume\> cannot contain a period '.' and an \<abbreviation\> cannot begin with a period (or any non-alpha char).  In legal documents, there is always one space (' ') and nothing else between the \<volume\> and \<abbreviation\>.



Your help is really appreciated!



Dinesh
#### 2009-03-21 13:46:21 - ptmcg
Dinesh -



I'm happy to be helpful, but I can't be your development support service.  Somewhere along the line, you will need to learn some Python and pyparsing, and make enhancements for yourself.



Expanding the page format from 'integer' to 'integer-integer' is just the kind of enhancement pyparsing was designed for.  See how I added the pageref expression, and it checks for both formats.



Abbrevs not starting with a digit or period is only marginally helpful.  I hope that my introduction of the ordinal expression will be sufficient for those abbrevs that include words like '2d'.  This *does* allow me to remove the negative lookahead for integers, though.



Optional 'at' (actually, it is optional(', at')) is another change that is perfect for pyparsing to accommodate.  Look at the inclusion of Optional(AT) in the defintion of a citation.



Here is the upgraded version of the parser.



    from pyparsing import *
    
    # some basic things to match for
    integer = Word(nums)
    ordinal = Combine(integer + oneOf('d nd st rd th'))
    ndot = Literal('n.')
    
    # need to forward declare citation, since we will also refer to it
    # within its own definition (as negative lookahead when parsing
    # page numbers)
    citation = Forward()
    
    source_abbrev = originalTextFor(
        OneOrMore(ordinal | Word(alphas, alphanums+'.'))
        )
    
    pageref = integer + Optional('-' + integer)
    pages = originalTextFor(delimitedList((Group(pageref + ndot + pageref) | ~citation + pageref)))
    
    AT = Suppress(Optional(',') + 'at')
    citation \<\< integer('volume') + source_abbrev('abbrev') + Optional(AT) + pages('pages')



Best of luck,

-- Paul
#### 2009-03-22 02:18:34 - dbv
Many thanks Paul.  I should be able to build on this further.  Cheers!



Dinesh

---
## 2009-03-23 07:53:32 - dbv - Special Characters
Hi! I want to use certain special chars (eg. '&', '') in srange when defining Word. I've tried both:



i) Word( srange('[A-Z]'), srange('[a-zA-Z0-9.&\xa7\]')



and



ii) Word( srange('[A-Z]'), srange('[a-zA-Z0-9.&]')



But, neither works.  Yet, these ranges work in Python using re.



Thanks!



Dinesh

#### 2009-03-23 08:51:59 - ptmcg
Dinesh -



I wont be able to get to your question for a day or two - could you peek into the srange method of pyparsing and see if anything obvious is causing this problem?



You could also just use simple string concatenation.  srange just returns a string, so you could try:



Word( srange('[A-Z]'), srange('[a-zA-Z0-9.]') + '&' )



or just:



Word( alphas.upper(), alphanums+'.&' )



-- Paul
#### 2009-03-25 03:56:55 - dbv
Paul



This is the only format that worked for special characters:



Word( alphas.upper(), alphanums+'.&' )



Cheers ...



Dinesh
#### 2009-03-25 06:53:51 - ptmcg
Ah, I see that srange returns a unicode string, whereas alphanums returns an ordinary string.  Try wrapping your original call to srange in str(), that is use str(srange('A-Z...'))



-- Paul
#### 2009-03-25 07:18:45 - dbv
Wrapping srange in str() didn't work either.  But, I'm happy using the alphanums version.



Dinesh

---
## 2009-03-25 10:03:23 - reeeh2000 - Recursive Grammer
I am working on parsing a grammar with a recursion that is basically like this:



A := OneOrMore(B)

B := C | D

C := name + 'as' + B | name

D := name + 'with' + B



name = Word(alphas)



I'm having problems with the recursion.   I tried using B = Forward() and B \<\< C | D,  but I think I'm doing something wrong, as I either enter an infinite loop or B only represents the first part of C.



I'd greatly appreciate some help with this.

#### 2009-03-25 11:28:55 - ptmcg
Sorry, this is an unfortunate side effect of my choice of '\<\<' as the alternate assignement operator for Forwards.  In your case, you must use 'B \<\< (C | D)'



Here are some test cases:



    tests = '''\
    bob
    joe with bob
    sam joe with bill with fred as bob'''.splitlines()
    
    for t in tests:
        print t
        print A.parseString(t)
        print



Assuming this is your grammar:



    name = Word(alphas, alphanums)
    B = Forward()
    C = name + 'as' + B | name
    D = name + 'with' + B
    B \<\< (C | D)
    A = OneOrMore(B)



You get this:



    bob
    ['bob']
    
    joe with bob
    ['joe', 'with', 'bob']
    
    sam joe with bill with fred as bob
    ['sam', 'joe', 'with', 'bill', 'with', 'fred', 'as', 'bob']



This grammar isn't doing much better for you than str.split().



If you are interested in keeping track of the nesting levels, you might consider adding Groups, something like:





    name = Word(alphas, alphanums)
    B = Forward()
    C = Group(name + 'as' + B) | name
    D = Group(name + 'with' + B)
    B \<\< (C | D)
    A = OneOrMore(B)



Hmmm, running this only gives:



    bob
    ['bob']
    
    joe with bob
    ['joe', 'with', 'bob']
    
    sam joe with bill with fred as bob
    ['sam', 'joe', 'with', 'bill', 'with', ['fred', 'as', 'bob']]



I expected the last test to result in:



    ['sam', ['joe', 'with', ['bill', 'with', ['fred', 'as', 'bob']]]]



There are a couple of interesting things going on, even in this simple little grammar.  I've made some changes on my system to get the nested result, but so I don't spoil your learning exercise, I'll just suggest that you add:



    name.setDebug()

to your program, and see what you get.  Eventually, with this string 'sam joe with jim as pete with fred as bob', you should get:



    ['sam', ['joe', 'with', ['jim', 'as', ['pete', 'with', ['fred', 'as', 'bob']]]]]



Write back and let me know how this exercise progresses.



-- Paul
#### 2009-03-25 12:42:48 - reeeh2000
thank you for your help.  I've got it working now.  Another thing to make sure to watch out for is the difference between ^ and |.  Now everything is working great.
#### 2009-03-25 13:18:46 - ptmcg
I'm not sure what the larger context is for this little grammar, but there were some other interesting things going on when I experimented.



- Accidentally parsing 'with' and 'as' as names.

- 'B \<\< (C|D)' is problematic with your current definitions, try 'B \<\< (D|C)' instead

- For a totally different approach, consider 'with' and 'as' as operators on an expression composed of names, and try various combinations of operatorPrecedence (oP does the recursive definitions for you, you just specify the format of the operand, and the precedence of the operations), such as one of the following:



    # equal precedence, pretty much what you have now
    B = operatorPrecedence(name,
        [
        (oneOf('as with'), 2, opAssoc.LEFT,),
        ]);
    
    # higher precedence to 'with'
    B = operatorPrecedence(name,
        [
        ('with', 2, opAssoc.LEFT,),
        ('as', 2, opAssoc.LEFT,),
        ]);
    
    # higher precedence to 'as'
    B = operatorPrecedence(name,
        [
        ('as', 2, opAssoc.LEFT,),
        ('with', 2, opAssoc.LEFT,),
        ]);

An advantage of operatorPrecedence is that it will also support grouping with ()'s to override the precedence of operations.



Try these different combinations out on the test string 'sam joe with jim as pete with fred as bob' and see the differences.



Just some other crazy ideas to think about...



-- Paul

---
## 2009-03-27 12:43:38 - pratdam - pyparsing  for  Windows INF  Files 
Hi, 

 I am trying to write code to convert  a few INF  Files  to  xml Files . I am using a tool  called USMT ( Microsoft User mig tool ) which used .inf files till version 2.6  and now they have migrated to xml . And unfortunately there is no tool  available . I think that this cannot be achieved by a regex and  I needed a generic parser generator .  So I am trying to use pyparsing  .  a part of the inf file  is below e.g 



[[  [ODBC Connections.Environment]%ODBCUserFileDSNDir% = Registry, HKR\Software\ODBC\ODBC.INI\ODBC File DSN [DefaultDSNDir]%ODBCSysFileDSNDir% = Registry, HKLM\Software\ODBC\ODBC.INI\ODBC File DSN [DefaultDSNDir][ODBC Connections.Instructions]ForceDestReg=ODBC Connections.ForceDestRegAddReg=ODBC Connections.AddRegDelFiles=ODBC Connections.DelFilesForceDestFile=ODBC Connections.ForceDestFileCopyFilesEx=ODBC Connections.CopyFilesEx[ODBC Connections.ForceDestReg]HKR\Software\ODBC\ODBC.INI\*HKLM\Software\ODBC\ODBC.INI\*[ODBC Connections.AddReg]HKR\Software\ODBC\ODBC.INI\*HKLM\Software\ODBC\ODBC.INI\*]]



I would like to know  if this is  a good fit to use PyParsing . A few tips would be  great 



Thanks

Pratik

#### 2009-03-27 12:47:24 - pratdam
The text lost all newlines so am resnding it 





    
    [ODBC Connections.Environment] 
    
    %ODBCUserFileDSNDir% = Registry, HKR\Software\ODBC\ODBC.INI\ODBC File DSN [DefaultDSNDir]
    
    %ODBCSysFileDSNDir% = Registry, HKLM\Software\ODBC\ODBC.INI\ODBC File DSN [DefaultDSNDir]
    
    [ODBC Connections.Instructions]
    
    ForceDestReg=ODBC Connections.ForceDestReg
    
    AddReg=ODBC Connections.AddReg
    
    DelFiles=ODBC Connections.DelFiles
    
    ForceDestFile=ODBC Connections.ForceDestFile
    
    CopyFilesEx=ODBC Connections.CopyFilesEx
    
    [ODBC Connections.ForceDestReg]
    
    HKR\Software\ODBC\ODBC.INI\*
    
    HKLM\Software\ODBC\ODBC.INI\*
    
    [ODBC Connections.AddReg]
    
    HKR\Software\ODBC\ODBC.INI\*
    
    HKLM\Software\ODBC\ODBC.INI\*
    


#### 2009-03-27 17:42:56 - ptmcg
There are already a couple of canned parsers that look like they might work.  ConfigParser is included with standard Python, and there is also ConfigObj.  The pyparsing examples page includes configParse.py: 



I'd give some of these a shot before starting from scratch.



But to answer your question, yes, in general, this is the kind of task that pyparsing is good for.



-- Paul
#### 2009-03-28 15:01:52 - pratdam
Hi paul , 

I was mentioning about INF Files which can have  a 'ConfigParser' section name  as a  value in the name=value pair .  So there could be  a few degrees of nesting like

[[code type='ini' ]]



[ a  ]



p =  q.c 

m =  n.p



[ n ]



a = ...



[ q ]

z 

c 





..





I had tried out with ConfigParser and ConfigObj(voidspace) but they dont seem to handle nesting . Am I missing something here .



Thanks

Pratik
#### 2009-03-28 20:44:17 - ptmcg
Well, this latest example is especially difficult, since you use values like 'q.c' before they have been defined in the later section '[q]'. I could see a potential way to adapt the pyparsing configParse.py example with the pyparsing macroExpander.py example, but this would still require using variables only after they have been defined.



Would this be a valid restriction on referencing values defined in other sections, that the other sections must come before the nested reference?



-- Paul
#### 2009-03-30 16:05:10 - pratdam
Hi Paul , 

  Ok I get it  . I think that they do forward references to sections/words that have not been   defined yet in the  same   file or perhaps in some other fine , The sad part of the story  is that this is mostly Windows DDK Code to represent data and every driver has been parsed by Custom code . Howver I Notice that the general trend is defining a  list of wpords that can appear and hence remove the forward reference issue since it is 'HARDCODED' List  . Here is some info in MSDN long and full of magic sequences .



 But for solving this issue I 'd  prob.  define a  SET OF  POSSIBlLE Tokens  . Can I read the grammer  from the xsd file and use it as a .def/grammer file in the parser .  



PS 

The  goal is to autogenerate xml ( in  USMT  3.0 )  for existing INF ( 2.6   usmt )  and people have to manually convert that !! 



Thanks again for letting me know the options of pyparser 









[[ code type='ini' ]]

INF Version Section



[Version]



Signature='signature-name'

[Class=class-name]

[ClassGuid={nnnnnnnn-nnnn-nnnn-nnnn-nnnnnnnnnnnn}]

[Provider=%INF-creator%]

[LayoutFile=filename.inf [,filename.inf]... ]

[CatalogFile=filename.cat]

[CatalogFile.nt=unique-filename.cat]

[CatalogFile.ntx86=unique-filename.cat]

[CatalogFile.ntia64=unique-filename.cat] (Windows XP and later versions of Windows)

[CatalogFile.ntamd64=unique-filename.cat] (Windows XP and later versions of Windows)

DriverVer=mm/dd/yyyy[,w.x.y.z]

[DontReflectOffline=1] (Windows Vista and later versions of Windows)

[PnpLockDown=0|1] (Windows Vista and later versions of Windows)

[DriverPackageDisplayName=%driver-package-description%]

[DriverPackageType=PackageType]



By convention, the Version section appears first in INF files. Every INF file must have this section.

Entries



Signature='signature-name'

    Must be $Windows NT$, $Windows 95$, or $Chicago$, indicating the operating systems for which this INF is valid. These signature values have the following meanings.

    Signature Value    Meaning

    $Windows NT$    NT-based operating systems

    $Windows 95$    Windows 9x/Me

    $Chicago$    All Windows operating systems



    The enclosing $'s are required but these strings are case-insensitive. If signature-name is none of these string values, the file is not accepted as a valid INF.



    Generally, Setup does not differentiate among these signature values. One of them must be specified, but it doesn't matter which one. You should specify the appropriate value so that someone reading an INF file can determine the operating systems for which it is intended. (For Windows Me installations, Setup generally chooses drivers with $Chicago$ or $Windows 95$ signatures, and only chooses drivers with $Windows NT$ signatures if no others are available.)



    Some class installers place additional requirements on how the signature value must be specified. Such requirements, if they exist, are discussed in device type-specific sections of this DDK.



    An INF must supply OS-specific installation information by appending system-defined extensions to its DDInstall sections, whether the signature-name is $Windows NT$, $Chicago$, or $Windows 95$. (See Creating INF Files for Multiple Platforms and Operating Systems for a discussion of these extensions.)



Class=class-name

    For any standard type of device, this specifies the class name, which is usually one of the system-defined class names like Net or Display as listed in devguid.h, for the type of device to be installed from this INF file. See System-Supplied Device Setup Classes.



    If an INF specifies a Class it should also specify the corresponding system-defined GUID value for its ClassGUID entry. Specifying the matching GUID value for a device of any predefined device setup class can install the device and its drivers faster since this helps the system setup code to optimize its INF searching.



    If an INF adds a new setup class of devices to the system, it should supply a unique, case-insensitive class-name value that is different from any of the system-supplied classes in devguid.h. The length of the class-name string must be 32 characters or less. The INF must specify a newly generated GUID value for the ClassGUID entry. Also see INF ClassInstall32 Section.



    This entry is irrelevant to an INF that installs neither a new device driver under a predefined device setup class nor a new device setup class.



ClassGuid = {nnnnnnnn-nnnn-nnnn-nnnn-nnnnnnnnnnnn}

    Specifies the device-class GUID, formatted as shown here, where each n is a hexadecimal digit.



    For a Windows 2000 and later (and Windows 98/Me) INF, such a GUID value determines the device setup class subkey in the registry ...\Class tree under which to write registry information for the drivers of devices installed from this INF file. This class-specific GUID value also identifies the device class installer for the type of device and class-specific property page provider, if any.



    For a new device setup class, the INF must specify a newly generated ClassGUID value. For more information about creating GUIDs, see Using GUIDs in Drivers. Also see Device Setup Classes.



Provider=%INF-creator%

    Identifies the provider of the INF file. Typically, this is specified as an %OrganizationName% token that is expanded later in the INF file's Strings section. The maximum length, in characters, of a provider name is LINE_LEN.



    For example, INF files supplied with the system typically specify the INF-creator as %Msft% and define %Msft% = 'Microsoft' in their Strings sections.



LayoutFile=filename.inf [,filename.inf]...

    Specifies one or more additional system-supplied INF files that contain layout information about the source media required for installing the software described in this INF. All system-supplied INF files specify this entry.



    INF files that are not distributed with the operating system should omit this entry, if possible. Instead, such INF files should use Include and Needs entries in DDInstall sections, or they should have SourceDisksNames and SourceDisksFiles sections, or both. INF files not distributed with the operating system should include a LayoutFile directive only if



        * they require OS-supplied files as part of their installation, and those files are not installable using OS-provided INF sections that can be referenced by Include and Needs entries, or

        * they are used to support Windows 9x/Me. (Windows 9x/Me does not automatically include the layout files referenced by INF files you specify in Include entries, so the including INF must reference the layout files itself with LayoutFile entries.)



CatalogFile=filename.cat

    Specifies a catalog (.cat) file to be included on the distribution media of a device/driver. Catalog files are supplied by the Microsoft Windows Hardware Quality Lab (WHQL), after WHQL has tested and assigned digital signatures to driver files. (Contact WHQL for more information about the testing and signing of IHV and/or OEM driver packages.)



    Catalog files are not listed in the SourceDisksFiles or CopyFiles sections of the INF. Setup assumes that the catalog file is in the same location as the INF file.



    System-supplied INF files never have CatalogFile= entries because the operating system validates the signature for such an INF against all system-supplied xxx.cat files.



[CatalogFile.nt=unique-filename.cat] |

[CatalogFile.ntx86=unique-filename.cat] |

[CatalogFile.ntia64=unique-filename.cat] |

[CatalogFile.ntamd64=unique-filename.cat]

    Specifies another INF-writer-determined, unique file name, with the .cat extension, of a catalog file that is specific to Windows 2000 or later.



    If these optional entries are omitted from a dual-operating system INF file, a given CatalogFile=filename.cat is used for validating WDM device/driver installations on all Windows 2000 and later and Windows 98/Me machines. If any decorated CatalogFile.xxx= entry exists in an INF's Version section together with an undecorated CatalogFile= entry, the undecorated entry is assumed to identify a filename.cat for validating device/driver installations only on Windows 98/Me machines.



    Note that any cross-platform and/or dual-operating system device/driver INF file that has CatalogFile= and CatalogFile.xxx= entries must supply a unique IHV/OEM-determined name for each such .cat file.



    For information about how to use the system-defined .nt, .ntx86, .ntia64, and .ntamd64 extensions, see Creating INF Files for Multiple Platforms and Operating Systems.



DriverVer=mm/dd/yyyy[,w.x.y.z]

    This entry specifies version information for drivers installed by this INF. This entry is required beginning with Windows 2000.

    For information about how to specify this entry, see INF DriverVer Directive.



DontReflectOffline=1

    This directive is for internal use only on Windows Vista and later versions of Windows. This directive must not be used for any reason in a third-party INF file. Note that this directive is present in some inbox driver INF files, and an INF file writer must be careful not to copy this directive along with other INF Version directives that the writer might copy from an inbox driver INF file.



[PnpLockDown=0|1]

    Specifies whether Plug and Play (PnP) prevents applications from directly modifying the files that a driver packages INF file specifies. If the PnpLockDown directive is set to 1, PnP prevents applications from directly modifying the files that are copied by INF CopyFiles directives. Otherwise, if the directive is not included in an INF file or the value of the directive is set to zero, an application with administrator privileges can directly modify these files. Driver files that are protected in this manner are referred to as third-party protected driver files.



    To ensure the integrity of a PnP driver installation, applications should not directly modify driver files that are copied by the driver package INF file. Applications should only use the device installation mechanisms provided by Windows to update PnP drivers. Beginning with Windows Vista, a driver package should set PnpLockDown to 1 to prevent an application from directly modifying driver files. Note, however, that some existing applications that uninstall driver packages do directly delete driver files. To maintain compatibility with these applications, the PnpLockDown directive for such driver package should be set to zero.



    Although PnP on Windows Vista and later versions of Windows does not require that an INF file include a PnpLockDown directive in order to install a driver, PnP in a future version of Windows might require that INF files for PnP driver packages include the PnpLockDown directive.



DriverPackageDisplayName=%driver-package-description%

    Specifies a string token that corresponds to a string key entry in an INF Strings section; the string key entry supplies the driver package display name. Driver Install Frameworks (DIFx) uses the driver package display name to describe the purpose of driver package to end users, as described in Specifying the Driver Package Display Name.



DriverPackageType=PackageType

    Specifies the driver package type. Driver Install Frameworks (DIFx) uses the driver package type to determine the type of driver package, as described in Specifying the Driver Package Type.



Comments



When a driver package passes Microsoft Windows Hardware Quality Lab (WHQL) testing, WHQL returns .cat catalog files to the IHV or OEM. Each .cat file contains a digitally encrypted signature for the driver package. The IHV or OEM must list these .cat files in the INF Version section and must supply the files on the distribution media, in the same location as the INF file. The .cat files must be uncompressed.



Note that if an INF Version section does not include at least one CatalogFile or CatalogFile.ntxxx entry, the driver is treated as unsigned, and dates listed in the DriverVer directive will not be displayed in the UI. For more information, see Driver Signing.

Example

The following example shows a Version section typical of a simple device-driver INF, followed by the required SourceDisksNames and SourceDisksFiles sections implied by the entries specified in this sample Version section:



[Version]

Signature='$Chicago$'

Class=SCSIAdapter

ClassGUID={4D36E97B-E325-11CE-BFC1-08002BE10318}

Provider=%INF_Provider%

CatalogFile=aha154_win98.cat

CatalogFile.ntx86=aha154_ntx86.cat

DriverVer=08/20/1999



[SourceDisksNames]

;

; diskid = description[, [tagfile] [, \<unused\>, subdir]]

;

1 = %Floppy_Description%,,,\Win98

2 = %Floppy_Description%,,,\WinNT



[SourceDisksFiles]

;

; filename_on_source = diskID[, [subdir][, size]]

;

aha154x.mpd = 1,,



[SourceDisksFiles.x86]

aha154x.sys = 2,\x86



; ...



[Strings]

INF_Provider='Adaptec'

Floppy_Description = 'Adaptec Drivers Disk'

; ...
#### 2009-03-30 20:44:37 - ptmcg
Pratdam -



See if this code gets you started.  (When posting, <em>please</em> use the [[code]] tags, each on a line by itself, per the wiki instructions.)





    sampleINF = '''
    [Version]
    Signature='$Chicago$'
    Class=SCSIAdapter
    ClassGUID={4D36E97B-E325-11CE-BFC1-08002BE10318}
    Provider=%INF_Provider%
    CatalogFile=aha154_win98.cat
    CatalogFile.ntx86=aha154_ntx86.cat
    DriverVer=08/20/1999
    
    [SourceDisksNames]
    ;
    ; diskid = description[, [tagfile] [, \<unused\>, subdir]]
    ;
    1 = %Floppy_Description%,,,\Win98
    2 = %Floppy_Description%,,,\WinNT
    
    [SourceDisksFiles]
    ;
    ; filename_on_source = diskID[, [subdir][, size]]
    ;
    aha154x.mpd = 1,,
    
    [SourceDisksFiles.x86]
    aha154x.sys = 2,\x86
    
    ; ...
    
    [Strings]
    INF_Provider='Adaptec'
    Floppy_Description = 'Adaptec Drivers Disk'
    '''
    
    from pyparsing import *
    
    LBRACK,RBRACK,SEMI,EQ,PCT = map(Suppress,'[];=%')
    comment = SEMI + restOfLine
    
    # pass 1 - scan first to find all substitution strings
    macro_ident = Word(alphas, alphanums+'_')
    macro_ref = Combine(PCT + macro_ident + PCT)
    macro_ref.ignore(comment)
    macro_refs = list(mr[0] for mr in macro_ref.searchString(sampleINF))
    
    # pass 2 - extract substitution definitions
    macro_defn = oneOf(macro_refs) + EQ + quotedString.setParseAction(removeQuotes)
    macro_defn.ignore(comment)
    substitutions = dict(macro_defn.searchString(sampleINF))
    
    # pass 3 - extract structure, use parse action to replace substitution 
    # strings with defined values
    section_head = LBRACK + CharsNotIn(']') + RBRACK
    section_head.setParseAction(lambda toks: toks[0].strip())
    inf_defn = Group(Word(alphas,alphanums+'_.')('id') + EQ + empty + restOfLine('defn'))
    section = Group(section_head('head') + OneOrMore(inf_defn)('body'))
    
    # print out parsed tokens using XML format
    asOpenTag = lambda t: '\<%s\>' % t
    asCloseTag = lambda t: '\</%s\>' % t
    for section_def in section.ignore(comment).searchString(sampleINF):
        section_def = section_def[0]
        print asOpenTag(section_def.head)
        for infdef in section_def.body:
            print '    ' + asOpenTag(infdef.id) + infdef.defn + asCloseTag(infdef.id)
        print asCloseTag(section_def.head)



Prints



    \<Version\>
        \<Signature\>'$Chicago$'\</Signature\>
        \<Class\>SCSIAdapter\</Class\>
        \<ClassGUID\>{4D36E97B-E325-11CE-BFC1-08002BE10318}\</ClassGUID\>
        \<Provider\>%INF_Provider%\</Provider\>
        \<CatalogFile\>aha154_win98.cat\</CatalogFile\>
        \<CatalogFile.ntx86\>aha154_ntx86.cat\</CatalogFile.ntx86\>
        \<DriverVer\>08/20/1999\</DriverVer\>
    \</Version\>
    \<SourceDisksFiles\>
        \<aha154x.mpd\>1,,\</aha154x.mpd\>
    \</SourceDisksFiles\>
    \<SourceDisksFiles.x86\>
        \<aha154x.sys\>2,\</aha154x.sys\>
    \</SourceDisksFiles.x86\>
    \<Strings\>
        \<INF_Provider\>'Adaptec'\</INF_Provider\>
        \<Floppy_Description\>'Adaptec Drivers Disk'\</Floppy_Description\>
    \</Strings\>


#### 2009-03-30 20:55:40 - ptmcg
Ooops, forgot to handle the substitutions:



    sampleINF = '''
    [Version]
    Signature='$Chicago$'
    Class=SCSIAdapter
    ClassGUID={4D36E97B-E325-11CE-BFC1-08002BE10318}
    Provider=%INF_Provider%
    CatalogFile=aha154_win98.cat
    CatalogFile.ntx86=aha154_ntx86.cat
    DriverVer=08/20/1999
    
    [SourceDisksNames]
    ;
    ; diskid = description[, [tagfile] [, \<unused\>, subdir]]
    ;
    1 = %Floppy_Description%,,,\Win98
    2 = %Floppy_Description%,,,\WinNT
    
    [SourceDisksFiles]
    ;
    ; filename_on_source = diskID[, [subdir][, size]]
    ;
    aha154x.mpd = 1,,
    
    [SourceDisksFiles.x86]
    aha154x.sys = 2,\x86
    
    ; ...
    
    [Strings]
    INF_Provider='Adaptec'
    Floppy_Description = 'Adaptec Drivers Disk'
    '''
    
    from pyparsing import *
    
    LBRACK,RBRACK,SEMI,EQ,PCT = map(Suppress,'[];=%')
    comment = SEMI + restOfLine
    
    # pass 1 - scan first to find all substitution strings
    macro_ident = Word(alphas, alphanums+'_')
    macro_ref = Combine(PCT + macro_ident + PCT)
    macro_ref.ignore(comment)
    macro_refs = list(mr[0] for mr in macro_ref.searchString(sampleINF))
    
    # pass 2 - extract substitution definitions
    macro_defn = oneOf(macro_refs) + EQ + quotedString.setParseAction(removeQuotes)
    macro_defn.ignore(comment)
    substitutions = dict((k,v) for k,v in macro_defn.searchString(sampleINF))
    
    # pass 3 - extract structure, use parse action to replace substitution 
    # strings with defined values
    section_head = LBRACK + CharsNotIn(']') + RBRACK
    section_head.setParseAction(lambda toks: toks[0].strip())
    inf_defn = Group(Word(alphas,alphanums+'_.')('id') + EQ + empty + (macro_ref | restOfLine)('defn'))
    macro_ref.setParseAction(lambda toks: substitutions[toks[0]])
    section = Group(section_head('head') + OneOrMore(inf_defn)('body'))
    
    # print out parsed tokens using XML format
    asOpenTag = lambda t: '\<%s\>' % t
    asCloseTag = lambda t: '\</%s\>' % t
    for section_def in section.ignore(comment).searchString(sampleINF):
        section_def = section_def[0]
        print asOpenTag(section_def.head)
        for infdef in section_def.body:
            print '    ' + asOpenTag(infdef.id) + infdef.defn + asCloseTag(infdef.id)
        print asCloseTag(section_def.head)
    



gives



    \<Version\>
        \<Signature\>'$Chicago$'\</Signature\>
        \<Class\>SCSIAdapter\</Class\>
        \<ClassGUID\>{4D36E97B-E325-11CE-BFC1-08002BE10318}\</ClassGUID\>
        \<Provider\>Adaptec\</Provider\>
        \<CatalogFile\>aha154_win98.cat\</CatalogFile\>
        \<CatalogFile.ntx86\>aha154_ntx86.cat\</CatalogFile.ntx86\>
        \<DriverVer\>08/20/1999\</DriverVer\>
    \</Version\>
    \<SourceDisksFiles\>
        \<aha154x.mpd\>1,,\</aha154x.mpd\>
    \</SourceDisksFiles\>
    \<SourceDisksFiles.x86\>
        \<aha154x.sys\>2,\</aha154x.sys\>
    \</SourceDisksFiles.x86\>
    \<Strings\>
        \<INF_Provider\>'Adaptec'\</INF_Provider\>
        \<Floppy_Description\>'Adaptec Drivers Disk'\</Floppy_Description\>
    \</Strings\>



---
## 2009-03-28 19:47:27 - reeeh2000 - Pyparsing for Python3
Me and my friend have gotten the pyparsing_py3.py file fully functional.  How can I upload this so that pyparsing can be used successfully with Python3?

#### 2009-03-28 20:39:09 - ptmcg
Easiest is to post it to pyparsing.pastebin.com, and post another note when it's ready.



Thanks for the effort!

-- Paul
#### 2009-03-29 01:55:53 - reeeh2000
Ok, the code is at .  As far as I can tell it's fully operational now with Python3
#### 2009-03-29 21:56:47 - ptmcg
Thanks, I just checked it into SVN.



-- Paul
#### 2009-04-13 10:01:48 - rereidy
Hi Paul,



Is this new Python 3 branch of the code available?  I would like to use this for a test I am running that is slated to go into production late May.



Thanks.



--

Ron
#### 2009-04-13 11:23:24 - ptmcg
Check out the latest version from the SourceForge SVN repository.



-- Paul
#### 2009-04-14 07:41:03 - rereidy
Thanks Paul.



I am a Python newbie, so how do I install this module on both Windows and Solaris by using the SVN distro?



Thanks for your patience.



--

Ron
#### 2009-04-14 09:57:16 - ptmcg
Pyparsing has a very small footprint, a single file.  In the past, you would have used pyparsing.py.  Drop this file into your source directory, or add to a directory in your PYTHONPATH, and use 'import pyparsing' or one of its variations in your code.  To use the Python3 version, get the file pyparsing_py3.py, and use 'import pyparsing_py3'.



-- Paul



(I *promise* I will have the 1.5.2 release ready for download from SF in the next day or so.  Then you should be able to use easy_install.)

---
## 2009-03-31 07:22:48 - john-l - Example and lessons learned: parsing SPARQL in RDFLib
Until recently,  depended on a compiled module for parsing  queries.  RDFLib's primary maintainer has been pressing to move away from dependencies on anything that needs to be compiled, however, so I decided to look into solutions that could replace the compiled module.  I chose to work with pyparsing, and I have had a lot of fun porting the existing parser to use pyparsing.  I've made good progress, and I wanted to report on that progress here; you can find  in .  Thank you very much for such a great tool; pyparsing is a pleasure to work with.



The code for the original parser has been maintained with BisonGen, and it has a number of callbacks to Python classes to construct a parse tree with custom objects.  My goal was to be able to faithfully reproduce these parse trees with pyparsing, and pyparsing's flexibility, particularly with the `setParseAction` method, allowed me to meet that goal.  The original parser passed tokens from the parse to class constructors as positional parameters.  On the other hand, pyparsing passes a single parse result object to the callback when using `setParseAction`, so I first wrote a translation function that would create a new function to map the contents of a parse result to the positional parameters for a constructor; this is the `refer_component` function.  This function also does other fun things like allowing additional static arguments, and argument reordering; see the docstring on that function for details.  There were cases when I needed to build up callbacks compositionally, and so we also have the functions `composition` and `composition2` (which take a single argument and a list of arguments, respectfully).  With these utilities, I was able to completely replace pyparsing parse results with the recursively constructed object tree expected by the higher level SPARQL processor.



While writing this parser, I ran into a number of problems that I had to work through along the way, and I thought I might list them here to try to help others avoid them in the future:



<ul><li></li><li>In some cases, my parse tree objects expected `None` as a positional argument.  There is, however, no way to return `None` from a lower-level callback, as returning `None` is the protocol for using the original results unmodified.  Thankfully, the empty list was an equivalent substitute, so I could return a list containing an empty list as a single token (`return <!-- ws:start:WikiTextAnchorRule:0:&lt;img src=&quot;/i/anchor.gif&quot; class=&quot;WikiAnchor&quot; alt=&quot;Anchor&quot; id=&quot;wikitext@@anchor@@&quot; title=&quot;Anchor: &quot;/&gt; --><a name=""></a><!-- ws:end:WikiTextAnchorRule:0 -->`), which did work.  It might be nice to have a way to return `None` as a valid token from callbacks, in the future.</li><li>When debugging my problem with `None` tokens, above, initially the tracebacks were printing with extremely long descriptions of each parse element, as the descriptions of each parse element are constructed recursively from its contents.  It is very helpful to use the `setName` method to replace this recursive description with a short name.</li><li>I am very interested in implementing a mechanism for providing more detailed error messages (for parse errors).  I really like , but I think pyparsing might be able to do a better job of this with a stack of failed leaf-level tokens, perhaps using  to track them.</li><li>Be careful with parse actions: if they throw certain types of exceptions (e.g. `IndexError`s), those exceptions will be caught by pyparsing and silently treated as a failed parse.  <em>Catch potential exceptions early and reraise them as custom exceptions!</em>  (The introduction of the `ProjectionMismatchException` in my code is an example of this.  For some reason, I thought that a `Group(Optional(...))` would always return at least an empty list token, although upon reflection this was an extremely silly assumption.)</li></ul>

This parser still needs work.  I have run it through some test cases, but I need to run it through more.  I wrote some test cases targeted directly at the parser, and I need to write more to get better coverage.  One of the beautiful things about pyparsing is that I can use any of the intermediate `ParserElement`s as a standalone parser for the corresponding subset of the syntax represented by that object.  As a result, I can test the parsing functionality incrementally.  It gets a bit unwieldy to manually construct the expected results of a parse, though, so I may need to factor out some common result components for reuse.  You can see the test cases in , which is intended to be driven with .  The parser is also not complete; SPARQL supports a number of different query types, but my initial work has targeted only `SELECT` queries.  I will be expanding that support to other types of queries soon.  Finally, I want to experiment with better error handling in the parser, using the techniques I discussed earlier.

#### 2009-03-31 23:44:10 - ptmcg
Nice summary of your lessons learned.



Let me suggest one way to improve your error messages in the face of syntax errors, it is to use the '-' operator in selected places instead of the '+' operator.



'-' inserts an ErrorStop expression into the parser so that if any errors occur after this operator, parsing stops immediately and no further alternatives are tested.



'-' is useful when your grammar includes command keywords which unambiguously dictate the next token or group of tokens.  Here is an example BNF:



    ip_address :: #.#.#.#
    ip_address_spec :: 'ip_addr' ip_address
    hostname :: identifier_starting_with_alpha
    host_spec :: 'host' hostname
    port_spec :: 'port' #
    remote_addr :: ( ip_address_spec | host_spec ) [ port_spec ]



As part of some web service config string, we are going to specify a remote IP address using either an explicit IP address (preceded by the keyword 'ip_addr') or a host name (preceded by 'host'), and optionally followed by a port specification (marked with 'port').



Here is the simplest implementation of such a BNF (setName is indeed a handy way to label expressions for nicer-reading exception messages):



    integer = Word(nums).setName('integer')
    identifier = Word(alphas,alphanums+'_').setName('identifier')
    ip_address = Combine(integer + ('.'+integer)*3)
    ip_addr_spec = 'ip_addr' + ip_address
    host_spec = 'host' + identifier
    port_spec = 'port' + integer
    remote_addr = (ip_addr_spec | host_spec) + Optional(port_spec)



Now let's say we parse this erroneous string: 'host eyegor port zebra'.  To be a little more illustrative of where the error is getting flagged, I'll parse it with this code:



    test = 'host eyegor port zebra'
    try:
        remote_addr.parseString(test, parseAll=True)
    except ParseBaseException, pe:
        print pe.msg
        print pe.pstr
        print ' '*pe.loc+'^'



This prints out:



    Expected end of text
    host eyegor port zebra
                ^



The error occurs in the port clause, but not because 'port' doesn't belong, but because an invalid port number was given.



Notice now, that in our grammar, once we see the keyword 'port', there is nothing else that could <em>possibly</em> come next, except for an integer port number.  So, I choose to replace the '+' operator in port_spec with '-', as in:



    port_spec = 'port' - integer



and I leave the rest of the parser and test code the same.  Now I get this output:



    Expected integer
    host eyegor port zebra
                     ^



You could go back through the grammar, and follow every keyword with '-' instead of '+' and get similarly improved output messages.



Some other tidbits to address some of your lessons learned:

- Optional expressions will accept a 'default' argument to be returned in case the optional expression is not found.  In your case of 'Optional(Group(something))', change it to 'Optional(Group(something),default=[])'.

- Another pyparsing user also wanted to return None from parse actions, and ran into the same problem you did.  The approach he took was to define his own constant 'NONE_RETURN_VALUE = object()' and returned that as his None placeholder.  Pyparsing treated this object like a non-None return and used it to replace the parsed tokens. Then his later code interpreted NONE_RETURN_VALUE as None.  Another thing you might try would be to just delete the contents of the tokens argument passed into the parse action, using 'del tokens[:]'.  But returning [] or  should work too.



When I get a chance, I'll add your link to the 'Who's Using Pyparsing' wiki page. (I've invited you to be a wiki member, so you could also make this change yourself.)



Thanks for the detailed post.



-- Paul
#### 2009-04-01 08:35:28 - john-l
Thanks for your explanation of the `-` operator; I think it'll be useful in providing better error messages in some cases, as you suggest.  The `NoMatch` sentinel approach, however, adds the ability to decorate parse failures for branching parse trees with messages indicating that there were several available options.  For example:



    from pyparsing import Keyword, Word, alphas, NoMatch, ParseException
    
    w = Word(alphas)
    branch0 = Keyword('branch0') - w
    branch1 = Keyword('branch1') - w
    grammar = branch0 | branch1
    
    try:
      print(grammar.parseString('branch0 foo').asList())
    except ParseException, e: print(e)
    
    try:
      print(grammar.parseString('branch2 foo').asList())
    except ParseException, e: print(e)
    
    def guidepost(expr, message):
      return NoMatch().setName(message) | expr
    
    grammar = guidepost(grammar, 'branch0 or branch1')
    
    try:
      print(grammar.parseString('branch2 foo').asList())
    except ParseException, e: print(e)

When run, this produces the following output:



    ['branch0', 'foo']
    Expected 'branch0' (at char 0), (line:1, col:1)
    Expected branch0 or branch1 (at char 0), (line:1, col:1)

This does, however, require you to decorate any branch points with a `NoMatch` instance with an appropriate name (i.e. call the `guidepost` function).  It would be convenient if pyparsing could keep track of the errors encountered on the different branches so that it could produce a summary like this automatically, although I imagine this might get subtle if different branches error out at different depths.  I might try to investigate this further at some point.



There were also a couple of things that I forgot to advertise in my original post.  If you look at , you will see a number of utility functions at the beginning of the module.  I wanted to take a moment to describe those, in case they might be useful to others.  First, the `apply_to_pyparser_tree` function takes a `ParseElement` and a function, and applies that function to every `ParseElement` contained in the tree rooted at the initial `ParseElement`.  I used this for setting homogenous debug actions to every `ParseElement` in a tree.  In particular, I added debug actions that would emit a complete XML parse trace whenever that `ParseElement` is used to parse content.  I found this useful for debugging.  The `ParseTracer` class allows you to create such an XML parse trace; it currently depends on 4Suite for XML processing, but it would probably be easy to port that to other XML libraries.  I wrote , allowing a user to 'explore' the entire parse results, looking for problems.  The output is a little ... bright, but I would still be curious if anyone else might be interested in this functionality.  If so, it might make sense to break these utilities away from where they currently live in RDFLib, to a more general location.



I also wrote a pair of general purpose functions for visualizing any (Python) object tree; the main function for this is called `struct_data`, and produces a nested dictionary that summarizes the available data in an object tree.

---
## 2009-03-31 16:25:30 - reeeh2000 - Tokenizing
I need some help with recursive tokens.



    statement = Forward()
    name = Word(alphas)
    assignment = (name.setResultsName('assignedName') + 'as' + statement.setResultsName('moreStatements'))
    statement \<\< (assignment ^ expression ^ call ^ name)  
    ''' expression and call are along similar lines



As you see, this would parse 'a as b as c'.

The problem is that once parsed into line, line.moreStatements will only return b.  

Basically this line mean b = c and a = b.  For my lexical analysis I need to get past b.

Any help would be appreciated.

#### 2009-03-31 23:00:13 - ptmcg
The issue you are struggling with here is that setResultsName enables the dict-like semantics of the returned ParseResults.  Like a Python dict, a given key can only have one value.  If that key is used multiple times to assign different values into a dict, only the last value assigned is retained.



Similarly, when using setResultsName on an expression that can occur multiple times in a parsed string, only the last value parsed will be retained.  Here is a very simple way to demonstrate this (for brevity/laziness, I use the abbreviated form of setResultsName, in which the name in parens simply follows the expression):



    word = Word(alphas)
    number = Word(nums)
    expr = OneOrMore(word('alpha') | number('integer'))
    
    res = expr.parseString('ABC 1231 DEF 3456 ZYX')
    print res.dump()



dump() will return first the list of parsed tokens, and then an indented list of all tokens that are named.



This example prints:



    ['ABC', '1231', 'DEF', '3456', 'ZYX']
    - alpha: ZYX
    - integer: 3456



The list shows that all the tokens were parsed, but only the last ones for each expression are retained.



Fortunately, setResultsName supports an optional argument, listAllMatches.  If set to True, then all matching token values are kept in a list, instead of just the last matching token.  Unfortunately, if we need to pass listAllMatches, then we must use the long .setResultsName form:



    expr = OneOrMore(word.setResultsName('alpha', listAllMatches=True) | 
            number.setResultsName('integer', listAllMatches=True))



Now if we parse the same input string and dump the results we get:



    ['ABC', '1231', 'DEF', '3456', 'ZYX']
    - alpha: ['ABC', 'DEF', 'ZYX']
    - integer: ['1231', '3456']



If you add listAllMatches=True to your setResultsName calls, you'll get this parsed structure:



    ['a', 'as', 'b', 'as', 'c']
    - assignedName: ['a', 'b']
    - moreStatements: ['c', 'b']



This is kind of confusing, since 'b' shows up in both named lists.



Now in your case, instead of having a name overwritten because of a repetitive expression (using OneOrMore), you have this because of a recursive expression (using Forward).  By default, pyparsing results accumulate tokens into a flat list of strings, with no nesting, so even though you recursively define an expression to parse 'a as b as c', you get '['a', 'as', 'b', 'as', 'c']' for a result.  This is done so that the token structure is not bound to whatever expression nesting may have been used in creating the expression in the first place.  Consequently, you also have a flat view of the results' dict behavior.  If instead, however, you use Group expressions to add structure to the results, then you can get nested results, and instead of getting back lists for your given names, you'll actually get a nested structure.



Here is how your grammar would look with Group (and some slight rearranging - removing the lone name from the possible values of statement):



    assignment = name('assignedName') + 'as' + (Group(statement)|name)('moreStatements')
    statement \<\< (assignment) # could be expanded as (assignment | call | etc.)



Now Group preserves the structure for any compound (that is, non-scalar name) statement, and our parsed results now look like:



    ['a', 'as', ['b', 'as', 'c']]
    - assignedName: a
    - moreStatements: ['b', 'as', 'c']
      - assignedName: b
      - moreStatements: c



I would prefer the Group approach myself, since this unambiguously retains the parsing structure, and you don't have to guess how you might reconstruct the a's, b's, and c's from some strange list results.



I would also caution against using '^' within the definition of statement, and instead try to define call and expression so that they would not be confused by the order of evaluation.  Using '^' within a recursive expression can easily lead to stack overflow exceptions.  Looking ahead, I'm also guessing that a lone name might qualify as a valid expression, so there is no point in trying to evaluate both within statement, just test for expression, and you can then omit the '|name' from the definition of assignment.



Have you written out a BNF for this grammar?  If you haven't, I suggest you take a minute and write one out, and add it to the module docstring of your parser.  It will really help you clarify your thinking, and you'll also have a pretty good idea of when you have finished.  If you post back with more questions, please include the BNF, so I can make suggestions that I wont have to undo later when you add more of the advanced details and features.



Good luck,

-- Paul
#### 2009-04-01 12:43:20 - reeeh2000
Thank you for your help.  I do have a BNF.  The reason that I must use '^' is that some of the commands start off the same, so I must match the longest, not the first.

---
## 2009-04-02 13:41:59 - dbv - 64-bit Windows
Paul



Will pyparsing work under 64-bit Windows Vista using 64-bit Python 2.5x?



Dinesh

#### 2009-04-02 16:29:03 - ptmcg
Pyparsing is pure Python, and makes no assumptions on integer size (that I can think of).  So there should be no problem in running pyparsing on 64-bit platforms of any kind.



I personally have run pyparsing on Windows, StrongARM embedded processor (running Linux), and Debian.  Pyparsing is bundled with a number of Linux distributions.  I've not gotten any reports that pyparsing *doesn't* run on 64-bit platforms.



-- Paul
#### 2009-04-03 01:00:00 - dbv
Thanks Paul.  So far, pyparsing is working under 64-bit Windows. Cheers.  Dinesh

---
## 2009-04-04 12:03:54 - ellisonbg - Subtle infinite loop with Optional
I am having trouble with Optional giving me an infinite loop when used with scanString.  Here is an example that shows the behavior:



from pyparsing import *



START_B, END_B = makeHTMLTags('b')

START_SPAN, END_SPAN = makeHTMLTags('span')



s = '''ExtraText...Elevation(s):\<b\>1882 KB\</b\>

\<b\>1881 DF\</b\>

\<b\>1862 GL\</b\>...ExtraText'''



UNIT = START_B.suppress() + SkipTo(START_SPAN)('depth') + \

    START_SPAN.suppress() + SkipTo(END_SPAN)('ref') + \

    END_SPAN.suppress() + END_B.suppress()



ELEVATION1 = Literal('Elevation(s):').suppress() + \

    Group(ZeroOrMore(UNIT))('elev_list')



ELEVATION2 = Optional(Literal('Elevation(s):')).suppress() + \

    Group(ZeroOrMore(UNIT))('elev_list')



<ol><li>This works fine</li></ol>g = ELEVATION1.scanString(s)

for i in g:

    print i



g = ELEVATION2.scanString(s)

<ol><li>This is an infinite loop</li></ol>for i in g:

    print i



I need the Optional element as the ELEVATION stuff doesn't always appear in the string and in reality, my parsing looks like (ELEVATION + other elements).scanString.



When I include ELEVATION in my actual overall scanString step, the call to scanString never returns.  My guess is that the infinite loop generated by the ELEVATION2 element get iterated over internally.



Why is Optional giving this infinite loop and how can I solve this problem?



Thanks for a great library!

#### 2009-04-04 12:31:18 - ellisonbg
I have figured out a little more about this.  If I add another non-optional element to the parser, it works:



HEAD = Liberal('HiThere')



ELEVATION = HEAD + Optional(Literal('Elevation(s):')).suppress() + \

Group(ZeroOrMore(UNIT))('elev_list')



(as long as HiThere is found).



I would still like to understand why I am getting the infinite loop with Optional and scanString used together.



Thanks!
#### 2009-04-09 08:22:44 - ptmcg
Ellisonbg - 



Sorry not to get back to you sooner, my house is currently being torn up by contractors after a water leak killed my kitchen floor.



So, to your question.  scanString (and by extension searchString) works by successively moving through the input string character by character, looking for a match to the given pyparsing expression.  If there is no match, then the character position is incremented by 1 and the expression tried again.  If there *is* a match, then the scanner moves to the end of the match and picks up the scan from there.



In your case, where you get the infinite loop, the expression you are matching consists of a leading Optional, followed by a Group(ZeroOrMore), which can match a zero-length string.  The scanner successfully matches nothing at position 1, advances 0 characters to position 1, and tries again, where it successfully matches nothing at position 1, advances 0 characters to position 1 again, and so on...



I think I could make scanString a little smarter by having it check to see if the next location after a successful match is the same as the current location, and if so, increment by one.  And if the returned ParseResults is empty, discard it.



In the meantime, make sure that the expression that you are scanning for is not *completely* optional or zero-length.



-- Paul

---
## 2009-04-10 07:10:57 - JaimeWyant - 1.5.2 pypi release?
Will you be putting 1.5.2 up on the pypi anytime soon?  I use easy_install and would really like to be able to fetch my versions from there.



Thanks!

jw

#### 2009-04-10 07:35:58 - ptmcg
I was just packaging up 1.5.2 yesterday to upload, I should have it done sometime this weekend (including pyparsing_py3 module for Python 3 users, and support for IronPython2.0.1).



-- Paul

---
## 2009-04-15 08:18:50 - Llanilek - Simple Question
I'm new to pyparsing and it's all getting a little out of hand when a push in the right direction could be all that i need to save a man from going bald... lol



Ok, heres what i Want



i have a string





    @tgfstatus #game Game Title | status text



basically i need to parse it as following





    ['@tgfstatus', '#', 'game', 'Game Title', '|', 'status text']



Here's what i have



    Text = '@tgfstatus #game Game Title | status text'
    searchterm = '@tgfstatus #' + Word(alphas) + OneOrMore(Word(alphas)) + '|' + OneOrMore(Word(alphas))
    result = searchterm.parseString(Text)



which prints





    ['@tgfstatus #', 'game', 'Game', 'title', '|', 'status', 'text']



I'm sure it's really simple but it'd be great if someone could point me in the right direction.

#### 2009-04-15 09:48:18 - ptmcg
Your first issue is easy to describe.  Because you define the opening literal as '@tgfstatus #', then you get that literal back as the matched token.  Since you want the '#' sign separate from the opening '@tgfstatus' tag, then list them as two different expressions in the search term.  Instead of:



    searchterm = '@tgfstatus #' + ...

use:



    searchterm = Literal('@tgfstatus') + '#' + ...

The intervening whitespace is automatically skipped by pyparsing.



The second issue has to do with matching the multi-word phrases, but returning them as a single string instead as each separate word.  I have 3 ways to give you that all work, and you can choose for yourself which you prefer.  In all cases, the treatment of 'Game Title' and 'status text' are the same, so I'll define a subexpression called phrase that you can use as:



    searchterm = Literal('@tgfstatus') + '#' + Word(alphas) + phrase + '|' + phrase



1. Add a parse action.



    phrase = OneOrMore(Word(alphas)).setParseAction(lambda toks:' '.join(toks))



This will post-process the matched tokens by joining them with intervening spaces.  Using parse actions is an important technique in pyparsing, so it is good to get comfortable with them.  The downside to this is that if you have words separated by two or more spaces, these will get collapsed down to single spaces.



2. Use Combine.



    phrase = Combine(OneOrMore(Word(alphas)), ' ', adjacent=False)



Combine was originally developed to wrap expressions like this:



    floatNumber = Word(nums) + '.' + Word(nums)

Unfortunately, this would return a parse of '3.1416' as ['3', '.', '1416'].  By changing this to:



    floatNumber = Combine(Word(nums) + '.' + Word(nums))

then the matched tokens get concatenated after being individually parsed.  Then I could even add a parse action like this:



    floatNumber.setParseAction(lambda toks: float(toks[0]))

and now my parsed results is not a string, but the actual float number 3.1416.



I did not want Combine to accidentally match 'The answer is 3. 1416 is an important date in history.' and find '3.1416', so Combine by default requires that the matching tokens have no intervening whitespace.  But in your example, there is space between the words, so I have to set the additional Combine initializer argument 'adjacent=False'.



This is actually my least preferred of the 3 options, but it is here for completeness and instructional value.  It also has the same whitespace-collapsing behavior as option 1.



3. Use originalTextFor helper wrapper.



    phrase = originalTextFor(OneOrMore(Word(alphas)))



this helper method was only recently added to pyparsing, but it was long overdue.  Several of pyparsing's methods and classes return a set of tokens when all that is desired is the original matching text.  By wrapping OneOrMore(Word(alphas)) in originalTextFor, you will get the original text from the input string, reconstituted as a single string, with all original whitespace preserved.



Please download the source or docs distributions from SourceForge - they will include some additional documentation that is not included with the Win32 binary install or the easy_install egg.



But please keep the questions coming, and welcome to pyparsing!

-- Paul
#### 2009-04-16 09:59:08 - Llanilek
ok... thats a great help



one more question... how can we stop this from returning an error if it does not match the query?
#### 2009-04-16 10:08:51 - ptmcg
I think the solution is not to stop from returning the error, but how to trap and handle the returned error, and this is more a Python question than a pyparsing one.  Something like this:



    try:
        result = searchterm.parseString(Text)
    except ParseException, pe:
        print pe.msg



ParseExceptions have a number of fields that include the location in the parser where the problem occurred, the reason for failure, etc. - check the docs.



-- Paul
#### 2009-04-16 10:25:15 - Llanilek
Hey paul,



I downloaded the docs from sourceforge but the file was empty there was nothing in there?
#### 2009-04-16 11:09:33 - ptmcg
Did you download from the SVN repository?  Sorry, I haven't put any files in there.



Try downloading the docs package from the project download page ). The file is 1.04Mb in size and contains a single directory docs.  Within docs are the examples and htmldoc directories, a class diagram in PNG and JPG format, two PDF files containing my presentations from PyCON 06, and an HTML file with some basic 'how to' instructions.



-- Paul
#### 2009-04-16 14:02:08 - Llanilek
ok ... i got that



also from the first question how can i make it accept the parse if the string is like this



@tgfstatus #game something & something | i have punctionation & i still must parse



i need that to still parse in the same format
#### 2009-04-16 20:23:57 - ptmcg
Ok, now your phrase is getting more than just Word(alphas) in there.  It should read kind of like a sentence:



'A phrase is one or more words or punctuation marks'



So it should translate to something like:



    punctuation = oneOf(list('&,.'?'))
    phrase = OneOrMore(Word(alphas) | punctuation)



Don't forget to add originalTextFor.



-- Paul
#### 2009-04-17 03:39:15 - Llanilek
what if punctuation has one or more items of punctuation...



if i try and call it with as this





    punctuation = OneOrMore(list('&,.'?'))



i get an error.





    'list' object has no attribute 'mayIndexError'


#### 2009-04-17 04:13:47 - Llanilek
ahh forget that ... it wasn't the one or more items causing a problem its the fact that i was using alphas instead of alphanums 



thanks paul!
#### 2009-04-17 06:42:08 - ptmcg
Your welcome, and good luck!  Post back when your application is finished, so I can add you to the 'Who's Using Pyparsing' page.



-- Paul

(... and there's worse things than going bald!)

---
## 2009-04-18 12:49:17 - pynguin - problems with setName()
I am trying to get the errmsg to print out the highest level name when the parsing fails.





    floatNumber = Combine(Word(nums) + Literal('.') + Word(nums)).setName('float')



I still get the response:



Expected W:(0123...)



but when I have:





    number = Word(nums).setName('number')



It responds as expected with:

Expected number



Any Ideas?

#### 2009-04-20 21:11:04 - ptmcg
Well, in its striving to give the most local and detailed exception message, it seems that I sent pyparsing in the exact opposite direction, that is reporting the *lowest* level name possible.



In another thread, gmonkey5 reports using this technique, which may be partially helpful:



    floatNumber = NoMatch().setName('real number') | Combine(Word(nums) + Literal('.') + Word(nums))



If there is no leading numeric digit, then both alternatives will fail, but the message for the first alternative is the one that will be reported.



This feels awkward to me, but that's the way my code behaves, so I can't really complain.



The option I had proposed was a new class derived from ParseElementEnhance.



    from pyparsing import *
    
    class NamedExpression(ParseElementEnhance):
        def parseImpl(self, instring, loc, doActions = True):
            try:
                return super(NamedExpression,self).parseImpl(instring, loc, doActions)
            except ParseBaseException,pbe:
                pbe.msg = 'Expected ' + str(self)
                pbe.loc = loc
                raise pbe
    
    USE_NAMED_EXPRESSION = True
    if not USE_NAMED_EXPRESSION:
        floatNumber = (
            Combine(Word(nums) + Literal('.') + Word(nums))
            ).setName('floating point number')
    else:
        floatNumber = NamedExpression(
            Combine(Word(nums) + Literal('.') + Word(nums))
            ).setName('floating point number')
    
    print floatNumber.parseString('123.345')
    print floatNumber.parseString('123')



You can try both versions of floatNumber - if you use NamedExpression, the exception will report 'Expected a floating point number' - if not, you'll get 'Expected '.''



-- Paul

---
## 2009-04-20 06:37:44 - stakhanov2 - memory issues with Py3k (reminder)
Hi!



...this is just a reminder that the

memory leak issue, I reported in the

thread 'memory issues with Py3k'

back in February is still unresolved.



I've tested this with 1.5.2 now, and

the symptoms are still the same.



cheers,



Richard

#### 2009-04-20 10:50:06 - ptmcg
Thanks for the reminder, Richard.  I am devoting only a few cycles to pyparsing these past few weeks, and wanted to at least get out the Py3 and IronPython2.0.1 compatibility features.  I'll look at this the next opportunity that I get.



-- Paul

---
## 2009-04-22 13:31:35 - stanchan - Top level parsing
This might be a newbie question, but what would be the best way to handle this type of parsing scenario?  I want to only extract the 'def name {}' sections.



class foo {

  test blah1 {

     echo(test)

  }

}

class bar {

  test blah2 {

     echo(test2)

  }

}

def baz {

  test blah3 {

     echo(test3)

  }

}



ParserElement.setDefaultWhitespaceChars(' \t')

NL = LineEnd().suppress()

LCURLB,RCURLB,COLON,QUOTE = map(Suppress,'{}:'')

KEYWORDS = Literal('class') | Literal('def')

STMT = Group(Literal('def') + Word(alphanums+'_-')('name') + LCURLB + NL + OneOrMore(restOfLine + NL) + RCURLB + NotAny(KEYWORDS + Word(alphanums+'_-') + LCURLB)



Obviously, NotAny is not being used correctly in this example.  I basically want to extract the contents of 'def' sections enclosed '{}' by without parsing anything within it up to the next 'def' or 'class' section without picking up any 'class' sections.  Any help would be appreciated.  No recursion is required.

#### 2009-04-22 13:34:20 - stanchan
Forgot to enclose the section with spacing.





    class foo {
       test blah1 {
          echo(test)
       }
    }
    class bar {
       test blah2 {
          echo(test2)
       }
    }
    def baz {
       test blah3 {
          echo(test3)
       }
    }


#### 2009-04-22 15:51:44 - ptmcg
If you are trying to extract some fragment from a larger body of text, searchString is the way to go.  Here is a minimal way to extract 'def' statements.



    from pyparsing import *
    
    ident = Word(alphas+'_', alphanums+'_')
    def_ = 'def' + ident('name') + originalTextFor(nestedExpr('{','}'))('body')
    
    for d in def_.searchString(test):
        print d.name
        print d.body
        print



If you have to step over defs that might occur within other constructs (like comments, or class definitions), then define expressions for them to, and have def ignore them using ignoreExpr.



-- Paul
#### 2009-04-22 16:56:51 - stanchan
Thanks... that works.  originalTextFor(nestedExpr('{','}')) was what I was looking for.  Unfortunately it doesn't work with ParserElement.setDefaultWhitespaceChars(' \t') set. Where can I apply .setWhitespaceChars(' \t\n\r') to the above example to override the default?
#### 2009-04-22 17:00:20 - ptmcg
What is the purpose of overriding the default whitespace characters?  Your sample input text does not look like it needs to do this.
#### 2009-04-22 17:08:39 - stanchan
It's part of a larger app that parses a pretty complex configuration file.  That overall configuration file can contain elements that can embed scriptlets.  Everything else is parsable, but I had to set setDefaultWhitespaceChars to make that happen and since the other parts of the config file require more extensive parsing, I can't unset it.
#### 2009-04-22 17:23:51 - stanchan
Running .setWhitespaceChars(' \t\n\r') on all elements/sub-elements of the parser statement doesn't seem to override the default pyparsing class attribute.
#### 2009-04-22 19:27:15 - ptmcg
There are a number of unnamed sub-elements created by nestedExpr and originalTextFor.  Use this recursive routine to set the whitespace characters for def_ and all its sub-elements:



    def setWhitespace(expr,ws,seen):
        if expr in seen: return
        seen.add(expr)
        expr.setWhitespaceChars(ws)
        if hasattr(expr,'expr'):
            setWhitespace(expr.expr, ws, seen)
        if hasattr(expr,'exprs'):
            for e in expr.exprs:
                setWhitespace(e, ws, seen)
    
    previously_seen = set()
    setWhitespace(def_, ' \t\n', previously_seen)



-- Paul
#### 2009-04-23 14:04:43 - stanchan
Thanks!  Works well.



- Stan

---
## 2009-04-28 06:47:49 - reynaudd - maximum recursion depth exceeded
Hello,



I'm trying to write a logic formula parser using pyparsing, and stumbled on this:





    formula = Forward()
    
    # this works:
    formula \<\< Group('or' + formula + formula)
    
    # this causes a Runtime Error: maximum recursion depth exceeded
    formula \<\< Group(formula + 'or' + formula)



What would be the right way to use the infix notation ?

Thanks for contributing pyparsing !

#### 2009-04-28 13:49:07 - ptmcg
Well, you've skipped a few steps, but that is ok.



For simplicity, let's just say that you are defining boolean expression notation, using standard 'and' and 'or' operators, and the base operand is either a lower-case letter, or 'T' for true or 'F' for false.  To define your grammar while preserving precedence of operations, here is a typical infix notation BNF (this is not a rigorous BNF syntax, but should get the meaning across):



    bool_expr ::= and_term [ or and_term ]...
    and_term ::= base_operand [ and base_operand ]...
    base_operand ::= 'a'..'z' | 'T' | 'F' | '(' bool_expr ')'



Note that the base_operand recursively refers back to the top-level bool_expr, if it is enclosed in ()'s.



Translating this to pyparsing looks like this:



    variable_ref = oneOf(list(alphas.lower()))
    TRUE = Literal('T')
    FALSE = Literal('F')
    
    bool_expr = Forward()
    base_operand = variable_ref | TRUE | FALSE | '(' + bool_expr + ')'
    and_expr = base_operand + ZeroOrMore('AND' + base_operand)
    bool_expr \<\< (and_expr + ZeroOrMore('OR' + and_expr))



You could also use the pyparsing builtin operatorPrecedence, which looks like this:



    bool_expr = operatorPrecedence( variable_ref | TRUE | FALSE,
        [
        ('AND', 2, opAssoc.LEFT),
        ('OR', 2, opAssoc.LEFT),
        ])



To add support for 'NOT', the operatorPrecedence example expands to:



    bool_expr = operatorPrecedence( variable_ref | TRUE | FALSE,
        [
        ('NOT', 1, opAssoc.RIGHT),
        ('AND', 2, opAssoc.LEFT),
        ('OR', 2, opAssoc.LEFT),
        ])



You can also see more examples in the fourFn.py, simpleBool.py, and simpleArith.py examples on the wiki examples page.  simpleBool.py shows you how to take the parsed output from the operatorPrecedence example and create an evaluatable object tree.



Good luck, and welcome to pyparsing!

-- Paul



(This problem of parsing infix notation was one of the first parsing applications that really captured my interest in parsing problems.)

---
## 2009-04-28 12:40:02 - JesterEE - Optional() TypeError Bug?
pyparsing community,



I am actively engaged in writing my first Python parser, and have found pyparsing very useful for making the task explicit and easy to follow.  Though, I think I may have stumbled over a bug using the Optional Class.  However, since I am a <em>newbie</em> to the module, I will let you all be the judge.



For a simple example, I was trying to write a grammar to define interget numbers with a optional sign for the set 

[..., -3, -2, -1, 0, +1, +2, +3 ,...].



My grammar looks as follows:



    intnum = Combine(Optional('+' | '-') + Word(nums))



This gives the debugger error:

`TypeError: unsupported operand type(s) for |: 'str' and 'str'`



If I reform my grammar as follows, I do not get the error;



    intnum = Combine(Optional(Literal('+') | Literal('-')) + Word(nums))



Though, I thought those two expressions are equivalent.  Am I mistaken?



I have since changed the grammar to:



    intnum = Combine(Optional(Suppress('+') | '-') + Word(nums))



Even though I have a working grammar, I would like to understand my mistake so I don't pitfall there again.



pyparsing version - 1.5.2

Thanks

-   <small>Apr 28, 2009</small>

#### 2009-04-28 14:10:17 - ptmcg
Well, sometimes I do my job too well.  One of my goals in designing the pyparsing API was to make the code look natural to the Python developer's eye, but in a few cases, what looks natural is not really what one intends.



The '|' operator ONLY uses pyparsing MatchFirst semantics if one of the two operands is a pyparsing ParserElement.  So you can write 'Literal('+') | '-'', ''+' | Literal('-')', or 'Literal('+') | Literal('-')', and Python will use the correct ParserElement.__or__ or ParserElement.__ror__ method to generate a MatchFirst from the two pieces (converting the one that is a string literal to a Literal).  But if you write ''+' | '-'' Python has no idea what this is supposed to mean, and thinks you have lost your senses and are trying to 'or' two literal strings!  By wrapping '+' in Suppress, your grammar has done the promotion to a ParserElement, which can then be '|'ed with the string '-', which gets promoted to a pyparsing Literal by the __or__ method.



You can also use the oneOf helper to quickly create an alternation among several literals.  You could write



    sign = oneOf('+ -')
    intnum = Combine(Optional(sign) + Word(nums))



You must take especial care when using '+', since this operator <em>is</em> valid for combining strings, except it probably wont create the pyparsing expression you would want.  Note the difference between these different expressions:



    COLON = Literal(':')
    patt1 = 'FIRST' + 'SECOND' + COLON
    patt2 = Literal('FIRST') + 'SECOND' + COLON
    patt3 = 'FIRST' + Literal('SECOND') + COLON
    
    for p in (patt1,patt2,patt3):
        try:
            print p.parseString('FIRSTSECOND:')
            print p.parseString('FIRSTSECOND :')
            print p.parseString('FIRST SECOND:')
            print p.parseString('FIRST SECOND :')
        except ParseException, pe:
            print 'failed on '%s'' % pe.pstr
        print



Lastly, let me suggest you investigate the use of parse actions to convert your signed integer strings into actual integers at parse time.  The code will look something like:



    intnum = Combine(... etc.... )
    intnum.setParseAction(lambda tokens: int(tokens[0]))



Good luck!  And Welcome!



-- Paul
#### 2009-04-28 14:53:12 - JesterEE
Paul,



Thanks for chiming in and clearing this up for me!  I understand the convention you made and even appreciate it as it's probably easier to read that way anyway when stuff starts getting messy.



About the code segment in the last part, that is very interesting indeed ... I would not have expected that at a quick first look.  Though, it seems that if I don't want to have nuances pop up while playing in the sandbox, when defining literals I should always user the Literal() block.  Is that pretty accurate?



And lastly, the setParseAction was the next line down :).



-   <small>Apr 28, 2009</small>

---
## 2009-04-29 13:15:02 - catherinedevlin - strange propagation of ignoreExprs into elements
I've found some pyparsing behavior I can't make sense of.  I don't know if it should be a bug report, or just leave it here in hopes that future websearchers will find it when they need it.



In this chunk, there are no surprises:



<ul class="quotelist"><ul class="quotelist"><ul class="quotelist"><li>import pyparsing</li><li>pyparsing.<u>version</u></li></ul></ul></ul>'1.5.2'

<ul class="quotelist"><ul class="quotelist"><ul class="quotelist"><li>target = 'notquoted 'quoted''</li><li>strend = pyparsing.stringEnd ^ 'EOF'</li><li>pyparsing.SkipTo(strend).parseString(target)</li></ul></ul></ul>(['notquoted 'quoted''], {})

<ul class="quotelist"><ul class="quotelist"><ul class="quotelist"><li>strend.ignore(pyparsing.quotedString)</li></ul></ul></ul>{stringEnd ^ 'EOF'}

<ul class="quotelist"><ul class="quotelist"><ul class="quotelist"><li>pyparsing.SkipTo(strend).parseString(target)</li></ul></ul></ul>(['notquoted'], {})



... but then,



<ul class="quotelist"><ul class="quotelist"><ul class="quotelist"><li>pyparsing.SkipTo(pyparsing.stringEnd ^ 'EOF').parseString(target)</li></ul></ul></ul>(['notquoted'], {})



... OK, so I guess the ignoreExprs get applied to pyparsing.stringEnd itself - which is, after all, a single object - instead of just to the aggregated object `strend`.



<ul class="quotelist"><ul class="quotelist"><ul class="quotelist"><li>pyparsing.SkipTo(pyparsing.stringEnd).parseString(target)</li></ul></ul></ul>(['notquoted 'quoted''], {})



... huh?  



<ul class="quotelist"><ul class="quotelist"><ul class="quotelist"><li>pyparsing.SkipTo(pyparsing.stringEnd ^ 'x').parseString(target)</li></ul></ul></ul>(['notquoted'], {})



... OK, now I really don't know what to make of it.  Because stringEnd was used in a grammar, and .ignore was applied to that grammar, stringEnd will always carry the effects of that .ignore... unless it's used on its own?



For now, to be on the safe side, I'm using pyparsing.StringEnd() so that I'll know I'm using a fresh object, and won't have to worry about what may have gotten into its ignoreExprs during previous usages.

#### 2009-05-01 19:19:37 - ptmcg
Yes, this is a hazard when using the built-in helper instances empty, lineEnd, lineStart, etc. in place of Empty, LineEnd, LineStart and so on.  Since ignore at a high-level expression has to propagate to all of the component expressions (or else it wont really be ignored very much), this will have some far-reaching effects on other expressions in your grammar.  Similar effects happen when calling leaveWhitespace.



The built-in instances are useful for simple expressions, but for large complex grammars with localized definitions of ignore and whitespace expressions, then using all freshly-created instances is a prudent approach.



Thanks for your continued use and support of pyparsing (and congratulations on the nice comments about you and your sqlpython lightning talk at Pycon, in Steve Holden's column in the April issue of Python Magazine).



-- Paul

---
## 2009-05-13 15:08:50 - oafilipoai - scanString problem
If a parsed string contains tabs ('\t') the scanString function fails to work correctly.





    from pyparsing import *
    
    text='''
    parameter p1=200;\t
    parameter p2=10;
    '''
    param_loc=Combine(Keyword('parameter') + SkipTo(';'))
    for p,s,e in param_loc.scanString(text):
        print text[s:e]
    



The output on my setup (Python 2.6, Linux) is:



    parameter p1=200
    ter p2=10;



If I remove the '\t' from the input text, the output is correct:



    parameter p1=200
    parameter p2=10



The output is also correct if I modify the the parsing expression to this:



    param_loc=Combine(Keyword('parameter') + SkipTo(';')).parseWithTabs()



Is this the expected behavior?



--Tavi

#### 2009-05-13 19:09:59 - ptmcg
Yes, the default behavior of parseString, scanString, and searchString is to first convert the tabs in the input string to spaces.  You'll see this mostly in calls to scanString, if you use the start and end locations to slice the input string (or with the col or line pyparsing methods).  As you've found, parseWithTabs is one way to work around this.



-- Paul

---
## 2009-05-13 16:20:23 - oafilipoai - White spaces not ignored
The following code produces an unexpected output:





    from pyparsing import *
    
    text='''
       parameter p1=200;
    parameter p2=10;
    '''
    param_loc=Combine(LineStart()+Keyword('parameter') + SkipTo(';'))
    for p,s,e in param_loc.scanString(text):
        print text[s:e]



Ouput:



    parameter p2=10



So the statement preceded by white spaces does not seem to be matched. Am I missing something?

#### 2009-05-13 19:26:02 - ptmcg
Combine's default behavior is to require all of the contained expressions to be contiguous.  It was originally written to deal with cases like



    realnumber = Word(nums) + '.' + Word(nums)

which, if parsing the string '3.1416' returns ['3', '.', '1416'].  So instead, write realnumber as



    realnumber = Combine(Word(nums) + '.' + Word(nums))



But I wanted this to *only* match real numbers, and not accidentally match the number in 'Dinner is at 3. 1415 Main St. is the address.' so Combine does not automatically skip whitespace.  That is why the leading whitespace between LineStart() and 'parameter' don't match your expression.  You can override this behavior using 'adjacent=False'.



    param_loc=Combine(LineStart()+Keyword('parameter') + SkipTo(';'), adjacent=False)



This change will now match:



    
       parameter p1=200
    parameter p2=10



-- Paul

---
## 2009-05-14 10:40:06 - oafilipoai - Excluding keywords in comments or strings
How can I exclude the keywords found in comments (/*...*/) or strings ('...')



My current approach is the parse the string twice, first using the ignore() statement on the parsing expression to determine the positions of the real keywords and then to parse the text from that position w/o the ignore statement. I'm doing this because I need to extract the comments and strings as well.



Is there a better way? 



--Tavi

#### 2009-05-14 12:12:26 - ptmcg
I assume you are using scanString.



Do this:



    (cStyleComment | keyword).scanString(sourcestring)



Since cStyleComment comes first in the grammar, it will be matched for first as scanString works through your source string.  Any keywords found inside cStyleComment will get parsed as part of cStyleComment, not as keywords.



Did I guess your intent?  If not, please post back with a little more detail.



-- Paul
#### 2009-05-14 13:51:16 - oafilipoai
I'm trying to parse some statements which look like:



    keyword some statement; // inline comments



I don't want the parser to match a keyword contained in a comment, but I do want to capture the inline comments when parsing



The following code correctly finds the keywords but it does not capture the inline comments:





    
    text='''
    key1 some statement;         // comment 1
    key2 other statement;        // comment 2
    key3 yet one more statement; // comment 3
    '''
    
    comments = cStyleComment | ('//' + SkipTo(LineEnd()))
    key=(Keyword('key1') | Keyword('key2') | Keyword('key3'))
    pat = (    key            # keyword
        + SkipTo(';')        # statements
        + ';'
        + SkipTo(LineEnd())    # inline comment
        ).ignore(comments)
    
    print pat.searchString(text)
    



The output is:



    
    [['key1', 'some statement', ';', ''], ['key2', 'other statement', ';', ''], ['key3', 'yet one more statement', ';', '']]
    



What is the best way to correctly match the keywords and to capture the inline comments at the same time
#### 2009-05-14 18:36:04 - ptmcg
Using your code, try:



    print (comments | key).searchString(text)

Since you want to see the comments, <em>don't</em> use ignore().



-- Paul
#### 2009-05-15 09:51:50 - oafilipoai
It works. Thanks

---
## 2009-05-18 05:03:29 - asb_india - Parsing selected
Hello 



I am trying to make the sql query parser. But i got stuck at one time 



I take one example 



In select query from clause can have 

1. tablename where ....

2. tablename as t where ... 

3. tablename t ...







    
    \>\>\> scalar = Word(alphanums + '_')
    \>\>\> tables = Word(alphanums + '_')
    \>\>\> tablename = delimitedList(tables) + Optional(('as' + scalar) | scalar)
    \>\>\> tablename.parseString('temp')
    (['temp'], {})
    \>\>\> tablename.parseString('temp t')
    (['temp', 't'], {})
    \>\>\> tablename.parseString('temp as t')
    (['temp', 'as', 't'], {})
    \>\>\> tablename.parseString('temp  t')
    (['temp', 't'], {})
    \>\>\> tablename.parseString('temp  where')
    (['temp', 'where'], {})
    \>\>\> where_ = 'where' + scalar
    \>\>\> t_ = tablename + where_
    \>\>\> tablename.parseString('temp  where')
    (['temp', 'where'], {})                    -------------------------------- X
    \>\>\> 
    
    



The last one is not expected as where is keyword and i have to process where seprately. 

How to restrict this. Is thr any other way i can say it the tablename is followed by where it shl stop.

#### 2009-05-18 09:45:16 - ptmcg
For scalar, you will need to guard against matching *any* SQL keyword.  Something like this:



    sql_keyword = oneOf('SELECT INSERT UPDATE DELETE WHERE AS ORDER BY GROUP')
    scalar = ~sql_keyword + Word(alphas,alphanums+'_')



Also, I think your list of tables will need to support repetition of the 'as' alias clause, to support 'EMPLOYEES AS EMP, SALARIES AS SAL, MANAGERS AS MGRS'.  So your tables expression would be:



    tables = delimitedList( Group(scalar + Optional(Optional('as') + scalar))



I Grouped the table names so that all of the tokens wouldn't just blend together into a single unstructured list.



Now you can write something like:



    sql_keyword = oneOf('SELECT INSERT UPDATE DELETE WHERE AS ORDER BY GROUP')
    scalar = ~sql_keyword + Word(alphas,alphanums+'_')
    scalar_list = delimitedList( Group(scalar + Optional(Optional('as') + scalar))
    where_clause = Forward() 
    # to be defined further later, for now just match an empty string
    where_clause \<\< Empty()
    select_statement = 'SELECT' + ('*' | scalar_list)('columns') + 'FROM' +
          scalar_list('tables') + Optional('WHERE' + where_clause)



-- Paul
#### 2009-05-20 06:31:22 - asb_india
Thanks a lot.



I am trying for sql parser. Will come here in case of success or doubts.
#### 2009-05-20 08:28:17 - ptmcg
Please check out some existing SQL parsing resources - google for 'pyparsing SQL' to get a look at a few parsers that have already been written.



Also, please start with at least a rudimentary BNF.  Otherwise, you will find yourself running in circles!



-- Paul

---
## 2009-05-29 16:18:10 - dbv - Windows amd64
Hi Paul



I just upgraded from Python 2.5 to Python 2.6.2 for Windows AMD64.  PyParsing 1.5.2 doesn't install and returns the message:



'No Python Installation Found In Registry'



I've also installed Numpy 1.3 for AMD64 and that doesn't work but Python by itself works fine.



Any ideas?



Dinesh

#### 2009-05-29 19:36:56 - ptmcg
Don't install the AMD64 versions just because your computer has a Athlon64 or other XXXion64 AMD chip.  You should only use them if you are running a 64-bit version of Windows or Linux.



Are you using easy_install to install pyparsing?  If this isn't working, try downloading the source .ZIP distribution from SourceForge and manually extract the distribution and run setup.py (see the README file for the full command line).



Otherwise, I don't have much to go on - there's no reason pyparsing should have problems with 2.6, and I haven't gotten any other error reports.



-- Paul
#### 2009-05-30 00:45:02 - dbv
I'm using a 64-bit Windows OS.  Using 'python setup.py install' from the zip distribution, I got this error:

...

running install

running build

running build_py

creating build

creating build\lib

copying pyparsing.py -\> build\lib

copying pyparsing_py3.py -\> build\lib

running install_lib

copying build\lib\pyparsing.py -\> C:\hp\bin\Python\Lib\site-packages

copying build\lib\pyparsing_py3.py -\> C:\hp\bin\Python\Lib\site-packages

byte-compiling C:\hp\bin\Python\Lib\site-packages\pyparsing.py to pyparsing.pyc

byte-compiling C:\hp\bin\Python\Lib\site-packages\pyparsing_py3.py to pyparsing_

py3.pyc

  File 'C:\hp\bin\Python\Lib\site-packages\pyparsing_py3.py', line 2470

    except ParseException as err:

                           ^

SyntaxError: invalid syntax



running install_egg_info

Writing C:\hp\bin\Python\Lib\site-packages\pyparsing-1.5.2-py2.5.egg-info

...



Dinesh
#### 2009-05-30 01:12:16 - ptmcg
Well this is a different error, that is progress.



This is not necessarily a fatal error.  The syntax exception occurs when trying to install the Python3-compatible version of pyparsing.  This version uses syntax that is not supported under Python 2.5 (although I thought it would be okay under Python 2.6...).  I don't know how to do version-dependent install commands in setup.py.



But even though you got this error during setup.py, your pyparsing installation for 2.5 should be complete - try it out.



-- Paul
#### 2009-05-30 14:16:22 - dbv
Hi!  I re-installed the Python 2.5.2 for the 64-bit AMD64 and pyparsing installs without a problem as before.  



The versions of Python 2.6.2, Numpy 1.3 and Pyparsing for the 64-bit Windows AMD64 all have problems.  It seems to me as if Python 2.6.2 is not getting registered in in the Registry which in turn pyparsing cannot sync with.  Anyway, I think it is a core Python and Numpy issue.



Dinesh
#### 2009-10-08 06:54:43 - dbv
Hi Paul



Since the last note I got sidetracked and just got back to installing PyParsing 1.5.2 on a Windows 64bit machine with Python 2.6.2.  It still fails to install and the output is:



C:\pyparsing-1.5.2\>python setup.py install

running install

running build

running build_py

creating build

creating build\lib

copying pyparsing.py -\> build\lib

copying pyparsing_py3.py -\> build\lib

running install_lib

byte-compiling C:\hp\bin\Python\Lib\site-packages\pyparsing_py3.py to pyparsing_

py3.pyc

  File 'C:\hp\bin\Python\Lib\site-packages\pyparsing_py3.py', line 2470

    except ParseException as err:

                           ^

SyntaxError: invalid syntax



running install_egg_info

Removing C:\hp\bin\Python\Lib\site-packages\pyparsing-1.5.2-py2.5.egg-info

Writing C:\hp\bin\Python\Lib\site-packages\pyparsing-1.5.2-py2.5.egg-info



I checked the site-packages directory and PyParsing hasn't installed.



Dinesh
#### 2009-10-08 13:53:30 - ptmcg
Are you looking for a directory named Pyparsing?  There isn't one.  Just two files named pyparsing.py and pyparsing_py3.py.  Try invoking the Python interpreter, and then typing (of course, leaving out the '\>\>\>' prompt):



<ul class="quotelist"><ul class="quotelist"><ul class="quotelist"><li>import pyparsing</li></ul></ul></ul>

Does this work?
#### 2009-10-08 14:48:52 - dbv
Hi Paul



I just copied the files pyparsing.py and pyparsing_py3.py into the site-packages folder and all is working.  Cheers ...



Dinesh

---
## 2009-06-02 05:36:06 - ivdkleyn - Problems with recursive parsing
Hi there,



Thanks for a great library. I bought the ebook a few days ago, hoping that it helps (it certainly helps me :-)



Saying that, I do encounter some problems, undoubtedly because I just cannot get my head around the underlying theory. 



Example: I have tried creating a parser for a minimal sub-set of Python. 



Here for example a snippet from the part of defining the parser which can cope with nested (repeated object attribute access). For example:





    attributeref_ = atom_ + OneOrMore('.' + identifier_)    
    primary_ =  attributeref_ | atom_  



it parses simple structures like ''var.amember' or 'var.amember.anotherChild'. This works well but does not support nested expressions. What you would expect is that the parser would support a nested definition like



primary_ = Forward()

attributeref_ = primary_ + '.' + identifier_

primary_ \<\< ( atom_ | attributeref_ )



Which would allow the inclusion of nested parse expressions. 

Instead it will raise ParseException: '--Expected stringEnd (at char 3), (line:1, col:4)' on a string  with valu...e









Down you will find the (unfinished) code.



Any hints or tips?



Regards,



Iwan van der Kleijn



    ############################################################3
    
    
    ## listing ##########################################################3

#number    

sign = Optional(oneOf('+ -'))    

nonzerodigits_ = '123456789'

digits_ = '0' + nonzerodigits_      

intpart_ = Word(nonzerodigits_,  digits_)

integer_  = ( sign + intpart_ ) | Literal('0')       

fraction_ = Word('.', digits_)

exponent_ = oneOf('e E') + sign + Word(digits_) #Python accepts 10#000004

pointfloat_ = Optional(integer_) + fraction_ | intpart_ + '.'    

expfloat_ = (pointfloat_ | intpart_ ) + exponent_  #( intpart_ | pointfloat_ ) + exponent_         

floatnumber_ = expfloat_ | pointfloat_    

number_ = floatnumber_ | integer_



#other literals                                         

null_ = Literal('null')

false_ = Literal('false')

true_ = Literal('true')



literal_ = dblQuotedString | number_ | null_ | false_ | true_              

identifier_ = Word(alphas + '_', alphas + nums + '_')

atom_ =  identifier_ | literal_  #could/should contain: enclosure    



#attributeref_ = atom_ + OneOrMore('.' + identifier_)    

#primary_ =  attributeref_ | atom_   #could/should contain: subscription |slicing | call



primary_ = Forward()

attributeref_ = primary_ + '.' + identifier_

primary_ \<\< ( atom_ | attributeref_ )



#### 2009-06-02 19:00:19 - ptmcg
Unless Python accepts '3. 14159' as a valid float, you should enclose your floating point number expressions inside pyparsing Combine classes:



    number_ = Combine(floatnumber_ | integer_)



Your definition of integer includes a leading sign, but the floatnumber doesn't.



Your recursion is probably not working because attributeref_ could start with an atom_ expression (an identifier).  Since atom_ is matched first, an attributeref_ will never get evaluated.  Try this instead:



    primary_ \<\< (attributeref_ | atom_)



If you download the Python source distribution, you can find the Python grammar file that will help you define your grammar using the same rules that the Python interpreter uses.



-- Paul
#### 2009-06-08 02:43:00 - ivdkleyn
Hi, thanks for the response. The main problem still exists though. It probably is due to my less than concise description of the problem.



If you check the following function you can see that I have taken the Python Grammar file and used it in a simplified form to translate it to PyParsing. For the purpose of the test I have limited the objective to parsing attribute accessors, including nested ones (i.e. 'root.member' and 'root.member.child'). 



I cannot get this to work (I'm using PyParsing 1.5.2 with Python 2.5). In the following code I'm getting a 'maximum recursion depth exceeded' error. When I'm  switching the forward declaration to 'attributeref_', the recursion error disappears, but then the nested attribute accessor is not recognized. 



This bothers me. I presumed PyParsing would support recursive definitions. I need to parse a simple python based expression language which should support constructs like root.member.member or (expr).member where 'expr' can contain other attribute accessors.



What would be the solution or is it indeed not possible?





    def guardGrammar():
        '''
        identifier ::= 
                 (letter|'_') (letter | digit | '_')*
    
        atom ::= 
                 identifier | literal 
    
        primary ::= 
                 atom | attributeref
    
    
        attributeref ::= 
                 primary '.' identifier
        '''
        identifier_ = Word(alphas + '_', alphas + nums + '_')
        literal_ = dblQuotedString
        atom_ = identifier_ | literal_
    
        primary_ = Forward()
        attributeref_ = primary_ + '.' + identifier_    
        primary_ \<\< (attributeref_ | atom_ )    
    
    
        return attributeref_



Regards,



Iwan van der Kleijn
#### 2009-06-09 06:38:01 - ptmcg
Once again, MatchFirst has gotten the better of you! : )



As-is, your recursive structure creates this infinite loop:

- a primary_ is an attributeref_ or something else

- an attributeref_ is a primary followed by more stuff



So pyparsing goes down this infinitely until it runs out of stack, like a never-ending nested Russian doll.



When you use Forward(), you need to guard against coming full-circle and testing for the same expression.  Change attributeref_ to:





    attributeref_ = identifier_ + '.' + primary_



and things are much better.



This is a common issue when directly converting a BNF to pyparsing.  BNF does not have a repetition syntax (like the '*' or '+' in regular expressions), so you get these recursive definitions like:



    list_of_arg = arg ',' list_of_arg | arg



Pyparsing provides a couple of builtin repetition structures, such as OneOrMore and ZeroOrMore, or the helper delimitedList.  You will be much better off with this kind of attribute parser:



    attributeref_ = Combine(ident + ZeroOrMore('.' + ident))



-- Paul

---
## 2009-06-03 01:32:24 - BlGene - C++ parser
I heard that some people are using pyparsing to parse c++ files, since these are rather complicated to parse I was wondering if a full, or even incomplete parser is avalible?

#### 2009-06-03 03:08:14 - BlGene
I the wiki would become easier to use if the current 'documentation' section werte to be renamed to 'Presentations'. Since the documentation in python is very good putting browsable documentation, of the type automatically generated from source code, into the Usage notes section, and renaming this section 'Documentation' would make it easier to find information about some minor functions. You might also consider moving 'Under Development' into a section of 'Examples' and putting the new 'Documentation', if you decide to change it, below 'Introduction' instead of below 'Examples'. That said the website is already very helpful.
#### 2009-06-03 09:53:27 - ptmcg
Thanks for the wiki organization suggestions.  One simple change that I could make quickly was to simply make 'Under Development' a navigational sub-bullet to the 'Examples' link in the menu.  I'm not sure how compatible wikispaces is with HTML docs, but there are several schools that maintain online copies of these docs, and I've added an external link to one of them on the Documentation page.



I also added a first cut at a C struct parser that I wrote ages ago, but it fell off the back burner and into the 'Someday' bin - maybe it will give you a start on your C++ parser.  I tacked this onto the 'Under Development' page, and added a Table of Contents, now that there are now <em>2</em> listings on that page!



Welcome to pyparsing, post back with any questions, or to have your pyparsing app added to the 'Who's Using Pyparsing' page!



-- Paul

---
## 2009-06-05 13:06:10 - ssscripting - need some help
Hello guys !



I have a file that contains many entries like this one :



define something

options are listed here

end



and I have the following script so far:



    define = Literal('define') + Word(alphanums)
    define_body = OneOrMore(Word(alphanums))
    define_end = Literal('end')
    entry = define + Optional('\n') + define_body + Optional('\n') + define_end

but, unfortunately, this throws an exception that says it was expecting 'end' at the final of the string. I think this comes up because of the OneOrMore(Word(alphanums)). Is there any way to write that OneOrMore(Word(alphanums)) should not include a certain string ( end in this case )?



I tried to do something like this :



    define_body = OneOrMore(Word(alphanums)) - Literal('end')



but this didn't work out the way I imagined it would.

#### 2009-06-05 16:18:23 - ptmcg
You are very close.  When you read this description:



    define something
    options are listed here
    end

try not to think of the whitespace as significant.  Here's a sample of what this could look like:



    define something option1 option2 option3 end



Now ask yourself how you know that 'end' is not a valid option.  You might answer, 'it just isn't!'  Okay, then let's redefine what your body of options is:



    define_body = OneOrMore(~define_end + Word(alphanums))



Now your OneOrMore expression will not include 'end' as one of your options.  Also, newlines are included in the default set of skippable whitespace, so you can simplify your definition of entry to:



    entry = define + define_body + define_end



I would also recommend that you add some results names to your expression - they make things *so* much easier when it comes time to picking apart the actual tokens parsed out of the input string.  Something like:



    entry = 'define' + Word(alphanums)('name') + define_body('body') + define_end



Now when you parse an input string, you can process the parsed tokens using these names like they were attributes of the parsed results:



    define_end = Keyword('end')
    define_body = OneOrMore(~define_end + Word(alphanums))
    entry = 'define' + Word(alphanums)('name') + define_body('body') + define_end
    
    result = entry.parseString('define something option1 option2 option3 end')
    print result.name
    print result.body

prints



    something
    ['option1', 'option2', 'option3']



or you could just write:



    print result.dump()

giving:



    ['define', 'something', 'option1', 'option2', 'option3', 'end']
    - body: ['option1', 'option2', 'option3']
    - name: something



Hope this help, welcome to pyparsing!



-- Paul
#### 2009-06-06 02:24:39 - ssscripting
Thank you Paul! I'm off to parsing more interesting things :)

---
## 2009-06-09 03:10:58 - nederhrj - [Newbie] Converting ad-hoc text into XML
Hello,



I am trying to convert a text template into XML. This task seems well suited for Pyparsing. However, due to my lack of knowledge of both python and pyparsing I continue experience problems. Is there somebody here who can help me?



I want to convert a text like this:



My name [is, 's] John, I [am, 'm] 20 years old and come [from] Birmingham, a big [city, town] in England.  \<...text continues...\>



in:





    \<ANSWER QTYPE='FIB'\>
        \<CONTENT TYPE='text/plain'\>\<![CDATA[My name ]]\>\</CONTENT\>
        \<CHOICE ID='0'\>
          \<CONTENT TYPE='NULL' WIDTH='2'\>\<![CDATA[]]\>\</CONTENT\>
        \</CHOICE\>
        \<CONTENT TYPE='text/plain'\>\<![CDATA[ John. I ]]\>\</CONTENT\>
        \<CHOICE ID='1'\>
          \<CONTENT TYPE='NULL' WIDTH='2'\>\<![CDATA[]]\>\</CONTENT\>
        \</CHOICE\>
        \<CONTENT TYPE='text/plain'\>\<![CDATA[ 20 years old and come ]]\>\</CONTENT\>
        \<CHOICE ID='2'\>
          \<CONTENT TYPE='NULL' WIDTH='4'\>\<![CDATA[]]\>\</CONTENT\>
        \</CHOICE\>
        \<CONTENT TYPE='text/plain'\>\<![CDATA[ Birmingham, a big ]]\>\</CONTENT\>
        \<CHOICE ID='3'\>
          \<CONTENT TYPE='NULL' WIDTH='4'\>\<![CDATA[]]\>\</CONTENT\>
        \</CHOICE\>
        \<CONTENT TYPE='text/plain'\>\<![CDATA[ in England.]]\>\</CONTENT\>
      \</ANSWER\>
      \<OUTCOME ID='0' ADD='1' CONTINUE='TRUE'\>
        \<CONDITION\>'0' MATCHES 'is' OR '0' MATCHES ''s'\</CONDITION\>
        \<CONTENT TYPE='NULL'\>\<![CDATA[is or 's]]\>\</CONTENT\>
      \</OUTCOME\>
      \<OUTCOME ID='1' ADD='1' CONTINUE='TRUE'\>
        \<CONDITION\>'1' MATCHES 'am' OR '1' MATCHES ''m'\</CONDITION\>
        \<CONTENT TYPE='NULL'\>\<![CDATA[am or 'm]]\>\</CONTENT\>
      \</OUTCOME\>
      \<OUTCOME ID='2' ADD='1' CONTINUE='TRUE'\>
        \<CONDITION\>'2' MATCHES 'city' OR '2' MATCHES 'town'\</CONDITION\>
        \<CONTENT TYPE='NULL'\>\<![CDATA[city or town]]\>\</CONTENT\>
      \</OUTCOME\>
      \<OUTCOME ID='3' ADD='1' CONTINUE='TRUE'\>
        \<CONDITION\>'3' MATCHES 'from'\</CONDITION\>
        \<CONTENT TYPE='NULL'\>\<![CDATA[from]]\>\</CONTENT\>
      \</OUTCOME\>
    \</ANSWER\>



You can find the code below that I have tried. It is not finished and does not generate the XML, but I would be really helped if I could get some pointers on how to create the BNF-like code and the XML-substitutions.





    import sys
    from pyparsing import *
    
    # BNF
    
    # parser for [word (,word, ...)]
    lbracket = Literal('[').suppress()
    rbracket = Literal(']').suppress()
    ident = Word(alphas+',.!?', alphanums)
    arg = ident
    args = Group(arg + ZeroOrMore(Literal(',').suppress() + arg))
    
    fib = Group(lbracket + args + rbracket)
    
    
    sentences = Forward()
    sentence = Group(ident + ZeroOrMore(ident)) | fib
    sentences \<\< sentence + ZeroOrMore(sentence)
    
    qmlstring = sentences.parseString( 'testh [a, b] Hello [c] big [b,o] big, World!' )
    
    print qmlstring
    
    for word in qmlstring:
        print qmlstring



Thank you in advance!



Rene

#### 2009-06-09 04:54:56 - nederhrj
Extra. My example code generates this:





    [['testh'], [['a', 'b']], ['Hello'], [['c']], ['big'], [['b', 'o']], ['big', ',', 'World', '!']]



To start IMO it should be like below so it should be 'easily' converted into xml. Right?





    ['testh', ['a', 'b'], 'Hello', ['c'], 'big', ['b', 'o'], 'big, World!']


#### 2009-06-09 05:42:34 - ptmcg
This is a case where parseString is not the best choice.  You have an expression that you can fairly precisely define (a list of choices within brackets), and then 'everything else'.  In this case, scanString is a more suitable path to take.



scanString returns a generator that scans through the input string, skipping over text that does not match your expression, and then gives you a tuple for each match.  The tuple contains:

- the matched tokens

- the start location of the match

- the end location of the match



Then, call scanString within a for loop (something like this):



    def printout(s):
        print '|%s|' % s
    
    test = 'testh [a, b] Hello [c] big [b,o] big, World!'
    
    last = 0
    for t,s,e in fib.scanString(test):
        printout(test[last:s])
        printout(t)
        last = e
    printout(test[last:])



where fib is your expression:



    fib = (lbracket + args + rbracket)

(no need for Group any more).



This prints out:



    |testh |
    |['a', 'b']|
    | Hello |
    |['c']|
    | big |
    |['b', 'o']|
    | big, World!|



From here, you can build up two lists, a list of intervening text and a list of the choices, with something like this:



    intervening = []
    choices = []
    
    last = 0
    for t,s,e in fib.scanString(test):
        intervening.append( test[last:s] )
        choices.append(t)
        last = e
    intervening.append(test[last:])



Now you can walk these two lists to generate your XML output.



Also, your definition of args could be simpler - pyparsing defines a helper for created comma-delimited lists, called delimitedList.  I defined args this way:



    argchars = printables.replace(',','').replace(']','')
    arg = Word(argchars)
    args = delimitedList(arg)



HTH and Welcome to Pyparsing!

-- Paul
#### 2009-06-09 05:44:46 - ptmcg
Replacing your test string with your initial example, I get:



    |My name |
    |['is', ''s']|
    | John, I |
    |['am', ''m']|
    | 20 years old and come |
    |['from']|
    | Birmingham, a big |
    |['city', 'town']|
    | in England.|



-- Paul
#### 2009-06-09 07:54:07 - nederhrj
Thank you Paul for a great parser and wonderful support. I am going to try your solution.

---
## 2009-06-11 02:24:51 - WernerBruhin - PayPal email - again
Please ignore the sourceforge post, I try to rephrase my problem here.



PayPal is really having fun in constantly changing the format of the email they send on a purchase. 



I have a problem with the latest change. 



The address portion can be: 

VERSION1 begin: 

Buyer: 

Roman Schiller 

roman.schiller=40edvconsult.com 

436764214725 



Unconfirmed shipping address 

Roman Schiller 

Wallensteinstra=DFe 56/21 

A-1200 Wien 

Austria 



Instructions to merchant: 



VERSION1 end: 



Or 



VERSION2 begin: 

<hr />
 JAN ONDERDIJK's UNCONFIRMED Address 

<hr />
 

JAN ONDERDIJK 

VOSHOLLEI 23 

BRASSCHAAT 2930 

Belgium 





Have you lifted your withdrawal and receiving limits?  



VERSION2 end: 



There are actually a few more variations which I try to handle with this code (only showing snippets): 





    # address
    str_address = (((pyp.Suppress('UNCONFIRMED Address') |\
                     pyp.Suppress('CONFIRMED Address') |\
                     pyp.Suppress('unconfirmed address')) |\
                     pyp.Suppress('Unconfirmed shipping address')  +\
                     pyp.Suppress(pyp.Word('-')))  +\
                    ((addLine + addLine + addLine + addLine +\
                      emptyLine.suppress()) |\
                     (addLine + addLine + addLine + addLine + addLine +\
                      emptyLine.suppress()) |\
                     (addLine + addLine + addLine + addLine + addLine +\
                      addLine + emptyLine.suppress()) ))

 

What are ways to simplify the above?



- Can I make things case insensitive in some way, e.g. just look for 'confirmed address' or 'unconfirmed' + anyword or something similar. 



- Also tried to use pyp.Optional instead of: 

pyp.Suppress(pyp.Word('-') 



But I can not get this to work. 



Appreciate a few tips on how I could go about this. 



Thanks 

Werner

#### 2009-06-11 03:34:38 - WernerBruhin
Looking through the doc I found CaselessLiteral, so now I have:





    # address
    str_address = ((pyp.Suppress(pyp.CaselessLiteral('confirmed address')) |\
                    pyp.Suppress(pyp.CaselessLiteral('confirmed shipping address')) |\
                    pyp.Suppress(pyp.CaselessLiteral('ship-to address:')) ) +\
                    pyp.Suppress(pyp.Optional(pyp.Word('-'))) +\
                    ((addLine + addLine + addLine + addLine + emptyLine.suppress()) |\
                    (addLine + addLine + addLine + addLine + addLine + emptyLine.suppress()) |\
                    (addLine + addLine + addLine + addLine + addLine + addLine + emptyLine.suppress()) ))
    



A bit better, but I wonder if this can't be improved more.



I thought there was a way of combining the 'addLine' stuff, but I can not find it at the moment.



Werner
#### 2009-06-11 03:55:18 - WernerBruhin
Scanning the Sourceforge archive I found the previous exchange we had on this early last year.



Tried this:





    ((addLine*(4, 6) + emptyLine.suppress() )



But this only works if I use addLine*(4), so the range does not work, unless I do something wrong or maybe my version (using 1.4.11)?



Werner
#### 2009-06-11 04:10:11 - WernerBruhin
Just tried with 1.5.2 but still the same, anyhow the code is now this:





    # address
    str_address = ((pyp.Suppress(pyp.CaselessLiteral('confirmed address')) |\
                    pyp.Suppress(pyp.CaselessLiteral('confirmed shipping address')) |\
                    pyp.Suppress(pyp.CaselessLiteral('ship-to address:')) ) +\
                    pyp.Suppress(pyp.Optional(pyp.Word('-'))) +\
    ##                ((addLine*(4, 6) + emptyLine.suppress() ))) # would be nice
                    ((addLine*(4) + emptyLine.suppress()) |\
                    (addLine*(5) + emptyLine.suppress()) |\
                    (addLine*(6) + emptyLine.suppress() )))



Werner
#### 2009-06-11 11:21:40 - ptmcg
Werner -



Can't completely diagnose what's happening here, but here are some points:



You can clean up str_address if you define some of the pieces as separate sub-expressions.  Also, you can minimize some of the () nesting and pyp. namespace refs if you use 'expr.suppress()' instead of 'pyp.Suppress(expr)' - the two are equivalent.



    label = pyp.oneOf( 'confirmed address;'
                       'confirmed shipping address;'
                       'ship-to address:'.split(';'), caseless=True)
    str_address = ( label.suppress() +
                    pyp.Optional(pyp.Word('-')).suppress() +
                    addLine*(4, 6) + emptyLine.suppress() )



I had no difficulty multiplying an expression by a tuple:



    x4_6 = pyp.Literal('X')*(4,6)
    print x4_6.parseString('XXXX')
    print x4_6.parseString('XXXXX')
    print x4_6.parseString('XXXXXX')
    print x4_6.parseString('XXXXXXXX')
    print x4_6.parseString('XXXYXXX')



prints:



    ['X', 'X', 'X', 'X']
    ['X', 'X', 'X', 'X', 'X']
    ['X', 'X', 'X', 'X', 'X', 'X']
    ['X', 'X', 'X', 'X', 'X', 'X']
    Traceback (most recent call last):
      File 'werner1.py', line 48, in \<module\>
        print x4_6.parseString('XXXYXXX')
      File 'C:\Documents and Settings\Paul\Desktop\pyparsing.py', line 1075, in parseString
        raise exc
    pyparsing.ParseException: Expected 'X' (at char 3), (line:1, col:4)

I suspect an issue with addLine is defeating this in your code.



If you need to hack around expr*(4,6) with a succession of expr*n terms, you must start with the longest and work your way down:



    expr*6 | expr*5 | expr*4


#### 2009-06-12 02:49:23 - WernerBruhin
Paul,



Thanks for looking at this.



My addLine is:

addLine = pyp.Combine((pyp.OneOrMore(alphanums) + pyp.restOfLine))



Anyhow, I adapted your tips and now I have:





    addressStart = pyp.oneOf('confirmed shipping address;' 
                             'confirmed address;'
                             'ship-to address:'.split(';'), caseless=True)
    
    emptyLine = pyp.oneOf('have you lifted your withdrawal;'
                          'thank you for using paypal;'
                          'instructions to;'
                          'address status;'
                          'sincerely,'.split(';'), caseless=True) + pyp.restOfLine
    
    ....
    # address
    str_address = ((addressStart.suppress() +\
                    pyp.Optional(pyp.Word('-')).suppress())  +\
    ##                       addLine*(4, 6) + emptyLine.suppress() )
                    (addLine*(4) + emptyLine.suppress()) |\
                    (addLine*(5) + emptyLine.suppress()) |\
                    (addLine*(6) + emptyLine.suppress()) )
    



I tried the commented line, and reversed the '4', '5', '6' lines, but that did not help with the following case:



In the debug I get:

...

Match address at loc 802(26,35)

Exception raised:Expected 'confirmed shipping address' (at char 802), (line:26, col:35)

Match address at loc 827(29,1)

Matched address -\> ['ejeanneret@romandie.com', '0244363539', 'Ship-to address: Eric Jeanneret', 'Chemin des Raytolats', 'CH-1423 Villars-Burquin', 'Switzerland']

Match address at loc 982(37,1)

...



I don't get it why I have the email, number and ship-to in the matched address.



From this:

<hr />
SHIPPING INFORMATION

<hr />


Buyer: Eric somename

somename=40romandie.com

0244363539

Ship-to address: Eric somename

Chemin des Raytolats

CH-code somevillage

Switzerland



Address status: Unconfirmed =20

Shipping method: Not specified



I thought it might suppress the first and last name as they are on the same line as 'ship-to' but instead it gets two more lines in front of the 'ship-to'.



I hope you can help on this.



Thanks

Werner
#### 2009-06-12 03:52:04 - WernerBruhin
Paul,



Got it, my () were not right.



Correcting it to:





    addressStart = pyp.oneOf('unconfirmed shipping address;' 
                             'confirmed shipping address;' 
                             'confirmed address;'
                             'ship-to address:'.split(';'), caseless=True)  +\
                             pyp.Optional(pyp.Word('-')).suppress()
    
    emptyLine = pyp.oneOf('have you lifted your withdrawal;'
                          'thank you for using paypal;'
                          'instructions to;'
                          'address status;'
                          'sincerely,'.split(';'), caseless=True) + pyp.restOfLine
    
    ....
    # address
    str_address = (addressStart.suppress() +\
    ##                        addLine*(4, 6) + emptyLine.suppress() )
                  (addLine*(4) + emptyLine.suppress() |\
                   addLine*(5) + emptyLine.suppress() |\
                   addLine*(6) + emptyLine.suppress()) )
    



Tried with the 4, 6 but that I don't get to work, but above is a lot nicer/cleaner then what I had before and also much easier to maintain next time PayPal changes something.



Thanks again for this great tool

Werner

---
## 2009-06-22 02:10:24 - asb_india - Parsing Selected values
Hello 



If i want to parse words with exception. Let mne explain with example



I have function say sum , min max etc .... and i want the normal text shld not be any of this But it can min_abc etc ... 



Is this possible.



I tried like this 



<ul class="quotelist"><ul class="quotelist"><ul class="quotelist"><li>func = oneOf('min max avg sum')</li><li>text = ~func + Word(alphanums + '_' +'.')</li><li>text.parseString('min_value')</li></ul></ul></ul>

But this will not parse min_value and will give error



Thnks in advance

#### 2009-06-22 09:23:21 - ptmcg
Because 'min_value' starts with 'min', which is one of the things you are trying not to match.



Perhaps you meant not to match 'min' if it was a word by itself?  There are two ways to do this:



1. expand func to include an ending word boundary after min, max, etc.:





    func = oneOf('min max avg sum')+WordEnd()



or define func as a list of Keywords:





    func = MatchFirst(map(Keyword,'min max avg sum'.split()))





2. add a parse action to text that raises an exception if any function name is matched:





    def nobuiltins(tokens):
        if tokens[0] in 'min max avg sum'.split():
            raise ParseException,'word cannot be a built-in name'
    text = Word(alphanums + '_' +'.')
    text.setParseAction(nobuiltins)





-- Paul
#### 2009-06-22 09:44:27 - asb_india
Thnks  :)

---
## 2009-06-25 17:51:45 - GrahamDennis - Inconsistent results
Hi,



I have a function that uses a pyparser to extract some results. Each time the function is called it constructs a new parser. (The code is available at  ). The strange thing is that I get different results out of this function depending on if I have called it before, and what with.



As a demonstration, in the linked code I have two testcases. If either is commented out, the other succeeds. But if both are left, 'test_ignoreChildComment' fails. What confuses me is that because I construct a new parser each time, I wouldn't expect what I have called before to influence future results.



I'm using pyparser 1.5.2 with Python 2.5.



Any help would be appreciated.

#### 2009-06-26 05:46:26 - GrahamDennis
Hi,



I eventually managed to fix the problem, but I don't know if this is a bug or not that I've found. The solution I found was to add '.copy()' to cppStyleComment and quotedString on line 14. The new line reads:



ignoreExpr = cppStyleComment.copy() | quotedString.copy()
#### 2009-06-27 04:46:33 - ptmcg
I've looked at this code for a while, and I'm having a tough time seeing how I could make pyparsing figure this out without your having to explicitly copy the quotedString and cppStyleComment built-ins.



The code problem stems from these lines:



    parser.ignore(cppStyleComment)
        parser.ignore(quotedString)



parser.ignore(expr) has to recursively call ignore() on all elements within the parser, so that nested structures within the parser also ignore the given expr.  For example, in this parser:



    parser = Literal('A') + 'B' + 'C'
        parser.ignore(''')

parser is really an And, containing 3 Literals.  By calling ignore, we want any of these to match:



    ABC
    A'B'C
    'A'B'C
    'ABC

If ignore doesn't call ignore on all of the contained expressions, then only the last 'ABC would match this little parser.



At a more conceptual level, there is just a <em>lot</em> of ignoring going on here!  At the parser level, you want to ignore any comments or quoted strings that might contain a matching string.  Within your defined baseExpr, you also want to suppress any comments or quoted strings within the nested []'s or ()'s.  And finally, nestedExpr itself uses the ignoreExpr to not match closing nesting delimiters that might be enclosed in quotes or a comment.  Since cppStyleComment and quotedString are used within parser, and then parser is told to ignore those expressions, these expressions never get to actually do their proper jobs within nestedExpr, because they essentially <em>ignore themselves</em>.



Your example <em>did</em> highlight for me a subtle bug in nestedExpr, and that is that the default value for ignoreExpr is given as quotedString - I should really change that to quotedString.copy(), to try to avoid similar side-effect problems.



Thanks for posting!



-- Paul
#### 2009-06-27 05:29:42 - GrahamDennis
Thanks for your help Paul. I now have a better understanding of what's going on here.



Am I right in thinking that the safe way of calling .ignore then is with a copy unless the parsing element will only be used in .ignore()? If this is the case, would it be safe to make .ignore() always copy its argument?



Cheers,

Graham
#### 2009-06-27 05:46:34 - ptmcg
Well, this certainly wouldn't hurt, and I've made that change in the code for the next release.  I'm not sure it catches all the corner cases, but its an improvement, and no harm to existing code.



-- Paul

---
## 2009-06-27 05:38:30 - GrahamDennis - C statement parser design / max recursion depth exceeded
Hi,



I've been trying to put together a fairly basic parser for C statements, and while it works fine for a range of statements, it seems to choke on more complex ones due to a recursion depth error. I'm guessing the correct way to solve this is to modify the way I have defined my parser. So if anyone has any suggestions, I'm all ears!



The parser is at  . I've commented out a simple testcase which produces expected results. The second test which is uncommented is the one causing the maximum recursion depth exceeded error.



I've defined my parser using operatorPrecedence (which rocks by the way!), and if I comment out a few of the operators that aren't used then the parsing goes OK. This would be a reasonable solution as I don't expect to use all of the operators anyway, but if there is a general flaw in the design then I'd love to hear it.



Also, this parser uses the packrat parser as otherwise the runtime is far too long.



Cheers,

Graham

#### 2009-06-27 17:24:57 - ptmcg
Which operators do you comment out to 'fix' the recursion problem?  This will help me in tracking down the offending code.



BTW, I suggest you modify your bitwise operators slightly, so that they don't accidently read the first character of a two-character operator - change:



    ('&', 2, opAssoc.LEFT),
            ('^', 2, opAssoc.LEFT),
            ('|', 2, opAssoc.LEFT),

to:



    (~Literal('&&') + '&', 2, opAssoc.LEFT),
            ('^', 2, opAssoc.LEFT),
            (~Literal('||') + '|', 2, opAssoc.LEFT),



This by itself doesn't fix the problem, but may help you avoid some others in the future.



-- Paul
#### 2009-06-27 18:06:33 - GrahamDennis
Hi Paul,



I haven't tested exhaustively, but if I comment out any two operators that I'm not using, the recursion problem goes away. What surprises me is that this problem goes away if I comment out two operators that use characters that aren't even in the source string. For example, if I make the operator list:





        (oneOf('++ --'), 1, opAssoc.LEFT),

        # (oneOf('. -\>'), 2, opAssoc.LEFT),

        (oneOf('+ - ! ~ * & ++ --'), 1, opAssoc.RIGHT),

        (oneOf('* / %'), 2, opAssoc.LEFT),

        (oneOf('+ -'), 2, opAssoc.LEFT),

        # (oneOf('\<\< \>\>'), 2, opAssoc.LEFT),

        (oneOf('\< \<= \> \>='), 2, opAssoc.LEFT),

        (oneOf('== !='), 2, opAssoc.LEFT),

        (~Literal('&&') + '&', 2, opAssoc.LEFT),

        ('^', 2, opAssoc.LEFT),

        (~Literal('||') + '|', 2, opAssoc.LEFT),

        ('&&', 2, opAssoc.LEFT),

        ('||', 2, opAssoc.LEFT),

        (('?',':'), 3, opAssoc.RIGHT),

        (oneOf('= += -= *= /= %= \<\<= \>\>= &= ^= |= =\>'), 2, opAssoc.RIGHT),

        (',', 2, opAssoc.LEFT),





then the problem goes away despite the string not containing any of the '.', '\<' or '\>' characters.



Cheers,



Graham
#### 2009-06-27 19:16:45 - ptmcg
Graham - 



Well, we seem to be pushing the envelope on recursion!  I tried just adding a nonsense operator and the problem seems to be just how many levels are defined.  So I doubled the default recursion limit from 1000 to 2000, and the full opPrec expression is now parseable.



Here is the latest version of operatorPrecedence, to avoid masking a longer operator with a shorter one:



    expr \<\< operatorPrecedence(
        operand,
        [
            (oneOf('++ --'), 1, opAssoc.LEFT),
            (oneOf('. -\>'), 2, opAssoc.LEFT),
            (~oneOf('-\> -= += *= &= |=') + oneOf('+ - ! ~ * & ++ --'), 1, opAssoc.RIGHT),
            (~oneOf('*= /= %=') + oneOf('* / %'), 2, opAssoc.LEFT),
            (~oneOf('++ -- -\> -= +=') + oneOf('+ -'), 2, opAssoc.LEFT),
            (~oneOf('\<\<= \>\>=') + oneOf('\<\< \>\>'), 2, opAssoc.LEFT),
            (~oneOf('\<\< \>\> \<\<= \>\>=') + oneOf('\< \<= \> \>='), 2, opAssoc.LEFT),
            (oneOf('== !='), 2, opAssoc.LEFT),
            (~oneOf('&& &=') + '&', 2, opAssoc.LEFT),
            ('^', 2, opAssoc.LEFT),
            (~oneOf('|| |=') + '|', 2, opAssoc.LEFT),
            ('&&', 2, opAssoc.LEFT),
            ('||', 2, opAssoc.LEFT),
            (('?',':'), 3, opAssoc.RIGHT),
            (~Literal('==') + oneOf('= += -= *= /= %= \<\<= \>\>= &= ^= |= =\>'), 2, opAssoc.RIGHT),
            (',', 2, opAssoc.LEFT),
        ]
    )



and here is the code to change the recursion limit (and a nicer looking way to dump the parse tree):



    import sys
    sys.setrecursionlimit(2000)
    
    from pprint import pprint
    try:
        pprint (statement.searchString('dphi1_dt = T[phi1] - 1.0/hbar*(V + Uint*(mod2(phi1) + mod2(phi0)) - eta*mu)*phi1;').asList()) # maximum recursion depth exceeded.
    except RuntimeError:
        print 'exceeded recursion limit again!'



Prints:



    [[['dphi1_dt',
       '=',
       ['T',
        ['[', 'phi1', ']'],
        '-',
        ['1.0',
         '/',
         'hbar',
         '*',
         ['V',
          '+',
          ['Uint',
           '*',
           ['mod2', ['(', 'phi1', ')'], '+', 'mod2', ['(', 'phi0', ')']]],
          '-',
          ['eta', '*', 'mu']],
         '*',
         'phi1']]]]]



-- Paul
#### 2009-06-27 20:04:41 - GrahamDennis
Paul,



Are you suggesting making the modifications to avoid masking longer operators with shorter ones for performance reasons? I don't see that they are necessary to ensure validity. For example, consider the following expression:



    A && B



If the parser tries to treat the second '&' as a unary '&' it is then left with the invalid expression:



    A & ( &B )

which is invalid and the parser must then backtrack.



Thanks for your suggested modifications, I'm just trying to understand the reason for your changes as it is slightly less maintainable.



Also, I'd like to avoid changing the recursion limit if possible as that seems to be going about it the wrong way. In this particular case, I wonder if an optimisation is possible because I have a large contiguous set of operators which are of the same type (binary, left-associative) but just different operators. I'll take a look at the definition of operatorPrecedence to see if I can think of anything.



Thanks very much for your help.



Graham
#### 2009-06-27 20:19:56 - GrahamDennis
I'm an idiot.



I just tried my exact suggestion and without your changes the parser does return 'A & (& B)', which is not clearly invalid. I clearly do need to prevent masking longer operators with short ones.



I have another question. Why are the 'FollowedBy' expressions needed in operatorPrecedence? None of the examples that implement operator precedence manually without using operatorPrecedence use FollowedBy.



Thanks,

Graham
#### 2009-06-28 18:21:39 - ptmcg
The FollowedBy expressions were added back around pyparsing 1.4.7.  They are there to prevent running parse actions at intermediate levels of the precedence list if the parse match is really at a lower level.

---
## 2009-06-30 05:30:38 - ptmcg - DSL article feedback (Mark Fink)
(received by e-mail from Mark Fink, before this wiki page was available)



Hi Paul,



I could not wait to get the article on DSL implementation in Python. Unfortunately I had to find out soon that your approach does not work on current/new Python implementations (2.6.2 is standard in Ubuntu 9.04). imputil is deprecated in 2.6 and will be removed in 2.7. Maybe nobody cares besides me because it does not even work in 2.6:



    \>\>\> import stateMachine
    \>\>\> import trafficLight
    Traceback (most recent call last):
      File '\<stdin\>', line 1, in \<module\>
      File '/usr/lib/python2.6/imputil.py', line 106, in _import_hook
        top_module = self._import_top_module(parts[0])
      File '/usr/lib/python2.6/imputil.py', line 193, in _import_top_module
        module = self.fs_imp.import_from_dir(item, name)
      File '/usr/lib/python2.6/imputil.py', line 546, in import_from_dir
        result = self._import_pathname(_os_path_join(dir, fqname), fqname)
      File '/usr/lib/python2.6/imputil.py', line 583, in _import_pathname
        return importFunc(filename, finfo, fqname)
      File 'stateMachine.py', line 169, in get_state_machine
        COMPILE_MODE)
      File 'trafficLight.pystate', line 2
    
        ^
    SyntaxError: invalid syntax
    Error in sys.excepthook:
    Traceback (most recent call last):
      File '/usr/lib/python2.6/dist-packages/apport_python_hook.py', line 38, in apport_excepthook
        from apport.packaging_impl import impl as packaging
      File '/usr/lib/python2.6/imputil.py', line 106, in _import_hook
        top_module = self._import_top_module(parts[0])
      File '/usr/lib/python2.6/imputil.py', line 193, in _import_top_module
        module = self.fs_imp.import_from_dir(item, name)
      File '/usr/lib/python2.6/imputil.py', line 548, in import_from_dir
        return self._process_result(result, fqname)
      File '/usr/lib/python2.6/imputil.py', line 307, in _process_result
        exec code in module.__dict__
      File '/usr/lib/python2.6/dist-packages/apport/__init__.py', line 1, in \<module\>
        from apport.report import Report
      File '/usr/lib/python2.6/imputil.py', line 135, in _import_hook
        return importer._finish_import(top_module, parts[1:], fromlist)
      File '/usr/lib/python2.6/imputil.py', line 231, in _finish_import
        bottom = self._load_tail(top, parts)
      File '/usr/lib/python2.6/imputil.py', line 327, in _load_tail
        m = self._import_one(m, part, fqname)
      File '/usr/lib/python2.6/imputil.py', line 277, in _import_one
        module = self._process_result(result, fqname)
      File '/usr/lib/python2.6/imputil.py', line 307, in _process_result
        exec code in module.__dict__
      File '/usr/lib/python2.6/dist-packages/apport/report.py', line 14, in \<module\>
        import subprocess, tempfile, os.path, urllib, re, pwd, grp, os, sys
      File '/usr/lib/python2.6/imputil.py', line 106, in _import_hook
        top_module = self._import_top_module(parts[0])
      File '/usr/lib/python2.6/imputil.py', line 193, in _import_top_module
        module = self.fs_imp.import_from_dir(item, name)
      File '/usr/lib/python2.6/imputil.py', line 548, in import_from_dir
        return self._process_result(result, fqname)
      File '/usr/lib/python2.6/imputil.py', line 307, in _process_result
        exec code in module.__dict__
      File '/usr/lib/python2.6/subprocess.py', line 359, in \<module\>
        import gc
      File '/usr/lib/python2.6/imputil.py', line 109, in _import_hook
        raise ImportError, 'No module named ' + fqname
    ImportError: No module named gc
    
    Original exception was:
    Traceback (most recent call last):
      File '\<stdin\>', line 1, in \<module\>
      File '/usr/lib/python2.6/imputil.py', line 106, in _import_hook
        top_module = self._import_top_module(parts[0])
      File '/usr/lib/python2.6/imputil.py', line 193, in _import_top_module
        module = self.fs_imp.import_from_dir(item, name)
      File '/usr/lib/python2.6/imputil.py', line 546, in import_from_dir
        result = self._import_pathname(_os_path_join(dir, fqname), fqname)
      File '/usr/lib/python2.6/imputil.py', line 583, in _import_pathname
        return importFunc(filename, finfo, fqname)
      File 'stateMachine.py', line 169, in get_state_machine
        COMPILE_MODE)
      File 'trafficLight.pystate', line 2
    
        ^
    SyntaxError: invalid syntax



A good article would show how to handle the 2.6 problems. Otherwise you should state that this only works with 2.5 (I hope it does).

From my perspective DSL is a very important topic and you are definitely hurting the Python community in demonstrating that building DSLs in Python is a real burden.



I am disappointed that the PythonMagazine lets you reprint a topic from 2006 without a major overhaul of the approach.







Best Regards,

Mark Fink

#### 2009-06-30 05:31:35 - ptmcg
Mark -



I'm certainly glad to get your feedback on my article, and I'm sorry for the compatibility issues you bring up.



Personally, I have kept my own environment at Python 2.5.2, in an effort to not accidentally use new language features that would put pyparsing out of the reach of users running on older versions of Python.  (I've tried to maintain compatibility back to Python 2.3.2.)  The Py3 compatibility releases of pyparsing have been mostly the work of contributors who have moved onto that version, and submitted changes back to me.  I have then incorporated their changes as best I can, while minimizing the differences between the Py2 and the Py3 code.



The body of my article *does* make mention of the imputil module's impending demise in Py3.x, but its removal from the v2.7 release is news to me.  I had assumed that the Python development team was going to stick to their promise of reserving any changes that would break 2.x (for x\<=5) code until Py3.0 and later.



I really haven't had a centralized place to capture feedback or errata on my articles or e-book, so I just added this page to the pyparsing wiki: .  I'll add your e-mail as a comment on the Discussion tab, and I'll try to address your questions and post a forward-compatible version of imputil that will at least minimally support the code examples in the article. (If you have already made any progress on an updated imputil, that would be greatly appreciated, and would make a great addition to the wiki!)



I see that you googled for pyparsing DSL examples and found my post to comp.lang.python from 2006.  That material, consisting of little more than the source code itself, was posted on my home page for about 3 years, but I didn't get *any* feedback on it.  Also, the article for Python Magazine includes a new example for the library book state handling, and nearly 5000 words of supporting explanatory information, on not only the Python code, but also the application's State pattern implementation.  The article content is a substantial expansion beyond my original c.l.py post, so I don't feel that I have done any disservice to Python Magazine or its readers.



I am baffled at how my article is bringing harm to the Python community.  I would think that imputil would be a natural fit to support implementation of DSL's, and it is a shame that it is being dropped - the only reasons I've heard for this is that it is generally perceived to be an unused module.  So perhaps this article will draw some attention to the plight of imputil, and rally some community support for it.  



Otherwise, you can still develop Python-based DSL's using object notation and operator overloading.  In that case, imparting domain-specific concepts to Python come from a suitable API design (like SQLAlchemy, for instance).



Thanks for writing,

-- Paul

---
## 2009-06-30 05:41:50 - ptmcg - OnLAMP article feedback (bearophile)
Hello, I have just read your quite interesting article:





Maybe something this can simplify the API a little:

.parseString() =\> .parse()

.setResultsName('salute')  =\> .setname()



APIs are very important, as you know :-) They are the 'user interface'

of a software.



I don't like this much:

SkipTo(tdEnd)

Maybe a better name can be found, random text? unspecified()? I don't

know...



Do Pyparsing compiles like RE? If not then this can be very useful (to

speed up searches), but I presume it's not an easy thing to do.

This is a very nice example of 'compiling' REs, from lwc:





Do you remember my post about reverb?





This is an interesting article about Perl6 REs:





Their syntax looks better than the old Perl REs used by CPython too.

Maybe CPython can copy Perl6 REs :o)



The people that create Mathematica are very intelligent, so looking at

their solutions is always interesting:







etc.



Bear hugs,

bearophile



-- 

 - Faster than the air-speed velocity of an

                          unladen european swallow

#### 2009-06-30 05:43:27 - ptmcg
Bearophile -



Grazie per tutte le vostre osservazioni!  (I assume you are Italian, since you referred me to the Italian Google page.) :)



Yes, I do remember exchanging notes regarding reverb.  I suspect that a proper 'verbification' of RE could end up making pyparsing largely unnecessary.  Still, pyparsing adds some features (recursion, parse actions) that are difficult or impossible with REs.



I'm afraid most of pyparsing's warts come from its organic development process.  I started with some basic design concepts for the parsing classes, but I'm not sure I anticipated all the directions that pyparsing would grow when I did this.



Absolutely, the API is king when designing a library such as pyparsing.  It dictates how much mental translation the user has to go through to map his application needs to the library's defined functions.  I feel that a subtle part of this API (and one of pyparsing's strengths) is the use of *a few* operator overloads to make the And's, Or's, etc. more readable in the client code.  By contrast, the API to regexp's is so complicated (and confusing between re symbology vs. character literals - God help you if one of the re special characters is part of your input string, the re ends up backslashed to death!) that whole books and websites are spawned to tutor and evaluate a given re's behavior.



parseString-\>parse seems like an achievable improvement without breaking backwards compatibility.  It turns out that ParserElement class already has a parse() method, but it is only used internally.



setResultsName -\> setName is not so easily done.  ParserElements already have a setName method, and the distinction between the two is subtle:

. setName - gives a name to the expression itself, and is used in debugging and exception messages

. setResultsName - gives a name to the tokens matched by the expression, and returned in the ParseResults

I too thought setResultsName was a little unwieldy, but could not really come up with a better one.



SkipTo is another interesting one.  I also considered naming it something like 'EverythingUpTo', 'AnythingUntil' but decided SkipTo was close enough, and much shorter than anything else. (Readable brevity is also good in an API.  Shorter names allow a larger gestalt when viewing the code - plus programmers don't really like typing thisIsALongMethodNameThatExplainsExactlyWhatTheFunctionDoes() when doFunction() is close enough.  But *nix takes this idea to too much of an extreme, with ls,cp,grep,etc.  Brevity and readability are almost always at odds, and sometimes the compromise between the two doesn't satisfy anybody.)



Pyparsing expressions *sort of* compile.  This is done behind the scenes using the internal streamline() method.  For example, when composing an And using '+' operators, as in expr = A + B + C + D, the resulting expression is And(And(And(A,B),C),D).  Streamline collapses these degenerately-nested Ands to And(A,B,C,D).  (Streamline is called internally by parseString, and a streamlined expression knows not to streamline itself more than once.)  Many of the constructors of the various types also do some pre-optimization.  The Word class generates and compiles an re for doing its matching (new in the latest version, 1.4).  I've had one person have some success in pickling a large pyparsing grammar, and have better performance unpickling the grammar than in composing it from scratch.



Is there any chance you will be at PyCon?  I am presenting two papers there (I submitted two hoping to get one in, and they called my bluff and accepted them both!) - if you can't attend, I'll send them to you separately.



Thanks again for your comments,

-- Paul
#### 2009-12-23 03:06:29 - ptmcg
Since the time of this e-mail exchange between Bearophile and me, pyparsing has adopted or addressed some of these points:



- `expr.setResultsName(&quot;name&quot;)` can now be abbreviated as `expr(&quot;name&quot;)`.

- a number of pyparsing expressions (notably Word and oneOf) use regular expressions internally to implement their matching logic



-- Paul

---
## 2009-06-30 05:45:53 - ptmcg - OnLAMP article feedback (Alex Shinn)
A friend of mine was asking me how to implement a parser for a simple

tree of data.  I immediately explained the regexp techniques he was

trying would never work, and that what he wanted was a recursive

descent parser.  Since he was using Python, I did a Google search and

your article was the first result:



  



In it you make the claim



  Pyparsing is a Python class library that helps you to quickly and

  easily create recursive-descent parsers.



and the rest of the article deals with non-recursive examples.



Pyparsing looks like a very nice and useful library, but it does not

allow recursive descent parsing (even if perhaps it's implemented

using that technique, I didn't look).  What it does is essentially let

you concatenate regular patterns, so you never get above the level of

expressivity of regular expressions.  A recursive descent parser, on

the other hand, lets subpatterns refer to other subpatterns, in a

possibly cyclic fashion, so that, for example, you could have an EXPR

clause that could expand into other EXPR clauses.  This lets you parse

all context-free grammars, such as XML.  Programming languages like C,

on the other hand, tend to be context-sensitive or even more complex,

and are usually handled by LR1 parsers like Yacc.



Pyparsing may well be useful as a tokenizer to be used by a recursive

descent parser, but if that was your intent the article was

misleading.



For a sample OSS recursive descent parser, you can refer to



  



and for more info in general on formal grammars, see



  



--

Alex

#### 2009-06-30 05:47:50 - ptmcg
Alex -



Thanks for your comments and critique of my article.  I've been writing e-mails and replies to pyparsing queries for the past 2-1/2 years, and the article is a summary of several of these e-mails - and I can't believe I left out a recursive example!



Pyparsing does support recursive grammars.  At least one is included in the examples directory that comes with pyparsing.  Here is a simple grammar from my upcoming presentation at PyCon.  It seems that about once a month, someone on comp.lang.python asks how to convert from a string representation of a list back to the original list, without using eval.  Here is a non-recursive version, which parses lists that are composed only of simple quoted strings, integers, and floats:





    from pyparsing import *
    
    # define basic building blocks
    lbrack = Suppress('[')
    rbrack = Suppress(']')
    integer = Word(nums+'+-',nums).setName('integer')
    real = Combine(Optional(oneOf('+ -')) + Word(nums) + '.' +
                   Optional(Word(nums))).setName('real')
    
    # add conversion actions for numerics
    cvtInt = lambda s,l,toks: int(toks[0])
    integer.setParseAction( cvtInt )
    cvtReal = lambda s,l,toks: float(toks[0])
    real.setParseAction( cvtReal )
    
    # define grammar for a list
    listItem = real | integer | quotedString.setParseAction(removeQuotes)
    listStr = lbrack + delimitedList(listItem) + rbrack
    
    # test the grammar
    test = '['a', 100, 3.14]'
    print listStr.parseString(test)

prints:



    ['a', 100, 3.1400000000000001]



To expand this grammar to one that will handle nested lists, we need to be able to include listStr as one of the options in the expression listItem - leading to a chicken-and-egg situation.  To resolve this, pyparsing includes the Forward class, allowing you to forward declare an expression before defining its contents.  To make this grammar recursive, we change the grammar definition from:



    listItem = real | integer | quotedString.setParseAction(removeQuotes)
    listStr = lbrack + delimitedList(listItem) + rbrack

To:



    listStr = Forward()
    listItem = real | integer | quotedString.setParseAction(removeQuotes) | Group(listStr)
    listStr \<\< ( lbrack + Optional(delimitedList(listItem)) + rbrack )

The \<\< operator indicates that we are not binding a new expression to listStr, but 'injecting' an expression into the placeholder previously defined as a Forward.



With this minor change, we can now parse nested lists, and get back a nested result:



    test = '['a', 100, [], 3.14, [ +2.718, 'xyzzy', -1.414] ]'
    print listStr.parseString(test)

prints:



    ['a', 100, [], 3.1400000000000001, [2.718, 'xyzzy', -1.4139999999999999]]



I think I need to submit another article to ONLamp, with some more advanced grammar examples.



But even if the example grammars in the article are not recursive, does this necessarily mean pyparsing's parsers are not 'recursive descent'?  The grammars themselves are composed of ParserElement objects organized into a hierarchy representing the grammar expressions and their respective composition from other, more basic grammar expressions.  In this case, the top level expression, listStr, is made up of 3 expressions, which must all match in their turn.  The first and third expressions are just literals, and they call their respective parse methods to see if there is a left and right bracket at the start and end of the string.  But the second expression is a composite, made up of an Optional element, containing a delimited list of listItems.  delimitedList(expr) is a helper function, that returns



    expr + ZeroOrMore(delim + expr)

where the default delim is a literal comma.  So from the top level listStr, we call the Optional expression's parse method, which calls the parse method of the And object created by delimitedList - recursively descending through the grammar until a basic token (such as a Literal or Word) is found, or no match is found and an exception raised.  It is because we have used recursion to traverse the grammar structure, that we can use exceptions and exception handlers to take care of backtracking through the grammar, with no separate maintenance of stacks or backtrack pointers.  On the other hand, this is the feature that makes pyparsing vulnerable to left recursion, which I have only partially succeeded in addressing with the validate() method.



I would be interested in seeing your friend's tree structure, to see if it is in fact amenable to being cracked using pyparsing.



Regards,

-- Paul McGuire
#### 2009-06-30 05:49:39 - ptmcg
Hi,



Thanks for the fast reply!



On 2/22/06, Paul McGuire \<paul@alanweberassociates.com\> wrote:

\>

<ul class="quotelist"><li>Pyparsing does support recursive grammars.</li></ul>

Oh, looks like I've put my foot in my mouth... I really should've

looked more into pyparsing before writing.  But I do think you

should post a followup article with some recursive examples.



I was thinking about how you might achieve recursion with a

pyparsing approach, and realized it could only be done with

a declaration plus later assignment (how Scheme implements

letrec).  And this is exactly what you do with Forward() and \<\<.



My friend wanted to parse a simple subset of Lisp sexps for

a linguistics course (even when not using Lisp linguists use

sexps as their standard data representation).  I see now this

would be quite easy to implement with pyparsing.  Instead

I started him off with a very simple five-minute manual parser -

attached for reference, but please keep in mind I'm not really

a pythonista.



--

Alex

---
## 2009-06-30 05:51:58 - ptmcg - OnLAMP article feedback (Dave Feustel)
Hi Paul,



Here's a message we received from a reader:



<ul class="quotelist"><li>From: Dave Feustel \<dfeustel@mindspring.com\></li><li>Date: February 1, 2006 4:48:05 AM PST</li></ul>

<ul class="quotelist"><li>Paul,</li></ul>\>

\> You wrote an Interesting article on parsing in Python. I'm tempted  

<ul class="quotelist"><li>to try it out</li><li>The telephone number example does not include the 3-3-4 (7 or 10</li><li>total digits)</li><li>requirement on (U.S.) telephone numbers. Then there is the country</li><li>prefix to deal with too.</li></ul>\>

\> Your example of IP decoding was also interesting. I wrote a short  

<ul class="quotelist"><li>snobol4 program to do</li><li>something similar a day ago. I include it so you can compare it to</li><li>your python program.</li><li>The snobol4 program first locates lines in tcpdump output that</li><li>record an attempt to login to</li><li>ssh, then prints the whois info for the ip address from which the</li><li>attempt to access ssh was made.</li><li><h6 id="toc0">=====================</h6>
</li></ul>

    \> #!/usr/local/bin/snobol4 -b   #can be invoked directly as script or
    \>                                               #indirectly as input to 
    snobol4
    \>
    \> start
    \>         num = span('0123456789')
    \>         sp = ' '
    \>         per = '.'
    \>         hit = sp num per num per num per num
    \>
    \> loop
    \>         line = input :f(eod)          #read a line of input previously  
    \> generated by tcpdump
    \>                                               #exit if end of file
    \>         line 'ME.22: ' :f(loop)       #was this packet an attempt to use  
    \> ssh?
    \> *       output = line                 #debug output (commented out)
    \>         line hit $ ipa :f(err)                #store the source ip address 
    into ipa
    \>         output = ipa                  #print the ip address
    \>         cmd = 'nslookup ' ipa #create nslookup command
    \>         host(1,cmd)                   #execute the nslookup command (output 
    to  
    \> console)
    \>         :(loop)                               #read next line
    \>
    \> err
    \>         output = 'ipa match failed' :(end)
    \> eod
    \> end
    \> ======================================



<ul class="quotelist"><li>I have used with great effect short snobol4 programs in alias-</li><li>invoked shell command lines.</li><li>You can get the C source for snobol4 from www.snobol4.org.</li><li>There is a very fast (not free) compiled version of snobol4</li><li>available from www.snobol4.com.</li><li>Bell Labs originally developed Snobol in the late 60s. It still</li><li>beats any other string</li><li>pattern matching/manipulation program I've encountered.</li></ul>\>

\> Dave Feustel

<ul class="quotelist"><li>Fort Wayne, IN</li></ul>

#### 2009-06-30 05:52:56 - ptmcg
I'm vaguely familiar with snobol, that is, I've discussed it with co-workers but have never used it myself.  The other languages that I've run into, that I think are specifically designed for text parsing are awk and the proprietary VAX/SCAN.  Awk and snobol are about the same vintage, late 60's-70's, SCAN came later, more late 80's, I think intended to be awk for VMS.  I used SCAN on several projects, including an arithmetic expression evaluator for a manufacturing statistical process control system.



I'm pretty happy at how pyparsing's class library, operator overloading, and Python's language syntax and object model work together to make a text processing environment that is reasonably easy to read and understand.  It still has some warts and quirks, but it can be very productive for these same simple one-off applications that Dave describes.  And for complex grammars, it generates programs that you can pick up months after having written them, and still make sense of them.



Dave's comments regarding the laxity (is this a word?) of the phone number example are certainly valid, but they reflect more my desire to keep simple examples simple.  The Word class supports min, max, and exact qualifiers on its constructor, so that you can control the length of matching words.  So a more rigorous phone number expression would look like:



    '(' + Word(nums,exact=3) + ')' + Word(nums,exact=3) + '-' + Word(nums,exact=4)



As for matching international numbers, I've yet to discern any consistency in the number or length of number groups, even within the same country.



Here is what I think a Python version of the tcpdump scanner would look like:





    from pyparsing import *
    import os
    
    integer = Word(nums)
    ipaddr = Combine(integer + '.' + integer + '.' + integer + '.' + integer)
    
    for line in file('tcpdump.log'):
        if 'ME.22:' in line:
            try:
               toks,start,end = ipaddr.scanString(line).next()
               ipa = toks[0]
            except:
               continue
            print ipa
            for outline in os.popen('nslookup ' + ipa):
                print outline
    

-- Paul

---
## 2009-06-30 07:14:09 - ptmcg - GSWP errata (Bruce van der Kooij)
Page number: 15

Location on the page: 2nd paragraph



The following code is invalid and produces an exception when parsing supposedly valid input:



Code: Word(alphas)+ '(' + Group( Optional(Word(nums)|Word(alphas) + ZeroOrMore(',' + Word(nums)|Word(alphas))) ) + ')'





Also I found what looks like a spelling error (typo) on page 27, namely

in the info box:



'The standard Python library includes the modules HTMLParser and htmllib

for processing HTML source, although they intolerant of HTML that is not

well behaved.' (should be 'they are intolerant of HTML')



Also maybe it's better to use the expression' 'well formed' compared to

'well behaved'.



Hope you forgive me.



Best regards,



Bruce

#### 2009-06-30 07:21:29 - ptmcg
Page 15:



Yes, I screwed up the order of operations, and forgot to wrap the Word(nums)|Word(alphas) in ()'s.  The code should be:



    Word(alphas)+ '(' + Group( Optional((Word(nums)|Word(alphas)) + 
        ZeroOrMore(',' + (Word(nums)|Word(alphas)))) ) + ')'





Page 27:

I omitted the word 'are', but where the editors that are supposed to catch this kind of thing, I ask you? :)  As for 'well-formed' vs. 'well-behaved', just chalk it up to my overly folksy narrative style.  If there ever is a 2nd ed. of this Short Cut, I'll use the more correct 'well-formed'.





Thanks for submitting these notes - all is certainly forgiven! :)



-- Paul





(By the way, I'd just like to apologize to my readers on how this e-book got laid out.  I intended all of the notes to be single column side bars, but they were inserted as full page width notes.  So some of the source listings get wrapped in weird places, when there is plenty of space available.  Another reason I'd like a chance to go back and do an updated edition.  IF ANYONE ELSE WOULD LIKE AN UPDATED EDITION OF THIS E-BOOK, PLEASE WRITE TO OREILLY AND EXPRESS YOUR INTEREST. -- Paul)

---
## 2009-07-01 05:56:43 - asb_india - Parsing mutiple line
Hello,



I am trying to parse multiple lines means line with breaks 

For eg : 'my name is 

asb 

i live on earth'





Clearly it have the character '\n'

So i made code like this 





    tokens = '(' + delimitedList(OneOrMore(Word(alphanums  + ' _ ' + '\n')),',',combine=True) + ')'
    print tokens.parseString('(dfdfdf,dfdfd,dfdf)')
    print tokens.parseString('(This is asb \
            This as b \
            HWELL , tttttttt)')
    
    ['(', 'dfdfdf,dfdfd,dfdf', ')']
    ['(', 'This is asb         This as b         HWELL , tttttttt', ')']
    



But the \n is missing :( from the parsed result. Is thr any way to retain that too.



Thnaks

#### 2009-07-01 10:15:45 - ptmcg
This is a Python syntax question.  Please try this:



    z = '(This is asb \
            This as b \
            HWELL , tttttttt)'
    print z



You'll find that there are no \n's in z to begin with.  Now try using ''' quotes:



    z = '''(This is asb 
            This as b 
            HWELL , tttttttt)'''
    print z



Now your code has \n's in it, and should parse properly too.



-- Paul
#### 2009-07-01 20:48:07 - asb_india
Thanks but the problem with mine is i get input as 





    (1,'asb','www.asb.com','asb_india','This is asb
    Leaving on earth 
    have done xxxx and doing yyyy
    Details
       * aaa
       * bbb
       * ccc
    Feel free to contact
    ','asb@asb.com',1)
    



The text can be anything from simple text to html to xml anything i just need to parse it and find the different elements i.e seperated by the ,



Thanks
#### 2009-07-02 06:15:28 - ptmcg
Here are a couple of approaches to your problem, one is a naive parser that just breaks on commas, the second is a little smarter an only breaks on commas that are not inside quotes.





    from pyparsing import *
    
    text = '''(1,'asb','www.asb.com','asb_india','This is asb
    Leaving on earth 
    have done xxxx and doing yyyy
    Details
       * aaa
       * bbb
       * ccc
    Feel free to contact
    ','asb@asb.com',1)'''
    
    # simple parser, but fails if a comma occurs inside a quoted string
    text_in_parens = QuotedString('(', endQuoteChar=')', multiline=True)
    print text_in_parens.parseString(text)[0].split(',')
    
    # smarter parser, checks for quoted strings first before breaking on commas
    quotedString.setParseAction(removeQuotes)
    csvList = delimitedList(quotedString | CharsNotIn(',)'))
    LPAR,RPAR = map(Suppress,'()')
    print (LPAR + csvList + RPAR).parseString(text)
    
    # prints:
    ['1', ''asb'', ''www.asb.com'', ''asb_india'', ''This is asb\nLeaving on 
    earth \nhave done xxxx and doing yyyy\nDetails\n   * aaa\n   * bbb\n   * 
    ccc\nFeel free to contact\n'', ''asb@asb.com'', '1']
    ['1', 'asb', 'www.asb.com', 'asb_india', ''This is asb\nLeaving on earth 
    \nhave done xxxx and doing yyyy\nDetails\n   * aaa\n   * bbb\n   * ccc\nFeel 
    free to contact\n'', 'asb@asb.com', '1']



-- Paul

---
## 2009-07-23 01:59:25 - asb_india - Parsing non latin and latin character
Hello



Is it possible to parse all type of characters (Latin , Non Latin).



Thanks in advance

#### 2009-07-23 07:39:15 - ptmcg
Yes it is possible.  You can create Words and Literals using Unicode character strings, and then parse a Unicode string containing them.

---
## 2009-07-26 01:52:23 - mbutow - not ignoring some "special" comments?
Hello,



Since this is my first post, thanks to Paul & all the other contributors who made pyparsing such a great tool!



I have a question whether it is possible not to ignore some special comments.



The files I am parsing contain C++-style comments, and I was using





    grammar.ignore( cppStyleComment )



However, I want to handle certain 'special' double-slash comments, which have been added by a preprocessor. They always start at the beginning of a line and have following format:





    //##loc## \<original filename\>::\<linenumber\>



I want to parse these so my parser can have the information about the location in the original #include file.



Can I somehow get pyparsing to ignore all C++-style comments except these?



Thanks in advance,

Mike

#### 2009-07-26 02:38:41 - ptmcg
Welcome to pyparsing, and thanks for such glowing compliments! :)



I think you can accomplish what you want by attaching a parse action to cppStyleComment, something like this:



    specialComment = '//##' + Word(alphanums) + '##' + Word(alphanums+'.') + '::' + Word(nums)
    
    def dontMatchSpecialComments(tokens):
        if tokens[0] == specialComment:
            raise ParseException('don't match special comments')
    cppStyleComment.setParseAction(dontMatchSpecialComments)



Here's the full test program:





    data = '''
    
    a = 10;
    
    //##loc1## xyzzy.cpp::12345
    
    // not a special comment
    
    a = 20;
    
    /*
     * a special comment inside an ordinary comment
     *
    //##loc2## xyzzy.cpp::12345
     */
    
    a = 30;
    '''
    from pyparsing import *
    
    assignment = Word(alphas) + '=' + Word(nums) + ';'
    specialComment = '//##' + Word(alphanums) + '##' + Word(alphanums+'.') + '::' + Word(nums)
    
    assert '//##loc1## xyzzy.cpp::12345' == specialComment
    
    def dontMatchSpecialComments(tokens):
        if tokens[0] == specialComment:
            raise ParseException('don't match special comments')
    cppStyleComment.setParseAction(dontMatchSpecialComments)
    
    assignment.ignore(cppStyleComment)
    
    for t in (assignment | specialComment).searchString(data):
        print t



Prints:





    ['a', '=', '10', ';']
    ['//##', 'loc1', '##', 'xyzzy.cpp', '::', '12345']
    ['a', '=', '20', ';']
    ['a', '=', '30', ';']



Now go back to the definition of specialComment and add some results names to the different fields, and you can easily access the fields after the parsing has found each special comment.



-- Paul
#### 2009-07-26 02:42:06 - ptmcg
You could also probably do what you need just by taking two passes through the source text, the first pass to get the special comments, the second to read the code.



Have you composed a C++ grammar?  This would be quite a project, I should think!



-- Paul
#### 2009-07-26 03:57:47 - mbutow
Hi Paul,



Thanks for the quick response and the elegant solution.

However, I get an AssertionError on this line





    assert '//##loc1## xyzzy.cpp::12345' == specialComment



I suppose I need something else to get that to work, but I don't know what. I have python 2.5.2 and pyparsing 1.5.1.



Best regards,

Michael
#### 2009-07-26 04:13:27 - mbutow
Please ignore my previous post. 



It turns out I was using pyparsing 1.4.7 instead of 1.5.1 as I should have been doing.



As I forgot to answer your earlier question: 

I'm not doing a C++ grammar (agreed - that would be quite a project!). The files I am parsing are in a data description language which has C++ -style comments.



Once it is completed, I will of course release it as free software!



Kind regards,

Michael

---
## 2009-07-28 07:17:06 - zaviichen - partial matching
hello, i get a problem when i want to parse this syntax:



decimal_number ::= [114] [95LRM 2.5.1] [ sign ] unsigned_number [28]

| [ size ] decimal_base unsigned_number 



unsigned_number ::= [28, 33, 157, 172] [95LRM 2.5.1] [206]

decimal_digit { _ | decimal_digit }



decimal_base ::= [28] [95LRM 2.5.1] d | D [26]



e.g: 4 and 4'd11_22 are all acceptable (this is Verilog syntax)



i write the parser : 

decimal_number = Optional(sign) + unsigned_number | Optional(size) + decimal_base + unsigned_number



but if given 4'd11_22, it will match only 4 as it matches the first part, how i do modify my code to solve this problem ?



thanks!

#### 2009-07-28 20:17:18 - ptmcg
When setting up a MatchFirst (using the '|' operator), put the longest or most specific possible match first.  In your case, the leading '4' of '4'd11_22' <em>is</em> a valid decimal_number.



You can fix this in two ways:

- change '|' which creates MatchFirst to '^' which creates Or, which does a match-longest evaluation.

- reverse the order of the two alternatives



-- Paul
#### 2009-07-30 08:30:52 - zaviichen
i have already got it, thanks!

---
## 2009-08-02 17:22:46 - gregglind - initchars, bodychars


For my application, valid 'names' are of this form (EBNF):



thing  ::=  thing_name + thing_count

thing_type ::= (alpha + alphanums)* + alpha

thing_count ::= digits*



Using Combine and Optional, the best I could come up with is:





from pyparsing import *





thing_name = Combine(Optional(Word(alphas,alphanums)) + Word(alphas))

<ol><li>fails with parseError, due to optional as first clause?</li></ol>thing_name.parseString('p2a')  







This fails with a parse error.  



What's the right way to get this to work?  



Thanks!  (Also, the ShortCuts book is great!)



Gregg

#### 2009-08-02 18:13:58 - ptmcg
Gregg -



Unlike regular expressions, pyparsing's Word class does not do any kind of lookahead or backtracking.  Word(initchars,bodychars) would be like a regex of '[initchars][bodychars]*'  That is, match one of the characters given in initchars, and then as many bodychars as you can.



If I read this part of your BNF correctly:



    thing_type ::= (alpha + alphanums)* + alpha



you are reading alpha and alphanum characters in 0 or more pairs, followed by an alpha.  You can match this using:



    alphachar = oneOf(list(alphas))
    alnumchar = oneOf(list(alphanums))
    thing_type = Combine(ZeroOrMore(alphachar + alnumchar) + alphachar)
    
    assert 'p2a' == thing_type



But I'm not entirely sure that I understand your BNF.  For one thing, you define thing_type, but use thing_name.  Assuming that you meant thing_type, you follow it with an integer counter mad of a number of digits.  Unless there is some whitespace or other separator, the trailing count digits can get confused with the digits in the opening pairs of alphas and alphanums.



It would help if you posted more examples, not just of thing_type's, but also how you would represent a complete thing.



(Thanks for the compliments on the book!  At the moment, I am struggling with copies showing up on filesharing sites - I appreciate anyone's efforts at helping me encourage people to do the right thing and order a legit copy from O'Reilly instead of downloading a bootleg copy.)



-- Paul
#### 2009-08-02 18:39:11 - gregglind
Re: the book!  It's $10, and totally worth it, even for the S-expressions chapter.  It's hard to know how useful it is until you own the thing, so if you do find it somewhere naughty, pay up!





I think my solve for this (the name part) is hackish:



Regex('[^\s\d]\w+[^\s\d]').parseString('k2tog')





my revised, and less error-full:  





thing ::= thing_name + thing_count

thing_name ::= alpha+  ((alphanums)* alpha)*

thing_count ::= digits*



My BNF is rusty, but I'm trying to say for thing_name:

   alpha, then some alphanums, but must end with an alpha.



And yes it really is that something like:



k2tog3  -\>  3 of k2tog   

p3   -\> 3 p's

k3bl2tog5 -\> 5  of k3bl2tog



That's why the 'name' can't end with digits, to make it possible to pull a count off of it.  I know this is getting into natural language territory, but it is unambiguous. 





Thanks for the help!



Gregg

---
## 2009-08-13 13:16:52 - john-l - indentedBlock performance problem with proposed solution
It looks like the current implementation of `indentedBlock` has a nasty performance problem.  Thankfully, I believe it is easy to fix.  I present the problem and my proposed solution here.



To begin, I give you a simple script which builds up a grammar for a simple nested, bulleted list format and then parses an example of that format.





    #!/usr/bin/env python
    from pyparsing import (indentedBlock, Regex, Suppress, Group, Optional,
                           OneOrMore, restOfLine, Forward, Literal,
                           ParserElement, Combine, StringEnd)
    import sys
    
    ParserElement.setDefaultWhitespaceChars(' \t')
    
    COLON = Suppress(Literal(':'))
    label = Suppress(Literal('-')) + Group(OneOrMore(Regex(r'\w+'))) + COLON
    nonWhite = Regex(r'\S+')
    value = Combine(nonWhite + restOfLine)
    item = Forward()
    indentStack = [1]
    item \<\< (label + Optional(value) +
             Optional(indentedBlock(item, indentStack, True)))
    
    parser = OneOrMore(indentedBlock(item, indentStack, False)) + StringEnd()
    
    parser.parseString('''
    - item 1:
      - item 1_1: value for item 1.1
      - item 1_2:
        - item 1_2_3: value for item 1.2.3
      - item 1_3: value for item 1.3
      - item 1_4: v
      - item 1_5: v
    
    - item 2:
      - item 2_1: v
      - item 2_2: v
      - item 2_3: v
      - item 2_4: v
    
    - item 3:
      - item 3_1:
        - item 3_1_1: v
        - item 3_1_2: v
      - item 3_2:
        - item 3_2_1: v
    ''')



If you run this script using a recent pyparsing release, it takes a fairly long time (several minutes) to correctly parse the sample list.  I believe the problem is line 3641 of pyparsing.py: `FollowedBy(blockStatementExpr) +`.  This line means that whenever `indentedBlock` starts to parse an indented block expression, it first checks to see if the current position is followed by an indented block expression.  This recursively tries to parse the next indented block expression, which will include another `FollowedBy` check.  I'm not sure of the running time of that (anyone want to try to help me derive that?), but it seems to be at least polynomial in the number of indented blocks in the string to be parsed.



To top it off, that `FollowedBy` check seems to be superflous, since after the `FollowedBy` check pyparsing goes on to then parse the following expressions anyway.  This parsing is, as you would expect, sufficient to determine if the following string matches the grammar.



In short, I commented out that line, and the grammar parses the string in a much more reasonable time (i.e. moments).

#### 2009-08-19 22:56:22 - ptmcg
John -



Thanks for the post, I'm not able to spend as much time on pyparsing as I'd like these days.



I'll have to go back and see what that FollowedBy is trying to do - I made you change and reran my unit tests, and nothing broke.  Yes, it definitely speeds things up.  But I don't think it just dropped in by accident, so I'll need to see what I was trying to do.



In your test string, try to have some indentations end more than 1 level.  That is, instead of this nice friendly test string:



    a
    b
     b1
     b2
    c
     c1
       c11
     c2
    d

in which each indentation is followed by another statement at the previous indent level, test also for this:



    a
    b
     b1
       b11
    c



Needs to be parsed properly as:



    [[a], [b, [b1, [b11]]], [c]



Thanks again,

-- Paul
#### 2009-08-20 13:09:02 - john-l
<ul class="quotelist"><li>In your test string, try to have some indentations</li><li>end more than 1 level.</li></ul>

Thanks for the suggestion.  I added a few list items like this to my test case, and they parsed correctly.
#### 2009-08-31 16:27:14 - jkrukoff
I'm so glad this got posted, as it was an instant solution to painfully long parsing times for me. My 30 line test case dropped from parsing in 13 minutes to a tenth of a second.



I've no idea what the FollowedBy line might be doing, but I've a line continuation bit in my parser that looks like this:



[[continuation = pyparsing.LineEnd( ) + pyparsing.FollowedBy( '\\' ) + addons.checkIndent( aIndentations ) + '\\']]



And while removing the FollowedBy in indentedBlock doesn't cause any problems, I can't get rid of it here without seeing indentation errors.

---
## 2009-08-19 14:10:33 - gregglind - list of the overloaded operators?
Is there a list of the overloaded operators and how to use them posted somewhere?  If so, I can't seem to locate it.

#### 2009-08-19 22:47:54 - ptmcg
This sounds like it would be a good addition to the docs.



    ~ : creates NotAny using the expression after the operator
    + : creates And using the expressions before and after the operator
    | : creates MatchFirst (first left-to-right match) using the expressions
        before and after the operator
    ^ : creates Or (longest match) using the expressions before and after the
        operator
    & : creates Each using the expressions before and after the operator
    * : creates And by multiplying the expression by the integer operand; if
        expression is multiplied by a 2-tuple, creates an And of (min,max)
        expressions (similar to '{min,max}' form in regular expressions); if
        min is None, intepret as (0,max); if max is None, interpret as 
        expr*min + ZeroOrMore(expr)
    - : creates And using the expressions before and after the operator (raised
        ParseException is converted to ParseSyntaxException)
    == : when comparing to a string, runs parseString(s, parseAll=True)
         returning True if no exception is thrown
    \<\< : inserts the expression following the operator as the body of the
         Forward expression before the operator



+, -, |, ^, and & implicitly promote strings to Literals (other operand must be a ParserElement).
#### 2009-08-20 08:03:06 - gregglind
Thank you!  This is revelatory!  I didn't know there was anything that did what '^' does, and I've been hacking around is using Regexen.
#### 2009-09-07 21:39:41 - ptmcg
Hey, these are already included in the HowToUserPyparsing.html file that comes with pyparsing.  (Although a few were left out, so I've updated the HTML file to include these new entries.)

---
## 2009-08-21 11:01:21 - gregglind - ParseResults item values "stringlike" sometimes, "listlike" other times
Please excuse the vague subject line, and any vague phrasing, or term imprecision.  That said:



Something I seem to be noticing is if you label the subparts of an expression, then it affects the result of the parsing.  



When unlabeled (as in 717,719 below), the result is a string.  



When a subexpression is labeled, then it comes back as a list.  (715,716)



This makes it harder to write parseActions!  



Questions:



1. Am I understanding the cause of this phenomena correctly, that is has to do with the labeling?



2. Any way to 'guarantee' that the result of a parsing 

will be a 'string'ish?



Thanks for the great tool!



Gregg





In [715]: Combine(Literal('a') + Word('9')('num'))('code').parseString('a9')['code']

Out[715]: (['a9'], {'num': [('9', 0)]})



In [716]: Combine('a' + Word('9')('num'))('code').parseString('a9')['code']

Out[716]: (['a9'], {'num': [('9', 0)]})



In [717]: Combine('a' + Word('9'))('code').parseString('a9')['code']

Out[717]: 'a9'



In [719]: Combine(Suppress('a') + Word('9'))('code').parseString('a9')['code']

Out[719]: '9'

#### 2009-08-21 21:23:43 - ptmcg
All I can say is that if you define a name within a sub-field of the expression, then the value you get has to be a ParseResults, not a string.  In 715,716, if you got back a string, there would be no way to get at the 'num' field.  Imagine just typing this:



'a9'.num



or 



'a9'['num']



Strings just don't have that attribute, so if you have a named field, you have to get at it with a ParseResults.



Glad you're enjoying pyparsing!



-- Paul

---
## 2009-08-24 17:09:58 - oafilipoai - parsing to object hierarchy 
Is there a recommended way to generate an object hierarchy as the final result of parsing?



For example to represent a C source file one could have the following (incomplete) hierarchy:





    class File:
      list of functions
    
    class Function:
      list of inputs
      return value
    
    class FunctionInput:
      name  
      type
    
    class FunctionReturn:
      name
      type
    



I can probably generate this hierarchy by defining a BNF for the entire source file, parsing the file with it, and post-processing the ParseResults object to construct the object hierarchy.



I'm wondering if there is a way for pyparsing to do the object creation and to generate the hierarchy.

#### 2009-08-24 21:21:33 - ptmcg
Attach parse actions to each expression, but here is the trick: use a class instead of a function.  The class's __init__ method will get called, and return an instance of that class.  You can see examples of this in the online examples  and .  This technique was also used in the Python Magazine article in May, 2008 creating a Brainf*ck interpreter/compiler.



The elegance here is that the instances are built at parse time - no post-processing required!



Write back if you want more examples.



-- Paul

---
## 2009-08-27 07:12:31 - akonsu - (newbie) setResultsName question
hello,



i have this grammar and a sample string:





    from pyparsing import *
    
    slug = Word(srange('[a-zA-Z0-9_-]'))
    
    date = Keyword('date') + '=' + Combine(Word(nums, exact = 4) + ('-' + Word(nums, exact = 2)) * 2).setResultsName('date')
    
    name = Keyword('name') + '=' + slug.setResultsName('name')
    
    template = (Combine('.' + slug) + '=' + QuotedString('{', endQuoteChar = '}', multiline = True)).setResultsName('template', listAllMatches = True)
    
    plate = Keyword('plate') + '=' + (date & name & ZeroOrMore(template)).setResultsName('plate', listAllMatches = True)
    
    grammar = ZeroOrMore(plate) + StringEnd()
    
    doc = '''
    plate=
        name=        1-my_Name
        date=        2009-08-22
        .summary=       {
                            some long text
                            consisting of two lines
                            }
        .default=       {
                            even longer text
                            that can span even more
                            than two lines
                            }
    '''
    tokens = grammar.parseString(doc)
    
    for t in tokens.plate[0].template :
        print t



it procudes this output:





    ['.summary', '=', '\r\n                        some long text\r\n
             consisting of two lines\r\n                        ']
    ['.default', '=', '\r\n                        even longer text\r\n
               that can span even more\r\n                        than two lines\r\n
                            ']
    [['.summary', '=', '\r\n                        some long text\r\n
              consisting of two lines\r\n                        '], ['.default', '=
    ', '\r\n                        even longer text\r\n                        that
     can span even more\r\n                        than two lines\r\n
             ']]



i expected just two lists in the output, where is the third list of lists come from? is this by design? if i set listAllMatches = False in the template definition below



    template = (Combine('.' + slug) + '=' + QuotedString('{', endQuoteChar = '}', multiline = True)).setResultsName('template')



the program outputs just templates (not lists/arrays) and there are just two templates instead of three. this is what i need. can someone explain why this is happening? i am trying to understand how pyparsing works.



thanks

konstantin

#### 2009-08-27 11:28:30 - akonsu
i was wrong.





template = (Combine('.' + slug) + '=' + QuotedString('{', endQuoteChar = '}', multiline = True)).setResultsName('template')





outputs all tokens in both templates as a flat list, i need to return the tokens that constitute each template just like the original code does (each token set in its own list or tuple), i only do not need the third array...

---
## 2009-08-31 23:05:00 - marjoj - (newbie) What is needed to run pyparsing?
Hi,



I have been doing scripting with Lex/Yacc and regular expressions and I just run into pyparsing by accident yesterday. So, what do I need to try it out? I checked the download/installation pages, but I still wonder if I already need to have python installed? If so where to get one for Linux?



Thanks for helping me out.

#### 2009-09-01 05:14:13 - ptmcg
You wonder correctly. You have to have Python installed, pyparsing is a class library written in Python.



Python is very often already installed in Linux, or if not, easily added using your Linux distributions package manager.  I would suggest that you install Python 2.6, as I have done only minimal testing with Python 3.x.



-- Paul

---
## 2009-09-01 16:26:53 - nemith - Trying to use dictOf on a group of objects
I have a huge dump file from an application of ours that has a list of users and attributes.  I would like pyparsing to return an array and for each user in that array I would like to use dictionary for the attributes



The dump file looks like this:



    #-----------------------------------------------------------------------------
    
    Name          :    N5239411
    
    Password      :    0x0020 61 01 fe 7c 22 36 f4 1a 74 fa 95 4e 36 c7 c1 fb 85 fd 2a 46 98 fe d5 4a 6f 08 cf 49 8d 8f 99 a4 
    
    Chap password :    0x0020 29 93 f4 52 10 c5 9a 5a b2 06 ab c9 2c 5f 76 15 b6 cc 06 08 e5 79 57 18 61 69 24 5d 6d bb dd fe 
    
    State         :    0
    
    S_flags       :    1
    
    Aging policy  :    group147
    
    Good count    :    0
    
    Warning count :    0
    
    Change count  :    0
    
    Last change Lo:    3270332864
    
    Last change Hi:    30024291
    
    Last auth Lo  :    0
    
    Last auth Hi  :    0
    
    Rights        :    1
    
    Type          :    2560
    
    EnableType    :    4
    
    Status        :    33
    
    Reset         :    1
    
    Expiry        :    262    109    0    4294955786    0    5
    
    MaxSession    :    65535
    
    MaxSess2      :    0
    
    Profile       :    147
    
    LogonHrs      :    0x0016 00 ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff 
    
    Alias         :    0    
    
    Value Flags   :    524337
    
    CounterVals_00:    0    0    0    0
    
    CounterRst_00 :    0     0
    
    CounterVals_01:    0    0    0    0
    
    CounterRst_01 :    0     0
    
    ##--- User End
    
    App00    USER_DEFINED_FIELD_0    STRING    PV Mohandas
    
    App00    USER_DEFINED_FIELD_1    STRING    NAP_India_Noida_HCL_SAP-Support
    
    App00    IP_ACS_POOLS_LENGTH    INTEGER    1
    
    App00    IP_ACS_POOLS    STRING    
    
    App00    IP_ALLOCATION_METHOD    INTEGER    5
    
    App00    IP_STATIC_ADDR_LENGTH    INTEGER    1
    
    App00    IP_STATIC_ADDR    STRING    
    
    App00    IP_NAS_POOL_LENGTH    INTEGER    1
    
    App00    IP_NAS_POOL    STRING    
    
    App00    user_callback_type    INTEGER    0
    
    App00    user_callback    STRING    
    
    App00    disp_callback    STRING    
    
    App01    Filters\NAS\records    MSTRING    All AAA Clients**
    
    App01    Filters\NAS\enabled    STRING    1
    
    App01    Filters\NAS\option    STRING    PERMIT
    
    App01    Filters\Dialup\records    MSTRING    
    
    App01    Filters\Dialup\enabled    STRING    0
    
    App01    Filters\Dialup\option    STRING    PERMIT
    
    App01    max_priv    STRING    0,0
    
    App01    max_priv_LENGTH    INTEGER    3
    
    App01    PROFILE    STRING    {default = deny default service = deny default cmd = ignore default attribute = deny}===={shell{3}{}{}}
    
    ##--- Values End
    
    #-----------------------------------------------------------------------------
    
    Name          :    N5239410
    
    Password      :    0x0020 bd 2b e5 ee 9d ff e2 b1 0d 4a 72 30 a4 5a 8c 43 7d 84 e1 b9 06 b6 f1 6a d7 ca 59 9a 44 0f 4f 35 
    
    Chap password :    0x0020 29 93 f4 52 10 c5 9a 5a 29 5e 23 fd 90 8c 0b 0f 3a 05 f6 59 60 9b f4 d5 34 ce 81 bb f0 76 74 51 
    
    State         :    0
    
    S_flags       :    1
    
    Aging policy  :    group147
    
    Good count    :    0
    
    Warning count :    0
    
    Change count  :    0
    
    Last change Lo:    3986732864
    
    Last change Hi:    30024291
    
    Last auth Lo  :    0
    
    Last auth Hi  :    0
    
    Rights        :    1
    
    Type          :    2560
    
    EnableType    :    4
    
    Status        :    33
    
    Reset         :    1
    
    Expiry        :    262    109    0    4294956042    0    5
    
    MaxSession    :    65535
    
    MaxSess2      :    0
    
    Profile       :    147
    
    LogonHrs      :    0x0016 00 ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff 
    
    Alias         :    0    
    
    Value Flags   :    524337
    
    CounterVals_00:    0    0    0    0
    
    CounterRst_00 :    0     0
    
    CounterVals_01:    0    0    0    0
    
    CounterRst_01 :    0     0
    
    ##--- User End
    
    App00    USER_DEFINED_FIELD_0    STRING    Vinod Yene
    
    App00    USER_DEFINED_FIELD_1    STRING    NAP_India_Noida_HCL_SAP-Support
    
    App00    IP_ACS_POOLS_LENGTH    INTEGER    1
    
    App00    IP_ACS_POOLS    STRING    
    
    App00    IP_ALLOCATION_METHOD    INTEGER    5
    
    App00    IP_STATIC_ADDR_LENGTH    INTEGER    1
    
    App00    IP_STATIC_ADDR    STRING    
    
    App00    IP_NAS_POOL_LENGTH    INTEGER    1
    
    App00    IP_NAS_POOL    STRING    
    
    App00    user_callback_type    INTEGER    0
    
    App00    user_callback    STRING    
    
    App00    disp_callback    STRING    
    
    App01    Filters\NAS\records    MSTRING    All AAA Clients**
    
    App01    Filters\NAS\enabled    STRING    1
    
    App01    Filters\NAS\option    STRING    PERMIT
    
    App01    Filters\Dialup\records    MSTRING    
    
    App01    Filters\Dialup\enabled    STRING    0
    
    App01    Filters\Dialup\option    STRING    PERMIT
    
    App01    max_priv    STRING    0,0
    
    App01    max_priv_LENGTH    INTEGER    3
    
    App01    PROFILE    STRING    {default = deny default service = deny default cmd = ignore default attribute = deny}===={shell{3}{}{}}
    
    ##--- Values End
    
    #-----------------------------------------------------------------------------
    
    #End Of Dump

Except for the fact it's about 300k lines long and has 2000+ users.



Pyparsing is working great but it seems to be creating my dictOf in the wrong spot and I can't wrap my head around it



Here is the pyparsing code 



    from pyparsing import *
    
    userStart = Literal('#-----------------------------------------------------------------------------').suppress()
    userEnd = Literal('##--- User End').suppress()
    valuesEnd = Literal('##--- Values End').suppress()
    dumpEnd = Literal('#End Of Dump').suppress()
    
    COLON = Literal(':').suppress()
    SPACE = Literal(' ').suppress()
    EOL = LineEnd().suppress()
    
    variable_chars = alphanums+ '_'
    
    variableName = Combine(OneOrMore(Word(variable_chars)), ' ', adjacent=False)
    userVariables = dictOf(variableName, COLON + restOfLine) + EOL
    
    userBlock = userStart + OneOrMore(userVariables)  + userEnd
    
    valuesTypes = Literal('INTEGER') | Literal('STRING') | Literal('MSTRING') | Literal('ESTRING') 
    valuesLine = Group(Combine(Literal('App') + Word(nums)) + Word(printables) + valuesTypes + restOfLine + EOL)
    
    valuesBlock = Optional(Group(OneOrMore(valuesLine))) + valuesEnd 
    
    parser = Group(OneOrMore(userBlock + valuesBlock)) + userStart + dumpEnd
    
    for user in parser.parseFile('test.dump'):
        print user
        print user['Name']
        print user.keys()
        print
        print



And the output



    [['Name', ' N5239411\r'], ['Password', ' 0x0020 61 01 fe 7c 22 36 f4 1a 74 fa 95 4e 36 c7 c1 fb 85 fd 2a 46 98 fe d5 4a 6f 08 cf 49 8d 8f 99 a4 \r'], ['Chap password', ' 0x0020 29 93 f4 52 10 c5 9a 5a b2 06 ab c9 2c 5f 76 15 b6 cc 06 08 e5 79 57 18 61 69 24 5d 6d bb dd fe \r'], ['State', ' 0\r'], ['S_flags', ' 1\r'], ['Aging policy', ' group147\r'], ['Good count', ' 0\r'], ['Warning count', ' 0\r'], ['Change count', ' 0\r'], ['Last change Lo', ' 3270332864\r'], ['Last change Hi', ' 30024291\r'], ['Last auth Lo', ' 0\r'], ['Last auth Hi', ' 0\r'], ['Rights', ' 1\r'], ['Type', ' 2560\r'], ['EnableType', ' 4\r'], ['Status', ' 33\r'], ['Reset', ' 1\r'], ['Expiry', ' 262     109     0       4294955786      0       5\r'], ['MaxSession', ' 65535\r'], ['MaxSess2', ' 0\r'], ['Profile', ' 147\r'], ['LogonHrs', ' 0x0016 00 ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff \r'], ['Alias', ' 0       \r'], ['Value Flags', ' 524337\r'], ['CounterVals_00', ' 0       0       0       0\r'], ['CounterRst_00', ' 0       0\r'], ['CounterVals_01', ' 0       0       0       0\r'], ['CounterRst_01', ' 0       0\r'], [['App00', 'USER_DEFINED_FIELD_0', 'STRING', '  PV Mohandas\r'], ['App00', 'USER_DEFINED_FIELD_1', 'STRING', '  NAP_India_Noida_HCL_SAP-Support\r'], ['App00', 'IP_ACS_POOLS_LENGTH', 'INTEGER', ' 1\r'], ['App00', 'IP_ACS_POOLS', 'STRING', '  \r'], ['App00', 'IP_ALLOCATION_METHOD', 'INTEGER', ' 5\r'], ['App00', 'IP_STATIC_ADDR_LENGTH', 'INTEGER', ' 1\r'], ['App00', 'IP_STATIC_ADDR', 'STRING', '  \r'], ['App00', 'IP_NAS_POOL_LENGTH', 'INTEGER', ' 1\r'], ['App00', 'IP_NAS_POOL', 'STRING', '  \r'], ['App00', 'user_callback_type', 'INTEGER', ' 0\r'], ['App00', 'user_callback', 'STRING', '  \r'], ['App00', 'disp_callback', 'STRING', '  \r'], ['App01', 'Filters\\NAS\\records', 'MSTRING', ' All AAA Clients\xc2\xac*\xc2\xac*\r'], ['App01', 'Filters\\NAS\\enabled', 'STRING', '  1\r'], ['App01', 'Filters\\NAS\\option', 'STRING', '  PERMIT\r'], ['App01', 'Filters\\Dialup\\records', 'MSTRING', ' \r'], ['App01', 'Filters\\Dialup\\enabled', 'STRING', '  0\r'], ['App01', 'Filters\\Dialup\\option', 'STRING', '  PERMIT\r'], ['App01', 'max_priv', 'STRING', '  0,0\r'], ['App01', 'max_priv_LENGTH', 'INTEGER', ' 3\r'], ['App01', 'PROFILE', 'STRING', '  {default = deny default service = deny default cmd = ignore default attribute = deny}===={shell{3}{}{}}\r']], ['Name', ' N5239410\r'], ['Password', ' 0x0020 bd 2b e5 ee 9d ff e2 b1 0d 4a 72 30 a4 5a 8c 43 7d 84 e1 b9 06 b6 f1 6a d7 ca 59 9a 44 0f 4f 35 \r'], ['Chap password', ' 0x0020 29 93 f4 52 10 c5 9a 5a 29 5e 23 fd 90 8c 0b 0f 3a 05 f6 59 60 9b f4 d5 34 ce 81 bb f0 76 74 51 \r'], ['State', ' 0\r'], ['S_flags', ' 1\r'], ['Aging policy', ' group147\r'], ['Good count', ' 0\r'], ['Warning count', ' 0\r'], ['Change count', ' 0\r'], ['Last change Lo', ' 3986732864\r'], ['Last change Hi', ' 30024291\r'], ['Last auth Lo', ' 0\r'], ['Last auth Hi', ' 0\r'], ['Rights', ' 1\r'], ['Type', ' 2560\r'], ['EnableType', ' 4\r'], ['Status', ' 33\r'], ['Reset', ' 1\r'], ['Expiry', ' 262     109     0       4294956042      0       5\r'], ['MaxSession', ' 65535\r'], ['MaxSess2', ' 0\r'], ['Profile', ' 147\r'], ['LogonHrs', ' 0x0016 00 ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff \r'], ['Alias', ' 0       \r'], ['Value Flags', ' 524337\r'], ['CounterVals_00', ' 0       0       0       0\r'], ['CounterRst_00', ' 0       0\r'], ['CounterVals_01', ' 0       0       0       0\r'], ['CounterRst_01', ' 0       0\r'], [['App00', 'USER_DEFINED_FIELD_0', 'STRING', '  Vinod Yene\r'], ['App00', 'USER_DEFINED_FIELD_1', 'STRING', '  NAP_India_Noida_HCL_SAP-Support\r'], ['App00', 'IP_ACS_POOLS_LENGTH', 'INTEGER', ' 1\r'], ['App00', 'IP_ACS_POOLS', 'STRING', '  \r'], ['App00', 'IP_ALLOCATION_METHOD', 'INTEGER', ' 5\r'], ['App00', 'IP_STATIC_ADDR_LENGTH', 'INTEGER', ' 1\r'], ['App00', 'IP_STATIC_ADDR', 'STRING', '  \r'], ['App00', 'IP_NAS_POOL_LENGTH', 'INTEGER', ' 1\r'], ['App00', 'IP_NAS_POOL', 'STRING', '  \r'], ['App00', 'user_callback_type', 'INTEGER', ' 0\r'], ['App00', 'user_callback', 'STRING', '  \r'], ['App00', 'disp_callback', 'STRING', '  \r'], ['App01', 'Filters\\NAS\\records', 'MSTRING', ' All AAA Clients\xc2\xac*\xc2\xac*\r'], ['App01', 'Filters\\NAS\\enabled', 'STRING', '  1\r'], ['App01', 'Filters\\NAS\\option', 'STRING', '  PERMIT\r'], ['App01', 'Filters\\Dialup\\records', 'MSTRING', ' \r'], ['App01', 'Filters\\Dialup\\enabled', 'STRING', '  0\r'], ['App01', 'Filters\\Dialup\\option', 'STRING', '  PERMIT\r'], ['App01', 'max_priv', 'STRING', '  0,0\r'], ['App01', 'max_priv_LENGTH', 'INTEGER', ' 3\r'], ['App01', 'PROFILE', 'STRING', '  {default = deny default service = deny default cmd = ignore default attribute = deny}===={shell{3}{}{}}\r']]]
     N5239410
    ['Value Flags', 'S_flags', 'LogonHrs', 'Last change Hi', 'Change count', 'State', 'Last auth Hi', 'EnableType', 'Type', 'Last change Lo', 'MaxSess2', 'Status', 'Expiry', 'CounterVals_01', 'CounterVals_00', 'Chap password', 'Password', 'MaxSession', 'Reset', 'Profile', 'Name', 'Rights', 'Alias', 'Warning count', 'Good count', 'Last auth Lo', 'Aging policy', 'CounterRst_00', 'CounterRst_01']
    
    
    [['Name', ' N5239411\r'], ['Password', ' 0x0020 61 01 fe 7c 22 36 f4 1a 74 fa 95 4e 36 c7 c1 fb 85 fd 2a 46 98 fe d5 4a 6f 08 cf 49 8d 8f 99 a4 \r'], ['Chap password', ' 0x0020 29 93 f4 52 10 c5 9a 5a b2 06 ab c9 2c 5f 76 15 b6 cc 06 08 e5 79 57 18 61 69 24 5d 6d bb dd fe \r'], ['State', ' 0\r'], ['S_flags', ' 1\r'], ['Aging policy', ' group147\r'], ['Good count', ' 0\r'], ['Warning count', ' 0\r'], ['Change count', ' 0\r'], ['Last change Lo', ' 3270332864\r'], ['Last change Hi', ' 30024291\r'], ['Last auth Lo', ' 0\r'], ['Last auth Hi', ' 0\r'], ['Rights', ' 1\r'], ['Type', ' 2560\r'], ['EnableType', ' 4\r'], ['Status', ' 33\r'], ['Reset', ' 1\r'], ['Expiry', ' 262     109     0       4294955786      0       5\r'], ['MaxSession', ' 65535\r'], ['MaxSess2', ' 0\r'], ['Profile', ' 147\r'], ['LogonHrs', ' 0x0016 00 ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff \r'], ['Alias', ' 0       \r'], ['Value Flags', ' 524337\r'], ['CounterVals_00', ' 0       0       0       0\r'], ['CounterRst_00', ' 0       0\r'], ['CounterVals_01', ' 0       0       0       0\r'], ['CounterRst_01', ' 0       0\r'], [['App00', 'USER_DEFINED_FIELD_0', 'STRING', '  PV Mohandas\r'], ['App00', 'USER_DEFINED_FIELD_1', 'STRING', '  NAP_India_Noida_HCL_SAP-Support\r'], ['App00', 'IP_ACS_POOLS_LENGTH', 'INTEGER', ' 1\r'], ['App00', 'IP_ACS_POOLS', 'STRING', '  \r'], ['App00', 'IP_ALLOCATION_METHOD', 'INTEGER', ' 5\r'], ['App00', 'IP_STATIC_ADDR_LENGTH', 'INTEGER', ' 1\r'], ['App00', 'IP_STATIC_ADDR', 'STRING', '  \r'], ['App00', 'IP_NAS_POOL_LENGTH', 'INTEGER', ' 1\r'], ['App00', 'IP_NAS_POOL', 'STRING', '  \r'], ['App00', 'user_callback_type', 'INTEGER', ' 0\r'], ['App00', 'user_callback', 'STRING', '  \r'], ['App00', 'disp_callback', 'STRING', '  \r'], ['App01', 'Filters\\NAS\\records', 'MSTRING', ' All AAA Clients\xc2\xac*\xc2\xac*\r'], ['App01', 'Filters\\NAS\\enabled', 'STRING', '  1\r'], ['App01', 'Filters\\NAS\\option', 'STRING', '  PERMIT\r'], ['App01', 'Filters\\Dialup\\records', 'MSTRING', ' \r'], ['App01', 'Filters\\Dialup\\enabled', 'STRING', '  0\r'], ['App01', 'Filters\\Dialup\\option', 'STRING', '  PERMIT\r'], ['App01', 'max_priv', 'STRING', '  0,0\r'], ['App01', 'max_priv_LENGTH', 'INTEGER', ' 3\r'], ['App01', 'PROFILE', 'STRING', '  {default = deny default service = deny default cmd = ignore default attribute = deny}===={shell{3}{}{}}\r']], ['Name', ' N5239410\r'], ['Password', ' 0x0020 bd 2b e5 ee 9d ff e2 b1 0d 4a 72 30 a4 5a 8c 43 7d 84 e1 b9 06 b6 f1 6a d7 ca 59 9a 44 0f 4f 35 \r'], ['Chap password', ' 0x0020 29 93 f4 52 10 c5 9a 5a 29 5e 23 fd 90 8c 0b 0f 3a 05 f6 59 60 9b f4 d5 34 ce 81 bb f0 76 74 51 \r'], ['State', ' 0\r'], ['S_flags', ' 1\r'], ['Aging policy', ' group147\r'], ['Good count', ' 0\r'], ['Warning count', ' 0\r'], ['Change count', ' 0\r'], ['Last change Lo', ' 3986732864\r'], ['Last change Hi', ' 30024291\r'], ['Last auth Lo', ' 0\r'], ['Last auth Hi', ' 0\r'], ['Rights', ' 1\r'], ['Type', ' 2560\r'], ['EnableType', ' 4\r'], ['Status', ' 33\r'], ['Reset', ' 1\r'], ['Expiry', ' 262     109     0       4294956042      0       5\r'], ['MaxSession', ' 65535\r'], ['MaxSess2', ' 0\r'], ['Profile', ' 147\r'], ['LogonHrs', ' 0x0016 00 ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff \r'], ['Alias', ' 0       \r'], ['Value Flags', ' 524337\r'], ['CounterVals_00', ' 0       0       0       0\r'], ['CounterRst_00', ' 0       0\r'], ['CounterVals_01', ' 0       0       0       0\r'], ['CounterRst_01', ' 0       0\r'], [['App00', 'USER_DEFINED_FIELD_0', 'STRING', '  Vinod Yene\r'], ['App00', 'USER_DEFINED_FIELD_1', 'STRING', '  NAP_India_Noida_HCL_SAP-Support\r'], ['App00', 'IP_ACS_POOLS_LENGTH', 'INTEGER', ' 1\r'], ['App00', 'IP_ACS_POOLS', 'STRING', '  \r'], ['App00', 'IP_ALLOCATION_METHOD', 'INTEGER', ' 5\r'], ['App00', 'IP_STATIC_ADDR_LENGTH', 'INTEGER', ' 1\r'], ['App00', 'IP_STATIC_ADDR', 'STRING', '  \r'], ['App00', 'IP_NAS_POOL_LENGTH', 'INTEGER', ' 1\r'], ['App00', 'IP_NAS_POOL', 'STRING', '  \r'], ['App00', 'user_callback_type', 'INTEGER', ' 0\r'], ['App00', 'user_callback', 'STRING', '  \r'], ['App00', 'disp_callback', 'STRING', '  \r'], ['App01', 'Filters\\NAS\\records', 'MSTRING', ' All AAA Clients\xc2\xac*\xc2\xac*\r'], ['App01', 'Filters\\NAS\\enabled', 'STRING', '  1\r'], ['App01', 'Filters\\NAS\\option', 'STRING', '  PERMIT\r'], ['App01', 'Filters\\Dialup\\records', 'MSTRING', ' \r'], ['App01', 'Filters\\Dialup\\enabled', 'STRING', '  0\r'], ['App01', 'Filters\\Dialup\\option', 'STRING', '  PERMIT\r'], ['App01', 'max_priv', 'STRING', '  0,0\r'], ['App01', 'max_priv_LENGTH', 'INTEGER', ' 3\r'], ['App01', 'PROFILE', 'STRING', '  {default = deny default service = deny default cmd = ignore default attribute = deny}===={shell{3}{}{}}\r']]]
     N5239410
    ['Value Flags', 'S_flags', 'LogonHrs', 'Last change Hi', 'Change count', 'State', 'Last auth Hi', 'EnableType', 'Type', 'Last change Lo', 'MaxSess2', 'Status', 'Expiry', 'CounterVals_01', 'CounterVals_00', 'Chap password', 'Password', 'MaxSession', 'Reset', 'Profile', 'Name', 'Rights', 'Alias', 'Warning count', 'Good count', 'Last auth Lo', 'Aging policy', 'CounterRst_00', 'CounterRst_01']



You can see that I only get the last user in a dictionary and the other users are one level deeper.



I can't seem to figure out how to fix this.



Thanks,



Brandon

#### 2009-09-01 16:30:08 - nemith
Woah... That came out ugly and it doesn't seem like i can edit it.



Sorry,



Brandon
#### 2009-09-02 14:20:04 - nemith
I actually figured it out.  I am going to create a new post that isn't 20million pixels long to post the results and my new issue

---
## 2009-09-02 14:30:15 - nemith - Using dictOf with many lines
I have an unusual input format that has the token that I want to use as a key on every line.  For example I would like to have App00 and App01 in the following example as a key with sub-dictionaries as values.



Input File



    #-----------------------------------------------------------------------------
    Name          :    N5239411
    Profile       :    147
    LogonHrs      :    0x0016 00 ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff f ff ff ff ff 
    Alias        :    0    
    Value Flags   :    524337
    CounterVals_00:    0    0    0    0
    CounterRst_00 :    0     0
    CounterVals_01:    0    0    0    0
    CounterRst_01 :    0     0
    ##--- User End
    App00    USER_DEFINED_FIELD_0    STRING    PV Mohandas
    App00    USER_DEFINED_FIELD_1    STRING    NAP_India_Noida_HCL_SAP-Support
    App01    Filters\NAS\records    MSTRING    All AAA Clients**
    App01    Filters\NAS\enabled    STRING    1
    ##--- Values End
    #----------------------------------------------------------------------------
    Nme          :    N5239410
    Profile       :    147
    LogonHrs      :    0x0016 00 ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff f ff ff ff ff 
    Alias        :    0    
    Value Flags   :    524337
    CounterVals_00:    0    0    0    0
    CounterRst_00 :    0     0
    CounterVals_01:    0    0    0    0
    CounterRst_01 :    0     0
    ##--- User End
    App00    USER_DEFINED_FIELD_0    STRING    Vinod Yene
    App00    USER_DEFINED_FIELD_1    STRING    NAP_India_Noida_HCL_SAP-Support
    App01    Filters\NAS\records    MSTRING    All AAA Clients**
    App01    Filters\NAS\enabled    STRING    1
    ##--- Values End
    #-----------------------------------------------------------------------------
    #End Of Dump



So for example I would like to to the following:





    for user in parser.parseFile('test.dump'):
        print user.Name
        print user.Values['App00']['USER_DEFINED_FIELD_0']



The problem is that dictOf on the valuesLine overwrite the previous sub-dictionary with a new one instead of adding to it.  So i only get the last parsed line.



Output



    [['App00', ['USER_DEFINED_FIELD_0', 'STRING', 'PV Mohandas']], ['App00', ['USER_DEFINED_FIELD_1', 'STRING', 'NAP_India_Noida_HCL_SAP-Support']],
     ['App01', ['Filters\\NAS\\records', 'MSTRING', 'All AAA Clients\xc2\xac*\xc2\xac*']], ['App01', ['Filters\\NAS\\enabled', 'STRING', '1']]]
    - App00: [['USER_DEFINED_FIELD_1', 'STRING', 'NAP_India_Noida_HCL_SAP-Support']]
      - USER_DEFINED_FIELD_1: ['STRING', 'NAP_India_Noida_HCL_SAP-Support']
        - type: STRING
        - value: NAP_India_Noida_HCL_SAP-Support
    - App01: [['Filters\\NAS\\enabled', 'STRING', '1']]
      - Filters\NAS\enabled: ['STRING', '1']
        - type: STRING
        - value: 1
    [['App00', ['USER_DEFINED_FIELD_0', 'STRING', 'Vinod Yene']], ['App00', ['USER_DEFINED_FIELD_1', 'STRING', 'NAP_India_Noida_HCL_SAP-Support']], ['App01', ['Filters\\NAS\\records', 'MSTRING', 'All AAA Clients\xc2\xac*
    \xc2\xac*']], ['App01', ['Filters\\NAS\\enabled', 'STRING', '1']]]
    - App00: [['USER_DEFINED_FIELD_1', 'STRING', 'NAP_India_Noida_HCL_SAP-Support']]
      - USER_DEFINED_FIELD_1: ['STRING', 'NAP_India_Noida_HCL_SAP-Support']
        - type: STRING
        - value: NAP_India_Noida_HCL_SAP-Support
    - App01: [['Filters\\NAS\\enabled', 'STRING', '1']]
      - Filters\NAS\enabled: ['STRING', '1']
        - type: STRING
        - value: 1



And here is the complete source so far



    from pyparsing import *
    
    ParserElement.setDefaultWhitespaceChars(' \t\r\n')
    
    COLON = Literal(':').suppress()
    SPACE = Literal(' ').suppress()
    EOL = LineEnd().suppress()
    
    userStart = Literal('#-----------------------------------------------------------------------------').suppress()
    userEnd = Literal('##--- User End').suppress()
    valuesEnd = Literal('##--- Values End').suppress()
    dumpEnd = Literal('#End Of Dump').suppress()
    
    globcount = 1
    
    def stripWhitespace(val):
        return val[0].strip()
    
    variable_chars = alphanums+ '_'
    
    
    variableName = Combine(OneOrMore(Word(variable_chars)), ' ', adjacent=False)
    userVariables = dictOf(variableName + COLON, restOfLine.setParseAction(stripWhitespace)) + EOL
    
    userBlock = userStart  + OneOrMore(userVariables)  + userEnd
    
    valuesTypes = Literal('INTEGER') | Literal('STRING') | Literal('MSTRING') | Literal('ESTRING') 
    valuesLine = dictOf(Combine(Literal('App') + Word(nums)), dictOf(Word(printables), valuesTypes('type') + restOfLine('value').setParseAction(stripWhitespace))) + EOL
    
    valuesBlock = Optional(OneOrMore(valuesLine)) + valuesEnd 
    
    parser = OneOrMore(Group(userBlock + valuesBlock('Values'))) + userStart + dumpEnd
    
    a = parser.parseFile('test.dump')
    
    for user in a:
        print user.Values.dump()



Any thoughts?

#### 2009-09-02 20:50:43 - ptmcg
Yes, it's a little unnatural to have dictOf work with input data with duplicated keys.  And nested dictOf's take some extra care anyway.



I would instead parse the clump of lines like 'App## etc.' as their own group (which you already do as a 'valuesBlock'), and attach a parse action to that group that creates the results names.



Something like:





    valuesLine = Group(Combine(Literal('App') + Word(nums))('key') +
                        Word(printables)('fieldname') +
                        valuesTypes('type') +
                        restOfLine('value')).setName('valuesLine').setDebug()
    valuesBlock = Optional(OneOrMore(valuesLine)) + valuesEnd 
    def setFieldNames(tokens):
        ret = ParseResults([])
        for valueline in tokens:
            if not valueline.key in ret:
                ret[valueline.key] = ParseResults([])
            ret[valueline.key][valueline.fieldname] = ParseResults([])
            ret[valueline.key][valueline.fieldname]['fieldname'] = valueline.fieldname
            ret[valueline.key][valueline.fieldname]['type'] = valueline.type
            ret[valueline.key][valueline.fieldname]['value'] = valueline.value
        return ret
    valuesBlock.setParseAction(setFieldNames)



Also, take care with some of your keys like 'Filters\NAS\records' - that nested '\r' bit me for a second there.





One other suggestion: this construct is very fragile:



    userStart = Literal('#-----------------------------------------------------------------------------').suppress()



If there is just one missing '-', or if the format gets changed later to add or remove a '-', things will start breaking for no seeming reason.



Try this instead:



    userStart = Word('#','-').suppress()



A cheap trick using Word to define a word group starting with the character '#', followed by zero or more '-'s.



HTH,

-- Paul

(and please try to wrap your posts - if you have something that you just can't rewrap, try using the pastebin at pyparsing.pastebin.com.)
#### 2009-09-02 20:53:44 - ptmcg
Oh, and you should remove '.setName('valuesLine').setDebug()' from the code example, I just needed to see where things were going bad, and it was at the occurrence of '\r' in your input text (I had pasted it into a Python string variable to simplify my testing).



-- Paul

---
## 2009-09-03 06:58:33 - reyman64 - recursive parsing on data
I have syntax like that : 

<ul class="quotelist"><li>plot-x(population desc, wealth desc) per-y-time(1650,2000) in-space(cities)</li></ul>

<ul class="quotelist"><li>plot-x(txV of population desc, txV of wealth desc) per-y-time (1650,2000) in-space(cities)</li></ul>

<ul class="quotelist"><li>group-by(plot-x(population desc),date) per-y-rank (1,max) in-space(cities desc,10000)</li></ul>

<ul class="quotelist"><li>group-by(plot-x(population desc),date) per-y-rank (1,1000) in-space(cities desc,10000)</li></ul>

<ul class="quotelist"><li>plot-x(primacy3 of population desc) per-y-time (1650,2000) in-space(cities)</li></ul>

<ul class="quotelist"><li>group-by(plot-x(population desc),sector) per-y-time(1650,2000) in-space(cities)</li></ul>

I'm starting with this grammar : 





    #keyword
            plotToken = Keyword('plot-x', caseless=True)
            groupToken = Keyword('group-by',caseless=True)
            descToken = Keyword('desc',caseless=True)
            ascToken = Keyword('asc',caseless=True)
            perYTimeToken = Keyword('per-y-time',caseless=True)
            perYRankToken = Keyword('per-y-rank',caseless=True)
    #verb
        perYVerb = '('+Word(nums, exact=4)+','+Word(nums,exact=4)+')'
            quitVerb = oneOf('QUIT Q', caseless=True)
        orderVerb = descToken | ascToken
            attributeVerb = Word(alphas) + Optional(orderVerb)
            plotVerb = delimitedList(attributeVerb)
    
    #command
        quitCommand = quitVerb
        plotCommand = plotToken + '(' + plotVerb + ')'
            perYCommand = (perYTimeToken | perYRankToken) + perYVerb
            groupCommand = groupToken + '(' + plotCommand + ',' + Word(alphas) +  ')'
        exprCommand = (plotCommand | groupCommand) + perYCommand
    



I went exprCommand control in first the syntax, and then run plotCommand.parseAction or groupCommand.parseAction with perYCommand.parseAction to finish.



I wrote this in first time : 





    quitCommand.setParseAction(
                self.makeCommandParseAction( QuitCommand ) )
    
            groupCommand.setParseAction(
                self.makeCommandParseAction( GroupCommand ) )
    
            plotCommand.setParseAction(
                self.makeCommandParseAction( PlotCommand ) )
    
            perYCommand.setParseAction(
                self.makeCommandParseAction( PerYCommand ) )
    
            exprCommand.setParseAction(
                self.makeCommandParseAction( ExprCommand ) )
    



ex : plot-x(population desc, wealth desc) per-y-time(1650,2000)



It is possible with pyparsing to run ExprCommand in first, which confirm global syntax. 



And immediately after that, pyparsing analyse and match : in first time the 'plot-x' keyword and launch a traitement which return a data-frame (R data), and in second time re-use this data when pyparsing match the 'per-y-time' keyword, which launch second traitment with this data ? 



Keyword launch function on data in my dev conception, with priority on keyword,so, can i do that with pyparsing?



Thx a lot for your help !

Seb

#### 2009-09-05 23:33:20 - ptmcg
It's taken me a little while to make my way through your post.  I think I follow now what you are doing, here's my responses to your questions:





1.) It is possible with pyparsing to run ExprCommand in first, which confirm global syntax?



No, ExprCommand is an expression that contains smaller expressions.  The smaller expressions will run first, as will their parse actions.  Then when all is matched and done, the overall container ExprCommand will match, and its parse action run.



I suggest you rethink this grammar just a little bit.  Instead of creating perYCommand as a command on the same level as plotCommand and groupCommand, change it to perYClause, which can be added to a plotCommand or a groupCommand:





    quitCommand = quitVerb
    perYClause = (perYTimeToken | perYRankToken) + perYVerb
    plotCommand = plotToken + '(' + plotVerb + ')' + Optional(perYClause('perY'))
    groupCommand = groupToken + '(' + plotCommand + ',' + Word(alphas) +  ')' +
                        Optional(perYClause('perY'))
    exprCommand = (plotCommand | groupCommand)



Now in the parse actions for plotCommand and groupCommand, you can test for the existence of a perYClause using the results name 'perY':





    if tokens.perY:
        # there was a perY clause added, access its fields



Now, I don't think you will need a parse action for exprCommand.





2.) And immediately after that, pyparsing analyse and match : in first time the 'plot-x' keyword and launch a traitement which return a data-frame (R data), and in second time re-use this data when pyparsing match the 'per-y-time' keyword, which launch second traitment with this data ? 



Should be handled now if you refactor your grammar as above in the answer to #1.







3.) Keyword launch function on data in my dev conception, with priority on keyword,so, can i do that with pyparsing?



I've no idea.  This may be something you choose to do as your own post-parsing processing, and is probably a good separation of functionality in your code.  Let the parser crack the user's input, and return a data structure or object that describes the input, or even is executable by itself.  Then process the resulting structure or object.  Then loop around and get more input from the user.  Not *everything* has to be done in pyparsing! :)





-- Paul
#### 2009-09-16 01:46:03 - reyman64
Thx a lot for your help :]
#### 2009-09-16 05:59:47 - ptmcg
I recently rehosted my content from my old Geocities free website (first written 15 years ago, the night before a job interview so I could say I had a website and show off a Java applet!).  Please check out the PyCon presentations I have on my simple adventure game engine, as it does similar command processing as you describe in your question.  Here is the link: 



-- Paul
#### 2009-09-16 07:01:54 - reyman64
Thx for this link, that's help me a lot for architecture my code :]



I have a question ,





Imagine, in your adventure game, i want to create one special potion with some other potions like this :



COOK( MIX(POTIONS( herb WITH frog ), POTIONS ( herb WITH squirrel's eye AT 200 degree) ), 'plate' )



You attach one action by keyword : POTIONS ( WITH AT | WITH ), MIX, COOK



I want to create potions, and after mix all in a final meal/cook :]

You have one action / class by command process, but, how i can to make this recursive in this case ? You parse expression COOK to verify the syntax, and after you launch one action by keyword, but how you respect the order of the trigger/action, and when this action need recursivity like COOK (POTIONS (WITH ...AT..), 'plate'), how did you do this  o_O ? 



-- In my case --



[code]

group-by(plot-y(population desc),date) per-x-time (1650,2000) in-space(cities desc,10000)

[/code]



I have a 'Big Array of data' on which i work at each action :



In first time, i want to select a period of time in my data \>\> per-x-time(1650,2000)

Then, select the 10000 first cities of this same data \>\>  in-space(cities desc,10000)

Then i select one attribute of this data (y of my graphic): the population in order 'desc' (like sql) \>\> plot-y(population desc)

At the end, i want to group all population data by date attribute (a sum of population by date in reality) and display this. \>\> group-by( ... , date)



Thanks a lot Paul for all your help (with noob in python/pyparsing/parser like me ...)



--Sbastien

---
## 2009-09-10 04:49:28 - asb_india - Thanks for all the help
Hello,



Thanks a lot for all your help on my silly questions. I have made the sql query parser that parse almost all the queries (select). 



Once again thanks a lot. :)

#### 2009-09-10 08:41:22 - ptmcg
That's terrific!  Good luck!
#### 2009-09-11 04:13:41 - codeape
Hi, I'm looking for a SQL parser I can use in Python. 



Is there any chance you will be making your code open-source?
#### 2012-05-08 19:44:39 - tarunrs
Hi asb_india,

I was looking for an SQL Parser as well for a project of mine. Any chance I could use your code as well?



Thanks,

Tarun
#### 2014-02-25 08:01:46 - danken6287
Hello, asb_india,

I am a student of National Cheng Kung University,Taiwan,ROC, and I am looking for an SQL Parser as well for master thesis of mine. Any chance I could use your code as well?



It will be great if you can help ,thank you

Danken Shih

---
## 2009-09-15 21:15:40 - crmccreary - parsing fortran type text files
I am currently using regex's to parse files that are delimited by their column as such:



    *        1         2         3         4         5         6         7
    
    *2345678901234567890123456789012345678901234567890123456789012345678901234567890
    
    OPTIONS      EN       SDUC   5 5        PT  PTPT          PT
    
    GRUP
    
    GRUP J 4         16.000 0.375 29.0011.6036.00 1    .700.700           490.00
    
    GRUP J21         16.000 0.438 29.0011.6036.00 1    .700.700           490.00
    
    GRUP J23         18.000 0.375 29.0011.6036.00 1    .700.700           490.00
    
    GRUP J53         18.000 0.500 29.0011.6036.00 1    .700.700           490.00
    
    GRUP J54         18.010 0.375 29.0011.6036.00 1    .700.700           490.00
    
    GRUP J55         16.000 0.656 29.0011.6036.00 1    .700.700           490.00
    
    GRUP JL3         34.500 0.750 29.0011.6036.00 1    1.001.00           490.00 5.0
    
    GRUP JL3         34.000 0.500 29.0011.6036.00 1    1.001.00           490.00 0.0
    
    GRUP JL3         34.500 0.750 29.0011.6036.00 1    1.001.00           490.00 5.0
    
    GRUP JL4         34.500 0.750 29.0011.6036.00 1    1.001.00           490.00 0.0
    
    MEMBER
    
    MEMBER 25042514 J 4                                                    0 16.00
    
    MEMBER 25142524 J 4
    
    MEMBER 26042624 J21
    
    MEMBER 24042414 J23                                                    0 18.00
    
    MEMBER 24142424 J23
    
    MEMBER123142404 J53                                                    0 18.00
    
    MEMBER OFFSETS                                   4.50 -8.62
    
    MEMBER123142424 J53                                                    0 18.00
    
    MEMBER OFFSETS                                   4.50  8.63
    
    MEMBER124142504 J54                                                    0 18.00
    
    MEMBER OFFSETS                                   4.50 -8.62
    
    MEMBER124142524 J54                                                    0 18.00
    
    MEMBER OFFSETS                                   4.50  8.63
    
    MEMBER125142604 J55                                                    0 16.00
    
    MEMBER OFFSETS                                   4.00 -8.62
    
    MEMBER125142624 J55                                                    0 16.00
    
    MEMBER OFFSETS                                   4.00  8.63
    
    MEMBER 23042404 JL3                                                    0 34.00
    
    MEMBER 23242424 JL3                                                    0 34.00
    
    MEMBER 24042504 JL3                                                    0 34.00
    
    MEMBER 24242524 JL3
    
    MEMBER 25042604 JL4
    
    MEMBER 25242624 JL4
    
    JOINT
    
    JOINT 2304    -13.   -0.0    82. -6.240  0.000        111111
    
    JOINT 2314    -47.   -0.0    82. -6.000  0.000        111111
    
    JOINT 2324    -81.   -0.0    82. -5.520  0.000        111111
    
    JOINT 2404    -15.   -0.0   123. -6.960  0.000        010000
    
    JOINT 2414    -45.   -0.0   123. -1.200  0.000
    
    JOINT 2424    -74.   -0.0   123. -7.680  0.000        010000
    
    JOINT 2504    -17.   -0.0   165. -8.040  0.000        010000
    
    JOINT 2514    -42.   -0.0   165. -6.960  0.000
    
    JOINT 2524    -67.   -0.0   165. -7.800  0.000        010000
    
    JOINT 2604    -19.   -0.0   207. -9.480  0.000        010000
    
    JOINT 2624    -60.   -0.0   207. -7.920  0.000        010000
    
    JOINT 2624 34000.034000.0                             ELASTI

Some of the keywords, such as MEMBER, may be followed by one or more modifier lines, such as MEMBER OFFSET. This is indicated by the '1' in column 7.



As an example,



    MEMBER125142604 J55                                                    0 16.00
    MEMBER OFFSETS                                   4.00 -8.62

should be parsed as follows:



    offset_option = int(line1[6]) # Column 7 
    jointA = line[7:11] # alphanumeric 
    jointB = line[11:15] # alphanumeric 
    group = line[16:19] # alphanumeric
    local_z_joint = line[41:45]
     #alphanumeric
    eff_d = line[72:78]) # real number

and



    JOINT 2624    -60.   -0.0   207. -7.920  0.000        010000

should be parsed as follows:



    label = line[6:10]
    
    coords_x_ft = line[11:18] # may be blank, in that case 0.0
    coords_y_ft = line[18:25] # likewise
    coords_z_ft = line[25:32]
    coords_x_in = line[32:39]
    coords_y_in = line[39:46]
    coords_z_in = line[46:53]
    fixity_1 = line[54] # either blank, '0', or '1'
    fixity_2 = line[55]
    fixity_3 = line[56]
    fixity_4 = line[57]
    fixity_5 = line[58]
    fixity_6 = line[59]

and so on, ad nauseum. The regex structure has been a real pain to maintain and extend. Would pyparsing be a good solution? I have gone through all of the examples but I have not found an analogue to what I am trying to accomplish.

#### 2009-09-15 21:18:19 - crmccreary
On the above post, there are no blank lines in the file. An error when pasting text.
#### 2009-09-16 00:33:39 - ptmcg
For data like this that is column-dependent, I should think that straight string slicing would be the best bet, it looks like you have only 3 or 4 various formats.  Pyparsing *can* do column sensitive parsing, but it feels like a square peg in a round hole to me.  Even regex's feel like you are trying too hard - just go with the string slicing.



Anyway, by way of some illustration, here is what a simple expression would look like to match your JOINT records:



    from pyparsing import *
    
    intId = Word(nums)
    real = Regex(r'[+-]?\d+\.\d{0,3}').setParseAction(lambda t:float(t[0]))
    flagword = Word('01')
    
    jointDef = ('JOINT' + intId('id') + 
                real('x_ft') + 
                real('y_ft') + 
                real('z_ft') + 
                real('x_in') + 
                real('y_in') + 
                Optional(real,default=0.0)('z_in') + 
                Optional(flagword,default='000000')('flags'))



If I put your sample text into a string called 'data' and run this code:



    for j in jointDef.searchString(data):
        print j.dump()



I get this output:



    ['JOINT', '2304', -13.0, 0.0, 82.0, -6.2400000000000002, 0.0, 0.0, '111111']
    - flags: 111111
    - id: 2304
    - x_ft: -13.0
    - x_in: -6.24
    - y_ft: 0.0
    - y_in: 0.0
    - z_ft: 82.0
    - z_in: 0.0
    ['JOINT', '2314', -47.0, 0.0, 82.0, -6.0, 0.0, 0.0, '111111']
    - flags: 111111
    - id: 2314
    - x_ft: -47.0
    - x_in: -6.0
    - y_ft: 0.0
    - y_in: 0.0
    - z_ft: 82.0
    - z_in: 0.0
    etc. ...



Each field could be accessed as j.id, j.x_ft, and so on.



Here is one way to add columnar verification of a particular field:



    def realAt(c):
        def parseAction(s,l,t):
            if col(l,s) != c:
                raise ParseException(s,l,'found token at wrong column')
        return real.copy().addParseAction(parseAction)
    
    jointDef = ('JOINT' + intId('id') + 
                realAt(11)('x_ft') + 
                realAt(18)('y_ft') + 
                realAt(25)('z_ft') + 
                realAt(32)('x_in') + 
                realAt(39)('y_in') + 
                Optional(real,default=0.0)('z_in') + 
                Optional(flagword,default='000000')('flags'))



But this wont work exactly because pyparsing skips over whitespace, and your documented field locations often include left-padded spaces.
#### 2009-09-16 00:35:36 - ptmcg
So to summarize, I would not really encourage using pyparsing for this data, since it is so regular in its columns and format - but if you are set on the idea, I hope these samples might give you a head start.



-- Paul

---
## 2009-09-17 01:03:14 - codeape - Missing from subversion repository + praise
Hi,

The directories doc and examples are empty in the sourceforge subversion repository. I assume they're supposed to contain the docs and examples?



The other issue:



pyparsing is a great library. I never took the compilers course in university. Never read the dragon book either.



Read the e-book on getting started (NOT a bootleg copy... - downloaded it from Safari books online).



Was able to create a parser and evaluator for (a subset of) SQL in less than a day (this includes time for reading the e-book).



Compare this with my previous attempt at building the parser. I used ANTLR (even got the book). For me, ANTLR was just too difficult to use. A few days of experimentation basically led nowhere.



This is not to say that ANTLR is a bad. It just wasn't suitable for me.

#### 2009-09-17 01:22:44 - ptmcg
Thanks for the testionial! :)  Yes, I think it is important for people to be able to do at least something simple fairly quickly, that you don't have to learn *everything* before you can do *anything.*  I have heard many times now of those having the same experience as yours - thanks, it is definitely reassuring!



When I set up the SVN storage for pyparsing, I was very new to SVN, so I was hesitant, afraid of setting things up 'wrong'. No such excuse now, I've been using SVN for a while.  I should also add a test directory, and upload the unit tests that I have that are non-proprietary, so that others can run them too.



I'll try to get going on this in the next few days - I started a new job recently and it is really demanding quite a bit of my time and attention (but they *do* send me regular paychecks, so it is only fair...).



Cheers!

-- Paul

---
## 2009-09-17 02:05:53 - bytecolor - AST
Hello pythonistas,



I've been playing with pyparsing for a week or so now. I haven't found an example that does quite what my neurons have in mind ;) The crux of the code below is the way I alter the return values from each parse action; building an AST with with operators as the first element of each `node'. Prefix expressions, much like lisp.



I have a language in mind I'd like to attempt to implement with pyparsing. I have the EBNF complete. I was struggling with how to create an AST until I finally reduced my code to what you see below, a simple infix parser. I need to read more on pyparsing error handling.



Any comments are more than welcome.





    #!/usr/bin/python -t
    
    '''pyparse_ast.py
    
    Parse an infix expression.
    Produce an Abstract Syntax Tree.
    Solve the tree.
    
    S. Edward Dolan \<bytecolor@gmail.com\>
    Wednesday, September 16 2009
    '''
    
    import sys, pprint
    import pyparsing as pp
    
    integer = pp.Word(pp.nums).setParseAction(lambda t: int(t[0]))
    real = pp.Regex(r'((\d+\.\d*)|(\d*\.\d+))([eE][-+]?\d+)?').setParseAction(
        lambda t: float(t[0]))
    lpar, rpar = map(pp.Suppress, '()')
    factor = pp.Forward()
    term = pp.Forward()
    expression = pp.Forward()
    
    # ======================================================================
    # ast
    class AST(object):
        def __init__(self):
            self.root = None
            self.opDict = {
                '+': lambda x, y: x + y,
                'u-': lambda x: -x,
                '-': lambda x, y: x - y,
                '*': lambda x, y: x * y,
                '/': lambda x, y: x / y,
                '^': lambda x, y: x ** y
                }
    
        def solve(self):
            def dfs(node):
                # not a list
                if not isinstance(node, type([])):
                    return node
                # call the lambda, keyed on the operator
                if node[0] == 'u-':
                    return self.opDict[node[0]](dfs(node[1]))
                else:
                    return self.opDict[node[0]](dfs(node[1]),
                                                dfs(node[2]))
            return dfs(self.root[0])
    
        def makeUnaryMinusNode(self, t):
            if len(t[1:]) \> 1:
                operand = t[1:]
            else:
                operand = t[1]
            if t[0] == '-':
                return [['u-', operand]]
            return [operand]
    
        def makeFactorNode(self, t):
            if t:
                return self.makeNodes(t)
    
        def makeTermNode(self, t):
            if t:
                return self.makeNodes(t)
    
        def makeExpressionNode(self, t):
            if t:
                # This is incorrect, as expression is recursive.
                # This will be correctly set at completion of
                # the as-of-yet to be coded start rule
                self.root = self.makeNodes(t)
                return self.root
    
        def makeNodes(self, t):
            node = t[0]
            for op, operand in zip(t[1::2], t[2::2]):
                node = [op, node, operand]
            return [node]
    
    ast = AST()
    
    # ======================================================================
    # grammar
    number = integer ^ real
    
    primary = \
        (lpar + (expression) + rpar) ^ \
        (pp.oneOf('- +') + term).setParseAction(ast.makeUnaryMinusNode) ^ \
        number
    
    factor \<\< (primary + pp.Optional('^' + factor))
    factor.setParseAction(ast.makeFactorNode)
    
    term \<\< (factor + pp.ZeroOrMore(pp.oneOf('* /') + factor))
    term.setParseAction(ast.makeTermNode)
    
    expression \<\< (term + pp.ZeroOrMore(pp.oneOf('+ -') + term))
    expression.setParseAction(ast.makeExpressionNode)
    
    # ======================================================================
    # main
    pretty = pprint.PrettyPrinter(width=50)
    print 'C-d to exit'
    while 1:
        try:
            expression.parseString(raw_input('\> '))
            print 'AST', '='*50
            pretty.pprint(ast.root)
            print 'solved', '='*47
            print ast.solve()
        except EOFError:
            print
            sys.exit(0)        
        except pp.ParseException, err:
            print err.line
            print ' ' * (err.column-1) + '^'
            print err



#### 2009-09-17 03:14:32 - bytecolor
I found one oversight (also known as a fsckup). The third line, starting at primary, should call factor, not term:





    (pp.oneOf('- +') + factor).setParseAction(ast.makeUnaryMinusNode) ^ \



I wrote a func to generate random expressions, then compared the ouput of the AST with python's eval(). Seems to be Ok now. ;)
#### 2009-09-17 05:23:40 - ptmcg
Try these alternatives, using '|' instead of '^':





    number = real | integer
    
    primary = \
        (lpar + (expression) + rpar) | \
        (pp.oneOf('- +') + term).setParseAction(ast.makeUnaryMinusNode) | \
        number



'^' can get very time-consuming, especially in Forwards.



I am curious, why did you not think that the fourFn.py example would help you.  It covers much of this same ground.



Welcome to pyparsing!



-- Paul
#### 2009-09-17 11:46:20 - bytecolor
Ah, 'I see', said the blind man. I had actually looked at most/all of the examples and I had seen reference to fourFn.py searching for an AST implementation. I looked at fourFn.py once, got as far as `push' and `stack' and thought, 'These aren't the droids you're looking for'.



After a good look at it there are quite a few nuggets in there ;)



Thanks for a cool lib and for being so active on this list.
#### 2014-07-11 15:13:48 - hobs
Thank you @bytecolor ! This is exactly the pattern I was looking for. I find it much more pythonic and approachable than Paul's intricate fourFn.py example. Your approach will help me do something useful with my [GDL parsing grammar]( 'solvable' AST or a searchable propositional logic network (propnet).





    import pyparsing as pp



    # capital letter followed by any number of alphanum or underscore chars

    function_constant = pp.Word(pp.srange('[A-Za-z]'), pp.srange('[a-zA-Z0-9_]'))

    identifier = pp.Word(pp.srange('[A-Za-z]'), pp.srange('[a-zA-Z0-9_]'))

    comment = pp.OneOrMore(pp.Word(';').suppress()) + pp.restOfLine('comment')



    # GDL keywords ('Relation Constants')

    role = pp.Keyword('role')  # role(a) means that a is a player name/side in the game.

    inpt = pp.Keyword('input') # input(t) means that t is a base proposition in the game.

    base = pp.Keyword('base')  # base(a) means that a is an action in the game.

    init = pp.Keyword('init')  # init(p) means that the datum p is true in the initial state.

    next = pp.Keyword('next')  # next(p) means that the datum p is true in the next state.

    does = pp.Keyword('does')  # does(r,a) means that player r performs action a in the current state.

    legal = pp.Keyword('legal')  # legal(r,a) means it is legal for r to play a in the current state.

    goal = pp.Keyword('goal')  # goal(r,n) means that player the current state has utility n for player r. n must be an integer from 0 through 100.

    terminal = pp.Keyword('terminal')  # terminal means that the current state is a terminal state.

    distinct = pp.Keyword('distinct')  # distinct(x,y) means that the values of x and y are different.

    true = pp.Keyword('true')  # true(p) means that the datum p is true in the current state.



    variable = pp.Word('?', pp.alphas)



    # GDL-II Relation Constants

    sees = pp.Keyword('sees')  # The predicate sees(?r,?p) means that role ?r perceives ?p in the next game state.

    random = pp.Keyword('random')  # A predefined player that choses legal moves randomly



    # GDL-I Relation Constants

    relation_constant = role | inpt | base | init | next | does | legal | goal | terminal | distinct | true



    # Numerical contant

    # FIXME: too permissive -- accepts 10 numbers, '00', '01', ... '09'

    number = (pp.Keyword('100') | pp.Word(pp.nums, min=1, max=2))



    # the only binary operator (relationship constant?)

    implies = pp.Keyword('\<=')



    token = (implies | variable | relation_constant | number | pp.Word(pp.alphas + pp.nums))



    # Define recursive grammar for nested paretheticals

    grammar = pp.Forward()

    nested_parentheses = pp.nestedExpr('(', ')', content=grammar) 

    grammar \<\< (implies | variable | relation_constant | number | pp.Word(pp.alphas + pp.nums) | nested_parentheses)

    sentence = grammar + (comment | pp.lineEnd.suppress())



    game_description = pp.OneOrMore(comment | sentence)

---
## 2009-09-23 15:29:24 - maxstylus - parsing an ini file store in parent-child relationship (lists)
Hello, 



I want to parse an ini file. The basic structure of the file is this: 



[Header]

name = value

name = value

name = value



[Header]

name = value

name = value



I'm confident I can figure out the grammar associated with defining the 'Header' and each 'name = value' line.  However, what's confusing me is that I need to keep the parent-child relationship of the header and it's 'name = value' children and also I need to grab multiple lines 



So I think what I eventually need is this: [(Header, ((name = value), (name = value), etc.)]



Eventually, I need to put this into a GUI tree structure with the parent level item being the header string, and the children. 



So I basically need to:



1. Pull out the header

2. Grab the data between the end of one header ']' and the beginning of the next header: '['. 

3. Somehow the 'children' of a particular 'header' need to go together. 



Any help out there for me? :-)

Lauren

#### 2009-09-23 16:16:54 - ptmcg
Lauren -



Thanks for your post, and welcome to the world of pyparsing!



Look over this example (), and see if it helps you in parsing your file.



-- Paul

---
## 2009-10-01 06:35:15 - Lllama - iptables parsing
Hello all,



I'm having some success with parsing iptables output but I'm bumping up against my ability with some of the rules.



Here's what I've got at the moment





    from pyparsing import Word, OneOrMore, alphanums, Literal, restOfLine, Group, LineEnd, nums, Optional
    
    
    TARGET = Word(alphanums+'~')
    PROTO = Word('!'+'tcp'+'udp'+'icmp'+'all'+nums)
    OPT = Word(alphanums+'-')
    SOURCE = Word(nums+'.'+'/'+'!')
    DEST = SOURCE
    OPTIONS = restOfLine
    
    RULE = Optional(TARGET) + PROTO + OPT + SOURCE + DEST + Optional(OPTIONS)
    
    CHAIN_HEADER = Literal('Chain').suppress() + Word(alphanums+'~') + Literal('(').suppress() + Word(nums) + Literal('references)').suppress()
    
    FILLER = Literal('target')+Literal('prot')+Literal('opt')+Literal('source')+Literal('destination')
    
    CHAIN = Group(CHAIN_HEADER + FILLER.suppress() + Group(OneOrMore(RULE)))
    RULESET = OneOrMore(CHAIN)
    
    print RULESET.parseString('''
    Chain BADTCP (2 references)
    target     prot opt source               destination         
    PSCAN      tcp  --  0.0.0.0/0            0.0.0.0/0           tcp flags:0x03/0x03 
    NEWNOTSYN  tcp  --  0.0.0.0/0            0.0.0.0/0           tcp flags:!0x17/0x02 state NEW 
    
    Chain DMZHOLES (1 references)
    target     prot opt source               destination         
    ACCEPT     tcp  --  192.168.0.1          10.0.0.1          tcp dpt:53 
    ACCEPT     udp  --  192.168.0.1          10.0.0.1          udp dpt:162 
    
    Chain ipac~o (1 references)
    target     prot opt source               destination         
               all  --  0.0.0.0/0            0.0.0.0/0           
    ''')



This works for the BADTCP and DMZHOLES chains but the ipac~o chain isn't being parsed. It seems that this is the lack of a target in the rule. i.e. changing it to:





    Chain ipac~o (1 references)
    target     prot opt source               destination         
    ACCEPT     all  --  0.0.0.0/0            0.0.0.0/0           



works. I tried to correct this with the Optional(TARGET) but no joy.



Can anyone offer any insight into what I'm doing wrong?



Thanks in advance,



Felix



P.S. I'm doing Optional(OPTIONS) as I'm going to add in parsing for them later.

#### 2009-10-02 00:57:26 - ptmcg
Well done, you really are very close with this.



Remember that pyparsing is a pure left-to-right parser, with no lookahead.  To get some insight into what is happening, temporarily change TARGET to:



    TARGET = Word(alphanums+'~').setName('TARGET').setDebug()



This will give this debugging output:



    Match TARGET at loc 91(4,1)
    Matched TARGET -\> ['PSCAN']
    Match TARGET at loc 173(5,1)
    Matched TARGET -\> ['NEWNOTSYN']
    Match TARGET at loc 268(7,1)
    Matched TARGET -\> ['Chain']
    Match TARGET at loc 360(9,1)
    Matched TARGET -\> ['ACCEPT']
    Match TARGET at loc 431(10,1)
    Matched TARGET -\> ['ACCEPT']
    Match TARGET at loc 505(12,1)
    Matched TARGET -\> ['Chain']
    Match TARGET at loc 606(14,12)
    Matched TARGET -\> ['all']



Ah! Look at that last match - 'all' is being read as TARGET, even though you made it optional.  Pyparsing, unlike regular expressions, does not have the same lookahead capabilities.  After parsing the suppressed header (I liked the way you did this in FILLER), it advances to the next non-whitespace, and tries to match the next bit of the parser, which is an Optional(TARGET).  TARGET is a Word(alphanums+'~'), which 'all' certainly matches.  So it matches TARGET, then goes on to match PROTO, but '--' is the next part, and that is not a PROTO, so no RULE match.



The solution is to add your own lookahead.  *You* know that 'all' is not a valid TARGET, and neither is 'tcp', 'udp', etc.  So add a negative lookahead to TARGET, using the '~' operator defined in pyparsing (after moving PROTO to just before TARGET):



    TARGET = ~PROTO + Word(alphanums+'~')



Voila!  Now your parser correctly recognizes the last CHAIN and RULE.



A couple of other points:

- You define PROTO as:



    PROTO = Word('!'+'tcp'+'udp'+'icmp'+'all'+nums)



Remember that Word is not Literal - Word means 'parse a group of contiguous characters in the following set'.  So your PROTO is actually a parser of Word('!tcpudpicmpall0123456789').  This will match not only 'tcp', 'udp', or 'icmp', but also 'tdi', 'itupcmicp', 'mp0tcid!', which I don't think is your intent.  I don't know your format here, but I'm going to guess that a PROTO is one of these 'tcp', 'udp', 'icmp', or 'all', with an optional '!' in front, and optional digit or digits on the end.  Here is a better definition:



    PROTO = Combine( Optional('!') + oneOf('tcp udp icmp all') + Word(nums) )



Why Combine?  Two reasons.  One, you want a PROTO such as '!all14' returned as a single string, not the individual tokens ['!', 'all', '14'], and Combine will concatenate them for you before returning the result.  Two, you *don't* want PROTO to match 'tcp 0', but only 'tcp0', and by default Combine only accepts matches if there is no intervening whitespace between its elements.



- You nicely used Group to group the tokens for each CHAIN, so that they don't all run together.  You'll probably want to do the same for RULE, so that within each CHAIN, the RULE tokens are kept in their own separate lists.



But overall, nice job, and welcome to pyparsing!
#### 2009-10-02 04:00:22 - Lllama
Hi Paul,



First off: thanks for the excellent reply. It's really appreciated.



I've tried your suggestions and I've had to make the following modification. Instead of



    PROTO = Combine( Optional('!') + oneOf('tcp udp icmp all') + Word(nums) )

I've used:



    PROTO = Combine( Optional('!') + Or(oneOf('tcp udp icmp all') + Word(nums) ) )

In the original version I was getting



    ParseException: Expected W:(0123...) (at char 105), (line:4, col:15)

errors. This is actually in line with the preferred syntax anyway, as the rules will be either 'tcp', 'udp', etc _or_ a number. e.g. tcp80 is not valid.



I've a couple of follow up questions as well.



1. When a TARGET is not found is it possible to provide a default/empty value? When I try to unpack a rule without a target I get a ValueError as the list isn't long enough.



2. A preferred alternative to the above would be returning the results as a series of Dicts. I'd like to have a dict of lists of dicts e.g.:



    ruleset = {
        'BADTCP':
             [
               {'target': 'PSCAN', 'proto': 'tcp', 'opts': '--', ..... },
               {'target': 'NEWNOTSYN', 'proto': 'tcp', 'opts': '--', ..... },
             ],
        'DMZHOLES': 
             [
               {'target': 'ACCEPT', 'proto': 'tcp', 'opts': '--', ..... },
               {'target': 'ACCEPT', 'proto': 'udp', 'opts': '--', ..... },
             ],
    
        'ipac~o':
             [
               {'target': '', 'proto': 'all', 'opts': '--', ......},
               .....
             ]
        }



I'm guessing I should be using setResultsName (or the callable syntax alternative), so I'll have an experiment with that first.



Thanks again for your help. It's starting to make a bit more sense...



Felix
#### 2009-10-02 05:23:43 - ptmcg
I just referred another poster to the JSON parser on the examples page, and if you want to pay for a back copy, or already have one, there is a detailed description of this parser in the August,'08 issue of Python magazine.  It uses results names and the Dict class to auto-generated the self-named structure you describe.



As to your question about a default value, Optional takes a second argument, a default value to be returned if the optional expression is not present.  But your first instinct is better, I think - results names are the way to go.



-- Paul
#### 2009-10-02 05:28:05 - ptmcg
Oh, and also, just a tweak to this:



    PROTO = Combine( Optional('!') + Or(oneOf('tcp udp icmp all') + Word(nums) ) )



should be:



    PROTO = Combine( Optional('!') + (oneOf('tcp udp icmp all') | Word(nums) ) )



Your construct works, but I don't quite understand why.  It actually expands to:



    PROTO = Combine( Optional('!') + Or(And(oneOf('tcp udp icmp all'), Word(nums) ) ) )



or at least it should, but it doesn't appear to behave this way.  Hmmmmm...
#### 2009-10-02 07:53:58 - Lllama


Thanks again for the quick response. I've had a play with setResultsName and it all seems to be working now. I'll post the full example below. I'm running into trouble with some rules but they're not important for what I want to do, so I'm jump trimming them from the file at the moment.



One gotcha:



    ...
    RULE = Optional(TARGET)('target') + PROTO('proto') + OPT('opt') + SOURCE('source') + DEST('dest') + Optional(OPTIONS('options'))
    rule = RULE.parseString('''PSCAN      tcp  --  0.0.0.0/0            0.0.0.0/0           tcp flags:0x3F/0x29''')
    print rule.target
    \>\>\> ['PSCAN']

After some digging I discovered that I'm setting the results name in the wrong place. The above should be



    PROTO = Combine(....)
    TARGET = ~PROTO + Word(alphanums+'~').setResultsName('target')
    RULE = Optional(TARGET) + PROTO('proto') + OPT('opt') + SOURCE('source') + DEST('dest') + Optional(OPTIONS('options'))
    rule = RULE.parseString('''PSCAN      tcp  --  0.0.0.0/0            0.0.0.0/0           tcp flags:0x3F/0x29''')
    print rule.target
    \>\>\> PSCAN

I can sort of see why this is happening.



Here's the full parser that I've come up with. Now I need to try and convert them into some class instances...



Thanks again for the help.



    PROTO = Combine( Optional('!') + (oneOf('tcp udp icmp all') | Word(nums) ))
    TARGET = ~PROTO + Word(alphanums+'~').setResultsName('target')
    OPT = Word(alphanums+'-')
    SOURCE = Word(nums+'.'+'/'+'!')
    DEST = SOURCE
    OPTIONS = restOfLine
    
    RULE = Optional(TARGET) + PROTO('proto') + OPT('opt') + SOURCE('source') + DEST('dest') + Optional(OPTIONS('options'))
    
    CHAIN_HEADER = Literal('Chain').suppress() + Word(alphanums+'~').setResultsName('chain_name') + Literal('(').suppress() + Word(nums)('refs') + Literal('references)').suppress()
    
    FILLER = Group(Literal('target')+Literal('prot')+Literal('opt')+Literal('source')+Literal('destination')).suppress()
    
    CHAIN = Dict(Group(CHAIN_HEADER + FILLER + OneOrMore(Group(RULE)).setResultsName('rules')))
    RULESET = Dict(OneOrMore(CHAIN))('chains')



(Not sure if I've got too many Dict calls in there...)

---
## 2009-10-02 04:34:58 - nielAtpyparsing - Problem with recursive/nested parsing
Hi,



I am trying to define a grammer for parsing formatted output which I wish to convert to CSV output. My formatted data will look something as follows:





    [record 
        size_of_rec 374
        dat [record
            field1 'valX'
            field2 [record
                fieldX 'val1'
                fieldY 'val2']
                ]
        fieldW 'valW'
    ]



My python code is below:





    from pyparsing import *;
    
    # The global bnf
    bnf = None;
    
    def STR_REP_BNF():
       # String patterns
       LBRACK     = '[';
       RBRACK     = ']';
       RECORD     = 'record';
       VECTOR     = 'vector';
       UNION      = 'union';
    
       '''
       Define the BNF for describing the language of the
       string formatted data records.
       '''
       global bnf;
    
       if not bnf:
          # Tokens
          #####################################
          # punctuation
          lbrack          = Literal(LBRACK).suppress();
          rbrack          = Literal(RBRACK).suppress();
    
          # Keywords
          recordToken_    = Keyword(RECORD);
          vectorToken_    = Keyword(VECTOR);
          unionToken_     = Keyword(UNION);
          #####################################
    
          # Rules
          #####################################
          # Primitives
          field           = Forward().setName('field');
          record          = Forward().setName('record');
          vector          = Forward().setName('vector');
          name            = ( Word(printables) ).setName('name');
          value           = ( Word(alphanums) | quotedString | Group(record) | Group(vector) ).setName('value');
    
          # name value(s)
          field \<\< Group( name + value );
    
          # [record
          #    field  --------
          #    ^              |
          #    |---------------
          #  ]
          record \<\< ( lbrack + recordToken_ + OneOrMore(field).setResultsName('record') + rbrack );
    
          # [vector
          #    value1,
          #    ...,
          #    valueN
          # ]
          vector \<\< ( lbrack + vectorToken_ + delimitedList(value, ',').setResultsName('vector') + rbrack );
    
          data = record;
          #####################################
    
          # Configure the BNF
          bnf = OneOrMore(data);
    
       # Return the BNF
       return bnf;
    
    def parseData(bnf, buffer):
       '''
       Parse the data
       '''
       try:
         # Parse the buffer using the bnf
         tokens = bnf.parseString(buffer);
         print 'tokens = ';
         print tokens.asList();
    
       except ParseException, err:
         print buffer + ' --\>';
         print err.line;
         print ' '*(err.column-1) + '^';
         print err;
    
    testString = '''[record 
        size_of_rec 374
        dat [record
            field1 'valX'
            field2 [record
                fieldX 'val1'
                fieldY 'val2']
                ]
        fieldW 'valW'
    ]
    ''';
    parseData(STR_REP_BNF(), testString);
    
    # Expected output
    #tokens = 
    #['record', ['size_of_rec', '374'], ['dat', ['record', ['field1', ''valX''], ['field2', ['record', ['fieldX', ''val1''], ['fieldY', ''val2'']]]]], ['fieldW', ''valW'']]
    
    # Exception getting at the moment
    #[record 
    #    size_of_rec 374
    #    dat [record
    #        field1 'valX'
    #        field2 [record
    #            fieldX 'val1'
    #            fieldY 'val2']
    #            ]
    #    fieldW 'valW'
    #]
    # --\>
    #    dat [record
    #    ^
    #Expected ']' (at char 33), (line:3, col:5)



I would expect the output to look as follows:





    ['record', ['size_of_rec', '374'], ['dat', ['record', ['field1', ''valX''], ['field2', ['record', ['fieldX', ''val1''], ['fieldY', ''val2'']]]]], ['fieldW', ''valW'']]



However, I am getting the following parse exception:





    [record 
        size_of_rec 374
        dat [record
            field1 'valX'
            field2 [record
                fieldX 'val1'
                fieldY 'val2']
                ]
        fieldW 'valW'
    ]
     --\>
        dat [record
        ^
    Expected ']' (at char 33), (line:3, col:5)



Any advice on how I need to adjust my grammar would be greatly appreciated.

#### 2009-10-02 05:10:37 - nielAtpyparsing
After reading some of the other posts I noticed the negative look ahead functions.



Looking into that I realized that my 'name' primitive was consuming my left/right brackets.



Updating the name specifier seems to have solve my problem:





    name            = ( ~lbrack + ~rbrack + Word(printables) ).setName('name');



Output now:





    ['record', ['size_of_rec', '374'], ['dat', ['record', ['field1', ''valX''], ['field2', ['record', ['fieldX', ''val1''], ['fieldY', ''val2'']]]]], ['fieldW', ''valW'']]



If I run into any other problems I'll post it here. (I am still learning how to use pyparsing)



PS. Thanks for this awesome library :)
#### 2009-10-02 05:20:28 - ptmcg
Great to have you joining in.



This is similar to the JSON parser on the examples page.  You can check it out, and also see the article in the Python Magazine from August, 2008 for other ideas.



I'm surprised that you need so many Forwards.  Try this:



<ol><li>at the very top</li></ol>value = Forward().setName('value')



<ol><li>define record, field, and array as ordinary expressions, using value</li></ol>field = Group( name + value ).setName('field')

<ol><li>etc.</li></ol>

<ol><li>finally</li></ol>value \<\< ( Word(alphanums) | quotedString | Group(record) | Group(vector) )



Also, there is no need for all those trailing ';'s. :)  



Parse on!

---
## 2009-10-08 03:58:35 - nielAtpyparsing - Installing pyparsing on HP-UX with python 2.5
Hi,



I wrote a small python parser using pyparsing to convert formatted data into CSV. I need to run this on a HP-UX platform, but am having trouble getting the pyparsing library installed.



This is the instruction from the README:





    Extract van README
    Installation
    ============
    
    Do the usual:
    
        python setup.py install
    
    (pyparsing requires Python 2.3.2 or later.)



When running the command we get an exception in pyparsing_py3.py file:





    #python setup.py install
    running install
    running build
    running build_py
    creating build
    creating build/lib
    copying pyparsing.py -\> build/lib
    copying pyparsing_py3.py -\> build/lib
    running install_lib
    copying build/lib/pyparsing.py -\> /usr/local/lib/python2.5/site-packages
    copying build/lib/pyparsing_py3.py -\> /usr/local/lib/python2.5/site-packages
    byte-compiling /usr/local/lib/python2.5/site-packages/pyparsing.py to pyparsing.pyc
    byte-compiling /usr/local/lib/python2.5/site-packages/pyparsing_py3.py to pyparsing_py3.pyc
      File '/usr/local/lib/python2.5/site-packages/pyparsing_py3.py', line 2470
        except ParseException as err:
                               ^
    SyntaxError: invalid syntax
    
    running install_egg_info
    Writing /usr/local/lib/python2.5/site-packages/pyparsing-1.5.2-py2.5.egg-info



We are running python version 2.5:





    Python 2.5.2 (r252:60911, Mar 12 2008, 18:10:13) [C] on hp-ux11
    Type 'help', 'copyright', 'credits' or 'license' for more information.
    \>\>\>



Any assistance would be greatly appreciated.

#### 2009-10-08 04:00:50 - nielAtpyparsing
This is a 64-bit HP-UX installation:





    $ uname -a
    HP-UX hostname B.11.31 U ia64 2902944534 unlimited-user license


#### 2009-10-08 04:38:38 - ptmcg
I have *got* to put out a new release that fixes this!



There really is nothing wrong with your installation.  What is happening is that I am trying to install both the Python2 and Python3 compatible versions of pyparsing.  When the installer goes to precompile the sources, it chokes on the one that is not your version, since Py2 and Py3 have incompatible syntax for exception handling.  (There *is* a cross-version compatible form, but it is about 10% slower.)



The error you are getting is benign, go ahead and start using pyparsing!

---
## 2009-10-12 14:33:39 - gregglind - isinstance considered harmful :)
So, I'm noticing that even for simple parses, there are *a lot* (sometimes 100+) of isinstance calls.  In profiling my code, they are one of the main bottlenecks in my code.  Any thoughts on this?  



I've done some timing tests, and it does seems isinstance itself is pretty fast.  



Cheers,



GL

#### 2009-10-13 09:11:53 - ptmcg
Gregg -



I've been doing some work with profiling, including Robert Kern's line_profiler, but nothing conclusive yet.



Pyparsing *does* have quite a number of isinstance calls, and there may be some merit in copying the global isinstance to a local in a few routines that call it a lot.  But I wouldn't look for huge performance gains.



I suspect isinstance may get expensive if working with classes with deep or multiple inheritance chains, since it has to navigate through the full list of superclasses to see if this is an instance or subclass of the given type or types.  Does your grammar create object instances as part of some parse action, and if so, do these objects have any complex inheritance hierarchy?



-- Paul

---
## 2009-10-16 14:17:23 - nathanielpeterson - extending fourFn.py 
I'd like to extend the definition of expr in  so that it is possible to parse '+(2.178)' and '-(2.178)'.



I seem to have managed to do this by changing



    ident = Word(alphas, alphas+nums+'_$')
            atom = (Optional('-') + ( pi | e | fnumber | ident + lpar + expr + rpar ).setParseAction( pushFirst ) | ( lpar + expr.suppress() + rpar )).setParseAction(pushUMinus) 

to



    ident = ZeroOrMore(Word(alphas, alphas+nums+'_$'))
            atom = ( Optional('-')+Optional('+') + ( pi | e | fnumber | ident + lpar + expr + rpar ).setParseAction( pushFirst ) | ( lpar + expr.suppress() + rpar )).setParseAction(pushUMinus) 

These are the tests that I added to fourFn.py:



    test( '-(2.178)', -2.178 )    
        test( '+(2.178)', 2.178 )

Although this allows the tests to pass, I have doubts about the correctness of my changes.  

In particular, Optional('-')+Optional('+') feels wrong. 

Is there a better way?



Many thanks in advance.

#### 2009-10-16 15:14:03 - ptmcg
Just change Optional('-') to Optional(oneOf('- +')) and this should get you what you want.  (Please wrap your code listings in the future.)



Welcome to pyparsing!



-- Paul
#### 2009-10-16 16:09:07 - nathanielpeterson
Thanks, for the great (and quick) help!



While trying to better understand what expr.suppress() does, I tried

running fourFn.py with





    atom = ((Optional(oneOf('- +')) +
                     (pi|e|fnumber|ident+lpar+expr+rpar).setParseAction(pushFirst))
    #                 | (lpar+expr.suppress()+rpar)
                    ).setParseAction(pushUMinus)         



and to my surprise, it still passes all the tests. Could it be that adding ZeroOrMore

to the definition of ident removed the need of (lpar+expr.suppress()+rpar) ?
#### 2009-10-17 08:44:17 - nathanielpeterson
I made a major mistake in all of my previous posts.



The edits I made to the bnf are wrong and do not pass the tests. (I mindlessly assumed if the program ended, the tests must have passed.)



Now that I semi-know what I'm doing, it appears a possible way to extend the fourFn parser to handle '+(2.178)' and '-(2.178)' is 



    ident = Word(alphas, alphas+nums+'_$')
            atom = ((Optional(oneOf('- +')) +
                     (pi|e|fnumber|ident+lpar+expr+rpar).setParseAction(pushFirst))
                    | Optional(oneOf('- +')) + (lpar+expr.suppress()+rpar)
                    ).setParseAction(pushUMinus)         

I still don't think I quite understand what expr.suppress() does. Could you please shed some light on how it works?
#### 2009-10-17 11:16:55 - ptmcg
Look at everything *around* expr.suppress() first.  Do you understand what is happening with pushFirst?  Each term, operator, and operand get pushed onto stack, converting the arithmetic infix notation (like '9+3') to reverse Polish notation on the stack ('9 3 +').  Now the stack can be evaluated using a recursive evalStack function.



The purpose of expr is to structure the parser so that operations nested within parens get parsed (and therefore pushed onto the stack) first.  But expr itself shouldn't push *anything* onto the stack - the operands and operator within expr get pushed as they are parsed individually - we don't want to push them on again when expr is finally evaluated.  So we suppress the results from expr.  Try parsing this expression '9*(1+2)' and look at the contents of the stack before it gets eval'ed - it should be something like '1 2 + 9 *'.  If we didn't suppress expr, the stack would have something like '1 2 + ['1', '+', '2'] 9 *'.



HTH,

-- Paul
#### 2009-10-17 13:42:38 - nathanielpeterson
Thank you for explanation Paul. Following your suggestion, I put a trace decorator on pushFirst and that helped quite a bit. But (as you'll see) I'm still confused on some points.



I downloaded a fresh copy of fourFn.py (from ) and changed expr.suppress() to expr.



I added these tests too:





    test( '9*(1+2)', 9*(1+2) )
        test( '9*(1+(3*4))', 9*(1+(3*4)) )        

fourFn.py still passes all the tests even with the call to suppress() removed. There is no change to exprStack. 





with suppress() intact (original version):



    s: 9*(1+2)
    expVal: 27
    results: ['9', '*']
    exprStack: ['9', '1', '2', '+', '*']
    val: 27.0



with suppress() removed:



    s: 9*(1+2)
    expVal: 27
    results: ['9', '*', '1', '+', '2']
    exprStack: ['9', '1', '2', '+', '*']
    val: 27.0



Does this mean the call to suppress is not necessary, or am I still missing something?



PS. The suppress method call was on the second instance of expr, not the first.

The second expr, in (lpar+expr.supress()+rpar) does not have a setParseAction(pushFirst) cal attached to it, only a pushUMinus. So I'm really confused what good the suppress call is doing.





    atom = ((Optional(oneOf('- +')) +
                     (pi|e|fnumber|ident+lpar+expr+rpar).setParseAction(pushFirst))
                    | Optional(oneOf('- +')) + (lpar+expr.supress()+rpar)
                    ).setParseAction(pushUMinus)         


#### 2009-10-17 14:33:49 - ptmcg
Hmmm, your analysis is right.  Here is what I think was the sequence of events (it has been a while since I first wrote fourFn.py).  I think the early version of fourFn.py did *not* support the sin,cos, etc. functions, so that atom originally looked like:



    atom = (Optional('-') + 
              ( pi | e | fnumber | lpar + expr.suppress() + rpar ).setParseAction( pushFirst )
           ).setParseAction(pushUMinus) 



In this incarnation, the suppress() *was* required for the reasons I stated earlier (this also required a minor change to pushFirst:



    if toks:
        exprStack.append( toks[0] )



Then when I added the 'ident + lpar + expr + rpar' option, I took out the 'lpar + expr + rpar' group outside of the expression with the pushFirst parse action, but I left the .suppress() in place.  But you are correct, the suppress is unnecessary.



In fact, if you change atom to read:



    atom = (Optional('-') + ( pi | e | fnumber | ident + lpar + expr + rpar ).setParseAction( pushFirst ) | 
                    Group( lpar + expr + rpar )).setParseAction(pushUMinus) 



Now the results get returned as a nice nested list reflecting the () grouping, and you still build the exprStack properly.



Good catch, I'll clean this up in 1.5.3!



-- Paul
#### 2009-10-17 17:40:40 - nathanielpeterson
Thanks Paul, you're awesome!

---
## 2009-10-19 12:23:49 - gregglind - Treating notAny as words?
from pyparsing import *



_kw_aliases = dict(s='site', l='limit')



class QueryParser(object):

    ''' parser for queries '''

    setting_name = oneOf(list(set(_kw_aliases.keys() + _kw_aliases.values())))('name')

    setting_value = OneOrMore(~setting_name + Word(alphanums + '._/-'))('value')

    setting = Group(setting_name + Suppress('=') + setting_value)('setting')

    _query = OneOrMore(setting)('settings') + lineEnd



    def parse(self,string):

        # default query

        parsed = self._query.parseString(string)

        return parsed





QP = QueryParser()

QP.parse( 's = abcde.net l = 10')



QP.parse( 's = s')  # bad!

QP.parse( 's = localhost')  # worse!



'''

<hr />
Question:



How do I get this to work?  The problem is that 'localhost' begins with 'l'

which is the name of a command.  I'm clearly not grokking something 

about 'WordStart' or something like that.  I'm sure this is an easy

fix, but it is eluding me at the moment.  What is a 'right' or 'better' way of handling it....





'''

#### 2009-10-19 12:42:26 - ptmcg
Look at these changes and see what you make of them.  I can't go into much explanation now, I'll try to write back this evening.



    class QueryParser(object):
        ''' parser for queries '''
        setting_name = oneOf(list(set(_kw_aliases.keys() + _kw_aliases.values())))('name')
        setting_name = MatchFirst(map(Keyword, _kw_aliases.keys() + _kw_aliases.values()))('name')
        setting_value = (Word(alphanums + '._/-') + ZeroOrMore(~setting_name + Word(alphanums + '._/-')))('value')
        setting = Group(setting_name + Suppress('=') + setting_value)#('setting')
        _query = OneOrMore(setting)('settings') + lineEnd
        _query = Dict(OneOrMore(setting)) + lineEnd
    
        def parse(self,string):
            # default query
            parsed = self._query.parseString(string)
            print parsed.dump()
            return parsed



-- Paul
#### 2009-10-19 14:45:01 - gregglind
I will review this!  Thanks for the advice!



Gregg
#### 2009-10-20 07:36:39 - gregglind
So, since you like a challenge :) 



How about this, more general varargs spec:



    Settings ::= OneOrMore(Setting)

    Setting ::= name '=' OneOrMore(Words)



I know you shown this kind of thing before.  Is this even formally LL(1)?
#### 2009-10-20 07:38:17 - gregglind
Oh, and here is a parser that will (hackishly!) do this, with rational assumptions ('=' not in words, etc.)





    def ppp(string):

        string = string.replace('=', ' = ')

        f = [(ii,x) for (ii,x) in enumerate(string.split())]

        L = len(f)

        e = [x[0]-1 for x in f if x[1] == '=']

        print 'e: ', e

        parts = zip( e, e[1:] + [L,])

        print 'parts', parts

        for (ii,(l,r)) in enumerate(parts):

            print 'part %i: ' % ii, [x[1] for x in f[l:r]]



        print ' '



    ppp( 's = abcde.net l = 10')

    ppp( 's = s l s s l=blah blah blah')  

    ppp( 's = localhost')
#### 2009-10-20 07:40:32 - gregglind
<ul class="quotelist"><li>def ppp(string):</li><li>string = string.replace('=', ' = ')</li><li>f = [(ii,x) for (ii,x) in enumerate(string.split())]</li><li>L = len(f)</li><li>e = [x[0]-1 for x in f if x[1] == '=']</li><li>print 'e: ', e</li><li>parts = zip( e, e[1:] + [L,])</li><li>print 'parts', parts</li><li>for (ii,(l,r)) in enumerate(parts):</li><li>print 'part %i: ' % ii, [x[1] for x in f[l:r]]</li></ul>\>

\>    print ' '

\>

<ul class="quotelist"><li>ppp( 's = abcde.net l = 10')</li><li>ppp( 's = s l s s l=blah blah blah')</li><li>ppp( 's = localhost')</li></ul>
#### 2009-10-20 11:24:14 - ptmcg
Gregg -



Something like this maybe?





    EQ ::= Literal('=')
    Settings ::= OneOrMore(Setting)
    Setting ::= name EQ OneOrMore(Words + ~EQ)





When you post code, could you use [[code]] tags?  This makes it easier to copy/paste your code.



Thanks,

-- Paul
#### 2009-10-20 15:18:01 - gregglind
Sorry about the code thing! I couldn't figure out what the tag was for posting it, and there's no preview in wikispaces!  Irritating.  Will try with your code.  Thanks for the ideas, as always.



Gregg
#### 2009-10-20 16:28:55 - gregglind
Building on Paul's suggestions:



[[EQ = Suppress(Literal('='))wordchars = ''.join(x for x in printables if x not in ('=',))setting_name = Word(wordchars)('name')  setting_value = OneOrMore(Word(wordchars) + ~EQ)('value')setting = Group(setting_name +  EQ  + setting_value)('setting')_query = Dict(OneOrMore(setting))('settings') + lineEndD = _query.parseString('s=abcde.net yahoop.edu mungo.big l = 10')for x in D.settings:      print x['name'], x['value']]]



Thank you!
#### 2009-10-20 16:33:35 - gregglind
Ugh!  Formatting again.... why doesn't wikispaces have preview!?!







    from pyparsing import *
    
    EQ = Suppress(Literal('='))
    wordchars = ''.join(x for x in printables if x not in ('=',))
    setting_name = Word(wordchars)('name')
    setting_value = OneOrMore(Word(wordchars) + ~EQ)('value')
    setting = Group(setting_name +  EQ  + setting_value)('setting')
    _query = Dict(OneOrMore(setting))('settings') + lineEnd
    
    
    D = _query.parseString('s=abcde.net yahoop.edu mungo.big l = 10')
    for x in D.settings:  print x['name'], x['value']
    



Thank you, this is much more robust than my original code.
#### 2009-10-20 16:46:02 - ptmcg
Great!  Here is what Dict does for you.  If you know the keys that the user gives, you can do this:



    \>\>\> print D.settings.s
    ['abcde.net', 'yahoop.edu', 'mungo.big']
    \>\>\> print D.settings.l
    10
    \>\>\>



Or treat D.settings like a dict:



    \>\>\> print D.settings.keys()
    ['s', 'setting', 'l']



(with Dict-style access, the 'setting' results name is not really needed anymore).



Or just use dump() to see all the nested keys and values:



    \>\>\> print D.dump()
    [[['s', 'abcde.net', 'yahoop.edu', 'mungo.big'], ['l', '10']]]
    - settings: [['s', 'abcde.net', 'yahoop.edu', 'mungo.big'], ['l', '10']]
      - l: 10
      - s: ['abcde.net', 'yahoop.edu', 'mungo.big']
        - name: s
        - value: ['abcde.net', 'yahoop.edu', 'mungo.big']
      - setting: ['l', '10']
        - name: l
        - value: ['10']



Happy parsing!

---
## 2009-10-29 04:19:17 - zike2000 - AttributeError: 'module' object has no attribute 'copy'
<ol><li>Will try to explain on following example</li></ol>#########

import os

import string

from pyparsing import *

import urllib

import pdb



integer = Word(nums)

ipAddress = Combine( integer + '.' + integer + '.' + integer + '.' + integer )

tdStart,tdEnd = makeHTMLTags('td')

timeServerPattern =  tdStart + ipAddress.setResultsName('ipAddr') + tdEnd + \

        tdStart + SkipTo(tdEnd).setResultsName('loc') + tdEnd



<ol><li>get list of time servers</li></ol>nistTimeServerURL = ''

serverListPage = urllib.urlopen( nistTimeServerURL )

serverListHTML = serverListPage.read()

serverListPage.close()



addrs = {}

for srvr,startloc,endloc in timeServerPattern.scanString( serverListHTML ):

    print srvr.ipAddr, '-', srvr.loc

    addrs[srvr.ipAddr] = srvr.loc



#### ############3



so then i desided to make procedure, so i could feed link into it 

#### ############



import os

import string

from pyparsing import *

import urllib

import pdb



def poc_prin (f): 

  integer = Word(nums)

  ipAddress = Combine( integer + '.' + integer + '.' + integer + '.' + integer )

  tdStart,tdEnd = makeHTMLTags('td')

  timeServerPattern =  tdStart + ipAddress.setResultsName('ipAddr') + tdEnd + \

        tdStart + SkipTo(tdEnd).setResultsName('loc') + tdEnd



  # get list of time servers

  nistTimeServerURL = link

  serverListPage = urllib.urlopen( nistTimeServerURL )

  serverListHTML = serverListPage.read()

  serverListPage.close()



  addrs = {}

  for srvr,startloc,endloc in timeServerPattern.scanString( serverListHTML ):

      print srvr.ipAddr, '-', srvr.loc

     addrs[srvr.ipAddr] = srvr.loc



link = ''

proc_prin(link)



<ol><li>and now start getting listed error and even first script doesn't work, but it used to!</li><li>have unicode strings in orriginal cript</li></ol>#Any suggestion ?





Entire error text is:

{Suppress:('\0x') W:(0123...)}         ---- here i  printed out self as pice of the code where error appears is: (cpy = copy.copy( self ))



C:\Python26\>python.exe c:\web_dev\site_rip\pars.py

{Suppress:('\0x') W:(0123...)}         ---- here i  printed out self as pice of the code where error appears is: (cpy = copy.copy( self ))

Traceback (most recent call last):

  File 'c:\web_dev\site_rip\pars.py', line 6, in \<module\>

    import pyparsing

  File 'C:\Python26\lib\site-packages\pyparsing.py', line 3303, in \<module\>

    _escapedHexChar = Combine( Suppress(_bslash + '0x') + Word(hexnums) ).setPar

seAction(lambda s,l,t:unichr(int(t[0],16)))

  File 'C:\Python26\lib\site-packages\pyparsing.py', line 2992, in <u>init</u>

    self.leaveWhitespace()

  File 'C:\Python26\lib\site-packages\pyparsing.py', line 2611, in leaveWhitespa

ce

    self.expr = self.expr.copy()

  File 'C:\Python26\lib\site-packages\pyparsing.py', line 703, in copy

    cpy = copy.copy( self )

AttributeError: 'module' object has no attribute 'copy'

#### 2009-10-29 04:42:09 - zike2000
Question is closed! Cause found.



there was a file name copy.py at the same folder , there fore it was used instad of right copy module...

---
## 2009-10-29 05:44:22 - marjoj - setResultsName() problem
I finally got the pyparsing module to my python. And it has been really easy (compared to lex/yacc and regular expressions) to build a parser, so this is an extremely great tool! 



I'm building a parser for a log file. There are multiple different commands with different number of attributes, which all should match the parse string. 



One example line to be matched is:

[2009-08-07 10:47:08.076] 123456/xxx/yyy *SOME_COMMAND* dog 0x262, cat 0x2000000, horse 0x2000020, mouse 0x85A079F7



My parser code is working, but I would like to be able to set result names to those attributes, not a certain string but the value of attributeName.





Here is a part of my code:



attributeName = Word(alphas)

AttributeValue = Word(alphanums)



attribute = attributeName('name') + AttributeValue('value') + Suppress (',')

lastAttribute = attributeName('name') + AttributeValue('value')



AnimalCommand = timeStamp('timeStamp') + location('location') + command('command')+ OneOrMore (attribute) + Optional(lastAttribute)





And here is what I get from dump():



- command: *SOME_COMMAND*

- location: ['123456/xxx/yyy']

  - country: 123456

  - town: xxx

  - street: yyy

- timeStamp: ['2009-08-07 10:48:43.032']

  - date: 2009-08-07

  - time: 10:48:43.032





Additionally I would also like to get this in dump():

- dog: ['dog', '0x262']

  - name: dog

  - value: 0x262

- cat: ['cat', '0x2000000']

  - name: cat

  - value: 0x2000000 

- horse: ['horse', ' 0x2000020']

  - name: horse

  - value:  0x2000020

- mouse: ['mouse', '0x85A079F7']

  - name: mouse

  - value: 0x85A079F7





This one I tried, but it did not work.



AnimalCommand = timeStamp('timeStamp') + location('location') + command('command')+ OneOrMore (attribute(attributeName)) + Optional(lastAttribute(attributeName))



Are there any examples of parsing a log file available?



Thanks for Your help.



BR,

Marjo

#### 2009-10-29 17:53:37 - ptmcg
(Please use [[code]] tags.)



To come up with a working example, I had to fill in a few gaps that you left out in your posting.  Here are the basics parser items:



    from pyparsing import *
    
    data = '[2009-08-07 10:47:08.076] 123456/xxx/yyy *SOME_COMMAND* '
                'dog 0x262, cat 0x2000000, horse 0x2000020, mouse 0x85A079F7'
    
    # fake in missing parser elements
    timeStamp = QuotedString('[',endQuoteChar=']')
    integer = Word(nums)
    wrd = Word(alphas)
    SLASH = Suppress('/')
    location = integer('country') + SLASH + wrd('town') + SLASH + wrd('street')
    command = QuotedString('*')
    
    attributeName = Word(alphas)
    # don't do this - use capitalized names for classes only
    #~ AttributeValue = Word(alphanums) 
    attributeValue = Word(alphanums)



Consistency in naming and casing is a good habit to get into, it will make your code *much* more maintainable later.  None of this 'oh, was this the one where I started with a capital letter or not?' confusion.  A name like this 'AttributeValue' is going to be a class - a name like this 'attributeValue' is going to be a variable of some sort.



Now to combine these bits into the total parser.  Here is how you handled the comma-delimited list of attributes at the end of the log line:





    attribute = attributeName('name') + attributeValue('value') + Suppress (',')
    lastAttribute = attributeName('name') + attributeValue('value')
    
    animalCommand = timeStamp('timeStamp') + location('location') + command('command')+ 
        OneOrMore (attribute) + Optional(lastAttribute)



Comma-delimited lists are so common, pyparsing has a built-in for them, called delimitedList.  Here is your original code redone to use delimitedList:





    attributeDefn = attributeName('name') + attributeValue('value')
    animalCommand = timeStamp('timeStamp') + location('location') + command('command')+ \
        delimitedList(attributeDefn)('attributes')



Now you have a cleaner separation of concepts.  You have something that describes how an attribute definition is composed of a name and a value, the the list of attributes is at the end of your animalCommand, in a comma-separated list.



The next problem we have is that the default behavior for pyparsing is to simply return parsed tokens as a flat list of strings, with no internal structure.  This also causes our results names of 'name' and 'value' for each attribute to overwrite each other.  We need to define our attribute definition to keep each attribute in its own sub list.  So we modify attributeDefn slightly to enclose the expression in a Group.  Group changes nothing about how the parsing process is done, it just adds structure to the returned results:





    attributeDefn = Group(attributeName('name') + attributeValue('value'))



Now if we parse the log message, we see the attributes are nested into a lists of lists, and we can iterate over this structure and access the separate name and value fields of each attribute.  Dumping the parsed results shows this for the attributes field:





    - attributes: [['dog', '0x262'], ['cat', '0x2000000'], ['horse', '0x2000020'], ['mouse', '0x85A079F7']]



And if we iterate over the list like this:





    for attrdef in results.attributes:
        print attrdef.asList()
        print '-', attrdef.name, '=', attrdef.value



We get:





    ['dog', '0x262']
    - dog = 0x262
    ['cat', '0x2000000']
    - cat = 0x2000000
    ['horse', '0x2000020']
    - horse = 0x2000020
    ['mouse', '0x85A079F7']
    - mouse = 0x85A079F7



(I could have just done attrdef.dump(), but I wanted to show the access to the individual results names.)



Okay, one last step.  You also wanted these separate attributes to be accessible as their own results names too, sort of like keys within a dictionary.  Pyparsing has another structure-imparting class like Group, that doesn't affect the parsing process at all, but *does* have a big impact on the returned results.  This class is called Dict.  You call it when you have some repetition of name-value pairs, kind of like this dict constructor in Python:





    attribs = dict( [['dog', '0x262'], ['cat', '0x2000000'], ['horse', '0x2000020'], ['mouse', '0x85A079F7']] )



Dict is kind of fussy though, and has some important constraints:

- there must be repetition, like OneOrMore, ZeroOrMore, or delimitedList

- the repeated elements *must* be enclosed in Group, so that the dict has a list of sublists to work with



When pyparsing uses Dict to create results names, it uses the 0'th element of each sublist as that item's name, and then the [1:] slice of items in the sublist as that item's value.



By amazing coincidence, our current structure of attributes as a delimitedList(attributeDefn) fits these requirements perfectly!  All we need to do is to enclose our attributes list in a Dict:





    # was
    animalCommand = timeStamp('timeStamp') + location('location') + command('command')+ \
        delimitedList(attributeDefn)('attributes')
    
    # now
    animalCommand = timeStamp('timeStamp') + location('location') + command('command')+ \
        Dict(delimitedList(attributeDefn))('attributes')



Now look at this code for processing the returned results:





    results = animalCommand.parseString(data)
    print results.dump()
    print results.attributes.keys()
    for attrdef in results.attributes:
        print '%(name)s = %(value)s' % attrdef
    for attrdef in results.attributes:
        print attrdef.name.title(), int(attrdef.value[2:],16)



For your input data, we get this output:





    ['2009-08-07 10:47:08.076', '123456', 'xxx', 'yyy', 'SOME_COMMAND', [['dog', '0x262'], 
        ['cat', '0x2000000'], ['horse', '0x2000020'], ['mouse', '0x85A079F7']]]
    - attributes: [['dog', '0x262'], ['cat', '0x2000000'], ['horse', '0x2000020'], 
        ['mouse', '0x85A079F7']]
      - cat: 0x2000000
      - dog: 0x262
      - horse: 0x2000020
      - mouse: 0x85A079F7
    - command: SOME_COMMAND
    - country: 123456
    - location: ['123456', 'xxx', 'yyy']
      - country: 123456
      - street: yyy
      - town: xxx
    - street: yyy
    - timeStamp: 2009-08-07 10:47:08.076
    - town: xxx
    ['horse', 'mouse', 'dog', 'cat']
    dog = 0x262
    cat = 0x2000000
    horse = 0x2000020
    mouse = 0x85A079F7
    Dog 610
    Cat 33554432
    Horse 33554464
    Mouse 2241886711



Look over the code that processes your new dict-like field, and see if there are some new ideas for your future parsing efforts.  Good luck!

---
## 2009-11-02 13:32:58 - derenrich - Reserved Words
So I define an identifier as 



id = Combine(Word(alphas,exact=1) + Optional(Word(alphanums)))



but I want to add a restriction that it does not equal a set of reserved words. Is this within the power of pyparsing? I could do this with a regex.



While I'm asking questions, can you give me some suggestions on how to deal with left recursive grammars in pyparsing. It blows up with infinite loops and unrolling the grammar seems to bad style/unpythonic.

#### 2009-11-02 22:31:26 - ptmcg
First off, did you know that there is a two-parameter version of Word, that does the same thing as your definition, but which will run much faster:





    id = Word(alphas, alphanums)



When given two arguments, Word uses the first string as the set of allowed initial characters, and the second string as the set of allowed body characters.



Now to avoid reserved words.  If you are running Python 2.6.2 or higher, you can use the keyword module to get a list of the reserved words in Python.





    import keyword
    pyKeyword = MatchFirst(map(Keyword, keyword.kwlist))



Now put a negation on the front of your id definition to avoid matching a Python keyword (I added '_' as a valid body character, just to show how to add other symbols to this expression):





    id = ~pyKeyword + Word(alphas,alphanums+'_')



Done!



The left-recursion topic is a bit more than I have time for this morning (I'm currently in Germany on a business trip and I'm late for breakfast).  I think Wikipedia has some good material on this.  Pyparsing does have one method called validate() that is not 100% reliable, but will give you some hint whether you have a problematic grammar:



    problem = Forward()
    problem \<\< (problem | Word(alphas))
    
    problem.validate()



(should throw a RecursiveGrammarException).
#### 2009-11-03 03:46:15 - derenrich
You are too kind. I'll look into the validate method. I did not know about that nor the second argument to the Word function. pyparsing continues to impress me.



As to left-recursion. Is the only solution to unroll to remove the left recursion altogether?
#### 2009-11-03 22:25:13 - ptmcg
If you can extract a small example, please feel free to post it.  Just remember to use [[code]] tags.  Or if it is just too large, use the pyparsing.pastebin.com paste bin.
#### 2010-02-11 20:27:41 - card7
Hi,



I tried the above to exclude keywords, but it doesn't appear to work for me.  Here is my code:





    RESERVED_KEYWORDS = ['template']
    VALIDNAME = \
        ~MatchFirst(map(CaselessKeyword, RESERVED_KEYWORDS)) +\
        Word(alphas, alphanums+'_%')



but I get something like this in python:





    \>\>\> VALIDNAME.searchString('template')
    ([(['emplate'], {})], {})



I would expect it to not match in this instance.  What am I doing wrong here and what can I do to make sure it returns no match?  Many thanks.

---
## 2009-11-14 07:41:13 - tomekpe - nestedExpr and whitespaces
Hello,



I've got a long text (\>80 KB) with nested braces: `and `, like this





    text='a aa {{bb {{cc}} d }} e {{ff }}' 



i want to remove all texts in braces to get this text:





    text='a aa  e '



I wrote this code to do this:





    expr = nestedExpr('{{','}}')
    text = '{{' + text + '}}'
    
    result=expr.parseString(text).asList()[0]
    result2 = list()
    
    for r in result:
       if not isinstance(r, list):
          result2.append(r)
    
    text=''.join(result2)



It works... but VERY slow due to tokenizing text with whitespaces.

I try to use leaveWhitespace, setDefaultWhitespaceChars, but all my attempts end in exceptions.



Could you please help me resolve this issue?



Best Regards

#### 2009-11-14 16:51:02 - ptmcg
What about using transformString instead?  This way, you don't have to parse the whole body of the input text, and instead your code scans through and just trims out the nested lists.



    print nestedExpr('{{','}}').suppress().transformString(text)



gives me:



    a aa  e 



Of course, I've only tested with your little test string, not an 80K source, but I think this should be okay.



If whitespace-skipping is taking a lot of time, then add this line <strong>immediately</strong> after the import of pyparsing, and before you create your nestedExpr:



    ParserElement.setDefaultWhitespaceChars('')



-- Paul

---
## 2009-11-15 06:18:49 - gabriele.lanaro - Ideas on parsing a particular format
Sorry I couldn't explain better in the header...

The format I'm looking to parse is something like this:

[code]

<ol><li>Lexical states for SCLEX_PYTHON</li></ol>lex Python

lex Nimrod

val SCE_P_DEFAULT=0

val SCE_P_COMMENTLINE=1



lex Metapost=SCLEX_METAPOST SCE_METAPOST_

val SCE_METAPOST_DEFAULT=0

val SCE_METAPOST_SPECIAL=1

val SCE_METAPOST_GROUP=2



lex Octave_

lex MSSQL

val SCE_MSSQL_DEFAULT=0

val SCE_MSSQL_COMMENT=1

val SCE_MSSQL_LINE_COMMENT=2

val SCE_MSSQL_NUMBER=3

val SCE_MSSQL_STRING=4

[/code]

there are 'lex' and 'val' directives.

I have to group each 'lex' with the closer 'val' list like:

[code]

[['lex', 'Python'],['val SCE_P_DEFAULT=0','val SCE_P_COMMENTLINE=1']],

[['lex', 'Nimrod'],['val SCE_P_DEFAULT=0','val SCE_P_COMMENTLINE=1']]

[/code]

So the problem is in this 'repetition' of the match. For example both 'lex Python' and 'lex Nimrod' match the same val list.

I can't figure out how to parse such thing: maybe a solution is to do a post-processing, but I prefer to make it in pyparsing to make it more mantainable.

#### 2009-11-15 10:30:22 - ptmcg
If you didn't have this special requirement for doubling up the processing of lex lines, you could have used nested Dicts to handle the definitions of vals within lexes, and have an over dict of lexes.  But since you have to post-process anyway, I'd skip the Dict class complexity, and just post-process the whole thing.



Here is my cut at a parser:



    from pyparsing import *
    
    langName = Word(alphas.upper(), alphas+'_')
    state = Word(alphas.upper(), alphas.upper()+'_')
    EQ = Suppress('=')
    integer = Word(nums).setParseAction(lambda t:int(t[0]))
    
    lexline = Group('lex' + langName('lang') + Optional(EQ + state + state))
    valline = Group('val' + state('state') + EQ + integer('value'))
    
    lexgroup = Group(OneOrMore(lexline)('lex') + 
                        OneOrMore(valline)('vals'))
    parser = OneOrMore(lexgroup)
    
    # post process results
    lexstates = {}
    grps = parser.parseString(data)



And then to post-process your code:



    grps = parser.parseString(data)
    
    lexstates = {}
    for lg in grps:
        # get list of vals
        vallist = []
        for valdef in lg.vals:
            vallist.append((valdef.state, valdef.value))
        # or just this list comp (easily converted to a dict if you prefer)
        # vallist = [(vd.state, vd.value) for vd in lg.vals]
    
        # set val list for each lex
        for langdef in lg.lex:
            lexstates[langdef.lang] = vallist
    
    # print out what we got
    for l,v in lexstates.items():
        print l
        for val in v:
            print val



Prints:





    Python
    ('SCE_P_DEFAULT', 0)
    ('SCE_P_COMMENTLINE', 1)
    
    Octave_
    ('SCE_MSSQL_DEFAULT', 0)
    ('SCE_MSSQL_COMMENT', 1)
    ('SCE_MSSQL_LINE_COMMENT', 2)
    ('SCE_MSSQL_NUMBER', 3)
    ('SCE_MSSQL_STRING', 4)
    
    MSSQL
    ('SCE_MSSQL_DEFAULT', 0)
    ('SCE_MSSQL_COMMENT', 1)
    ('SCE_MSSQL_LINE_COMMENT', 2)
    ('SCE_MSSQL_NUMBER', 3)
    ('SCE_MSSQL_STRING', 4)
    
    Nimrod
    ('SCE_P_DEFAULT', 0)
    ('SCE_P_COMMENTLINE', 1)
    
    Metapost
    ('SCE_METAPOST_DEFAULT', 0)
    ('SCE_METAPOST_SPECIAL', 1)
    ('SCE_METAPOST_GROUP', 2)



If you *really* want pyparsing to create the dict of vals for you, write back, or google this site for use of the 'Dict' class or the 'dictOf' helper.
#### 2009-11-15 11:48:34 - gabriele.lanaro
I think that this approach is good enough for me. Thank you very much for helping me!

---
## 2009-11-25 05:00:17 - nfirvine - Parsing a unixy command line language
I'm having trouble wrapping my head around pyparsing.  Obviously I didn't pay enough attention in Compilers class :)



I'm looking to parse commands similar to those used to execute commands in UNIX/Linux:





    set-location +verbose -debug -mood=good +terse /home/user



It consists of a command name followed by any number of boolean arguments (bargs), mapped arguments (margs) and locational arguments (largs), which can be in any order.  The order doesn't matter, except for largs, which only matter relative to one another.



I would like the results to come out like this:





    command:
    - command_name: set-location
    - bargs: 
      - verbose: True
      - debug: False
      - terse: True
    - margs:
      - mood: good
    - largs:
      - /home/user



(The output's not quite .dump(), but I try :)



I've got it to a point where I've pretty much got it, except that it only stores the last value into the dict.  I've read other post on the subject, but I can't seem to understand the combination of ZeroOrMore, Group, Dict, and the setResultsName flags.  One thing I've found out: you can't have ZeroOrMore inside a ZeroOrMore.



Here's my code.  Any help very appreciated.





    arg_name = Word(alphanums, alphanums + '-')
    shell_chars = '{}[]\\$#\'\''
    non_shell_chars = alphanums + ':/*%_'
    arg_value = Word(non_shell_chars)
    arg_prefix = Literal('-')
    
    barg_false = Literal('-')
    barg_false.setParseAction(lambda: False)
    barg_true = Literal('+')
    barg_true.setParseAction(lambda: True)
    barg = (barg_true ^ barg_false) + arg_name
    barg.setParseAction(lambda toks: [toks[1], toks[0]])
    bargg = Group(barg)
    bargs = Dict(Group(barg)).setResultsName('bargs')
    
    larg = arg_value
    largs = larg.setResultsName('largs', True)
    
    marg = arg_prefix.suppress() + arg_name + Literal('=').suppress() + arg_value
    margs = Dict(Group(marg)).setResultsName('margs')
    
    args = ZeroOrMore(margs ^ bargs ^ largs)
    
    command_name = Word(alphanums, alphanums + '-')('command_name')
    command_args = args
    command = Group(command_name + command_args)('command')



#### 2009-11-26 20:04:26 - ptmcg
I see that you found the second argument to setResultsName, and set it to True in your definition of largs.  Do the same for bargs and margs, and I think you'll get all of your results.



Welcome to pyparsing!



-- Paul

---
## 2009-12-03 02:27:07 - drake1337 - nested #ifdef ... #endif
I'm learning to use pyparsing by writing a simple, C-like preprocessor with the following grammar:



'#ifdef' ident '\n'

    line*

[ '#elsif' ident '\n'

    line* ]*

[ '#else' '\n'

    line* ]?

'#endif'



line* == ZeroOrMore(line)



A group of lines may contain arbitrarily deep nested #ifdef ... #endif statements. So naturally, I constructed my recursive definition with the Forward() statement.



Example:



#ifdef FOO    <em> outer



#define BAR



   #ifdef BAR </em> inner

       ...

   #endif



#endif



I was expecting the outer #ifdef to match first, but it is actually the inner #ifdef that matches first. So for this example, the parser will actually find the inner #ifdef first, expand it, then find the outer #ifdef and expand it.



This example has a problem. Since the inner #ifdef is found before the outer, the parser doesn't do the '#define BAR' until after expanding the inner.



So is there some obvious solution that I have not considered here? I based my original code around the macroExpander in the distribution. Has anyone else already solved this problem?



I am looking for a way to perhaps track the depth of the currently matched expression and only expand at depth 0, but so far have not figured out how to accomplish this magic.

#### 2009-12-03 09:35:19 - drake1337
Sorry for not formatting the pseudo-code very well.



I was over-thinking this problem. I re-implemented it without using recursion, and makes more sense now.



Using SkipTo is very convenient. After #ifdef, you just skip to either the next #elsif, #else, #endif OR #ifdef. When you hit an #ifdef, push a FOUND variable onto a stack, and when you hit an #endif, pop one off.
#### 2009-12-03 18:02:04 - ptmcg
Drake1337 -



Welcome to pyparsing! I'm sorry I didn't get to answer your post earlier - looks like you've got things moving forward.



You only need SkipTo if you are trying to eat the whole file using parseString.  If you look at using scanString or searchString instead, you only need to define the specific patterns you are looking for, and everything else gets passed over by the scan.



Good luck in your parsing work - write back if you have more questions!

-- Paul
#### 2009-12-03 21:56:16 - drake1337
Hey thanks for looking into it. As it turns out, I was going around in circles on this particular issue for a while. My SkipTo solution had some issues.



I settled on trying to figure out how to force my grammar to search the input in top-down fashion (since original, bottom-up implementation was broken).



1. A recursive #ifdef, that just echoes itself.

2. A top-level #ifdef that contains the recursive definition. This #ifdef calls transformString on any contained text.



It feels a little inefficient, but it processes all the text macros in the correct order.



BTW, even though I have some concern over performance, I very much like the feel of pyparsing. After getting used to it, I feel it is much more intuitive than the lex/yacc approach. Any plan to push for adding it to the python standard library?
#### 2009-12-09 06:00:38 - ptmcg
Thanks for the vote of confidence. I get asked about submitting pyparsing to the stdlib about once a year, and maybe we are getting closer to being able to do that.  The code is really very stable now, I've not had a release in over 6 months.  I would probably have to convert to PEP8 names though.  And pyparsing's approach to mutators returning self is a little unPythonic (very SmallTalkish, though - I also see this in JQuery).  I suspect that a stdlib version of pyparsing would go through some changes that would not be backward compatible, maybe spoiling some of the attraction in the first place.

---
## 2009-12-07 03:34:41 - nielAtpyparsing - Ignoring embedded keywords/punctuation in quoted string characters
Hi,



I am using pyparsing to convert formatted text into CSV. I have defined my grammar and it is working perfectly, except for cases where keywords/punctuation is inside some of my quotedString values.



My current grammar is as follows:



    # Define a parser that will parse string_representation output
    def STR_REP_BNF(debug):
        '''
        Define the BNF (Backus-Naur notation) for describing the language of the
        string_representation output of the Ab Initio data files.
        '''
        global bnf;
    
        if not bnf:
            # Tokens
            #####################################
            # punctuation
            lbrack          = Literal(LBRACK).suppress();
            rbrack          = Literal(RBRACK).suppress();
    
            # Keywords
            recordToken_    = Keyword(RECORD);
            vectorToken_    = Keyword(VECTOR);
            unionToken_     = Keyword(UNION);
            voidToken_      = Keyword(VOID);
            #####################################
    
            # Rules
            #####################################
            # Primitives
            hexadecimal     = Combine( Literal('0x') + Word(alphanums, max=2) ).setName('hexadecimal');
            decimal         = Combine( Literal('-' ) + Word(nums) ).setName('decimal');
            field           = Forward().setName('field');
            record          = Forward().setName('record');
            vector          = Forward().setName('vector');
            union           = Forward().setName('union');       # ??? Still need to define one that has unions
            void            = Forward().setName('void');
            name            = Forward().setName('name');
            value           = Forward().setName('value');
            bnf             = Forward();
    
            # Optionally set debug mode
            if (debug == True):
                field.setDebug();
                record.setDebug();
                vector.setDebug();
                union.setDebug();
                void.setDebug();
                name.setDebug();
                value.setDebug();
                bnf.setDebug();
    
            name            \<\< ( ~lbrack + ~rbrack + Word(printables) ).setName('name');
            value           \<\< ( decimal | Word(alphanums) | quotedString | Group(record) | Group(vector) | Group(void) ).setName('value')
    
            # name value(s)
            field \<\< Group( name + value );
    
            # [record
            #    field  --------
            #    ^              |
            #    |---------------
            #  ]
            record \<\< ( lbrack + recordToken_ + OneOrMore(field).setResultsName('record') + rbrack );
    
            # [vector
            #    value1,
            #    ...,
            #    valueN
            # ]
            # NOTE: Need to get the below one working to cater for directly embedded vectors (doubly-arrays)
            #vector \<\< ( lbrack + vectorToken_ + delimitedList(MatchFirst(field | value), ',').setResultsName('vector') + rbrack );
            vector \<\< ( lbrack + vectorToken_ + Optional(delimitedList(value, ',')).setResultsName('vector') + rbrack );
    
            # [void 0xXX 0xYY]
            void \<\< ( lbrack + voidToken_ + ZeroOrMore(hexadecimal).setResultsName('void') + rbrack );
            #####################################
    
            # Configure the BNF
            bnf \<\< OneOrMore(record);
    
        # Return the BNF that will be used to parse the string_representation
        return bnf;



The test case that I am having trouble with is as follows:



    [record
      hash_md5             '\x90\x00\>\x9fE^uG0\x81]'
      file_uid             6
      sequence_number      89
      date_time            '20090510'
      datetime_written     '20090703121929'
      msisdn               '27822140612    '
      imsi                 NULL
      start_event_datetime '20090510235530'
      end_event_datetime   '20090510235530']



In this example the ']' is picked up as a 'keyword' and as a result the remaining data fails to parse. The debugging log output is show below:



    Match value at loc 18(2,11)
    Matched value -\> [''\xbd\\x90\\x00\xb5\xfd\>\xe7\\x9fE\xec^uG0\\x81]'']
    Matched field -\> [['hash_md5', ''\xbd\\x90\\x00\xb5\xfd\>\xe7\\x9fE\xec^uG0\\x81]'']]
    Match field at loc 61(2,1)
    Match name at loc 61(2,1)
    Exception raised:Expected W:(0123...) (at char 62), (line:3, col:1)
    Exception raised:Expected W:(0123...) (at char 62), (line:3, col:1)
    Exception raised:Expected ']' (at char 62), (line:3, col:1)
    Exception raised:Expected ']' (at char 62), (line:3, col:1)
    Info: 1     [record
    2       hash_md5             '\x90\x00\>\x9fE^uG0\x81]'
    
    Info: [record
      hash_md5             '\x90\x00\>\x9fE^uG0\x81]'
     --\>
    Info: 
    Info: ^
    Info: Expected ']' (at char 62), (line:3, col:1)
    1     [record
    2       hash_md5             '\x90\x00\>\x9fE^uG0\x81]'
    
    [record
      hash_md5             '\x90\x00\>\x9fE^uG0\x81]'
     --\>
    
    ^
    Expected ']' (at char 62), (line:3, col:1)



Any help in updating my grammar to cater for this will be greatly appreciated.



Kind regards,

Niel

#### 2009-12-07 04:34:08 - nielAtpyparsing
The problem was not actually in my grammar, but in another part of my program where I was looking for '['/']'.



I've no solved that problem, so no need to reply to this topic.

---
## 2009-12-09 22:23:47 - drake1337 - streamlined issue?
Hello again,



I'm writing a SystemVerilog expression parser. When I try to use constant_expression.parseString(), python seems to lock up. I discovered that the interpreter is stuck in the following code:





    2269     def streamline( self ):
    -\> 2270         super(ParseExpression,self).streamline()
       2271
       2272         for e in self.exprs:



Here is a dump of my code, so you can try it out:





    pre_op     = oneOf('+ - ! ~ & | ^ ~& ~| ~^ ^~ ++ --')
    post_op    = oneOf('++ --')
    exp_op     = Literal('**')
    mult_op    = oneOf('* / %')
    add_op     = oneOf('+ -')
    shift_op   = oneOf('\<\< \>\> \<\<\< \>\>\>')
    comp_op    = oneOf('\< \<= \> \>=')
    eq_op      = oneOf('== != === !== ==? !=?')
    band_op    = Literal('&')
    bxor_op    = oneOf('^ ~^ ^~')
    bor_op     = Literal('|')
    and_op     = Literal('&&')
    or_op      = Literal('||')
    implies_op = Literal('-\>')
    
    primary_literal = number \
                    | time_literal \
                    | unbased_unsized_literal \
                    | string_literal
    
    constant_expression = Forward()
    
    constant_concatenation = \
        LBRACE + constant_expression + ZeroOrMore(COMMA + constant_expression) + RBRACE
    
    constant_multiple_concatenation = \
        LBRACE + constant_expression + constant_concatenation + RBRACE
    
    constant_primary = primary_literal \
                     | LPAREN + constant_expression + RPAREN \
                     | constant_concatenation \
                     | constant_multiple_concatenation
    
    last = constant_primary
    # right-associative unary ops
    this = Forward()
    this \<\< (FollowedBy(pre_op + this) + Group(pre_op + this) | last)
    last = this
    
    # left-associative unary ops
    this = FollowedBy(last + post_op) + Group(last + OneOrMore(post_op)) | last
    last = this
    
    # binary ops
    for op in [ exp_op, mult_op, add_op, shift_op,
                comp_op, eq_op,
                band_op, bxor_op, bor_op,
                and_op, or_op ]:
        this = FollowedBy(last + op + last) + \
               Group(last + OneOrMore(op + last)) | last
        last = this
    
    # -\> operator
    this = Forward()
    this \<\< (FollowedBy(last + implies_op + this) + \
             Group(last + OneOrMore(implies_op + this)) | last)
    last = this
    
    # ternary op
    this = Forward()
    this \<\< (FollowedBy(last + QUESTION + this + COLON + this) + \
             Group(last + QUESTION + this + COLON + this) | last)
    last = this
    
    # Constant Expressions
    constant_expression \<\< last



#### 2009-12-09 22:36:05 - drake1337
Sorry, forgot to give the expressions for number, time_literal, unbased_unsized_literal, string_literal



Those aren't necessary though. Just try out using the following:



primary_literal = Word(nums)
#### 2009-12-19 08:41:50 - ptmcg
I think you got yourself into an infinitely looping cyclic expression, inside your definition of your binary ops. You are using '=' assignment, when you should be creating a new Forward, and inserting each successive nested operation using '\<\<'.  You are also doing some cases of `a &lt;&lt; b | c`, this <em>must</em> be parenthesized to read `a &lt;&lt; (b | c)`.



This code works for me ().  Compare it with what you have.



-- Paul
#### 2009-12-22 15:37:48 - drake1337
In Verilog, the binary operators in my loop are left-associative. Haven't you just changed their definitions so that they are right-associative?
#### 2009-12-22 16:06:18 - ptmcg
You're correct.  Left associative is done using `last op last`; right associative is `last op this`.  But the main change I made was in creating the internal Forward's, instead of just reassigning to `this`.
#### 2009-12-22 17:17:34 - drake1337
Thanks, it seems to work now. You can obviously tell that I was copying this methodology from the operatorPrecedence function. Obviously I screwed up the left-associative binary ops.



I would have just used operatorPrecedence, but the definition of 'constant_primary' from the LRM doesn't quite fit the 'baseExpr' parameter, due to Verilog concatenations {...}. So this solution was a bit hackish to begin with :).

---
## 2009-12-13 03:32:17 - zzarko - Parse action executed, but it shouldn't
First, thanks for pyparsing! I just started to use it (I was using flex/bison for a long time), and I ran into a problem (no kidding!)... I'm not sure if this is pyparsing problem, or my grammar's. I'm making a parser for compiler course (very simplified C), and this is the code that makes trouble (stripped-down version):



    from pyparsing import *
    
    class Enumerate(object):
        def __init__(self, names):
            for number, name in enumerate(names.split()):
                setattr(self, name, number)
    
    sym_type = Enumerate('INT UNSIGNED')
    
    class MyParser(object):
        def __init__(self):
            self.tId = Word(alphas+'_',alphanums+'_')
            self.tInteger = Word(nums).setParseAction(lambda x : [int(x[0]), sym_type.INT])
            self.tUnsigned = Regex(r'[0-9]+[uU]').setParseAction(lambda x : [int(x[0][:-1]), sym_type.UNSIGNED])
            self.tConstant = self.tUnsigned | self.tInteger
            self.tType = Keyword('int').setParseAction(lambda x : sym_type.INT) | Keyword('unsigned').setParseAction(lambda x : sym_type.UNSIGNED)
    
            self.rVariable = (self.tType('type') + self.tId('name')).setParseAction(self.variable_action)
            self.rVariableList = Group(ZeroOrMore(self.rVariable + Suppress(';')))
    
            self.rParameters = Optional(self.rVariable + ZeroOrMore(Suppress(',') + self.rVariable))
            self.rFunction = Group(self.tType + self.tId + Literal('(') + self.rParameters + Literal(')') + Literal(';')).setParseAction(self.function_action)
            self.rFunctionList = Group(self.rFunction + ZeroOrMore(self.rFunction))
    
            self.rProgram = (self.rVariableList + self.rFunctionList)
    
        def variable_action(self, text, loc, var):
            print 'VAR:',var
    
        def function_action(self, text, loc, fun):
            print 'FUN:',fun
    
        def parse(self,text):
            return self.rProgram.parseString(text,parseAll=True)
    
    mp = MyParser()
    
    test1 = '''    int a;
        int b;
        int c;
        unsigned d;
        int main(int x, int y);
        int fun(int z);
    '''
    
    mp.parse(test1)

This code gives output:



    VAR: [0, 'a']
    VAR: [0, 'b']
    VAR: [0, 'c']
    VAR: [1, 'd']
    VAR: [0, 'main']
    VAR: [0, 'x']
    VAR: [0, 'y']
    FUN: [[0, 'main', '(', 0, 'x', 0, 'y', ')', ';']]
    VAR: [0, 'z']
    FUN: [[0, 'fun', '(', 0, 'z', ')', ';']]



My question is why is int main(int x, int y); first recognised as a variable, and then as a function (why are both parse actions called)? Note that int fun(int z); is only recognised as a function (as it should be). If the problem is in my grammar, how to solve this?



Best regards,

Zarko

#### 2009-12-13 04:47:08 - zzarko
I forgot to mention (it may be important): pyparsing 1.5.2, Python 2.6.4, Ubuntu 9.10 32bit
#### 2009-12-18 15:07:46 - zzarko
OK, I solved my problem with FollowedBy. But, I'm still interested in my question. Is the output I have in the first post right behavior? Should pyparsing call both parse actions for 'int main(int x, int y);' or shouldn't? I know pyparsing isn't flex/bison equivalent (I looked at many posts last few days about pyparsing, backtracking, etc.), but I just want to know is this 'in it's nature' or not.



Once again, thanks for great tool. Best regards,

Zarko
#### 2009-12-19 08:14:28 - ptmcg
Thanks for the kind words, and welcome to pyparsing!



First some general comments on your parser:



1. Good catch on testing unsigned before int in your constant expression, as the leading digits of an unsigned could get read as an int, and they you get stuck with this dangling 'u' character.



2. Your definition of parameters is



    self.rParameters = Optional(self.rVariable + ZeroOrMore(Suppress(',') + 
                                   self.rVariable))



This construct of `blah + ZeroOrMore(Suppress(delimiter) + blah)` is so common that pyparsing has a built-in called `delimitedList`, that does the same thing, with ',' as the default delimiter.  So you could shorten your expression, with no loss of understanding to:





    self.rParameters = Optional(delimitedList(self.rVariable))



although I think I would Group them, so that you could maintain the identity of this sublist of the parsed tokens.



In addition (this is really more a style point, but I'll add it anyway), I would leave the Optional out at this point, and just declare parameters as





    self.rParameters = Group(delimitedList(self.rVariable))



The 'optionalness' of this expression is really determined in the containing parent, and it is in this context that you can also define a reasonable default.





    self.rFunction = Group(self.tType + self.tId + 
                        '(' + Optional(self.rParameters, default=[]) + ')' + 
                        ';').setParseAction(self.function_action)



But this is purely stylistic, and your definitions will work just as well.



3. You have this expression:





    self.rFunctionList = Group(self.rFunction + ZeroOrMore(self.rFunction))



Why not just use `OneOrMore(self.rFunction)`?  I actually find this clearer to understand, that you want one or more of something, instead of something, followed by zero or more of the same thing.







Now on to your question.  Pyparsing calls parse actions as expressions are matched, even if they are part of a larger expression that ultimately fails to match.  Here is your overall expression (summarized):





    self.rVariableList = Group(ZeroOrMore(self.rVariable + Suppress(';')))
    self.rFunctionList = Group(self.rFunction + ZeroOrMore(self.rFunction))
    
    self.rProgram = (self.rVariableList + self.rFunctionList)



and your input text is:



    test1 = '''    int a;
        int b;
        int c;
        unsigned d;
        int main(int x, int y);
        int fun(int z);
    '''



So in parsing the leading variable list, pyparsing finds 'int a' and a ';', then 'int b' and a ';', and so on.  Then it finds 'int main', a perfectly valid variable definition, so it goes ahead and calls the parse action.  However, 'int main' is not followed by a semicolon, so the ZeroOrMore repetition fails, and stops at the last fully successful expression, which was 'unsigned d' followed by a ';'.  Then pyparsing picks right up at 'int main( etc.', and starts matching functions in a function list, beginning with the successful match of 'int main(int x, int y)', followed by a ';'.  As you found out, you could work around this using FollowedBy, which suppresses the calling of parse actions, since its intent is really to look ahead to see if a match might work, without actually advancing and consuming the matched tokens.



Hope this helps, sorry to take so long to respond, work was just crazy this week!



-- Paul
#### 2009-12-19 13:44:35 - zzarko
Paul,

Thank you so much for your response. I'm new to pyprsing and your advices are more than welcome. I just finished small compiler which translates (very) simplified C into assembly language. Compiler is based on flex/bison compiler that is used on my faculty's course on compilers and I hope that pyparsing version colud be used next year for student's projects. Now, I need to polish it according to your advices and to add more comments. If you are interested, I'll send it to you, so it can be included in examples for pyparsing (if it has enough quality, that is).



Best regards,

Zarko
#### 2009-12-19 15:43:20 - ptmcg
Yes, by all means, forward me your compiler, I'll be happy to include it.  You might also want to look into the May, 2008 () issue of Python Magazine, in which I describe creating a simple interpreter/compiler with pyparsing.  A back-issue will set you back $7, but it seems like it is right in line with what you are working on.
#### 2009-12-28 23:40:15 - zzarko
pymag's site is currently unavailable, and I didn't had the chance to look at your article, but I'll try to do it later (thanks for link!). I have finished (and polished to some extent) Python/pyparsing microC compiler, and I just want to ask you where to send it (should I copy it here or in another section (pyparsers, maybe?), enclosed in code tags?). In this version, I didn't use operatorPrecedence (and maybe some other helpers that may be useful in this case), because I didn't had the time to play with it, and I need to move on to my next subject (this may be the task for those attending the compiler course).

---
## 2009-12-18 13:38:34 - tk2600 - Delimiter is multiple <space>
I am trying to parse log output which has syntax like:




```
<br />
My dispatcher     User action       Rest of description<br />
Handler           Internal ID       More dumy text.<br />

```




So how do I parse the above by delimiter of 2 or more space?



Obviously my attempt failed:


```
<br />
spacing = Suppress(White(min=2))<br />
process = OneOrMore(Word(alphasnums)).setParseAction(lambda \<br />
tokens: &quot; &quot;.join(tokens))<br />
logProcess = process + spacing + process + spacing+ restOfLine<br />

```


#### 2009-12-18 13:44:30 - tk2600
Oops, I thought double curly braces is for quoting code?  And I can't edit my post.  Sorry.



Here is another attempt:

`My dispatcher     User action      Rest of description`

`Handler           Internal ID      More dumy text.`
#### 2009-12-18 13:45:33 - tk2600
Arrrg!



My dispatcher \<5 spaces\> User action \<5 spaces\> Rest of description

Handler \<11 spaces\> Internal ID \<5 spaces\> More dumy text.
#### 2009-12-19 07:22:55 - ptmcg
tk2600



(The wikispaces markup you are looking for is [[code]] - put this tag on a line by itself, just before, and just after your code block.)



When your input text is so regularly spaced, that you can identify each field of each line by starting column, then I recommend <em>not</em> using pyparsing, but just basic string slicing.  Here is my code to slice and dice your log file:





    text = '''\
    My dispatcher     User action     Rest of description
    Handler           Internal ID     More dumy text.
    Handler2          Internal ID2    More dumy text2
    Handler3          Internal ID3    More dumy text3
    '''.splitlines()
    
    # zero-based locations for each column - add None on the end
    # so that the last column will read to the end of the line
    cols = [0, 18, 34, None]
    
    def splitByCols(cols, s):
        ret = []
        for thisCol,nextCol in zip(cols[:-1], cols[1:]):
            ret.append(s[thisCol:nextCol].strip())
        return ret
    
        # this for loop can also be collapsed to this list comprehension
        return [s[thisCol:nextCol].strip() 
                    for thisCol,nextCol in zip(cols[:-1], cols[1:])]
    
    
    # extract headers from first line of text
    headers = splitByCols(cols, text[0])
    print headers
    
    # extract each subsequent line, and create a dict with the headers
    for line in text[1:]:
        data = dict(zip(headers, splitByCols(cols, line)))
        print data
    
    # or a single line list comprehension will give you a list of all the dicts
    alldicts = [dict(zip(headers, splitByCols(cols, line))) for line in text[1:]]



This prints



    ['My dispatcher', 'User action', 'Rest of description']
    {'My dispatcher': 'Handler', 'User action': 'Internal ID', 
        'Rest of description': 'More dumy text.'}
    {'My dispatcher': 'Handler2', 'User action': 'Internal ID2', 
        'Rest of description': 'More dumy text2'}
    {'My dispatcher': 'Handler3', 'User action': 'Internal ID3', 
        'Rest of description': 'More dumy text3'}



If you post a few lines of the actual log file, then I might propose an alternative script.  For instance, if there is a limited number of user actions (PUT, GET, SELECT, INSERT, DELETE), then instead of picking them out by column, you could create a pyparsing expression to match the actions, something like oneOf('PUT GET SELECT INSERT DELETE').  If your internal IDs are all numeric, then you could match them with a Word(nums).  Then the unformatted description could get picked up with a restOfLine.  Something like this:





    text = '''\
    My dispatcher     User action     Rest of description
    PUT               313             Lorem ipsum dolor sit amet, 
    GET               313             consectetur adipisicing elit, 
    SELECT            316             sed do eiusmod tempor incididunt
    '''.splitlines()
    
    from pyparsing import *
    
    # define a pyparsing expression for your log line - empty is there to advance
    # the parser to the start of the description, since restOfLine does not
    # skip whitespace
    logline = oneOf('PUT GET SELECT DELETE') + Word(nums) + empty + restOfLine
    
    for line in text[1:]:
        print logline.parseString(line).asList()



Giving:



    ['PUT', '313', 'Lorem ipsum dolor sit amet, ']
    ['GET', '313', 'consectetur adipisicing elit, ']
    ['SELECT', '316', 'sed do eiusmod tempor incididunt']



So it is up to you how to approach this problem.



-- Paul
#### 2009-12-21 06:09:23 - tk2600
Thank you very much Paul!  I used the oneOf() option as my User Action so far is a small set.  I still have a lot to learn about Python and pyparsing, but it is going to advance our automation greatly.  The challenge is, 'Handler' and 'UserAction', you never know when developer will add something.  I am thinking of writing a validater to weed out any log that we have never seen.  Then we can act on them immediately.



Thanks again.
#### 2009-12-21 08:55:58 - tk2600
Someone told me to use re.split()



print re.split('\s{2,}', s)



This take care of the multi space delimiter.  I will then use pyparsing to further parse the split string.

---
## 2009-12-18 14:19:46 - tk2600 - Matching possibly over 100 grammars
I am sure I have not grasped the power of pyparsing.  All the examples I have come across on the net, they all seem to parse 1 grammar.  For example:







What I am trying to parse is similar to parsing dmesg.  They are repeating patterns, but each event has different grammars.



I am sure there are smarter way than:



grammar1 = .....

grammar2 = .....

.....

grammar100 = ....



input = open('logfile.txt', 'r')

for line in input:

    event1 = grammar1.parseString(line)

    event2 = grammar2.parseString(line)

    .....

    event100 = grammar100.parseString(line)

#### 2009-12-19 08:21:20 - ptmcg
Yes, I do hope there is a smarter way than creating 100 different grammars!  The idea is to create a single grammar that can match <em>any</em> line in `logfile.txt`.  Now this grammar might have to account for a wide variety of different logged items, identifiers, user commands, component names, etc., but each one is defined and processed separately.  The the whole thing is combined into a single expression, and it is used over and over again, matching each element of each line in the input log file.



How many different kinds of event do you have?  Are their formats really that different?  If you could post a fragment of one of your logfiles, I might have some more specific suggestions I could make.



Parsing log files is a common application for pyparsing, even if they are pretty regularly formatted and spaced.  Using pyparsing, I've done things like computing the interval <em>between</em> log entries, and including that in the emitted data.  So don't give up yet!  I think we can still make some progress with your problem.



-- Paul
#### 2009-12-21 06:30:42 - tk2600
Hello Paul,



What I am worry about is, every line in the log has to repeat the matching process for 100 grammar definition.  Our log is around 80MB in size or even more if we are running longevity test for a week.



I suppose I can call python every time logrotate is called (I hope).  Then I can start parsing as the test run.



So in grammar definition, we define



grammar1 = XXX + YYY + ZZZ



in terms of pyparsing performance, if XXX does not match, I assume pyparsing will no continue to check, assuming I am using parseString() instead of scanString()?



In this case, I don't really have to worry about performance?  Hmmm... Every time you compare a grammar you allocate memory for the input string and delocate it after no match.  I wonder how many people will do something like me, parsing log which potentially have over 100 grammars?  In this case, if there is a grouping of grammars definition, then pyparsing can check thru 100 grammars before leaving the scope.



Unfortunately I cannot really post the log I am working on.  But the format looks just like syslog or dmesg.  Here is a syslog sample.  There are many grammars, and you are right from my last post.  They are aligned by column number depending on the specific grammar.



Dec 20 07:44:18 qalab-desktop rsyslogd: [origin software='rsyslogd' swVersion='4.2.0' x-pid='478' x-info=''] rsyslogd was HUPed, type 'lightweight'.

Dec 20 07:44:18 qalab-desktop anacron[3187]: Job `cron.daily' terminated

Dec 20 07:44:18 qalab-desktop anacron[3187]: Normal exit (1 job run)

Dec 20 08:17:01 qalab-desktop CRON[3354]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)

Dec 20 09:04:25 qalab-desktop ntpd[2827]: kernel time sync status change 2001

Dec 21 06:25:01 qalab-desktop CRON[3513]: (root) CMD (test -x /usr/sbin/anacron || ( cd / && run-parts --report /etc/cron.daily ))

Dec 21 07:17:01 qalab-desktop CRON[3517]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)

Dec 21 07:30:01 qalab-desktop CRON[3522]: (root) CMD (start -q anacron || :)

Dec 21 07:30:01 qalab-desktop anacron[3526]: Anacron 2.3 started on 2009-12-21

Dec 21 07:30:01 qalab-desktop anacron[3526]: Will run job `cron.daily' in 5 min.

Dec 21 07:30:01 qalab-desktop anacron[3526]: Jobs will be executed sequentially

Dec 21 07:33:40 qalab-desktop ntpd[2827]: kernel time sync status change 6001

Dec 21 07:35:01 qalab-desktop anacron[3526]: Job `cron.daily' started

Dec 21 07:35:01 qalab-desktop anacron[3533]: Updated timestamp for job `cron.daily' to 2009-12-21

Dec 21 07:50:46 qalab-desktop ntpd[2827]: kernel time sync status change 2001





I really appreciate your help!



Alan
#### 2009-12-21 06:56:31 - tk2600
I just thought about it, the above syslog example probably didn't reflect the complexity of the log I am working on.  The above syslog, 'qalab-dektop' is a small set.  The process name after it is what causes these 100+ grammars.  Sure above you see maybe 4 types of process only, but in my log, there are 40+.  For each of these 40+ processes, each process has different grammars.  Take one process as example:



sessionID=123 some test prarm1=abc, tty=1.

sessionID=123 param2=aaa other text param3=eee

sessionID=123 text text text param4=hhh.

SystemError for tty=1:  Other text and key1=qqq, key2=fff, etc.



To complicate things more, the logs is generated from multi threaded environment.  So The above log is all mixed in with 10,000 threads.  10k threads is not really a problem since they are all repetition.



Because of this multi threads log gets mixed in, so I cannot define one single grammar to take care of the 3 different lines started with 'sessionID='.  As you can see, I will have other important info mixed in between.



My approach is, I have to define grammar for each different line, then use a dictionary to store these concurrent 10,000 sessions.  (Yes, I don't know if dictionary is a good idea to keep everything in memory.  Some kind of file bases DB would be much safer.)



Another tricky thing is, when the process cause an error, I have to correlate by system tty then put the error back to the corresponding sessionID record.  However, this is out of the scope of pyparsing and shouldn't be difficult.
#### 2009-12-21 07:48:03 - ptmcg
Wow, this is really quite a messy log you are having to deal with!  Are there *any* parts of the line that are predictable, like a leading timestamp, and maybe a session id, or thread id, or severity field (ERR, WARN, INFO, DEBUG, something like that)?  I've done my own log parsing, and have been successful in rethreading the log by thread id to see what the interactions between separate threads looks like.  And there is always a 'rest of the line' unformatted comment at the end of most log formats.  But it sounds like you need to extract even some info from this unformatted piece.  If you could post even some kind of sanitized or redacted form of your log, that could help some.
#### 2009-12-21 08:52:26 - tk2600
It is complicated, that's exactly why I appreciate your pyparsing so much!!! :)  It is working like a champ.  I just need to get use to python syntax.



Again, thanks for your help!
#### 2009-12-21 12:40:02 - tk2600
Hello Paul,



Need your help.  Can I do something like this?



grammar1 = ....

grammar1.setParseAction(proc1)

grammar2 = ....

grammar2.setParseAction(proc2)

....

grammar100 = ....

grammar100.setParseAction(proc100)



for i in range (1, 100):

    found = \<pointer to array of grammar[i].parseString(myStr)\>

    if len(found) \> 0:

        break



But I don't know how to call pointer to grammars in Python.
#### 2009-12-21 12:40:03 - tk2600
Hello Paul,



Need your help.  Can I do something like this?



grammar1 = ....

grammar1.setParseAction(proc1)

grammar2 = ....

grammar2.setParseAction(proc2)

....

grammar100 = ....

grammar100.setParseAction(proc100)



for i in range (1, 100):

    found = \<pointer to array of grammar[i].parseString(myStr)\>

    if len(found) \> 0:

        break



But I don't know how to call pointer to grammars in Python.
#### 2009-12-21 19:34:01 - ptmcg
pyparsing will do this for you.  Your for loop is essentially implementing a MatchFirst construct.  Just create a single grammar like this:



    grammarOfAllGrammars = MatchFirst([grammar1, grammar2, ..., grammar100])



Then call `grammarOfAllGrammars.parseString(myStr, parseAll=True)`.



-- Paul
#### 2009-12-22 06:34:05 - tk2600
Thanks!  You are the best!

---
## 2009-12-22 13:09:29 - tk2600 - setParseAction() to take external param?
I want to write to CSV file within setParseAction().  How do I pass the writer handle into setParseAction()?

#### 2009-12-22 13:43:17 - ptmcg
Your options are much the same as how any Python method would access some external data value.



1. You could access the variable as a global variable.



2. You could use a closure to create the parse action method within a larger method having your file handle





    def makeParseAction(fp):
        def parseAction(tokens):
            fp.write(token.dump())
        return parseAction
    
    expr.setParseAction(makeParseACtion(sys.stdout))



3. You could use an object to contain variables about the runtime environment, or values set by other parse actions, and use instance methods of the object as parse actions:





    class TokenProcessor(object):
        def __init__(self, outfp):
            self.out = outfp
    
        def parseAction1(self, tokens):
            self.out.write(tokens.dump())
            self.importantToken = tokens['importantToken']
    
        def parseAction2(self, tokens):
            # reference self.importantToken as assigned in 
            # parseAction1


#### 2009-12-22 13:56:01 - tk2600
Thank you again, Paul!

---
## 2009-12-22 13:15:06 - tk2600 - Reference across grammars?
How do I reference to logTimeStamp inside setSessionRequest()?





#----- Grammar Definition -----

logEntry = logTimeStamp('logTimeStamp') + logStatus + logProc + restDesc('logDesc')

sessRequest = logSession + Literal('Session allocation requested:') + sessId + COMMA + dest + COMMA + initiator

sessRequest.setParseAction(setSessRequest)



#----- ParseAction Definition -----

Def setSessionRequest(tokens):

     # How do I get access to logTimeStamp here?





#---- Main -----

logFull = logEntry.parseString(line)

found = sessRequest.parseString(logFull.logDesc)

#### 2009-12-22 13:44:04 - ptmcg
This is similar to your other question - see if my answer there () gives you some ideas.



-- Paul
#### 2009-12-22 13:58:23 - tk2600
Thanks!

---
## 2009-12-23 02:59:11 - ptmcg - New DSL code, compatible with Python 2.7 and onward
Matt Anderson sent a modified version of stateMachine.py, that addresses the removal of the imputil module from the Python standard lib as of Python 2.7.  If you have upgraded to one of these later versions, please give it a try.



Here is a link to the new stateMachine2.py: <div class="objectEmbed"><div>
<ul><li></li><li></li><li style="color: #666">8 KB</li></ul></div></div>



Thanks, Matt!


---
## 2009-12-24 10:21:41 - drake1337 - pyparsing core
Merry Christmas!



I work as a logic designer at a large, semiconductor manufacturing company. After implementing a large portion of the IEEE 1800-2005 (SystemVerilog) spec in a pyparsing module, I am becoming concerned with performance issues for non-trivial grammars.



On a beefy Suse Linux x86-64 machine, it is taking my poor script several seconds to parse module headers. Also, the way scanString uses exceptions forced me to write a portion of my preprocessor scanner in C since large files were literally taking five minutes to process.



First let me just say two things:

1. I never expect peak performance from any python application. Duh.

2. I have not squeezed out every drop of performance in my script.



That said, I would like to wonder out loud what level of effort it would require to write a C parsing core for pyparsing. Is it impossible?, ... perhaps merely very difficult?



I have grown to love the pyparsing API, but in order to expand its application to performance-sensitive projects, a fast core would make it a serious competitor with tools like lex/yacc, ANTLR and Boost::Spirit.



Any thoughts?

#### 2009-12-24 21:37:24 - ptmcg
I spent quite a bit of time tuning my own Verilog parser, and it runs at about 240 lines/second. One thing that I've found in most complex parsers, especially those with support for arithmetic notation, is that it is worth converting the most atomic elements to Regex expressions.  Here is how I build up a numeric literal for Verilog:





    hexnums = nums + 'abcdefABCDEF' + '_?'
    base = Regex(''[bBoOdDhH]').setName('base')
    basedNumber = Combine( Optional( Word(nums + '_') ) + base + 
                           Word(hexnums+'xXzZ'),
                           joinString=' ', adjacent=False ).setName('basedNumber')
    number = ( basedNumber | \
               Regex(r'[+-]?[0-9_]+(\.[0-9_]*)?([Ee][+-]?[0-9_]+)?') \
              ).setName('numeric')



Since Verilog permits spaces within numeric literals with bases, I needed to be able to build up a 'based' number as you see in the basedNumber expression.  But for typical numerics, I used a Regex expression to match a floating point number - this will get used <em>many</em> times, and so using a Regex will be much faster than trying to navigate through the typical pyparsing `Optional(oneOf(&quot;+ -&quot;)) + Word(nums) + Optional(&quot;.&quot;+Optional(Word(nums))) + etc.`.



You can look for hotspots in your parser by setting names for different expressions (as I've set names 'base', 'basedNumber', and 'numeric'), and then selectively setting the debug flag using:





    number.setDebug()



(Note that setName is *not* the same as setResultsName.  setName assigns a name to the expression itself - setResultsName designates a name to be given to the parsed tokens in the results from parsing the expression.  Usually you use setName to describe the expression in an abstract way vs. using setResultsName to attach an meaningful context name, for example, 'integer' vs. 'age' or 'modelYear'.  If you do not set a name to an expression, then the debug output will be full of default composed expression descriptions, such as '{{[Re:('[+\\-]')] W:(0123...)} [{'.' [W:(0123...)]}]}', instead of the more meaningful 'real number'.)



In debug, you can see whenever the expression is tested against the input string, and whether the parse matches or fails.  If the parse matches, you'll see the matched tokens, and get the line and column number.



You can also define your own debug callbacks to get called when the expression is to be parsed, when it succeeds and when it fails.  I've used this to create a running tally of the different expressions in a large grammar, updating a 3-element list keyed by expression name.  At the end, I sort the tally (a Python dict) by descending number of parse attempts, and this shows me the hotspots in my parser.



If you have a MatchFirst sequence of equivalent items, you can use this information to also reorder the items by likely frequency.  This is a hit-or-miss type of optimization, as it will be very dependent on your source strings.  Still, many languages and grammars have features that are generally only very little used, so you might as well test for them after the more common features have been tested first.



You should also check for alternatives that you have defined using '^' (Or) instead of '|' (MatchFirst).  If at all possible, define your grammar using MatchFirst, as Or expressions have to evaluate *all* the given alternatives before returning the matching expression.  Here is a wasteful expression:





    httpVerb = Literal('HEAD') ^ Literal('GET') ^ Literal('PUT')



There is *no way* that there could be any confusion among these verbs.  Imagine if I were parsing a server log with 1000's of 'HEAD' commands - for each one, even after matching 'HEAD' successfully, my parser still has to check for a match with 'GET' or 'PUT', because Or's exhaustively check all the alternatives.



Another performance killer is ignoring of comments.  If you can avoid this, then your performance will improve dramatically.  One way is to do a preprocessing scan to strip all comments from the code, and then use a parser without '.ignore(verilogComment)' being set.  If you must do it all at once, make sure that your comment definition is as lean as possible, or defined using a Regex.



I have since Day 1 with pyparsing put my stock in the Python interpreter.  Pyparsing is 100% Python, so it pretty much runs on any Python on any platform, and this fits pretty well with my available time/energy resources, considering that pyparsing is free.  I can't imagine trying to keep up with various platforms with an internal C implementation.  My impression is that much of the time is spent making lots of function calls and raising and catching exceptions as the parser works its way through the input string, so to make any headway in the face of this, a C implementation would have to figure out how to run the parser while staying purely in the C world.  That is to say, just converting, say Literal to running in C would likely have little overall benefit, as we would have improved the actual performance of parsing a Literal, but we  *still have to make the function call*.  Of course, the history of software performance optimization is a very rich catalog of misplaced assumptions and guesses as to where the performance hotspots might be, but all my experience to date supports my theory.



You could also try enabling packrat parsing.  With my Verilog parser, I get about a 10% boost from this, but YMMV.



I think I need to write another article on pyparsing performance tuning, to capture some of these ideas in a set of examples... someday. :)



-- Paul
#### 2009-12-25 00:26:51 - drake1337
Wow, thanks for the detailed answer.



The question that pops into my mind is: what kind of a masochist would want to tackle the Verilog LRM? :) Don't tell me that your day job is logic design too.



I think an article on tweaking performance would be a valuable resource. After adjusting major hot-spots, my application may not be lex/yacc fast, but it's definitely fast enough (w/ far fewer LOC).
#### 2009-12-25 01:12:01 - ptmcg
:) No, one of my earliest test cases was to write a Verilog parser, based on this BNF: .  I spent about 8 weeks doing it, and I give it out for noncommercial use (I spent so much time on it, I hated to think it could just get slurped into somebody's product without even a token compensation, but if somebody wanted, I'd offer a for-modest-fee commercial license).



As it happens, I've been in the semiconductor industry myself since 1986, but on the manufacturing end, not design.

---
## 2009-12-26 12:02:48 - cgkanchi - Thanks for the feedback!
Wow, never thought my post on the Taz forum would catch your attention! Thanks a lot for the feedback, I do plan to work a bit more on the parser, so your suggestions will go into it. I plan to make it a bit more complex as well, to allow for handling of charge balances etc. when I have the time. 



Cheers,

cgkanchi


---
## 2009-12-28 00:41:34 - marjoj - LineStart and whitespace
Hi,



I would like to parse lines where the exclamation mark is a third character in the line (leading charachters are whitespaces). F. ex



  !        000600/dspF         000600/dsp10       000600/dsp11



Next works only if I remove leading whitespaces, but not when there are some.

overallContext = LineStart() + Literal('!')('command') + restOfLine 





I tried also to add whitespaces in definition, but it does not work either

overallContext = LineStart() + OneOrMore(White()) + Literal('!')('command') + restOfLine



Could someone help me out, please?

#### 2009-12-28 19:13:48 - ptmcg
LineStart is one of the fussier pyparsing classes, that is very sensitive to pyparsing's default whitespace-skipping logic.



But before I go much further, are you sure that pyparsing is what you want to use here?  Picking out your target lines is probably easier by iterating over the list of lines, and selecting those with line.startswith('  !').  Then use split(' ') and split('/') to pick out the various sub-fields.



But if you're really intent on using pyparsing, then here's how to get those expressions working.



I'm going to assume that you are trying to write a parser for an individual line, reading your input data one line at a time.  LineStart is failing to match in your parser because pyparsing first skips over whitespace before trying to match the LineStart expression.  Unfortunately, after skipping over the leading spaces, the parser location is no longer at the start of the line, so LineStart fails. You can disable pyparsing's linespace skipping by calling leaveWhitespace on your LineStart, telling pyparsing to *not* skip whitespace before trying to match the LineStart.  So if you replace LineStart() with LineStart().leaveWhitespace() in your code, you should sart getting better results.



Which raises another question - if you are parsing a line at a time, why start the parser with LineStart?  You're already guaranteed to be at the start of a line.  Now if you are in fact trying to parse multiple lines at once, then you will also need to add a LineEnd() after your restOfLine elements, so that the next LineStart will actually be at the start of the next line.



If you need more guidance, please post more of your code so I don't have to guess so much. :)
#### 2009-12-29 06:43:09 - marjoj
My Firefox seems to be really unstable today. I wrote alreadyYes, I want to use pyparsing, since this is just the part of trace and error log parser I'm writing.



LineStart().leaveWhitespace() did not work any better if there were white spaces in the beginning. The reason to use LineStart is that otherwise overallContext matches also all the other lines where there are any exclamation mark. I'm using scanString for parsing, since I want just to define grammar for interesting parts and don't care the rest of the lines.



However it does not matter (this time), since I needed to change overallContext definition.





    lineToParse = '  !                                                       IPE         ----\>TFO EP                                             T42    NbCS   002100/dsp9 \n                                                                                                                        CTX61  T61    NbCS   000600/dsp13 \n                                                                                                                               T62    TDM    000600/dsp13'
    
    for tokens,start,end in interestingLine.scanString(lineToParse):
        print tokens.dump()
    
    
    context = Combine(Literal('C') + Word(alphanums))
    termination = Combine(Literal('T') + Word(nums))
    noInfo = Word(alphanums+':'+'.'+'_')
    board = Word(nums, exact=6)
    dsp = Word(alphas, alphanums)
    
    contextInfo = Optional(context)('context') + termination('termination') + noInfo + Combine(board('board') + '/') + dsp('dspid')
    
    overallContext = Literal('!')('command') + OneOrMore(Group(Suppress(SkipTo(contextInfo)) + contextInfo))('info')



This is working all right as long as I'm using double indexing for info. I cannot use dictionary here since the contextInfos must stay in order.

It would be of course more pretty, if I could use tokens.info.XXX.context etc.



I would also like to know the place (string index) beginning of CTX61. Can that be done by pyparsing?





    !                                               +-------EC-+       +----------+        +----------+                  CTX41  T41    TDM    000600/dsp12
    
    ['!', ['CTX41', 'T41', 'TDM', '000600/', 'dsp12']]
    - command: !
    - info: [['CTX41', 'T41', 'TDM', '000600/', 'dsp12']]
    
      !                                                       IPE         ----\>TFO EP                                             T42    NbCS   002100/dsp9
                                                                                                                           CTX61  T61    NbCS   000600/dsp13
                                                                                                                                  T62    TDM    000600/dsp13   
    
    ['!', ['T42', 'NbCS', '002100/', 'dsp9'], ['CTX61', 'T61', 'NbCS', '000600/', 'dsp13'], ['T62', 'TDM', '000600/', 'dsp13']]
    - command: !
    - info: [['T42', 'NbCS', '002100/', 'dsp9'], ['CTX61', 'T61', 'NbCS', '000600/', 'dsp13'], ['T62', 'TDM', '000600/', 'dsp13']]



BR,

Marjo
#### 2009-12-29 06:46:26 - marjoj
And thanks for clearing LineStart class behaviour :) It was helpful.
#### 2009-12-30 06:34:09 - ptmcg
Just a quick reply before I go off to work.  To see an example of how to pick out the location of a parsed token, look at the implementation in pyparsing of originalTextFor.  Also, you'll need to decide whether to keep tabs in your source string or expand them with Python's expandtabs method.  Pyparsing expands tabs by default, which you can override using the parseWithTabs method.



-- Paul

---
## 2009-12-29 18:39:03 - sighup - Greedy delimited list?
I'm using pyparsing to try and parse sql [ or to be more precise, a limited subset of MYSQL SQL]. Part of the definition is meant to capture the values (...) list. Here's an extract that contains most of where I've defined that.



[code]

tok_sql_identifier = Word(alphanums + '_')

tok_sql_quoted_value = \

    QuotedString(''', '\\', '\'', True, False) ^ \

    QuotedString(''', '\\', '\'', True, False)

tok_sql_computed_value = tok_sql_identifier + tok_sql_op + \

    (tok_sql_identifier | tok_sql_quoted_value)

tok_sql_loose_val = tok_sql_quoted_value | tok_sql_computed_value | tok_sql_identifier

tok_sql_vals = delimitedList(tok_sql_loose_val)

[/code]



Then later, the actual insert...



[code]

...

tok_sql_literal_values + \

tok_sql_open_paren + \

tok_sql_vals.setResultsName('vals') + \

tok_sql_close_paren + \

Optional(tok_sql_literal_on + \

  tok_sql_literal_duplicate + \

  tok_sql_literal_key + \

  tok_sql_literal_update + \

  tok_sql_kvp_list.setResultsName('dup_list')

) + \

Optional(tok_sql_literal_semicol))

[/code]



When the Optional section isn't present, the INSERTS parse fine -- but, when it is present, <strong>tok_sql_vals.setResultsName('vals')</strong> acts greedily. For example:



[code]

<em>INSERT INTO email_track (email_code,sent_count,open_count,email_date,ctime) VALUES ('ms1',1,0,'2009-12-22','2009-12-22 10:41:22') ON DUPLICATE KEY UPDATE sent_count = sent_count + 1, mtime = '2009-12-22 10:41:22';</em>



Expected ')'

Expected ')' (at char 212), (line:1, col:213)

[code]



Which makes sense given:



[code]

<ul class="quotelist"><ul class="quotelist"><ul class="quotelist"><li>sp.tok_sql_vals.parseString(''''ms1',1,0,'2009-12-22','2009-12-22 10:41:22') ON DUPLICATE KEY UPDATE sent_count = sent_count + 1, mtime = '2009-12-22 10:41:22';''')</li></ul></ul></ul>([''ms1',1,0,'2009-12-22','2009-12-22 10:41:22') ON DUPLICATE KEY UPDATE sent_count = sent_count + 1, mtime = '2009-12-22 10:41:22''], {})

[/code]



Excuse me if this is something obvious, I'm new to the framework and working my way through the online docs.



Gary

#### 2009-12-29 19:38:51 - ptmcg
Yes, there <em>is</em> some greedy behavior going on here, but it's not in delimitedList.  It turns out that QuotedString is the culprit, and you have found a bug in my implementation (an overly greedy regex, can you imagine?).  If you want to patch your version of pyparsing, change this line in QuotedString.<u>init</u>:





    self.pattern += (r')*%s' % re.escape(self.endQuoteChar))



to





    self.pattern += (r')*?%s' % re.escape(self.endQuoteChar))





If you don't want to patch your pyparsing source code, you can work around this QuotedString problem using the pyparsing helper quotedString to define your own expression using:





    tok_sql_quoted_value = quotedString.setParseAction(removeQuotes)



Thanks for turning up this subtle bug! I'll get it fixed in the next release, which should be sometime in the next month, I hope.  (There has been a lot of interest in a SQL parser, but my own example is pretty feeble.  Is there any chance you would be willing to donate this parser to the pyparsing examples directory?)



-- Paul
#### 2009-12-29 20:49:13 - sighup
Thank you for the quick response. 



I tried the quick patch, which did fix that particular problem, but caused a couple of other test cases to fail. Rather than rush another  response I'll take the time to understand what went wrong [it's a decent amount of code] -- but it seems to be related to literal [escaped] strings in the statement. 



Will also be happy to add the parser to the examples when its finished up.



Thanks.

Gary
#### 2009-12-30 06:27:56 - ptmcg
\<sigh\>



It turns out that the definitions for the quotedString helpers suffer from a similar problem.  Here are the correct definitions for the quotedString's:



    dblQuotedString = Regex(r''(?:[^'\n\r\\]|(?:'')|(?:\\x[0-9a-fA-F]+)|(?:\\.))*?'').setName('string enclosed in double quotes')
    sglQuotedString = Regex(r''(?:[^'\n\r\\]|(?:'')|(?:\\x[0-9a-fA-F]+)|(?:\\.))*?'').setName('string enclosed in single quotes')
    quotedString = Regex(r'''(?:'(?:[^'\n\r\\]|(?:'')|(?:\\x[0-9a-fA-F]+)|(?:\\.))*?')|(?:'(?:[^'\n\r\\]|(?:'')|(?:\\x[0-9a-fA-F]+)|(?:\\.))*?')''').setName('quotedString using single or double quotes')



These changes pass all of my unit tests, but maybe that just means I need better unit tests.  As soon as you get your test cases working, and we determine if it is your grammar or pyparsing, I'll get some fixes put out, at least to the SVN repository.



-- Paul
#### 2010-01-03 23:49:59 - ptmcg
Gary -



I was just updating my unit tests, and adding tests for this particular case, and I think we mis-diagnosed this problem.  In the process, I've reverted my changes to pyparsing, and I am now looking more closely at your QuotedString definitions.



Your original definition for QuotedString was:



    tok_sql_quoted_value = \
            QuotedString(''', '\\', '\'', True, False) ^ \
            QuotedString(''', '\\', '\'', True, False)



I should have looked at this closer, as your usage of these constructors is the core problem.  The third argument to the constructor is a sequence that is supposed to map to a single quote character if found in the input string.  The purpose of this argument is specifically for syntaxes like many SQL's which use '\' for a generic escape character, but use a doubled quote ('' or '') for quotes.  So inserting this text like `&quot;I don't like you&quot;` is escaped as `&quot;I don''t like you&quot;`.  You gave \' and \' as your escape chars, which Python interprets as just ' and ', which ended up causing my generated regex to greedily include the terminating ' or ' in the body of the string.



To correct this in your code, please revert the changes to pyparsing that I proposed earlier, and change your definition of tok_sql_quoted_value to one of the following (my understanding of SQL is that the first is probably what you want):



    tok_sql_quoted_value = (
            QuotedString(''', '\\', '''', True, False) ^ 
            QuotedString(''', '\\', '''', True, False))
    
        tok_sql_quoted_value = (
            QuotedString(''', '\\', r'\'', True, False) ^ 
            QuotedString(''', '\\', r'\'', True, False))



I've still made updates to my unit tests, using your test case as input.



Thanks again,

-- Paul
#### 2010-01-04 09:20:46 - sighup
Paul -- 



Thanks. MYSQL actually supports both forms, and in this case the log format is actually using the second form. My mistake was not using a raw string for escaped quote arg. 



That gets me a little further in the parser. Going to work on function calls now, hopefully will have something soon I can post as an example.



Gary
#### 2010-01-08 12:15:44 - sighup
The parser is now available here: 



It's doing a pretty good job of parsing most of the log files I'm throwing at it -- though there is still an exception when we have nested select statements in the column list. The last testcase in the parser source illustrates the problem. And I'm sure a few other issues that I'll work through in time. 



Hopefully useful to someone though, as an example or something to build on.



Gary



